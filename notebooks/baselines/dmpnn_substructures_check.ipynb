{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tap import Tap\n",
    "from typing import List, Optional, Tuple\n",
    "import torch\n",
    "from typing_extensions import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonArgs(Tap):\n",
    "    \"\"\":class:`CommonArgs` contains arguments that are used in both :class:`TrainArgs` and :class:`PredictArgs`.\"\"\"\n",
    "\n",
    "    smiles_column: str = None\n",
    "    \"\"\"Name of the column containing SMILES strings. By default, uses the first column.\"\"\"\n",
    "    checkpoint_dir: str = None\n",
    "    \"\"\"Directory from which to load model checkpoints (walks directory and ensembles all models that are found).\"\"\"\n",
    "    checkpoint_path: str = None\n",
    "    \"\"\"Path to model checkpoint (:code:`.pt` file).\"\"\"\n",
    "    checkpoint_paths: List[str] = None\n",
    "    \"\"\"List of paths to model checkpoints (:code:`.pt` files).\"\"\"\n",
    "    no_cuda: bool = False\n",
    "    \"\"\"Turn off cuda (i.e., use CPU instead of GPU).\"\"\"\n",
    "    gpu: int = None\n",
    "    \"\"\"Which GPU to use.\"\"\"\n",
    "    features_generator: List[str] = None\n",
    "    \"\"\"Method(s) of generating additional features.\"\"\"\n",
    "    features_path: List[str] = None\n",
    "    \"\"\"Path(s) to features to use in FNN (instead of features_generator).\"\"\"\n",
    "    no_features_scaling: bool = False\n",
    "    \"\"\"Turn off scaling of features.\"\"\"\n",
    "    max_data_size: int = None\n",
    "    \"\"\"Maximum number of data points to load.\"\"\"\n",
    "    num_workers: int = 8\n",
    "    \"\"\"Number of workers for the parallel data loading (0 means sequential).\"\"\"\n",
    "    batch_size: int = 50\n",
    "    \"\"\"Batch size.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\"The :code:`torch.device` on which to load and process data and models.\"\"\"\n",
    "        if not self.cuda:\n",
    "            return torch.device('cpu')\n",
    "\n",
    "        return torch.device('cuda', self.gpu)\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, device: torch.device) -> None:\n",
    "        self.cuda = device.type == 'cuda'\n",
    "        self.gpu = device.index\n",
    "\n",
    "    @property\n",
    "    def cuda(self) -> bool:\n",
    "        \"\"\"Whether to use CUDA (i.e., GPUs) or not.\"\"\"\n",
    "        return not self.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    @cuda.setter\n",
    "    def cuda(self, cuda: bool) -> None:\n",
    "        self.no_cuda = not cuda\n",
    "\n",
    "    @property\n",
    "    def features_scaling(self) -> bool:\n",
    "        \"\"\"Whether to apply normalization with a :class:`~chemprop.data.scaler.StandardScaler` to the additional molecule-level features.\"\"\"\n",
    "        return not self.no_features_scaling\n",
    "\n",
    "    def add_arguments(self) -> None:\n",
    "        self.add_argument('--gpu', choices=list(range(torch.cuda.device_count())))\n",
    "        self.add_argument('--features_generator', choices=get_available_features_generators())\n",
    "\n",
    "    def process_args(self) -> None:\n",
    "        # Load checkpoint paths\n",
    "        self.checkpoint_paths = get_checkpoint_paths(\n",
    "            checkpoint_path=self.checkpoint_path,\n",
    "            checkpoint_paths=self.checkpoint_paths,\n",
    "            checkpoint_dir=self.checkpoint_dir,\n",
    "        )\n",
    "\n",
    "        # Validate features\n",
    "        if self.features_generator is not None and 'rdkit_2d_normalized' in self.features_generator and self.features_scaling:\n",
    "            raise ValueError('When using rdkit_2d_normalized features, --no_features_scaling must be specified.')\n",
    "\n",
    "\n",
    "class TrainArgs(CommonArgs):\n",
    "    \"\"\":class:`TrainArgs` includes :class:`CommonArgs` along with additional arguments used for training a Chemprop model.\"\"\"\n",
    "\n",
    "    # General arguments\n",
    "    data_path: str\n",
    "    \"\"\"Path to data CSV file.\"\"\"\n",
    "    target_columns: List[str] = None\n",
    "    \"\"\"\n",
    "    Name of the columns containing target values.\n",
    "    By default, uses all columns except the SMILES column and the :code:`ignore_columns`.\n",
    "    \"\"\"\n",
    "    ignore_columns: List[str] = None\n",
    "    \"\"\"Name of the columns to ignore when :code:`target_columns` is not provided.\"\"\"\n",
    "    dataset_type: Literal['regression', 'classification', 'multiclass']\n",
    "    \"\"\"Type of dataset. This determines the loss function used during training.\"\"\"\n",
    "    multiclass_num_classes: int = 3\n",
    "    \"\"\"Number of classes when running multiclass classification.\"\"\"\n",
    "    separate_val_path: str = None\n",
    "    \"\"\"Path to separate val set, optional.\"\"\"\n",
    "    separate_test_path: str = None\n",
    "    \"\"\"Path to separate test set, optional.\"\"\"\n",
    "    split_type: Literal[\n",
    "        'random', 'scaffold_balanced', 'predetermined', 'crossval', 'index_predetermined', 'one_out_crossval'] = 'random'\n",
    "    \"\"\"Method of splitting the data into train/val/test.\"\"\"\n",
    "    split_sizes: Tuple[float, float, float] = (0.8, 0.1, 0.1)\n",
    "    \"\"\"Split proportions for train/validation/test sets.\"\"\"\n",
    "    num_folds: int = 1\n",
    "    \"\"\"Number of folds when performing cross validation.\"\"\"\n",
    "    folds_file: str = None\n",
    "    \"\"\"Optional file of fold labels.\"\"\"\n",
    "    val_fold_index: int = None\n",
    "    \"\"\"Which fold to use as val for leave-one-out cross val.\"\"\"\n",
    "    test_fold_index: int = None\n",
    "    \"\"\"Which fold to use as test for leave-one-out cross val.\"\"\"\n",
    "    crossval_index_dir: str = None\n",
    "    \"\"\"Directory in which to find cross validation index files.\"\"\"\n",
    "    crossval_index_file: str = None\n",
    "    \"\"\"Indices of files to use as train/val/test. Overrides :code:`--num_folds` and :code:`--seed`.\"\"\"\n",
    "    seed: int = 0\n",
    "    \"\"\"\n",
    "    Random seed to use when splitting data into train/val/test sets.\n",
    "    When :code`num_folds > 1`, the first fold uses this seed and all subsequent folds add 1 to the seed.\n",
    "    \"\"\"\n",
    "    pytorch_seed: int = 0\n",
    "    \"\"\"Seed for PyTorch randomness (e.g., random initial weights).\"\"\"\n",
    "    metric: Literal['auc', 'prc-auc', 'rmse', 'mae', 'mse', 'r2', 'accuracy', 'cross_entropy'] = None\n",
    "    \"\"\"Metric to use during evaluation. Defaults to \"auc\" for classification and \"rmse\" for regression.\"\"\"\n",
    "    save_dir: str = None\n",
    "    \"\"\"Directory where model checkpoints will be saved.\"\"\"\n",
    "    save_smiles_splits: bool = False\n",
    "    \"\"\"Save smiles for each train/val/test splits for prediction convenience later.\"\"\"\n",
    "    test: bool = False\n",
    "    \"\"\"Whether to skip training and only test the model.\"\"\"\n",
    "    quiet: bool = False\n",
    "    \"\"\"Skip non-essential print statements.\"\"\"\n",
    "    log_frequency: int = 10\n",
    "    \"\"\"The number of batches between each logging of the training loss.\"\"\"\n",
    "    show_individual_scores: bool = False\n",
    "    \"\"\"Show all scores for individual targets, not just average, at the end.\"\"\"\n",
    "    cache_cutoff: int = 10000\n",
    "    \"\"\"\n",
    "    Maximum number of molecules in dataset to allow caching.\n",
    "    Below this number, caching is used and data loading is sequential.\n",
    "    Above this number, caching is not used and data loading is parallel.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model arguments\n",
    "    bias: bool = False\n",
    "    \"\"\"Whether to add bias to linear layers.\"\"\"\n",
    "    no_substructures_hidden_size: int = 300\n",
    "    substructures_hidden_size: int = 300\n",
    "    \"\"\"Dimensionality of hidden layers in MPN.\"\"\"\n",
    "    no_substructures_depth: int = 3\n",
    "    substructures_depth: int = 3\n",
    "    \"\"\"Number of message passing steps.\"\"\"\n",
    "    dropout: float = 0.0\n",
    "    \"\"\"Dropout probability.\"\"\"\n",
    "    activation: Literal['ReLU', 'LeakyReLU', 'PReLU', 'tanh', 'SELU', 'ELU'] = 'ReLU'\n",
    "    \"\"\"Activation function.\"\"\"\n",
    "    no_substructures_atom_messages: bool = False\n",
    "    substructures_atom_messages: bool = False\n",
    "    \"\"\"Centers messages on atoms instead of on bonds.\"\"\"\n",
    "    substructures_merge: bool = False\n",
    "    \"\"\"Merges neighboring rings.\"\"\"\n",
    "    substructures_use_substructures: bool = False\n",
    "    \"\"\"Generates chemical substructures.\"\"\"\n",
    "    no_substructures_undirected: bool = False\n",
    "    substructures_undirected: bool = False\n",
    "    \"\"\"Undirected edges (always sum the two relevant bond vectors).\"\"\"\n",
    "    ffn_hidden_size: int = None\n",
    "    \"\"\"Hidden dim for higher-capacity FFN (defaults to hidden_size).\"\"\"\n",
    "    ffn_num_layers: int = 2\n",
    "    \"\"\"Number of layers in FFN after MPN encoding.\"\"\"\n",
    "    features_only: bool = False\n",
    "    \"\"\"Use only the additional features in an FFN, no graph network.\"\"\"\n",
    "    separate_val_features_path: List[str] = None\n",
    "    \"\"\"Path to file with features for separate val set.\"\"\"\n",
    "    separate_test_features_path: List[str] = None\n",
    "    \"\"\"Path to file with features for separate test set.\"\"\"\n",
    "    config_path: str = None\n",
    "    \"\"\"\n",
    "    Path to a :code:`.json` file containing arguments. Any arguments present in the config file\n",
    "    will override arguments specified via the command line or by the defaults.\n",
    "    \"\"\"\n",
    "    ensemble_size: int = 1\n",
    "    \"\"\"Number of models in ensemble.\"\"\"\n",
    "\n",
    "    # Training arguments\n",
    "    epochs: int = 30\n",
    "    \"\"\"Number of epochs to run.\"\"\"\n",
    "    warmup_epochs: float = 2.0\n",
    "    \"\"\"\n",
    "    Number of epochs during which learning rate increases linearly from :code:`init_lr` to :code:`max_lr`.\n",
    "    Afterwards, learning rate decreases exponentially from :code:`max_lr` to :code:`final_lr`.\n",
    "    \"\"\"\n",
    "    early_stopping: int = None\n",
    "    init_lr: float = 1e-4\n",
    "    \"\"\"Initial learning rate.\"\"\"\n",
    "    max_lr: float = 1e-3\n",
    "    \"\"\"Maximum learning rate.\"\"\"\n",
    "    final_lr: float = 1e-4\n",
    "    \"\"\"Final learning rate.\"\"\"\n",
    "    grad_clip: float = None\n",
    "    \"\"\"Maximum magnitude of gradient during training.\"\"\"\n",
    "    class_balance: bool = False\n",
    "    \"\"\"Trains with an equal number of positives and negatives in each batch (only for single task classification).\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super(TrainArgs, self).__init__(*args, **kwargs)\n",
    "        self._task_names = None\n",
    "        self._crossval_index_sets = None\n",
    "        self._task_names = None\n",
    "        self._num_tasks = None\n",
    "        self._features_size = None\n",
    "        self._train_data_size = None\n",
    "\n",
    "    @property\n",
    "    def minimize_score(self) -> bool:\n",
    "        \"\"\"Whether the model should try to minimize the score metric or maximize it.\"\"\"\n",
    "        return self.metric in {'rmse', 'mae', 'mse', 'cross_entropy'}\n",
    "\n",
    "    @property\n",
    "    def use_input_features(self) -> bool:\n",
    "        \"\"\"Whether the model is using additional molecule-level features.\"\"\"\n",
    "        return self.features_generator is not None or self.features_path is not None\n",
    "\n",
    "    @property\n",
    "    def num_lrs(self) -> int:\n",
    "        \"\"\"The number of learning rates to use (currently hard-coded to 1).\"\"\"\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def crossval_index_sets(self) -> List[List[List[int]]]:\n",
    "        \"\"\"Index sets used for splitting data into train/validation/test during cross-validation\"\"\"\n",
    "        return self._crossval_index_sets\n",
    "\n",
    "    @property\n",
    "    def task_names(self) -> List[str]:\n",
    "        \"\"\"A list of names of the tasks being trained on.\"\"\"\n",
    "        return self._task_names\n",
    "\n",
    "    @task_names.setter\n",
    "    def task_names(self, task_names: List[str]) -> None:\n",
    "        self._task_names = task_names\n",
    "\n",
    "    @property\n",
    "    def num_tasks(self) -> int:\n",
    "        \"\"\"The number of tasks being trained on.\"\"\"\n",
    "        return len(self.task_names) if self.task_names is not None else 0\n",
    "\n",
    "    @property\n",
    "    def features_size(self) -> int:\n",
    "        \"\"\"The dimensionality of the additional molecule-level features.\"\"\"\n",
    "        return self._features_size\n",
    "\n",
    "    @features_size.setter\n",
    "    def features_size(self, features_size: int) -> None:\n",
    "        self._features_size = features_size\n",
    "\n",
    "    @property\n",
    "    def train_data_size(self) -> int:\n",
    "        \"\"\"The size of the training data set.\"\"\"\n",
    "        return self._train_data_size\n",
    "\n",
    "    @train_data_size.setter\n",
    "    def train_data_size(self, train_data_size: int) -> None:\n",
    "        self._train_data_size = train_data_size\n",
    "\n",
    "    def process_args(self) -> None:\n",
    "        super(TrainArgs, self).process_args()\n",
    "\n",
    "        global temp_dir  # Prevents the temporary directory from being deleted upon function return\n",
    "\n",
    "        # Load config file\n",
    "        if self.config_path is not None:\n",
    "            print(os.getcwd())\n",
    "            with open(self.config_path) as f:\n",
    "                config = json.load(f)\n",
    "                for key, value in config.items():\n",
    "                    setattr(self, key, value)\n",
    "\n",
    "        # Create temporary directory as save directory if not provided\n",
    "        if self.save_dir is None:\n",
    "            temp_dir = TemporaryDirectory()\n",
    "            self.save_dir = temp_dir.name\n",
    "\n",
    "        # Fix ensemble size if loading checkpoints\n",
    "        if self.checkpoint_paths is not None and len(self.checkpoint_paths) > 0:\n",
    "            self.ensemble_size = len(self.checkpoint_paths)\n",
    "\n",
    "        # Process and validate metric and loss function\n",
    "        if self.metric is None:\n",
    "            if self.dataset_type == 'classification':\n",
    "                self.metric = 'auc'\n",
    "            elif self.dataset_type == 'multiclass':\n",
    "                self.metric = 'cross_entropy'\n",
    "            else:\n",
    "                self.metric = 'rmse'\n",
    "\n",
    "        if not ((self.dataset_type == 'classification' and self.metric in ['auc', 'prc-auc', 'accuracy']) or\n",
    "                (self.dataset_type == 'regression' and self.metric in ['rmse', 'mae', 'mse', 'r2']) or\n",
    "                (self.dataset_type == 'multiclass' and self.metric in ['cross_entropy', 'accuracy'])):\n",
    "            raise ValueError(f'Metric \"{self.metric}\" invalid for dataset type \"{self.dataset_type}\".')\n",
    "\n",
    "        # Validate class balance\n",
    "        if self.class_balance and self.dataset_type != 'classification':\n",
    "            raise ValueError('Class balance can only be applied if the dataset type is classification.')\n",
    "\n",
    "        # Validate features\n",
    "        if self.features_only and not (self.features_generator or self.features_path):\n",
    "            raise ValueError('When using features_only, a features_generator or features_path must be provided.')\n",
    "\n",
    "        # Handle FFN hidden size\n",
    "        if self.ffn_hidden_size is None:\n",
    "            self.ffn_hidden_size = self.no_substructures_hidden_size + self.substructures_hidden_size\n",
    "\n",
    "        # Handle MPN variants\n",
    "        if self.no_substructures_atom_messages and self.no_substructures_undirected:\n",
    "            raise ValueError('Undirected is unnecessary when using atom_messages '\n",
    "                             'since atom_messages are by their nature undirected.')\n",
    "\n",
    "        if self.substructures_atom_messages and self.substructures_undirected:\n",
    "            raise ValueError('Undirected is unnecessary when using atom_messages '\n",
    "                             'since atom_messages are by their nature undirected.')\n",
    "\n",
    "        # Validate split type settings\n",
    "        if not (self.split_type == 'predetermined') == (self.folds_file is not None) == (\n",
    "                self.test_fold_index is not None):\n",
    "            raise ValueError('When using predetermined split type, must provide folds_file and test_fold_index.')\n",
    "\n",
    "        if not (self.split_type == 'crossval') == (self.crossval_index_dir is not None):\n",
    "            raise ValueError('When using crossval split type, must provide crossval_index_dir.')\n",
    "\n",
    "        if not (self.split_type in ['crossval', 'index_predetermined']) == (self.crossval_index_file is not None):\n",
    "            raise ValueError('When using crossval or index_predetermined split type, must provide crossval_index_file.')\n",
    "\n",
    "        if self.split_type in ['crossval', 'index_predetermined']:\n",
    "            with open(self.crossval_index_file, 'rb') as rf:\n",
    "                self._crossval_index_sets = pickle.load(rf)\n",
    "            self.num_folds = len(self.crossval_index_sets)\n",
    "            self.seed = 0\n",
    "\n",
    "        # Test settings\n",
    "        if self.test:\n",
    "            self.epochs = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chemprop]",
   "language": "python",
   "name": "conda-env-chemprop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
