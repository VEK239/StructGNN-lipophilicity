{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Neural Fingerprint scripts\n",
    "\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('../scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from build_vanilla_net import build_morgan_deep_net\n",
    "from build_convnet import build_conv_deep_net\n",
    "from util import normalize_array, build_batched_grad\n",
    "from optimizers import adam\n",
    "from util import rmse\n",
    "\n",
    "from autograd import grad\n",
    "import os\n",
    "\n",
    "from pathlib2 import Path\n",
    "import time\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"-f\", \"--fp_length\", dest=\"fp_length\",\n",
    "                    help=\"length of fingerprint\",default = 50,type=int)\n",
    "parser.add_argument(\"-d\", \"--fp_depth\",\n",
    "                    dest=\"fp_depth\", default=4,\n",
    "                    help=\"depth of fingerprint (radius)\",type=int)\n",
    "parser.add_argument(\"-i\", \"--init_scale\",\n",
    "                    dest=\"init_scale\", default=-4,\n",
    "                    help=\"power of exponent for scale of weights initialization\",type=float)\n",
    "parser.add_argument(\"-l\", \"--learn_rate\",\n",
    "                    dest=\"learn_rate\", default=-3,\n",
    "                    help=\"power of exponent for learning rate\",type=float)\n",
    "parser.add_argument(\"-s\", \"--l2_penalty\",\n",
    "                    dest=\"l2_penalty\", default=-2,\n",
    "                    help=\"power of exponent for size of l2 penalty\", type=float)\n",
    "parser.add_argument(\"-n\", \"--num_exp\",\n",
    "                    dest=\"NUM_EXP\", default='',\n",
    "                    help=\"number of current experiment\")\n",
    "parser.add_argument(\"-p\", \"--l1_penalty\",\n",
    "                    dest=\"l1_penalty\", default=0.0,\n",
    "                    help=\"power of exponent for size of l1 penalty\", type=float)\n",
    "parser.add_argument(\"-a\", \"--h1_size\",\n",
    "                    dest=\"h1_size\", default=100,\n",
    "                    help=\"Size of layer after fingerprint\",type=int)\n",
    "parser.add_argument(\"-c\", \"--conv_width\",\n",
    "                    dest=\"conv_width\", default=20,\n",
    "                    help=\"size of convolutions\",type=int)\n",
    "parser.add_argument(\"-t\", \"--data_file\",\n",
    "                    dest=\"data_file\", default='logp_mean',\n",
    "                    help=\"Prefix of filename with data\",type=str)\n",
    "parser.add_argument(\"-e\", \"--num_epochs\",\n",
    "                    dest=\"num_epochs\", default=10,\n",
    "                    help=\"Number of epochs\",type=int)\n",
    "\n",
    "# set number of experiment\n",
    "args = parser.parse_args(['-n','3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../../../mol_properties/data/3_final_data/split_data\"\n",
    "\n",
    "EXPERIMENTS_DATA = \"../\"\n",
    "\n",
    "\n",
    "#logs path\n",
    "LOG_PATH=os.path.join(EXPERIMENTS_DATA, \"logs\")\n",
    "Path(LOG_PATH).mkdir(exist_ok=True)\n",
    "\n",
    "path = os.path.join(LOG_PATH,'exp_'+args.NUM_EXP)\n",
    "Path(path).mkdir(exist_ok=True)\n",
    "LOG_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(LOG_PATH,args.NUM_EXP+'_parameters.json'),'w') as f:\n",
    "    json.dump(vars(args), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log files\n",
    "f_log=open(os.path.join(LOG_PATH,args.NUM_EXP+'_logs.txt'),'w')\n",
    "f_log.close()\n",
    "with open(os.path.join(LOG_PATH,args.NUM_EXP+'_logs_metrics_morgan.txt'),'w') as f_log_metrics:\n",
    "    f_log_metrics.write('train RMSE\\tval RMSE\\ttrain R2\\tval R2\\n')\n",
    "with open(os.path.join(LOG_PATH,args.NUM_EXP+'_logs_metrics_conv.txt'),'w') as f_log_metrics:\n",
    "    f_log_metrics.write('train RMSE\\tval RMSE\\ttrain R2\\tval R2\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing logs to txt\n",
    "def write_log_file(path, filename, log_message):\n",
    "    with open(os.path.join(path, filename),'a') as f_log:\n",
    "        f_log.write(log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, params, seed=0,\n",
    "             validation_smiles=None, validation_raw_targets=None, filename_fix=''):\n",
    "    \"\"\"Function to train model based on pred_fun and loss_fun\"\"\"\n",
    "    write_log_file(LOG_PATH, args.NUM_EXP+'_logs.txt', \"\\nTotal number of weights in the network: \"+str(num_weights)+'\\n')\n",
    "    print (\"Total number of weights in the network:\", num_weights)\n",
    "    init_weights = npr.RandomState(seed).randn(num_weights) * params['init_scale']\n",
    "\n",
    "    num_print_examples = len(train_smiles)\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 10 == 0:\n",
    "            print (\"max of weights\", np.max(np.abs(weights)))\n",
    "            write_log_file(LOG_PATH, args.NUM_EXP+'_logs.txt', \"\\nmax of weights \"+str(np.max(np.abs(weights)))+'\\n')\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n",
    "            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n",
    "            training_curve.append(cur_loss)\n",
    "            print (\"Iteration\", iter, \"loss\", cur_loss,\\\n",
    "                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples])),\n",
    "            print \"Train R2\", iter, \":\", \\\n",
    "                    r2_score(train_raw_targets, train_preds)\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print (\"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets)),\n",
    "                print \"Validation R2\", iter, \":\", \\\n",
    "                    r2_score(validation_raw_targets, validation_preds),\n",
    "            write_log_file(LOG_PATH, args.NUM_EXP+'_logs_metrics_'+filename_fix+'.txt',\\\n",
    "                           str(rmse(train_preds, train_raw_targets[:num_print_examples]))+'\\t'+\\\n",
    "                           str(rmse(validation_preds, validation_raw_targets))+'\\t'+\\\n",
    "                           str(r2_score(train_raw_targets, train_preds))+'\\t'+\\\n",
    "                           str(r2_score(validation_raw_targets, validation_preds))+'\\t'+\\\n",
    "                           '\\n')\n",
    "            return r2_score(validation_raw_targets, validation_preds)\n",
    "    # Build gradient using autograd.\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    # Optimize weights.\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=params['num_iters'], step_size=params['learn_rate'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(pred_func):\n",
    "    \"\"\"Print and log quality metrics\"\"\"\n",
    "    train_preds = pred_func(train_inputs)\n",
    "    test_preds = pred_func(test_inputs)\n",
    "    print \"\\nPerformance on \" + task_params['target_name'] + \":\"\n",
    "    print \"\\nTrain RMSE:\", rmse(train_preds, train_targets)\n",
    "    print \"\\nTrain R2:\", r2_score(train_targets, train_preds)\n",
    "    print \"\\nTest RMSE: \", rmse(test_preds,  test_targets)\n",
    "    print \"\\nTest R2: \", r2_score(test_targets, test_preds)\n",
    "    print \"-\" * 80\n",
    "    write_log_file(LOG_PATH, args.NUM_EXP+'_logs.txt', \\\n",
    "                   \"\\nPerformance on \" + task_params['target_name'] + \":\"+\\\n",
    "                   \"\\nTrain RMSE: \"+str(rmse(train_preds, train_targets))+\\\n",
    "                   \"\\nTrain R2: \"+str(r2_score(train_targets, train_preds))+\\\n",
    "                   \"\\nTest RMSE: \"+str(rmse(test_preds,  test_targets))+\\\n",
    "                   \"\\nTest R2: \"+str(r2_score(test_targets, test_preds))+\n",
    "                   '\\n')\n",
    "    return r2_score(test_targets, test_preds)\n",
    "\n",
    "def run_morgan_experiment():\n",
    "    global params\n",
    "    loss_fun, pred_fun, net_parser = \\\n",
    "        build_morgan_deep_net(params['fp_length'],\n",
    "                              params['fp_depth'], vanilla_net_params)\n",
    "    num_weights = len(net_parser)\n",
    "    predict_func, trained_weights, conv_training_curve = \\\n",
    "        train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                 params, validation_smiles=val_inputs, validation_raw_targets=val_targets, filename_fix='morgan')\n",
    "    return print_performance(predict_func)\n",
    "\n",
    "def run_conv_experiment():\n",
    "    conv_layer_sizes = [params['conv_width']] * params['fp_depth']\n",
    "    conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                        'fp_length' : params['fp_length'], 'normalize' : 1}\n",
    "    loss_fun, pred_fun, conv_parser = \\\n",
    "        build_conv_deep_net(conv_arch_params, vanilla_net_params, params['l2_penalty'])\n",
    "    num_weights = len(conv_parser)\n",
    "    predict_func, trained_weights, conv_training_curve = \\\n",
    "        train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                 params, validation_smiles=val_inputs, validation_raw_targets=val_targets, filename_fix='conv')\n",
    "    test_predictions = predict_func(test_inputs)\n",
    "    return rmse(test_predictions, test_targets)\n",
    "\n",
    "def run_avg_experiment():\n",
    "    y_train_mean = np.mean(train_targets)\n",
    "    return print_performance(lambda x : y_train_mean*np.ones(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function for loading our datasets\n",
    "def load_data(dataset_path, prefix_name, VALUE_COLUMN = 'logP', SMILES_COLUMN='smiles'):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    data_splits = ['train', 'test', 'validation']\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for split in data_splits:\n",
    "        data = pd.read_csv(os.path.join(dataset_path,prefix_name+'_'+split+'.csv'))\n",
    "        datasets[split] = (data[SMILES_COLUMN].values, data[VALUE_COLUMN].values)\n",
    "        \n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data parameters\n",
    "task_params = {'target_name' : 'logP',\n",
    "           'data_file'   : args.data_file}\n",
    "\n",
    "# load data\n",
    "data = load_data(dataset_path=DATASET_PATH, prefix_name = task_params['data_file'], VALUE_COLUMN = task_params['target_name'])\n",
    "\n",
    "train_inputs, train_targets = data['train']\n",
    "val_inputs,   val_targets   = data['validation']\n",
    "test_inputs,  test_targets  = data['test']\n",
    "\n",
    "# set parameters of training    \n",
    "params = {'fp_length': args.fp_length,\n",
    "        'fp_depth': args.fp_depth,\n",
    "        'init_scale': np.exp(args.init_scale),\n",
    "        'learn_rate': np.exp(args.learn_rate),\n",
    "        'l2_penalty': np.exp(args.l2_penalty),\n",
    "        'l1_penalty': 0,\n",
    "          'h1_size':args.h1_size,\n",
    "        'conv_width':args.conv_width,\n",
    "         'batch_size':100}\n",
    "\n",
    "params['num_iters'] = args.num_epochs*len(train_inputs)/params['batch_size']\n",
    "\n",
    "# create predictive model (regressor)\n",
    "vanilla_net_params = dict(\n",
    "layer_sizes = [params['fp_length'], params['h1_size']],  # One hidden layer.\n",
    "normalize=True, L2_reg = params['l2_penalty'], L1_reg = params['l1_penalty'], nll_func = rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "print \"Task params\", task_params, params\n",
    "print\n",
    "print \"Starting Morgan fingerprint experiment...\"\n",
    "write_log_file(LOG_PATH, args.NUM_EXP+'_logs.txt', \"\\nStarting Morgan fingerprint experiment...\"+'\\n')\n",
    "test_loss_morgan = run_morgan_experiment()\n",
    "print \"Starting Average experiment...\"\n",
    "write_log_file(LOG_PATH, args.NUM_EXP+'_logs.txt', \"\\nStarting Average experiment...\"+'\\n')\n",
    "test_loss_avg = run_avg_experiment()\n",
    "print \"Starting neural fingerprint experiment...\"\n",
    "write_log_file(LOG_PATH, args.NUM_EXP+'_logs.txt', \"\\nStarting neural fingerprint experiment...\"+'\\n')\n",
    "test_loss_neural = run_conv_experiment()\n",
    "print\n",
    "print \"\\nMorgan test R2:\", test_loss_morgan, \"\\nAvg test R2:\", test_loss_avg, \"\\nNeural test R2:\", test_loss_neural\n",
    "write_log_file(LOG_PATH, args.NUM_EXP+'_logs.txt', \"\\nMorgan test R2: \"+\\\n",
    "               str(test_loss_morgan) +\n",
    "               \"\\nAvg test R2: \"+ str(test_loss_avg) + \n",
    "               \"\\nNeural test R2: \" + str(test_loss_neural) +'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neural_fingerprint_27] *",
   "language": "python",
   "name": "conda-env-neural_fingerprint_27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
