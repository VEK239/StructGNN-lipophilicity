{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Neural Fingerprint scripts\n",
    "\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('../neuralfingerprint')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import DrawingOptions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from build_vanilla_net import build_morgan_deep_net, relu, build_standard_net\n",
    "from build_convnet import build_conv_deep_net, build_convnet_fingerprint_fun\n",
    "from util import normalize_array, build_batched_grad\n",
    "from optimizers import adam\n",
    "from util import rmse\n",
    "from mol_graph import degrees\n",
    "from data_util import remove_duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function for loading our datasets\n",
    "def load_data(dataset_path = '../../mol_properties/data/3_final_data/split_data', prefix_name='logP_pH_range_mean', VALUE_COLUMN = 'logP', SMILES_COLUMN='smiles'):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from rdkit.Chem import MolFromSmiles\n",
    "    \n",
    "    def check_molecules(smiles):\n",
    "        mol = MolFromSmiles(smiles)\n",
    "        for atom in mol.GetAtoms():\n",
    "            if atom.GetDegree() not in [0, 1, 2, 3, 4, 5]:\n",
    "                with open('broken_smiles_'+prefix_name+'.txt', 'a') as f:\n",
    "                    f.write(smiles+'\\n')\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    with open('broken_smiles_'+prefix_name+'.txt', 'w') as f:\n",
    "        pass\n",
    "    \n",
    "    data_splits = ['train', 'test', 'validation']\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    for split in data_splits:\n",
    "        data = pd.read_csv(os.path.join(dataset_path,prefix_name+'_'+split+'.csv'))\n",
    "#         data = data[data[SMILES_COLUMN].map(check_molecules)]\n",
    "        datasets[split] = (data[SMILES_COLUMN].values, data[VALUE_COLUMN].values)\n",
    "        \n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_training_params(params):\n",
    "    nn_train_params = {'num_epochs'  : num_epochs,\n",
    "                       'batch_size'  : batch_size,\n",
    "#                        'learn_rate'  : params['learn_rate'],\n",
    "#                        'b1'          : params['b1'],\n",
    "#                        'b2'          : params['b2'],\n",
    "                       'param_scale' : params['init_scale']}\n",
    "\n",
    "    vanilla_net_params = {'layer_sizes':[params['fp_length']],  # Linear regression.\n",
    "                          'normalize':normalize,\n",
    "                          'L2_reg': params['l2_penalty'],\n",
    "#                           'L1_reg': params['l1_penalty'],\n",
    "                          'activation_function':activation}\n",
    "    return nn_train_params, vanilla_net_params\n",
    "\n",
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params,\n",
    "             validation_smiles=None, validation_raw_targets=None):\n",
    "    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n",
    "    print \"Total number of weights in the network:\", num_weights\n",
    "    npr.seed(0)\n",
    "    init_weights = npr.randn(num_weights) * train_params['param_scale']\n",
    "\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 10 == 0:\n",
    "            print \"max of weights\", np.max(np.abs(weights))\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles))\n",
    "            cur_loss = loss_fun(weights, train_smiles, train_targets)\n",
    "            training_curve.append(cur_loss)\n",
    "            print \"Iteration\", iter, \"loss\", cur_loss, \"train RMSE\", \\\n",
    "                np.sqrt(np.mean((train_preds - train_raw_targets)**2)),\n",
    "            print \"Train R2\", iter, \":\", \\\n",
    "                    r2_score(train_preds, train_raw_targets),\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print \"Validation RMSE\", iter, \":\", \\\n",
    "                    np.sqrt(np.mean((validation_preds - validation_raw_targets) ** 2)),\n",
    "                print \"Validation R2\", iter, \":\", \\\n",
    "                    r2_score(validation_preds, validation_raw_targets),\n",
    "            dub_preds = undo_norm(pred_fun(weights, dub_wo_params_smiles))\n",
    "            uniq_preds = undo_norm(pred_fun(weights, uniq_wo_params_smiles))\n",
    "            print \"Dub RMSE\", iter, \":\", rmse(dub_preds, dub_wo_params_targets)\n",
    "            print \"Unique RMSE\", iter, \":\", rmse(uniq_preds,  uniq_wo_params_targets)\n",
    "            print \"Dub R2\", iter, \":\", r2_score(dub_preds, dub_wo_params_targets)\n",
    "            print \"Unique R2\", iter, \":\", r2_score(uniq_preds,  uniq_wo_params_targets)\n",
    "\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    num_iters = train_params['num_epochs'] * len(train_smiles) / train_params['batch_size']\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=num_iters)#, step_size=train_params['learn_rate'],\n",
    "                           #b1=train_params['b1'], b2=train_params['b2'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve\n",
    "\n",
    "\n",
    "def train_neural_fingerprint():\n",
    "    print \"Loading data...\"\n",
    "    data = load_data(prefix_name = task_params['data_file'], VALUE_COLUMN = task_params['target_name'])\n",
    "\n",
    "    train_inputs, train_targets = data['train']\n",
    "    val_inputs,   val_targets   = data['validation']\n",
    "    test_inputs,  test_targets  = data['test']\n",
    "\n",
    "    print \"Regression on\", len(train_inputs), \"training points.\"\n",
    "    def print_performance(pred_func):\n",
    "        train_preds = pred_func(train_inputs)\n",
    "        test_preds = pred_func(test_inputs)\n",
    "        dub_preds = pred_func(dub_wo_params_smiles)\n",
    "        uniq_preds = pred_func(uniq_wo_params_smiles)\n",
    "        print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n",
    "        print \"Train:\", rmse(train_preds, train_targets)\n",
    "        print \"Test: \", rmse(test_preds,  test_targets)\n",
    "        print \"Dub:\", rmse(dub_preds, dub_wo_params_targets)\n",
    "        print \"Unique: \", rmse(uniq_preds,  uniq_wo_params_targets)\n",
    "        \n",
    "        print \"\\nPerformance (R2) on \" + task_params['target_name'] + \":\"\n",
    "        print \"Train:\", r2_score(train_preds, train_targets)\n",
    "        print \"Test: \", r2_score(test_preds,  test_targets)\n",
    "        print \"Dub:\", r2_score(dub_preds, dub_wo_params_targets)\n",
    "        print \"Unique: \", r2_score(uniq_preds,  uniq_wo_params_targets)\n",
    "        print \"-\" * 80\n",
    "        return rmse(test_preds,  test_targets)\n",
    "\n",
    "    print \"-\" * 80\n",
    "    print \"Mean predictor\"\n",
    "    y_train_mean = np.mean(train_targets)\n",
    "    print_performance(lambda x : y_train_mean*np.ones(len(x)))\n",
    "\n",
    "    print \"Task params\", params\n",
    "    nn_train_params, vanilla_net_params = parse_training_params(params)\n",
    "    conv_arch_params['return_atom_activations'] = False\n",
    "\n",
    "    print \"Convnet fingerprints with neural net\"\n",
    "    loss_fun, pred_fun, conv_parser = \\\n",
    "        build_conv_deep_net(conv_arch_params, vanilla_net_params, params['l2_penalty'])\n",
    "    num_weights = len(conv_parser)\n",
    "    predict_func, trained_weights, conv_training_curve = \\\n",
    "         train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                 nn_train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "    print_performance(predict_func)\n",
    "    return trained_weights\n",
    "\n",
    "\n",
    "def draw_molecule_with_highlights(filename, smiles, highlight_atoms):\n",
    "    drawoptions = DrawingOptions()\n",
    "    drawoptions.selectColor = highlight_color\n",
    "    drawoptions.elemDict = {}   # Don't color nodes based on their element.\n",
    "    drawoptions.bgColor=None\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fig = Draw.MolToMPL(mol, highlightAtoms=highlight_atoms, size=figsize, options=drawoptions,fitImage=False)\n",
    "\n",
    "    fig.gca().set_axis_off()\n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def construct_atom_neighbor_list(array_rep):\n",
    "    atom_neighbour_list = []\n",
    "    for degree in degrees:\n",
    "        atom_neighbour_list += [list(neighbours) for neighbours in array_rep[('atom_neighbors', degree)]]\n",
    "    return atom_neighbour_list\n",
    "\n",
    "\n",
    "def plot(trained_weights):\n",
    "    print \"Loading data...\"\n",
    "    data = load_data(prefix_name = task_params['data_file'], VALUE_COLUMN = task_params['target_name'])\n",
    "\n",
    "    train_smiles, train_targets = data['train']\n",
    "    val_inputs,   val_targets   = data['validation']\n",
    "    test_inputs,  test_targets  = data['test']\n",
    "\n",
    "    print \"Convnet fingerprints with neural net\"\n",
    "    conv_arch_params['return_atom_activations'] = True\n",
    "    output_layer_fun, parser, compute_atom_activations = \\\n",
    "       build_convnet_fingerprint_fun(**conv_arch_params)\n",
    "    atom_activations, array_rep = compute_atom_activations(trained_weights, train_smiles)\n",
    "\n",
    "    if not os.path.exists('figures'): os.makedirs('figures')\n",
    "\n",
    "    parent_molecule_dict = {}\n",
    "    for mol_ix, atom_ixs in enumerate(array_rep['atom_list']):\n",
    "        for atom_ix in atom_ixs:\n",
    "            parent_molecule_dict[atom_ix] = mol_ix\n",
    "\n",
    "    atom_neighbor_list = construct_atom_neighbor_list(array_rep)\n",
    "\n",
    "    def get_neighborhood_ixs(array_rep, cur_atom_ix, radius):\n",
    "        # Recursive function to get indices of all atoms in a certain radius.\n",
    "        if radius == 0:\n",
    "            return set([cur_atom_ix])\n",
    "        else:\n",
    "            cur_set = set([cur_atom_ix])\n",
    "            for n_ix in atom_neighbor_list[cur_atom_ix]:\n",
    "                cur_set.update(get_neighborhood_ixs(array_rep, n_ix, radius-1))\n",
    "            return cur_set\n",
    "\n",
    "    # Recreate trained network.\n",
    "    nn_train_params, vanilla_net_params = parse_training_params(params)\n",
    "    conv_arch_params['return_atom_activations'] = False\n",
    "    _, _, combined_parser = \\\n",
    "        build_conv_deep_net(conv_arch_params, vanilla_net_params, params['l2_penalty'])\n",
    "\n",
    "    net_loss_fun, net_pred_fun, net_parser = build_standard_net(**vanilla_net_params)\n",
    "    net_weights = combined_parser.get(trained_weights, 'net weights')\n",
    "    last_layer_weights = net_parser.get(net_weights, ('weights', 0))\n",
    "\n",
    "    for fp_ix in range(params['fp_length']):\n",
    "        print \"FP {0} has linear regression coefficient {1}\".format(fp_ix, last_layer_weights[fp_ix][0])\n",
    "        combined_list = []\n",
    "        for radius in all_radii:\n",
    "            fp_activations = atom_activations[radius][:, fp_ix]\n",
    "            combined_list += [(fp_activation, atom_ix, radius) for atom_ix, fp_activation in enumerate(fp_activations)]\n",
    "\n",
    "        unique_list = remove_duplicates(combined_list, key_lambda=lambda x: x[0])\n",
    "        combined_list = sorted(unique_list, key=lambda x: -x[0])\n",
    "\n",
    "        for fig_ix in range(num_figs_per_fp):\n",
    "            # Find the most-activating atoms for this fingerprint index, across all molecules and depths.\n",
    "            activation, most_active_atom_ix, cur_radius = combined_list[fig_ix]\n",
    "            most_activating_mol_ix = parent_molecule_dict[most_active_atom_ix]\n",
    "            highlight_list_our_ixs = get_neighborhood_ixs(array_rep, most_active_atom_ix, cur_radius)\n",
    "            highlight_list_rdkit = [array_rep['rdkit_ix'][our_ix] for our_ix in highlight_list_our_ixs]\n",
    "\n",
    "            print \"radius:\", cur_radius, \"atom list:\", highlight_list_rdkit, \"activation\", activation\n",
    "            draw_molecule_with_highlights(\n",
    "                \"figures/fp_{0}_highlight_{1}.pdf\".format(fp_ix, fig_ix),\n",
    "                train_smiles[most_activating_mol_ix],\n",
    "                highlight_atoms=highlight_list_rdkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating molecules with unique and several measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import seaborn as sns\n",
    "# from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_PATH = \"../../mol_properties/data/3_final_data\"\n",
    "SMILES_COLUMN = 'smiles'\n",
    "VALUE_COLUMN = 'logP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logP_dataset = pd.read_csv(os.path.join(DATASETS_PATH,'logP.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logP_mean_dataset = pd.read_csv(os.path.join(DATASETS_PATH,'logp_mean.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logP_wo_params_dataset = pd.read_csv(os.path.join(DATASETS_PATH,'logP_wo_parameters.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14111 entries, 0 to 14110\n",
      "Data columns (total 2 columns):\n",
      "smiles    14111 non-null object\n",
      "logP      14111 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 220.6+ KB\n"
     ]
    }
   ],
   "source": [
    "logP_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13759 entries, 0 to 13758\n",
      "Data columns (total 3 columns):\n",
      "Unnamed: 0    13759 non-null int64\n",
      "smiles        13759 non-null object\n",
      "logP          13759 non-null float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 322.5+ KB\n"
     ]
    }
   ],
   "source": [
    "logP_mean_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12626 entries, 0 to 12625\n",
      "Data columns (total 2 columns):\n",
      "smiles    12626 non-null object\n",
      "logP      12626 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 197.4+ KB\n"
     ]
    }
   ],
   "source": [
    "logP_wo_params_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_smiles = logP_dataset.groupby([SMILES_COLUMN]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_dub = list(duplicates_smiles[duplicates_smiles[VALUE_COLUMN]>1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_uniq = list(duplicates_smiles[duplicates_smiles[VALUE_COLUMN]==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data_mean = logP_mean_dataset[logP_mean_dataset[SMILES_COLUMN].isin(smiles_dub)]\n",
    "uniq_data_mean = logP_mean_dataset[logP_mean_dataset[SMILES_COLUMN].isin(smiles_uniq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_mean_smiles, dub_mean_targets = list(dub_data_mean[SMILES_COLUMN]), list(dub_data_mean[VALUE_COLUMN])\n",
    "uniq_mean_smiles, uniq_mean_targets = list(uniq_data_mean[SMILES_COLUMN]), list(uniq_data_mean[VALUE_COLUMN])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data_wo_params = logP_wo_params_dataset[logP_wo_params_dataset[SMILES_COLUMN].isin(smiles_dub)]\n",
    "uniq_data_wo_params = logP_wo_params_dataset[logP_wo_params_dataset[SMILES_COLUMN].isin(smiles_uniq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_wo_params_smiles, dub_wo_params_targets = list(dub_data_wo_params[SMILES_COLUMN]), list(dub_data_wo_params[VALUE_COLUMN])\n",
    "uniq_wo_params_smiles, uniq_wo_params_targets = list(uniq_data_wo_params[SMILES_COLUMN]), list(uniq_data_wo_params[VALUE_COLUMN])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logP_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {'target_name' : 'logP',\n",
    "               'data_file'   : 'logp_mean'}\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "normalize = 1\n",
    "dropout = 0\n",
    "activation = relu\n",
    "params = {'fp_length': 20,\n",
    "            'fp_depth': 3,\n",
    "            'init_scale':np.exp(-4),\n",
    "            'learn_rate':np.exp(-4),\n",
    "                    'b1':np.exp(-4),\n",
    "                    'b2':np.exp(-4),\n",
    "            'l2_penalty':np.exp(-4),\n",
    "            'l1_penalty':np.exp(-5),\n",
    "            'conv_width':10}\n",
    "\n",
    "conv_layer_sizes = [params['conv_width']] * params['fp_depth']\n",
    "conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                    'fp_length' : params['fp_length'],\n",
    "                    'normalize' : normalize,\n",
    "                    'return_atom_activations':False}\n",
    "\n",
    "all_radii = range(params['fp_depth'] + 1)\n",
    "\n",
    "# Plotting parameters\n",
    "num_figs_per_fp = 11\n",
    "figsize = (100, 100)\n",
    "highlight_color = (30.0/255.0, 100.0/255.0, 255.0/255.0)  # A nice light blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Regression on 800 training points.\n",
      "--------------------------------------------------------------------------------\n",
      "Mean predictor\n",
      "\n",
      "Performance (RMSE) on logP:\n",
      "Train: 1.859267165016053\n",
      "Test:  1.8550727952633121\n",
      "--------------------------------------------------------------------------------\n",
      "Task params {'learn_rate': 0.01831563888873418, 'fp_depth': 3, 'b1': 0.01831563888873418, 'b2': 0.01831563888873418, 'init_scale': 0.01831563888873418, 'fp_length': 20, 'l2_penalty': 0.01831563888873418, 'l1_penalty': 0.006737946999085467, 'conv_width': 10}\n",
      "Convnet fingerprints with neural net\n",
      "Total number of weights in the network: 8791\n",
      "max of weights 0.06962983567500523\n",
      "Iteration 0 loss 1.052217399906365 train RMSE 1.9070721742622474 Validation RMSE 0 : 1.8954592417051612 max of weights 0.19345587019725458\n",
      "Iteration 10 loss 1.4459786015306226 train RMSE 2.23564794998938 Validation RMSE 10 : 2.20525463097695 max of weights 0.27519345919657223\n",
      "Iteration 20 loss 1.5785792888927956 train RMSE 2.3359102700386187 Validation RMSE 20 : 2.3025809827731667 max of weights 0.42947114207204856\n",
      "Iteration 30 loss 1.482088452409438 train RMSE 2.2633781101812698 Validation RMSE 30 : 2.232301751452704 max of weights 0.5943308290775344\n",
      "Iteration 40 loss 1.4730021500111767 train RMSE 2.256401396503377 Validation RMSE 40 : 2.225509007075981 max of weights 0.7331904822671493\n",
      "Iteration 50 loss 1.4565349620614225 train RMSE 2.2437030137822265 Validation RMSE 50 : 2.2130960691445702 max of weights 0.8659093995624885\n",
      "Iteration 60 loss 1.4387449470684728 train RMSE 2.229904662694033 Validation RMSE 60 : 2.199222593694951 max of weights 0.9971822941108915\n",
      "Iteration 70 loss 1.3615730216443431 train RMSE 2.1691949115758287 Validation RMSE 70 : 2.139042196683845 max of weights 1.110831605325512\n",
      "Iteration 80 loss 1.2633489424241993 train RMSE 2.0894080921991867 Validation RMSE 80 : 2.058525401001527 max of weights 1.179648358741741\n",
      "Iteration 90 loss 1.2464269416691334 train RMSE 2.075295187346878 Validation RMSE 90 : 2.047798203458118 max of weights 1.2244874172942224\n",
      "Iteration 100 loss 1.2937753266704277 train RMSE 2.1142919471043062 Validation RMSE 100 : 2.0853626863652956 max of weights 1.315774792331212\n",
      "Iteration 110 loss 1.27207980590484 train RMSE 2.0964154506032364 Validation RMSE 110 : 2.0668789285948903 max of weights 1.4309015100852442\n",
      "Iteration 120 loss 1.0114442774261108 train RMSE 1.8691573664685706 Validation RMSE 120 : 1.8459020167535016 max of weights 1.6098553021991842\n",
      "Iteration 130 loss 1.1835697328401238 train RMSE 2.0220146734530204 Validation RMSE 130 : 1.993390950165208 max of weights 1.785704468519206\n",
      "Iteration 140 loss 1.140554442778708 train RMSE 1.984859052838721 Validation RMSE 140 : 1.9608752941996266 max of weights 1.9175017709233209\n",
      "Iteration 150 loss 1.1078208876093991 train RMSE 1.9560715587524673 Validation RMSE 150 : 1.9317153260388864 max of weights 2.062559160180677\n",
      "Iteration 160 loss 1.0730538610341784 train RMSE 1.9250432335759395 Validation RMSE 160 : 1.898182518494336 max of weights 2.1269725525969414\n",
      "Iteration 170 loss 1.0300508207514052 train RMSE 1.8859783429969346 Validation RMSE 170 : 1.859996203682496 max of weights 2.1544624612857577\n",
      "Iteration 180 loss 1.024735963720058 train RMSE 1.8810551780560263 Validation RMSE 180 : 1.855719432229885 max of weights 2.1723322539449135\n",
      "Iteration 190 loss 1.0141788939959788 train RMSE 1.8712727292921938 Validation RMSE 190 : 1.8414712524540822 max of weights 2.1896636858557446\n",
      "Iteration 200 loss 1.0236243490569528 train RMSE 1.8799379949547874 Validation RMSE 200 : 1.8495518890758906 max of weights 2.1966781349968323\n",
      "Iteration 210 loss 1.0571445243653792 train RMSE 1.9104445861384978 Validation RMSE 210 : 1.8866054983307623 max of weights 2.213920528520455\n",
      "Iteration 220 loss 0.9819668496748494 train RMSE 1.841127905183229 Validation RMSE 220 : 1.818444149627059 max of weights 2.223860131406744\n",
      "Iteration 230 loss 0.993567743009315 train RMSE 1.8519439553024692 Validation RMSE 230 : 1.8173636147175578 max of weights 2.2333429047133655\n",
      "Iteration 240 loss 1.000954309763168 train RMSE 1.8587754271866452 Validation RMSE 240 : 1.8293176025174476 max of weights 2.236898620357977\n",
      "Iteration 250 loss 0.9997364954347187 train RMSE 1.8576073446983477 Validation RMSE 250 : 1.817983325322551 max of weights 2.2467990841143366\n",
      "Iteration 260 loss 0.936872779212132 train RMSE 1.7981320611706455 Validation RMSE 260 : 1.7667664431315915 max of weights 2.249465953314801\n",
      "Iteration 270 loss 0.9072221126532496 train RMSE 1.7693664233299042 Validation RMSE 270 : 1.738717533951773 max of weights 2.2542799931684425\n",
      "Iteration 280 loss 0.9648657564412779 train RMSE 1.8247519569690533 Validation RMSE 280 : 1.8012027273090274 max of weights 2.267176796184124\n",
      "Iteration 290 loss 0.9138876050033862 train RMSE 1.7757688307946309 Validation RMSE 290 : 1.7510883610737253 max of weights 2.26951045152762\n",
      "Iteration 300 loss 0.9900749562703848 train RMSE 1.8483713593568276 Validation RMSE 300 : 1.8185675333412221 max of weights 2.2700242306525453\n",
      "Iteration 310 loss 0.9341511697305573 train RMSE 1.7952552442917522 Validation RMSE 310 : 1.770633328178057 max of weights 2.280754330212259\n",
      "Iteration 320 loss 0.9357962583167685 train RMSE 1.796802940725421 Validation RMSE 320 : 1.772365786164744 max of weights 2.28442362971909\n",
      "Iteration 330 loss 0.9549914942861453 train RMSE 1.8151308911398907 Validation RMSE 330 : 1.7883864250594808 max of weights 2.280308016166477\n",
      "Iteration 340 loss 0.8755796061421689 train RMSE 1.7378316609690747 Validation RMSE 340 : 1.7103838928145523 max of weights 2.286312874413109\n",
      "Iteration 350 loss 0.9223861745614682 train RMSE 1.7837319503892015 Validation RMSE 350 : 1.7540272794941159 max of weights 2.2959038393756477\n",
      "Iteration 360 loss 0.9420263471070388 train RMSE 1.8025975992825443 Validation RMSE 360 : 1.7673655073032484 max of weights 2.2995083464741044\n",
      "Iteration 370 loss 0.8673684919859382 train RMSE 1.729477240206239 Validation RMSE 370 : 1.700508463134297 max of weights 2.3011998267644826\n",
      "Iteration 380 loss 0.8304167943559713 train RMSE 1.6920920272807674 Validation RMSE 380 : 1.663370990627352 max of weights 2.307847349051647\n",
      "Iteration 390 loss 0.797475715334147 train RMSE 1.658079650326491 Validation RMSE 390 : 1.6324226241908864 max of weights 2.309587595872455\n",
      "Iteration 400 loss 0.9235458534800478 train RMSE 1.7846268844642477 Validation RMSE 400 : 1.7544399532077204 max of weights 2.3170993838397616\n",
      "Iteration 410 loss 0.8614821271370638 train RMSE 1.7234325912505548 Validation RMSE 410 : 1.6895563868961818 max of weights 2.323325353304421\n",
      "Iteration 420 loss 0.9489548683666288 train RMSE 1.809019211742087 Validation RMSE 420 : 1.7843245199747741 max of weights 2.3264241646124892\n",
      "Iteration 430 loss 0.9202612784222668 train RMSE 1.781325607883454 Validation RMSE 430 : 1.7465553459655248 max of weights 2.33293403384599\n",
      "Iteration 440 loss 0.8719783543178095 train RMSE 1.7337975874314109 Validation RMSE 440 : 1.6933639518270207 max of weights 2.334832661304869\n",
      "Iteration 450 loss 0.9338925738014193 train RMSE 1.794418302156662 Validation RMSE 450 : 1.7651218003289866 max of weights 2.341250541804136\n",
      "Iteration 460 loss 0.7824316846990071 train RMSE 1.6420385725268916 Validation RMSE 460 : 1.61331146676746 max of weights 2.342906930726457\n",
      "Iteration 470 loss 0.8567703516488401 train RMSE 1.7184482148756473 Validation RMSE 470 : 1.6909544152589184 max of weights 2.3645406063781316\n",
      "Iteration 480 loss 0.8305441737413937 train RMSE 1.691814876991994 Validation RMSE 480 : 1.6722083433357096 max of weights 2.378811900356856\n",
      "Iteration 490 loss 0.9140410134724423 train RMSE 1.7750084399284027 Validation RMSE 490 : 1.7520569059748483 max of weights 2.3878235901663385\n",
      "Iteration 500 loss 0.8419376888175129 train RMSE 1.7032804918570321 Validation RMSE 500 : 1.684468634042203 max of weights 2.382529786592958\n",
      "Iteration 510 loss 0.7778520969268071 train RMSE 1.6369229222206738 Validation RMSE 510 : 1.621910041499602 max of weights 2.37091219216557\n",
      "Iteration 520 loss 0.9134794913275276 train RMSE 1.7743336915028312 Validation RMSE 520 : 1.7469580698955265 max of weights 2.5434207785294998\n",
      "Iteration 530 loss 0.9193501469874075 train RMSE 1.7799968110674975 Validation RMSE 530 : 1.755525060068516 max of weights 2.7193178036882193\n",
      "Iteration 540 loss 0.8243884399345734 train RMSE 1.6851912892006768 Validation RMSE 540 : 1.6599029574925612 max of weights 2.8645498187997127\n",
      "Iteration 550 loss 0.8169064277796892 train RMSE 1.6774445849664252 Validation RMSE 550 : 1.6477659612535922 max of weights 3.0285407877877546\n",
      "Iteration 560 loss 0.7678378834030478 train RMSE 1.6260561693242863 Validation RMSE 560 : 1.5972005444961637 max of weights 3.179717986709029\n",
      "Iteration 570 loss 0.8307683667498673 train RMSE 1.6915958517333902 Validation RMSE 570 : 1.6662860368096366 max of weights 3.291188560255954\n",
      "Iteration 580 loss 0.7853926729582664 train RMSE 1.6445454387848164 Validation RMSE 580 : 1.621260174053126 max of weights 3.336171187554329\n",
      "Iteration 590 loss 0.7869394531917716 train RMSE 1.646130911640377 Validation RMSE 590 : 1.6211196192168709 max of weights 3.3763826709738582\n",
      "Iteration 600 loss 0.8144461983934641 train RMSE 1.674727687073516 Validation RMSE 600 : 1.6444896553424693 max of weights 3.412774411095845\n",
      "Iteration 610 loss 0.8142891893327636 train RMSE 1.6745384681137054 Validation RMSE 610 : 1.6494164925563366 max of weights 3.432129034299839\n",
      "Iteration 620 loss 0.8089891474605165 train RMSE 1.6690264262999122 Validation RMSE 620 : 1.6396329373978602 max of weights 3.464634114601809\n",
      "Iteration 630 loss 0.8620427231339576 train RMSE 1.7230343785876847 Validation RMSE 630 : 1.6927638914190672 max of weights 3.4850329733729377\n",
      "Iteration 640 loss 0.8350218465105423 train RMSE 1.6956625902809617 Validation RMSE 640 : 1.6571876766051103 max of weights 3.494129085180529\n",
      "Iteration 650 loss 0.8455400459415294 train RMSE 1.7063262904555105 Validation RMSE 650 : 1.6734161722671024 max of weights 3.504594769209634\n",
      "Iteration 660 loss 0.7506166348239658 train RMSE 1.6072466398155785 Validation RMSE 660 : 1.5664585896276482 max of weights 3.520148013274862\n",
      "Iteration 670 loss 0.7797760752603914 train RMSE 1.638253701430515 Validation RMSE 670 : 1.6146155301277223 max of weights 3.5262510561857674\n",
      "Iteration 680 loss 0.84774803248081 train RMSE 1.7084399648910142 Validation RMSE 680 : 1.6761433851906677 max of weights 3.5353207960054545\n",
      "Iteration 690 loss 0.8934502146930755 train RMSE 1.7540183825628757 Validation RMSE 690 : 1.7295337693054558 max of weights 3.5396148382582617\n",
      "Iteration 700 loss 0.7595676224517677 train RMSE 1.616655484510253 Validation RMSE 700 : 1.5964622784061941 max of weights 3.5513426645714326\n",
      "Iteration 710 loss 0.8915021365062349 train RMSE 1.7520223069095011 Validation RMSE 710 : 1.7346532538367232 max of weights 3.5597438094683316\n",
      "Iteration 720 loss 0.8002330421899012 train RMSE 1.6594925173095914 Validation RMSE 720 : 1.6308541518300137 max of weights 3.5653477594120964\n",
      "Iteration 730 loss 0.7712452267778085 train RMSE 1.6289514611242029 Validation RMSE 730 : 1.607551532550168 max of weights 3.57559780708885\n",
      "Iteration 740 loss 0.8367573067927679 train RMSE 1.6969733280018091 Validation RMSE 740 : 1.6732437672565559 max of weights 3.592340810739533\n",
      "Iteration 750 loss 0.7994133997403015 train RMSE 1.6584471122929048 Validation RMSE 750 : 1.6202990189918296 max of weights 3.598999780770358\n",
      "Iteration 760 loss 0.7812755688547929 train RMSE 1.639390319437768 Validation RMSE 760 : 1.614235858105939 max of weights 3.610010297634185\n",
      "Iteration 770 loss 0.7142113085049914 train RMSE 1.5670743503344229 Validation RMSE 770 : 1.541754772887183 max of weights 3.6154329444381803\n",
      "Iteration 780 loss 0.6878416927790515 train RMSE 1.5376956423614356 Validation RMSE 780 : 1.512556507489448 max of weights 3.625301591447055\n",
      "Iteration 790 loss 0.871599884855369 train RMSE 1.7319452061566851 Validation RMSE 790 : 1.7082982514462444 max of weights 3.6338906507183757\n",
      "Iteration 800 loss 0.7691383887310886 train RMSE 1.6264281444023 Validation RMSE 800 : 1.601171515970273 max of weights 3.6408555667395213\n",
      "Iteration 810 loss 0.8022967549692748 train RMSE 1.6612673310938824 Validation RMSE 810 : 1.6354432451288363 max of weights 3.650570248591205\n",
      "Iteration 820 loss 0.8214607847305643 train RMSE 1.6810338472292752 Validation RMSE 820 : 1.6595358590340223 max of weights 3.6563049692345655\n",
      "Iteration 830 loss 0.7846657875472289 train RMSE 1.6427186867187626 Validation RMSE 830 : 1.612714923627133 max of weights 3.660495759286154\n",
      "Iteration 840 loss 0.7716548174920546 train RMSE 1.6289203748544894 Validation RMSE 840 : 1.5983069623390453 max of weights 3.6630444383056195\n",
      "Iteration 850 loss 0.6959334753117503 train RMSE 1.5464674054961804 Validation RMSE 850 : 1.5145223018390683 max of weights 3.673108021127222\n",
      "Iteration 860 loss 0.9132387093884767 train RMSE 1.7727309128200377 Validation RMSE 860 : 1.7542739645027061 max of weights 3.675851668522155\n",
      "Iteration 870 loss 0.8052010796367528 train RMSE 1.664044358498119 Validation RMSE 870 : 1.6511145359181292 max of weights 3.6793278756486614\n",
      "Iteration 880 loss 0.8409081697847345 train RMSE 1.7006773061932743 Validation RMSE 880 : 1.6848895976354676 max of weights 3.684030138661495\n",
      "Iteration 890 loss 0.766754050320472 train RMSE 1.6235021758795296 Validation RMSE 890 : 1.6046296193341751 max of weights 3.6871444622088383\n",
      "Iteration 900 loss 0.7571315808895006 train RMSE 1.6132127942098125 Validation RMSE 900 : 1.596700448688319 max of weights 3.6920592442768587\n",
      "Iteration 910 loss 0.7878413531245811 train RMSE 1.6457613900169688 Validation RMSE 910 : 1.6224693640129926 max of weights 3.697385618667559\n",
      "Iteration 920 loss 0.7742002102854606 train RMSE 1.6313219852604972 Validation RMSE 920 : 1.60443834952066 max of weights 3.708184218045095\n",
      "Iteration 930 loss 0.784156576997048 train RMSE 1.6417647708330585 Validation RMSE 930 : 1.6232948158869633 max of weights 3.7612818385710076\n",
      "Iteration 940 loss 0.715967309116049 train RMSE 1.5682584124371115 Validation RMSE 940 : 1.5348055436214914 max of weights 3.7724276532366936\n",
      "Iteration 950 loss 0.799420195145756 train RMSE 1.6576349284045913 Validation RMSE 950 : 1.6330112598804396 max of weights 3.8229404265674223\n",
      "Iteration 960 loss 0.7543995434160307 train RMSE 1.609998855053315 Validation RMSE 960 : 1.5926309259228308 max of weights 3.8467180297179637\n",
      "Iteration 970 loss 0.7104460717856655 train RMSE 1.5620639829519656 Validation RMSE 970 : 1.545474929033599 max of weights 3.870750780982423\n",
      "Iteration 980 loss 0.7475183705457636 train RMSE 1.6025298569772504 Validation RMSE 980 : 1.5829542848248808 max of weights 3.904590314680893\n",
      "Iteration 990 loss 0.8411653206174882 train RMSE 1.700489485552344 Validation RMSE 990 : 1.6813178058027607 max of weights 3.9647007624524138\n",
      "Iteration 1000 loss 0.7698440156784351 train RMSE 1.6263539224431336 Validation RMSE 1000 : 1.6033162635174845 max of weights 3.981539885451621\n",
      "Iteration 1010 loss 0.7574496102463955 train RMSE 1.6131109628952383 Validation RMSE 1010 : 1.5894213473060579 max of weights 4.090691500277777\n",
      "Iteration 1020 loss 0.7767442632227408 train RMSE 1.6336233511707474 Validation RMSE 1020 : 1.6110414913348483 max of weights 4.143888365730233\n",
      "Iteration 1030 loss 0.8007282718876464 train RMSE 1.6587470511851032 Validation RMSE 1030 : 1.6285445310516455 max of weights 4.179824551581211\n",
      "Iteration 1040 loss 0.7271368388565131 train RMSE 1.580175366250105 Validation RMSE 1040 : 1.5559499722715415 max of weights 4.209138515547483\n",
      "Iteration 1050 loss 0.7322654243298731 train RMSE 1.5857216719634104 Validation RMSE 1050 : 1.5620743318869472 max of weights 4.21084694034744\n",
      "Iteration 1060 loss 0.7792927571629339 train RMSE 1.6361410402027914 Validation RMSE 1060 : 1.6203606402916315 max of weights 4.247433574873958\n",
      "Iteration 1070 loss 0.8399110560524345 train RMSE 1.6989336492121252 Validation RMSE 1070 : 1.6837241359790216 max of weights 4.291956338041784\n",
      "Iteration 1080 loss 0.8569470720523829 train RMSE 1.7161181357621553 Validation RMSE 1080 : 1.7035055938384078 max of weights 4.294919516374181\n",
      "Iteration 1090 loss 0.6212905791463273 train RMSE 1.4595557358223867 Validation RMSE 1090 : 1.4368583867049856 max of weights 4.370386518588104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1100 loss 0.8275428552112589 train RMSE 1.6861810065244007 Validation RMSE 1100 : 1.670135073740472 max of weights 4.4007104879977526\n",
      "Iteration 1110 loss 0.7869945017613229 train RMSE 1.644052002729193 Validation RMSE 1110 : 1.6254906646420275 max of weights 4.478216359334269\n",
      "Iteration 1120 loss 0.7460780713307693 train RMSE 1.6003886251166748 Validation RMSE 1120 : 1.585689058842461 max of weights 4.488513335261613\n",
      "Iteration 1130 loss 0.7520146137037418 train RMSE 1.6067226253396465 Validation RMSE 1130 : 1.5884993760103834 max of weights 4.538805065029834\n",
      "Iteration 1140 loss 0.7552935160738973 train RMSE 1.610211356390134 Validation RMSE 1140 : 1.5899921794918455 max of weights 4.652584899648605\n",
      "Iteration 1150 loss 0.7371438920554406 train RMSE 1.590557928301801 Validation RMSE 1150 : 1.5705022886370295 max of weights 4.750433823819734\n",
      "Iteration 1160 loss 0.7118374973268448 train RMSE 1.562803300072586 Validation RMSE 1160 : 1.5521829483860539 max of weights 4.837202203300558\n",
      "Iteration 1170 loss 0.7766808070180211 train RMSE 1.632930869824845 Validation RMSE 1170 : 1.6151100704172119 max of weights 4.8131149615706255\n",
      "Iteration 1180 loss 0.8170772422855764 train RMSE 1.6751189856244164 Validation RMSE 1180 : 1.661597056112118 max of weights 4.906391897169586\n",
      "Iteration 1190 loss 0.7595355937871578 train RMSE 1.6146179190713337 Validation RMSE 1190 : 1.5973044515530292 max of weights 4.880455733142509\n",
      "Iteration 1200 loss 0.7456434023736049 train RMSE 1.5996568237225748 Validation RMSE 1200 : 1.5802589333118016 max of weights 4.953492977669787\n",
      "Iteration 1210 loss 0.7697153504551142 train RMSE 1.6254349286024605 Validation RMSE 1210 : 1.607705912990588 max of weights 4.92194518627966\n",
      "Iteration 1220 loss 0.7985346686178598 train RMSE 1.6557604134973813 Validation RMSE 1220 : 1.629450390757463 max of weights 4.964336245057034\n",
      "Iteration 1230 loss 0.7093355351970648 train RMSE 1.5598103066602924 Validation RMSE 1230 : 1.5290010875300524 max of weights 5.0144073800135365\n",
      "Iteration 1240 loss 0.7397740106719466 train RMSE 1.5931549405288559 Validation RMSE 1240 : 1.5756943635224123 max of weights 5.040472324365763\n",
      "Iteration 1250 loss 0.8282716745388016 train RMSE 1.6863811338494727 Validation RMSE 1250 : 1.6759163333688656 max of weights 5.01779374897944\n",
      "Iteration 1260 loss 0.7396818968462174 train RMSE 1.5929892324311663 Validation RMSE 1260 : 1.5792473623631493 max of weights 5.038298669985359\n",
      "Iteration 1270 loss 0.818760534913962 train RMSE 1.6765352268968918 Validation RMSE 1270 : 1.6626172182354404 max of weights 5.084707531374679\n",
      "Iteration 1280 loss 0.7586815728109448 train RMSE 1.6133453265856346 Validation RMSE 1280 : 1.6007887016989946 max of weights 5.028892538818484\n",
      "Iteration 1290 loss 0.7791084910012571 train RMSE 1.6350546819285994 Validation RMSE 1290 : 1.6266158853883572 max of weights 5.11608912522166\n",
      "Iteration 1300 loss 0.7942825938319935 train RMSE 1.6509713347860508 Validation RMSE 1300 : 1.6343652600327403 max of weights 5.2657641134884585\n",
      "Iteration 1310 loss 0.7258326014122711 train RMSE 1.5776297155295924 Validation RMSE 1310 : 1.5597641958002633 max of weights 5.2293185492232785\n",
      "Iteration 1320 loss 0.7765366331368533 train RMSE 1.6321977061142567 Validation RMSE 1320 : 1.621669117994376 max of weights 5.30427661872706\n",
      "Iteration 1330 loss 0.7838826176943973 train RMSE 1.6398897854459629 Validation RMSE 1330 : 1.6193405106879093 max of weights 5.38541171214617\n",
      "Iteration 1340 loss 0.7505532712293254 train RMSE 1.6043399176226827 Validation RMSE 1340 : 1.5874664439143296 max of weights 5.42075360582959\n",
      "Iteration 1350 loss 0.6966529380948473 train RMSE 1.5451717560306972 Validation RMSE 1350 : 1.5333518149350582 max of weights 5.5061559677019964\n",
      "Iteration 1360 loss 0.6530093705095796 train RMSE 1.4955294761816584 Validation RMSE 1360 : 1.48313293615027 max of weights 5.55082374828266\n",
      "Iteration 1370 loss 0.7934908035965564 train RMSE 1.6498929656415326 Validation RMSE 1370 : 1.6325690807442512 max of weights 5.485341401455099\n",
      "Iteration 1380 loss 0.722182609137551 train RMSE 1.573388715869365 Validation RMSE 1380 : 1.5496461809987805 max of weights 5.527054594212037\n",
      "Iteration 1390 loss 0.8266434234585059 train RMSE 1.684218397871743 Validation RMSE 1390 : 1.6669775733427978 max of weights 5.5660859784531125\n",
      "Iteration 1400 loss 0.7919079078221104 train RMSE 1.6481448376319114 Validation RMSE 1400 : 1.6317931078858594 max of weights 5.56800355054741\n",
      "Iteration 1410 loss 0.7630892305692348 train RMSE 1.6176198229572059 Validation RMSE 1410 : 1.5954560292199553 max of weights 5.521551471937629\n",
      "Iteration 1420 loss 0.7890915472592424 train RMSE 1.6450901687390225 Validation RMSE 1420 : 1.6214698554933533 max of weights 5.5243434924947525\n",
      "Iteration 1430 loss 0.6763588058947534 train RMSE 1.5220250991830087 Validation RMSE 1430 : 1.5011993268832051 max of weights 5.597224896849173\n",
      "Iteration 1440 loss 0.7545831889779172 train RMSE 1.6083391336108332 Validation RMSE 1440 : 1.5943328140347457 max of weights 5.573382817857483\n",
      "Iteration 1450 loss 0.727330224352945 train RMSE 1.5787537945726375 Validation RMSE 1450 : 1.5682234616962902 max of weights 5.568995903147286\n",
      "Iteration 1460 loss 0.8217649114425633 train RMSE 1.678911633604212 Validation RMSE 1460 : 1.6690345718094104 max of weights 5.667002203315364\n",
      "Iteration 1470 loss 0.7583281997389402 train RMSE 1.6122205118454291 Validation RMSE 1470 : 1.6021646712329989 max of weights 5.677926389195591\n",
      "Iteration 1480 loss 0.6954776887481825 train RMSE 1.543345148410533 Validation RMSE 1480 : 1.5376425519531731 max of weights 5.71670772274916\n",
      "Iteration 1490 loss 0.8046209602703938 train RMSE 1.6610345429886977 Validation RMSE 1490 : 1.644757819424233 max of weights 5.800648791148273\n",
      "Iteration 1500 loss 0.7975612216061388 train RMSE 1.653630623111159 Validation RMSE 1500 : 1.6377211455196752 max of weights 5.847972238318786\n",
      "Iteration 1510 loss 0.7258226958761987 train RMSE 1.5768129176044647 Validation RMSE 1510 : 1.5645338731917524 max of weights 5.882143744771013\n",
      "Iteration 1520 loss 0.7252615582767149 train RMSE 1.5761649994630602 Validation RMSE 1520 : 1.565346414945984 max of weights 5.900362604236372\n",
      "Iteration 1530 loss 0.6942995392841924 train RMSE 1.5417883534211405 Validation RMSE 1530 : 1.5190245433798835 max of weights 5.9140935199608045\n",
      "Iteration 1540 loss 0.7539587574119924 train RMSE 1.6072617277652272 Validation RMSE 1540 : 1.5871255406244822 max of weights 5.974500098837025\n",
      "Iteration 1550 loss 0.7014840981251804 train RMSE 1.5497615665800817 Validation RMSE 1550 : 1.5391043229028412 max of weights 6.031499494112735\n",
      "Iteration 1560 loss 0.6967596778515961 train RMSE 1.54444545379898 Validation RMSE 1560 : 1.5269486180618204 max of weights 5.968122762956526\n",
      "Iteration 1570 loss 0.7393415400332163 train RMSE 1.5913706261478118 Validation RMSE 1570 : 1.5751113059286048 max of weights 5.973280694139442\n",
      "Iteration 1580 loss 0.7292809735928797 train RMSE 1.5803786047109003 Validation RMSE 1580 : 1.5649757017074633 max of weights 5.977500776285077\n",
      "Iteration 1590 loss 0.7293231620463992 train RMSE 1.5804125762121701 Validation RMSE 1590 : 1.564432621115429 max of weights 6.034402734770797\n",
      "Iteration 1600 loss 0.7716538168509833 train RMSE 1.6260187551261185 Validation RMSE 1600 : 1.6054672433857073 max of weights 5.948778550536373\n",
      "Iteration 1610 loss 0.7498035638520153 train RMSE 1.602590066499545 Validation RMSE 1610 : 1.5753158604173896 max of weights 6.009093146401254\n",
      "Iteration 1620 loss 0.7594377053234261 train RMSE 1.612903326640656 Validation RMSE 1620 : 1.5937483132629715 max of weights 6.031391654257451\n",
      "Iteration 1630 loss 0.681421290686268 train RMSE 1.526956316680259 Validation RMSE 1630 : 1.5017607622366502 max of weights 6.052080277097395\n",
      "Iteration 1640 loss 0.7308902418175142 train RMSE 1.5819123787695242 Validation RMSE 1640 : 1.5663712025059071 max of weights 6.0288452010097435\n",
      "Iteration 1650 loss 0.7817128283197328 train RMSE 1.636479777013732 Validation RMSE 1650 : 1.624712371998885 max of weights 6.068753234569879\n",
      "Iteration 1660 loss 0.803205085047564 train RMSE 1.6589616274517904 Validation RMSE 1660 : 1.6478072340181833 max of weights 6.099932591585207\n",
      "Iteration 1670 loss 0.7018991653072304 train RMSE 1.5497644416349587 Validation RMSE 1670 : 1.5384667869838575 max of weights 6.007433331288733\n",
      "Iteration 1680 loss 0.7898914949376409 train RMSE 1.6449423777582393 Validation RMSE 1680 : 1.64038669383 max of weights 6.061801848451099\n",
      "Iteration 1690 loss 0.7445261415999669 train RMSE 1.5965136890687228 Validation RMSE 1690 : 1.587532723745102 max of weights 6.173776955889266\n",
      "Iteration 1700 loss 0.7083488116991223 train RMSE 1.5567939072112729 Validation RMSE 1700 : 1.5490825509845878 max of weights 6.136676898946958\n",
      "Iteration 1710 loss 0.755220729662211 train RMSE 1.6079672597202517 Validation RMSE 1710 : 1.5978601885550898 max of weights 6.248620201837519\n",
      "Iteration 1720 loss 0.7457135561663772 train RMSE 1.597663951949553 Validation RMSE 1720 : 1.5793753808878719 max of weights 6.27813578779306\n",
      "Iteration 1730 loss 0.728575667256097 train RMSE 1.5789784590766784 Validation RMSE 1730 : 1.561945750804959 max of weights 6.248125071461153\n",
      "Iteration 1740 loss 0.6697577148006681 train RMSE 1.5132158821589763 Validation RMSE 1740 : 1.4984636575495138 max of weights 6.317866271339981\n",
      "Iteration 1750 loss 0.6415589515174148 train RMSE 1.4806238814461075 Validation RMSE 1750 : 1.4641061187497095 max of weights 6.326814252199435\n",
      "Iteration 1760 loss 0.8091604008697624 train RMSE 1.6648105413237282 Validation RMSE 1760 : 1.647082339736992 max of weights 6.392154458109358\n",
      "Iteration 1770 loss 0.7133742769630053 train RMSE 1.5621641821502827 Validation RMSE 1770 : 1.5464725532897614 max of weights 6.4292323385583\n",
      "Iteration 1780 loss 0.7353664281190659 train RMSE 1.5862850911237814 Validation RMSE 1780 : 1.5675407721102195 max of weights 6.451482027413305\n",
      "Iteration 1790 loss 0.7783224063062859 train RMSE 1.6323939323139762 Validation RMSE 1790 : 1.6155181705523656 max of weights 6.430166120190599\n",
      "Iteration 1800 loss 0.747746150563589 train RMSE 1.5996727490473417 Validation RMSE 1800 : 1.5791240606299408 max of weights 6.430755649002547\n",
      "Iteration 1810 loss 0.718435128500916 train RMSE 1.5676069682844929 Validation RMSE 1810 : 1.5469769344005133 max of weights 6.4179332814700425\n",
      "Iteration 1820 loss 0.6368891398013024 train RMSE 1.4749355071791876 Validation RMSE 1820 : 1.4520083830160877 max of weights 6.471280056543336\n",
      "Iteration 1830 loss 0.8680418226302938 train RMSE 1.7246293324735182 Validation RMSE 1830 : 1.7096264618959023 max of weights 6.480882086740405\n",
      "Iteration 1840 loss 0.7634495439469521 train RMSE 1.6163651269634949 Validation RMSE 1840 : 1.6023712767038694 max of weights 6.471198621422531\n",
      "Iteration 1850 loss 0.7987221467943595 train RMSE 1.653599079938653 Validation RMSE 1850 : 1.6409747165982589 max of weights 6.494777572473854\n",
      "Iteration 1860 loss 0.7368595508003833 train RMSE 1.5875670992243645 Validation RMSE 1860 : 1.5767225947031365 max of weights 6.442726690648365\n",
      "Iteration 1870 loss 0.7224002752319668 train RMSE 1.5717198421875247 Validation RMSE 1870 : 1.564486131841445 max of weights 6.392670574068456\n",
      "Iteration 1880 loss 0.7504629433252121 train RMSE 1.6022504213641415 Validation RMSE 1880 : 1.5857200119661026 max of weights 6.461608179233266\n",
      "Iteration 1890 loss 0.7245601791087309 train RMSE 1.5740032322574424 Validation RMSE 1890 : 1.554764056808881 max of weights 6.444126021921257\n",
      "Iteration 1900 loss 0.7455871964599857 train RMSE 1.5968845720022775 Validation RMSE 1900 : 1.5843739782179527 max of weights 6.54996743594407\n",
      "Iteration 1910 loss 0.6649470307878507 train RMSE 1.5070001263698425 Validation RMSE 1910 : 1.4886975196469372 max of weights 6.541271315535192\n",
      "Iteration 1920 loss 0.7692986764602812 train RMSE 1.6222504829198627 Validation RMSE 1920 : 1.6019907369404536 max of weights 6.526229567469273\n",
      "Iteration 1930 loss 0.7192206519210641 train RMSE 1.5679886874884978 Validation RMSE 1930 : 1.5535415776483765 max of weights 6.618764366704899\n",
      "Iteration 1940 loss 0.6630450943075958 train RMSE 1.5047664730685137 Validation RMSE 1940 : 1.4873820723303692 max of weights 6.623338419051403\n",
      "Iteration 1950 loss 0.7034283854842037 train RMSE 1.550432877925396 Validation RMSE 1950 : 1.5306491811482996 max of weights 6.559875073049248\n",
      "Iteration 1960 loss 0.7876262249263365 train RMSE 1.6416013649470798 Validation RMSE 1960 : 1.6255062841929613 max of weights 6.565191041109574\n",
      "Iteration 1970 loss 0.7367538113076523 train RMSE 1.5870995712110008 Validation RMSE 1970 : 1.5682473348558177 max of weights 6.609540251792693\n",
      "Iteration 1980 loss 0.7189189161186191 train RMSE 1.5675391216393957 Validation RMSE 1980 : 1.546127475157073 max of weights 6.708844753902238\n",
      "Iteration 1990 loss 0.7305860826864575 train RMSE 1.580334659278528 Validation RMSE 1990 : 1.5602789839302054 max of weights 6.749961063889619\n",
      "Iteration 2000 loss 0.7693693037768012 train RMSE 1.622139418088102 Validation RMSE 2000 : 1.5971730696542619 max of weights 6.748346297016359\n",
      "Iteration 2010 loss 0.6924226031498129 train RMSE 1.5379218621322803 Validation RMSE 2010 : 1.517815112437962 max of weights 6.770441228055528\n",
      "Iteration 2020 loss 0.7011540070900237 train RMSE 1.5476387718439453 Validation RMSE 2020 : 1.52762431272829 max of weights 6.7244878315748995\n",
      "Iteration 2030 loss 0.7568067477610502 train RMSE 1.6085512886661066 Validation RMSE 2030 : 1.5977511011908745 max of weights 6.729655877244486\n",
      "Iteration 2040 loss 0.808040743137983 train RMSE 1.6626678597330873 Validation RMSE 2040 : 1.6529524974996732 max of weights 6.778591521041374\n",
      "Iteration 2050 loss 0.8251451776350132 train RMSE 1.6803047806685765 Validation RMSE 2050 : 1.6703551111955672 max of weights 6.776201783187159\n",
      "Iteration 2060 loss 0.6061890876340315 train RMSE 1.4374864078242928 Validation RMSE 2060 : 1.4205310917422183 max of weights 6.848132327734078\n",
      "Iteration 2070 loss 0.7930669742935486 train RMSE 1.646903249816628 Validation RMSE 2070 : 1.6334766646452814 max of weights 6.816516331950867\n",
      "Iteration 2080 loss 0.7447583321962656 train RMSE 1.5953590597904326 Validation RMSE 2080 : 1.5807619932572068 max of weights 6.84994852292519\n",
      "Iteration 2090 loss 0.7255517812548921 train RMSE 1.5743405839362117 Validation RMSE 2090 : 1.5637340002392761 max of weights 6.915846051790418\n",
      "Iteration 2100 loss 0.7164557168744099 train RMSE 1.5642809337161188 Validation RMSE 2100 : 1.5488563458340725 max of weights 6.909543432054592\n",
      "Iteration 2110 loss 0.7110788798318566 train RMSE 1.558288463244637 Validation RMSE 2110 : 1.5350713101790623 max of weights 6.907328996516485\n",
      "Iteration 2120 loss 0.7113258388453474 train RMSE 1.5585380937805589 Validation RMSE 2120 : 1.539702164920109 max of weights 6.956750205510057\n",
      "Iteration 2130 loss 0.6832217634441051 train RMSE 1.527028284927285 Validation RMSE 2130 : 1.5128485763784538 max of weights 6.915944226693604\n",
      "Iteration 2140 loss 0.7251383202463854 train RMSE 1.5737415099888739 Validation RMSE 2140 : 1.5530227160007013 max of weights 6.940672561062538\n",
      "Iteration 2150 loss 0.7907873561404302 train RMSE 1.6442415603410525 Validation RMSE 2150 : 1.6254382134822383 max of weights 6.872370463544085\n",
      "Iteration 2160 loss 0.7307955732688444 train RMSE 1.5798850119806445 Validation RMSE 2160 : 1.5624259938744838 max of weights 6.943617003995933\n",
      "Iteration 2170 loss 0.7215116542263211 train RMSE 1.5696669767350158 Validation RMSE 2170 : 1.5474656758615415 max of weights 6.9472996722456735\n",
      "Iteration 2180 loss 0.7428358698651655 train RMSE 1.5929493213730037 Validation RMSE 2180 : 1.5754568654928935 max of weights 6.999572638492111\n",
      "Iteration 2190 loss 0.7612935249363221 train RMSE 1.6128294456568553 Validation RMSE 2190 : 1.58906251659675 max of weights 7.031603303559078\n",
      "Iteration 2200 loss 0.6924635557629152 train RMSE 1.5372279450797992 Validation RMSE 2200 : 1.5058861655146756 max of weights 7.034583898581697\n",
      "Iteration 2210 loss 0.6977709868541302 train RMSE 1.543142827496208 Validation RMSE 2210 : 1.5146919474472258 max of weights 7.025361905314786\n",
      "Iteration 2220 loss 0.7911246506779975 train RMSE 1.6443273922212545 Validation RMSE 2220 : 1.6246742690422642 max of weights 7.0579730561511775\n",
      "Iteration 2230 loss 0.721201951132636 train RMSE 1.5690779706361522 Validation RMSE 2230 : 1.5557548414836444 max of weights 7.081683591405226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2240 loss 0.7800294818819324 train RMSE 1.6325409769574042 Validation RMSE 2240 : 1.6181995010572763 max of weights 7.072494604508286\n",
      "Iteration 2250 loss 0.738867919949835 train RMSE 1.5882948493479627 Validation RMSE 2250 : 1.5727024272219554 max of weights 7.1599879219775\n",
      "Iteration 2260 loss 0.7534846576062167 train RMSE 1.6040851603303183 Validation RMSE 2260 : 1.5908270426594806 max of weights 7.160947480739447\n",
      "Iteration 2270 loss 0.7649524030975133 train RMSE 1.616346934156678 Validation RMSE 2270 : 1.598639334027432 max of weights 7.200135173174307\n",
      "Iteration 2280 loss 0.6909149291035113 train RMSE 1.5350541800730086 Validation RMSE 2280 : 1.5098452283440598 max of weights 7.161224721919244\n",
      "Iteration 2290 loss 0.7554551510194911 train RMSE 1.606055706354382 Validation RMSE 2290 : 1.5883242378507232 max of weights 7.225761334196772\n",
      "Iteration 2300 loss 0.7448758680143666 train RMSE 1.594552880375717 Validation RMSE 2300 : 1.566577270242524 max of weights 7.241305186617356\n",
      "Iteration 2310 loss 0.7269233544617141 train RMSE 1.5749423863464176 Validation RMSE 2310 : 1.549858995367023 max of weights 7.306226869257874\n",
      "Iteration 2320 loss 0.6759982648744803 train RMSE 1.518024979162729 Validation RMSE 2320 : 1.498502767746985 max of weights 7.273490172502556\n",
      "Iteration 2330 loss 0.6319060698480335 train RMSE 1.4669294571627975 Validation RMSE 2330 : 1.4436960521009796 max of weights 7.257514755700951\n",
      "Iteration 2340 loss 0.7722241111903823 train RMSE 1.6238567673276803 Validation RMSE 2340 : 1.6031433349867155 max of weights 7.261755531904824\n",
      "Iteration 2350 loss 0.696363276700112 train RMSE 1.5409771050684613 Validation RMSE 2350 : 1.5141529941247034 max of weights 7.334067821420009\n",
      "Iteration 2360 loss 0.7887900058812946 train RMSE 1.641334404628648 Validation RMSE 2360 : 1.6191806538532487 max of weights 7.361582240442122\n",
      "Iteration 2370 loss 0.7431588334783369 train RMSE 1.5925264178874972 Validation RMSE 2370 : 1.5737579140387914 max of weights 7.354652586543134\n",
      "Iteration 2380 loss 0.7371905792734755 train RMSE 1.5860254457626188 Validation RMSE 2380 : 1.558055899437754 max of weights 7.418855284008941\n",
      "Iteration 2390 loss 0.765948710673737 train RMSE 1.6169974365313826 Validation RMSE 2390 : 1.591881911212902 max of weights 7.416143152420387\n",
      "Iteration 2400 loss 0.6634166057282234 train RMSE 1.5033839144168097 Validation RMSE 2400 : 1.4797718484304079 max of weights 7.390860500367633\n",
      "Iteration 2410 loss 0.7448387114063485 train RMSE 1.5941731401576298 Validation RMSE 2410 : 1.5739888019168224 max of weights 7.456117841822522\n",
      "Iteration 2420 loss 0.7152291806920805 train RMSE 1.5616994338706418 Validation RMSE 2420 : 1.5486261455403971 max of weights 7.487127002286485\n",
      "Iteration 2430 loss 0.8051897577998408 train RMSE 1.6582272842290116 Validation RMSE 2430 : 1.6436266276757947 max of weights 7.474642852730942\n",
      "Iteration 2440 loss 0.7401649410858538 train RMSE 1.5889461078880713 Validation RMSE 2440 : 1.5727804756788237 max of weights 7.501717681946068\n",
      "Iteration 2450 loss 0.6657274170571991 train RMSE 1.5057726223838825 Validation RMSE 2450 : 1.4964357553054217 max of weights 7.51654889194941\n",
      "Iteration 2460 loss 0.7494358831079245 train RMSE 1.5989289239337177 Validation RMSE 2460 : 1.581989190254176 max of weights 7.530882975441263\n",
      "Iteration 2470 loss 0.7634438907217224 train RMSE 1.6139230849529895 Validation RMSE 2470 : 1.5937870360406914 max of weights 7.548621021303482\n",
      "Iteration 2480 loss 0.7087498961665543 train RMSE 1.5541882159558298 Validation RMSE 2480 : 1.5359931806860174 max of weights 7.5718252038660445\n",
      "Iteration 2490 loss 0.6974859738101061 train RMSE 1.5415719885941377 Validation RMSE 2490 : 1.5207600575437448 max of weights 7.6117326858033625\n",
      "Iteration 2500 loss 0.677447177892885 train RMSE 1.5188840643382178 Validation RMSE 2500 : 1.486298397965558 max of weights 7.640484412657847\n",
      "Iteration 2510 loss 0.730705593810602 train RMSE 1.5783085629611198 Validation RMSE 2510 : 1.5550653964633843 max of weights 7.65366525993366\n",
      "Iteration 2520 loss 0.6831745686735764 train RMSE 1.5253298316921013 Validation RMSE 2520 : 1.5104552704972833 max of weights 7.711646735831087\n",
      "Iteration 2530 loss 0.6751135022624171 train RMSE 1.5161238855362702 Validation RMSE 2530 : 1.4911915261468938 max of weights 7.713249193045943\n",
      "Iteration 2540 loss 0.7267589505185951 train RMSE 1.5738859430763814 Validation RMSE 2540 : 1.5480488718818224 max of weights 7.717588784204851\n",
      "Iteration 2550 loss 0.7091532679288861 train RMSE 1.5543816145359866 Validation RMSE 2550 : 1.5349844059598354 max of weights 7.730277870224116\n",
      "Iteration 2560 loss 0.7074582530738547 train RMSE 1.5524473955475924 Validation RMSE 2560 : 1.5320118375798422 max of weights 7.7959008057698735\n",
      "Iteration 2570 loss 0.7461845388637282 train RMSE 1.594915133335954 Validation RMSE 2570 : 1.5716806034814699 max of weights 7.82254943119265\n",
      "Iteration 2580 loss 0.7349831882708583 train RMSE 1.5826961217143498 Validation RMSE 2580 : 1.5546833921725567 max of weights 7.837581743680562\n",
      "Iteration 2590 loss 0.7527988625011752 train RMSE 1.601975333338776 Validation RMSE 2590 : 1.5755752971973762 max of weights 7.877629446549859\n",
      "Iteration 2600 loss 0.66685520518982 train RMSE 1.506341664211314 Validation RMSE 2600 : 1.4757770124435405 max of weights 7.918004051497487\n",
      "Iteration 2610 loss 0.7025567110470758 train RMSE 1.5466781240149723 Validation RMSE 2610 : 1.5234119305995304 max of weights 7.903463544004538\n",
      "Iteration 2620 loss 0.754795144807813 train RMSE 1.60397813004559 Validation RMSE 2620 : 1.5875214052412012 max of weights 7.958476353251859\n",
      "Iteration 2630 loss 0.7892247668504854 train RMSE 1.6405674500808494 Validation RMSE 2630 : 1.6255813890458255 max of weights 7.923153884076285\n",
      "Iteration 2640 loss 0.6913354881451834 train RMSE 1.533907030044332 Validation RMSE 2640 : 1.517420247200065 max of weights 7.950247054101049\n",
      "Iteration 2650 loss 0.7637318537989531 train RMSE 1.6133633990857892 Validation RMSE 2650 : 1.6008607611666474 max of weights 7.947544591987491\n",
      "Iteration 2660 loss 0.7134942064857765 train RMSE 1.5585693669579486 Validation RMSE 2660 : 1.5399043038644804 max of weights 7.973469796959708\n",
      "Iteration 2670 loss 0.6914252529775561 train RMSE 1.5338119235143006 Validation RMSE 2670 : 1.5198380086561185 max of weights 8.006364874294587\n",
      "Iteration 2680 loss 0.7465218915888305 train RMSE 1.5946732738482639 Validation RMSE 2680 : 1.577531214000368 max of weights 8.042498576654419\n",
      "Iteration 2690 loss 0.7256016531472428 train RMSE 1.5717744383639196 Validation RMSE 2690 : 1.5447384203141923 max of weights 8.041534379437723\n",
      "Iteration 2700 loss 0.7051302114219035 train RMSE 1.5490756436080764 Validation RMSE 2700 : 1.526231248713945 max of weights 8.081717020024934\n",
      "Iteration 2710 loss 0.6509546197315584 train RMSE 1.4874070595175712 Validation RMSE 2710 : 1.4691204985561717 max of weights 8.045447486623443\n",
      "Iteration 2720 loss 0.629105117120214 train RMSE 1.4617839724428432 Validation RMSE 2720 : 1.4388477855489639 max of weights 8.100652388519253\n",
      "Iteration 2730 loss 0.7997058540750857 train RMSE 1.6512267767749806 Validation RMSE 2730 : 1.6299538658683068 max of weights 8.03252361488118\n",
      "Iteration 2740 loss 0.7014630723858949 train RMSE 1.544906752619962 Validation RMSE 2740 : 1.52216356824878 max of weights 8.122387616696278\n",
      "Iteration 2750 loss 0.7272642115087151 train RMSE 1.573427355908657 Validation RMSE 2750 : 1.552287162642102 max of weights 8.160892041025154\n",
      "Iteration 2760 loss 0.7612111718753851 train RMSE 1.6102344817516274 Validation RMSE 2760 : 1.5903842075459729 max of weights 8.21051134989547\n",
      "Iteration 2770 loss 0.7295074939658327 train RMSE 1.5758043754307023 Validation RMSE 2770 : 1.5510371197702664 max of weights 8.272225511268353\n",
      "Iteration 2780 loss 0.7085569582564053 train RMSE 1.5525738609646185 Validation RMSE 2780 : 1.5247084512325766 max of weights 8.294568829642248\n",
      "Iteration 2790 loss 0.6278350838913355 train RMSE 1.4599241933276625 Validation RMSE 2790 : 1.4321382294299259 max of weights 8.273031286441142\n",
      "Iteration 2800 loss 0.8312233303821377 train RMSE 1.6835164853023097 Validation RMSE 2800 : 1.6621810871046014 max of weights 8.345702465443154\n",
      "Iteration 2810 loss 0.7440388213033082 train RMSE 1.5914567863598077 Validation RMSE 2810 : 1.577343408884535 max of weights 8.366766830040213\n",
      "Iteration 2820 loss 0.7850247448261493 train RMSE 1.6352933974587447 Validation RMSE 2820 : 1.622702705417716 max of weights 8.333287507489988\n",
      "Iteration 2830 loss 0.7356937863145612 train RMSE 1.5822255284866105 Validation RMSE 2830 : 1.5691578387039593 max of weights 8.35568045315143\n",
      "Iteration 2840 loss 0.7021910811516036 train RMSE 1.5451364150025877 Validation RMSE 2840 : 1.534755635293475 max of weights 8.356345160909479\n",
      "Iteration 2850 loss 0.7170011600955688 train RMSE 1.5615667204470185 Validation RMSE 2850 : 1.545960880644207 max of weights 8.39177543121721\n",
      "Iteration 2860 loss 0.7040975386773946 train RMSE 1.5471475533537316 Validation RMSE 2860 : 1.5289105801842007 max of weights 8.354435382678053\n",
      "Iteration 2870 loss 0.7236605328158566 train RMSE 1.5688049816063196 Validation RMSE 2870 : 1.554381365571518 max of weights 8.394353108502424\n",
      "Iteration 2880 loss 0.6522132850020919 train RMSE 1.4879546493945917 Validation RMSE 2880 : 1.465043328400233 max of weights 8.393580083649743\n",
      "Iteration 2890 loss 0.7492164496578567 train RMSE 1.5966251662529012 Validation RMSE 2890 : 1.5721997935309717 max of weights 8.463312988082805\n",
      "Iteration 2900 loss 0.7030683310416669 train RMSE 1.5458627174471062 Validation RMSE 2900 : 1.526978267007661 max of weights 8.43067268162682\n",
      "Iteration 2910 loss 0.6448182567525251 train RMSE 1.4792814590926953 Validation RMSE 2910 : 1.4579612214992204 max of weights 8.459405721495493\n",
      "Iteration 2920 loss 0.6862291548282337 train RMSE 1.526904342615341 Validation RMSE 2920 : 1.5023289467623668 max of weights 8.481794080593342\n",
      "Iteration 2930 loss 0.7815489543139265 train RMSE 1.6311972552083498 Validation RMSE 2930 : 1.6045064365977573 max of weights 8.432826273865478\n",
      "Iteration 2940 loss 0.7235695203513328 train RMSE 1.5685033287620564 Validation RMSE 2940 : 1.5425658330033398 max of weights 8.436320071607224\n",
      "Iteration 2950 loss 0.695686188140211 train RMSE 1.537428924522283 Validation RMSE 2950 : 1.5094931587986498 max of weights 8.463133559993407\n",
      "Iteration 2960 loss 0.7066593326124803 train RMSE 1.5496757251309514 Validation RMSE 2960 : 1.522655160115733 max of weights 8.501590020966132\n",
      "Iteration 2970 loss 0.7544535132266337 train RMSE 1.6020096976817393 Validation RMSE 2970 : 1.5743654195901815 max of weights 8.515592381511063\n",
      "Iteration 2980 loss 0.6938794628538522 train RMSE 1.535238921151672 Validation RMSE 2980 : 1.5086030139942712 max of weights 8.5460149765199\n",
      "Iteration 2990 loss 0.6817247395315762 train RMSE 1.5214379130817677 Validation RMSE 2990 : 1.4950601826683416 max of weights 8.61472315976087\n",
      "Iteration 3000 loss 0.7245559769481548 train RMSE 1.5692761127404715 Validation RMSE 3000 : 1.5520153130798502 max of weights 8.61546340272985\n",
      "Iteration 3010 loss 0.7862352710189725 train RMSE 1.6357655491117475 Validation RMSE 3010 : 1.6208739687249665 max of weights 8.624517672377937\n",
      "Iteration 3020 loss 0.8022565449745348 train RMSE 1.6525302345644015 Validation RMSE 3020 : 1.6360722126557197 max of weights 8.620527114276824\n",
      "Iteration 3030 loss 0.5982828651436365 train RMSE 1.423218452790998 Validation RMSE 3030 : 1.4040953356817243 max of weights 8.627246068137136\n",
      "Iteration 3040 loss 0.7795623930819863 train RMSE 1.6285394313373118 Validation RMSE 3040 : 1.6127806466890728 max of weights 8.620358800227244\n",
      "Iteration 3050 loss 0.7205855489713966 train RMSE 1.564632756950496 Validation RMSE 3050 : 1.5481966646958687 max of weights 8.648870283901067\n",
      "Iteration 3060 loss 0.7072266529009229 train RMSE 1.5497409103837898 Validation RMSE 3060 : 1.5388353790792189 max of weights 8.684023116821114\n",
      "Iteration 3070 loss 0.7057244956530933 train RMSE 1.5480223364882582 Validation RMSE 3070 : 1.530168255737311 max of weights 8.679923024278128\n",
      "Iteration 3080 loss 0.7004126945190112 train RMSE 1.5420423456395702 Validation RMSE 3080 : 1.5206507870411954 max of weights 8.687237254743826\n",
      "Iteration 3090 loss 0.7020133660643875 train RMSE 1.5438293427687761 Validation RMSE 3090 : 1.5214636459728963 max of weights 8.768488097144056\n",
      "Iteration 3100 loss 0.6734569126440841 train RMSE 1.5115009661476866 Validation RMSE 3100 : 1.49563526691337 max of weights 8.725464934614523\n",
      "Iteration 3110 loss 0.7042327085609652 train RMSE 1.5462780686012898 Validation RMSE 3110 : 1.5240738001398746 max of weights 8.793797524281299\n",
      "Iteration 3120 loss 0.7866506576294163 train RMSE 1.635802502514009 Validation RMSE 3120 : 1.612889484491121 max of weights 8.760244680806053\n",
      "Iteration 3130 loss 0.7165471387184018 train RMSE 1.5598964992523088 Validation RMSE 3130 : 1.537800898777034 max of weights 8.842611932000432\n",
      "Iteration 3140 loss 0.7129383717727497 train RMSE 1.5558311992517933 Validation RMSE 3140 : 1.5333422580522968 max of weights 8.85516754417915\n",
      "Iteration 3150 loss 0.7262888277987184 train RMSE 1.5705538583317016 Validation RMSE 3150 : 1.546734047239593 max of weights 8.939651553677493\n",
      "Iteration 3160 loss 0.7523587356015995 train RMSE 1.5989379631685026 Validation RMSE 3160 : 1.5717739484777198 max of weights 8.99127034006973\n",
      "Iteration 3170 loss 0.6878552637720411 train RMSE 1.5275522515831141 Validation RMSE 3170 : 1.4951769393617582 max of weights 9.018757184991715\n",
      "Iteration 3180 loss 0.6656264112360997 train RMSE 1.502165362865857 Validation RMSE 3180 : 1.4738215729113022 max of weights 8.971632829399594\n",
      "Iteration 3190 loss 0.7553657352544517 train RMSE 1.6020309602835967 Validation RMSE 3190 : 1.5823310901644274 max of weights 9.00443883852729\n",
      "Iteration 3200 loss 0.6962090328398075 train RMSE 1.5368486142329891 Validation RMSE 3200 : 1.5180452544391994 max of weights 8.978464143418567\n",
      "Iteration 3210 loss 0.7546081756814289 train RMSE 1.6011196739082505 Validation RMSE 3210 : 1.5819703282278943 max of weights 8.978345346292647\n",
      "Iteration 3220 loss 0.7260584353454665 train RMSE 1.56994131297575 Validation RMSE 3220 : 1.5517459620604843 max of weights 9.016444877529308\n",
      "Iteration 3230 loss 0.7331812219028189 train RMSE 1.5777227586033304 Validation RMSE 3230 : 1.5626518587950613 max of weights 9.008525963366445\n",
      "Iteration 3240 loss 0.7365025051107138 train RMSE 1.5813189477491536 Validation RMSE 3240 : 1.562527368046752 max of weights 9.061728407131609\n",
      "Iteration 3250 loss 0.6736058159846592 train RMSE 1.5109492575042462 Validation RMSE 3250 : 1.492867311925333 max of weights 9.020134949942113\n",
      "Iteration 3260 loss 0.73680815076073 train RMSE 1.58154053656799 Validation RMSE 3260 : 1.5658187508056403 max of weights 9.110871444922843\n",
      "Iteration 3270 loss 0.7198619850702964 train RMSE 1.5628535056069692 Validation RMSE 3270 : 1.5415513718879563 max of weights 9.09700731675225\n",
      "Iteration 3280 loss 0.7155696388056403 train RMSE 1.5580525616658831 Validation RMSE 3280 : 1.5375426916400705 max of weights 9.131899901594087\n",
      "Iteration 3290 loss 0.6729571010088646 train RMSE 1.510047539619217 Validation RMSE 3290 : 1.4920016676319265 max of weights 9.099659815424475\n",
      "Iteration 3300 loss 0.6223468273738324 train RMSE 1.4509286690516112 Validation RMSE 3300 : 1.4291410474033008 max of weights 9.132549188098846\n",
      "Iteration 3310 loss 0.7600498301059301 train RMSE 1.6066074533864354 Validation RMSE 3310 : 1.584507124236958 max of weights 9.158271055188674\n",
      "Iteration 3320 loss 0.6800217199229615 train RMSE 1.5180312351841798 Validation RMSE 3320 : 1.49050536481639 max of weights 9.266771625959572\n",
      "Iteration 3330 loss 0.7785088265186255 train RMSE 1.6262368481388523 Validation RMSE 3330 : 1.603137765847118 max of weights 9.2733490447541\n",
      "Iteration 3340 loss 0.7244900723486181 train RMSE 1.56771646229418 Validation RMSE 3340 : 1.543512019741417 max of weights 9.2784051798407\n",
      "Iteration 3350 loss 0.7197162971833542 train RMSE 1.5623955275430195 Validation RMSE 3350 : 1.5354125954021878 max of weights 9.316309891103506\n",
      "Iteration 3360 loss 0.7511783056063145 train RMSE 1.596748236714874 Validation RMSE 3360 : 1.5676917037540172 max of weights 9.38078189032609\n",
      "Iteration 3370 loss 0.663670681244204 train RMSE 1.499040018872496 Validation RMSE 3370 : 1.4721664167136848 max of weights 9.369506741203187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3380 loss 0.7257673668117092 train RMSE 1.5689404421101236 Validation RMSE 3380 : 1.5498984786688235 max of weights 9.419189490843175\n",
      "Iteration 3390 loss 0.6932872114310349 train RMSE 1.5327063356173576 Validation RMSE 3390 : 1.5193058978259149 max of weights 9.399465629493136\n",
      "Iteration 3400 loss 0.7742472371032477 train RMSE 1.6213689817805537 Validation RMSE 3400 : 1.6062760815797794 max of weights 9.343251188189551\n",
      "Iteration 3410 loss 0.7261645671849288 train RMSE 1.5692402074668002 Validation RMSE 3410 : 1.5549429888545385 max of weights 9.357922905128115\n",
      "Iteration 3420 loss 0.6589481367462365 train RMSE 1.4933455302175707 Validation RMSE 3420 : 1.4833502862549786 max of weights 9.349940304426859\n",
      "Iteration 3430 loss 0.7395644267630144 train RMSE 1.58387690796536 Validation RMSE 3430 : 1.5683580469412768 max of weights 9.376497156270782\n",
      "Iteration 3440 loss 0.7388363863801245 train RMSE 1.583028117364277 Validation RMSE 3440 : 1.566670675533332 max of weights 9.376408788379202\n",
      "Iteration 3450 loss 0.6921135834713302 train RMSE 1.5311218146798653 Validation RMSE 3450 : 1.515454691118191 max of weights 9.375011239654654\n",
      "Iteration 3460 loss 0.6904559020308196 train RMSE 1.5291640487487044 Validation RMSE 3460 : 1.513362918803299 max of weights 9.44297253482898\n",
      "Iteration 3470 loss 0.6585706833195387 train RMSE 1.4926392748100978 Validation RMSE 3470 : 1.470605127177047 max of weights 9.466371077040565\n",
      "Iteration 3480 loss 0.7226406191135549 train RMSE 1.5650786320079866 Validation RMSE 3480 : 1.5440360852123247 max of weights 9.43608051002755\n",
      "Iteration 3490 loss 0.6713957911622591 train RMSE 1.5073588584249982 Validation RMSE 3490 : 1.4908301097347758 max of weights 9.47343213033289\n",
      "Iteration 3500 loss 0.6593781617439524 train RMSE 1.493485954183674 Validation RMSE 3500 : 1.4663190611079968 max of weights 9.507689516177397\n",
      "Iteration 3510 loss 0.7190621119024172 train RMSE 1.5610342782199975 Validation RMSE 3510 : 1.5333464600183981 max of weights 9.5313329468553\n",
      "Iteration 3520 loss 0.7003148678786097 train RMSE 1.5400824495571401 Validation RMSE 3520 : 1.5144166185456032 max of weights 9.598017376257529\n",
      "Iteration 3530 loss 0.6953255336072551 train RMSE 1.5344172486616292 Validation RMSE 3530 : 1.5072765577188247 max of weights 9.699734276052364\n",
      "Iteration 3540 loss 0.7267476925570769 train RMSE 1.5693298146863013 Validation RMSE 3540 : 1.5420275854790133 max of weights 9.770897949777096\n",
      "Iteration 3550 loss 0.7276647837845867 train RMSE 1.5703050374403247 Validation RMSE 3550 : 1.5388608739667857 max of weights 9.809867367538136\n",
      "Iteration 3560 loss 0.7551355005387362 train RMSE 1.600197706838046 Validation RMSE 3560 : 1.5701772893569719 max of weights 9.841145362310284\n",
      "Iteration 3570 loss 0.6551428116451411 train RMSE 1.4882689223860335 Validation RMSE 3570 : 1.4566077348114046 max of weights 9.83351227184524\n",
      "Iteration 3580 loss 0.6869396008907953 train RMSE 1.524683093968057 Validation RMSE 3580 : 1.5040855360819523 max of weights 9.805483415883172\n",
      "Iteration 3590 loss 0.7497255081879777 train RMSE 1.5942690876241412 Validation RMSE 3590 : 1.5747054149095805 max of weights 9.80345114933014\n",
      "Iteration 3600 loss 0.7683563863577186 train RMSE 1.614264428509821 Validation RMSE 3600 : 1.5951688182305055 max of weights 9.806641131893649\n",
      "Iteration 3610 loss 0.6809565497584174 train RMSE 1.5177489596221245 Validation RMSE 3610 : 1.5011966051374048 max of weights 9.850132400127533\n",
      "Iteration 3620 loss 0.7601351257135849 train RMSE 1.6053575237357602 Validation RMSE 3620 : 1.5939474468669708 max of weights 9.847890865374955\n",
      "Iteration 3630 loss 0.7046296398092337 train RMSE 1.544416736754468 Validation RMSE 3630 : 1.530389302447843 max of weights 9.850641567028713\n",
      "Iteration 3640 loss 0.6780411299741538 train RMSE 1.5142836900039947 Validation RMSE 3640 : 1.50513058603577 max of weights 9.878871837682828\n",
      "Iteration 3650 loss 0.7309746989486946 train RMSE 1.5734796107881726 Validation RMSE 3650 : 1.5591585661437728 max of weights 9.916614263313512\n",
      "Iteration 3660 loss 0.7152760534993746 train RMSE 1.5560880261081835 Validation RMSE 3660 : 1.537634196691087 max of weights 9.920990443097368\n",
      "Iteration 3670 loss 0.6955696856469702 train RMSE 1.5340364045757087 Validation RMSE 3670 : 1.514526967147848 max of weights 9.886812526801851\n",
      "Iteration 3680 loss 0.6510904689864813 train RMSE 1.4830683641388684 Validation RMSE 3680 : 1.4648271676210014 max of weights 9.81876806524386\n",
      "Iteration 3690 loss 0.626406429701464 train RMSE 1.4539825557925956 Validation RMSE 3690 : 1.4295297774173397 max of weights 9.92479733156928\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0814652d0502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_network_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_neural_fingerprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_network_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plotting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-436a2f226490>\u001b[0m in \u001b[0;36mtrain_neural_fingerprint\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mnum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     predict_func, trained_weights, conv_training_curve =          train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n\u001b[0;32m---> 82\u001b[0;31m                  nn_train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mprint_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-436a2f226490>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, validation_smiles, validation_raw_targets)\u001b[0m\n\u001b[1;32m     41\u001b[0m     trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n\u001b[1;32m     42\u001b[0m                            \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learn_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                            b1=train_params['b1'], b2=train_params['b2'])\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_smiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/optimizers.pyc\u001b[0m in \u001b[0;36madam\u001b[0;34m(grad, x, callback, num_iters, step_size, b1, b2, eps)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m      \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m  \u001b[0;31m# First  moment estimate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m  \u001b[0;31m# Second moment estimate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-436a2f226490>\u001b[0m in \u001b[0;36mcallback\u001b[0;34m(weights, iter)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"max of weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mundo_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_smiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_smiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mtraining_curve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train RMSE\"\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_preds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_raw_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/build_vanilla_net.pyc\u001b[0m in \u001b[0;36mloss_fun\u001b[0;34m(weights, smiles, targets)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mfingerprint_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mfingerprints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfingerprint_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfingerprint_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mnet_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_loss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfingerprints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfingerprint_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfp_l2_penalty\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36moutput_layer_fun\u001b[0;34m(weights, smiles)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moutput_layer_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_layer_fun_and_atom_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36moutput_layer_fun_and_atom_activations\u001b[0;34m(weights, smiles)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mwrite_to_fingerprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             atom_features = update_layer(weights, layer, atom_features, bond_features, array_rep,\n\u001b[1;32m     94\u001b[0m                                          normalize=normalize)\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36mwrite_to_fingerprint\u001b[0;34m(atom_features, layer)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mcur_out_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'layer output weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mcur_out_bias\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'layer output bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0matom_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_out_bias\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_out_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0matom_activations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# Sum over all atoms within a moleclue:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/anaconda3/envs/neural_fingerprint_27/lib/python2.7/site-packages/autograd/tracer.pyc\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_network_weights = train_neural_fingerprint()\n",
    "with open('results.pkl', 'w') as f:\n",
    "    pickle.dump(trained_network_weights, f)\n",
    "\n",
    "# Plotting.\n",
    "with open('results.pkl') as f:\n",
    "    trained_weights = pickle.load(f)\n",
    "plot(trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {'target_name' : 'logP',\n",
    "               'data_file'   : 'logp_mean'}\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "normalize = 1\n",
    "dropout = 0\n",
    "activation = relu\n",
    "params = {'fp_length': 50,\n",
    "            'fp_depth': 4,\n",
    "            'init_scale':np.exp(-4),\n",
    "            'learn_rate':np.exp(-4),\n",
    "                    'b1':np.exp(-4),\n",
    "                    'b2':np.exp(-4),\n",
    "            'l2_penalty':np.exp(-4),\n",
    "            'l1_penalty':np.exp(-5),\n",
    "            'conv_width':20}\n",
    "\n",
    "conv_layer_sizes = [params['conv_width']] * params['fp_depth']\n",
    "conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                    'fp_length' : params['fp_length'],\n",
    "                    'normalize' : normalize,\n",
    "                    'return_atom_activations':False}\n",
    "\n",
    "all_radii = range(params['fp_depth'] + 1)\n",
    "\n",
    "# Plotting parameters\n",
    "num_figs_per_fp = 11\n",
    "figsize = (100, 100)\n",
    "highlight_color = (30.0/255.0, 100.0/255.0, 255.0/255.0)  # A nice light blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Regression on 9629 training points.\n",
      "--------------------------------------------------------------------------------\n",
      "Mean predictor\n",
      "\n",
      "Performance (RMSE) on logP:\n",
      "Train: 1.859267165016053\n",
      "Test:  1.8550727952633121\n",
      "--------------------------------------------------------------------------------\n",
      "Task params {'learn_rate': 0.01831563888873418, 'fp_depth': 4, 'b1': 0.01831563888873418, 'b2': 0.01831563888873418, 'init_scale': 0.01831563888873418, 'fp_length': 50, 'l2_penalty': 0.01831563888873418, 'l1_penalty': 0.006737946999085467, 'conv_width': 20}\n",
      "Convnet fingerprints with neural net\n",
      "Total number of weights in the network: 27441\n",
      "max of weights 0.08144291040373952\n",
      "Iteration 0 loss 0.9841043233938501 train RMSE 1.844335332789677 Validation RMSE 0 : 1.8476555767592138 max of weights 0.2205990397952285\n",
      "Iteration 10 loss 2.0191148807011183 train RMSE 2.641851518701157 Validation RMSE 10 : 2.602044866427725 max of weights 0.3992600263094507\n",
      "Iteration 20 loss 1.9018455863735966 train RMSE 2.5639679974276675 Validation RMSE 20 : 2.5260111541113273 max of weights 0.5482411144110912\n",
      "Iteration 30 loss 1.717793334401654 train RMSE 2.436714490663774 Validation RMSE 30 : 2.4013050757781347 max of weights 0.6928924409104861\n",
      "Iteration 40 loss 1.642917538843514 train RMSE 2.382980074839209 Validation RMSE 40 : 2.3506126965362095 max of weights 0.8563865112221112\n",
      "Iteration 50 loss 1.6501539379779233 train RMSE 2.3881845310722207 Validation RMSE 50 : 2.355229783317962 max of weights 1.0225259594288338\n",
      "Iteration 60 loss 1.674509998039239 train RMSE 2.4057172041788473 Validation RMSE 60 : 2.370859481585436 max of weights 1.1967779582895384\n",
      "Iteration 70 loss 1.5899268450884718 train RMSE 2.3441274082554697 Validation RMSE 70 : 2.310184769244144 max of weights 1.345307096606175\n",
      "Iteration 80 loss 1.5428954439930118 train RMSE 2.3091648145892174 Validation RMSE 80 : 2.274148528099177 max of weights 1.4327835576790464\n",
      "Iteration 90 loss 1.4870815936013562 train RMSE 2.2669634059197605 Validation RMSE 90 : 2.2370233106444433 max of weights 1.6043234546434528\n",
      "Iteration 100 loss 1.6580388853624546 train RMSE 2.3937264715853326 Validation RMSE 100 : 2.3587905838192773 max of weights 1.6479689244143487\n",
      "Iteration 110 loss 1.7101756157996488 train RMSE 2.431051070528348 Validation RMSE 110 : 2.3918467198777793 max of weights 1.7788290574893733\n",
      "Iteration 120 loss 1.316751242163997 train RMSE 2.133043284033848 Validation RMSE 120 : 2.097767306139857 max of weights 1.9231112028381638\n",
      "Iteration 130 loss 1.519342957813746 train RMSE 2.291296814495957 Validation RMSE 130 : 2.2547237869769843 max of weights 2.0342808002195096\n",
      "Iteration 140 loss 1.4257043733247907 train RMSE 2.2195112486913198 Validation RMSE 140 : 2.1863827840334262 max of weights 2.1778545432426664\n",
      "Iteration 150 loss 1.3973198117032248 train RMSE 2.1972528942353495 Validation RMSE 150 : 2.163952585127524 max of weights 2.270700438539921\n",
      "Iteration 160 loss 1.3724133532186344 train RMSE 2.1775385626862502 Validation RMSE 160 : 2.1416142183559113 max of weights 2.3796198526036174\n",
      "Iteration 170 loss 1.3558004878586651 train RMSE 2.164271738922503 Validation RMSE 170 : 2.1227044026369266 max of weights 2.4507123452189767\n",
      "Iteration 180 loss 1.3328354800048363 train RMSE 2.1458263203991277 Validation RMSE 180 : 2.112199803377744 max of weights 2.554594079845559\n",
      "Iteration 190 loss 1.3295226911668736 train RMSE 2.143113969041143 Validation RMSE 190 : 2.09904953132352 max of weights 2.6641481168780863\n",
      "Iteration 200 loss 1.3187237823687774 train RMSE 2.1343692625747406 Validation RMSE 200 : 2.0955536461449684 max of weights 2.637519367936404\n",
      "Iteration 210 loss 1.4508577938659528 train RMSE 2.2387827082662657 Validation RMSE 210 : 2.1990976368565884 max of weights 2.704727918938556\n",
      "Iteration 220 loss 1.3058283698882975 train RMSE 2.123826718008942 Validation RMSE 220 : 2.0865030163730265 max of weights 2.8614480602416865\n",
      "Iteration 230 loss 1.2587047301144394 train RMSE 2.0850945101896663 Validation RMSE 230 : 2.0417137774853242 max of weights 3.0146444919390336\n",
      "Iteration 240 loss 1.305365366001858 train RMSE 2.1233800178458813 Validation RMSE 240 : 2.081904332595681 max of weights 3.090537287849174\n",
      "Iteration 250 loss 1.3242085164080415 train RMSE 2.1386346402858245 Validation RMSE 250 : 2.0922106471324615 max of weights 3.2197325972668747\n",
      "Iteration 260 loss 1.2462435895291208 train RMSE 2.0746279440005124 Validation RMSE 260 : 2.029593584823916 max of weights 3.33504278584941\n",
      "Iteration 270 loss 1.1725230939653208 train RMSE 2.012259283872814 Validation RMSE 270 : 1.9693660615833604 max of weights 3.48881200760832\n",
      "Iteration 280 loss 1.201479557282561 train RMSE 2.036945094485804 Validation RMSE 280 : 1.994953227242005 max of weights 3.5388909849501977\n",
      "Iteration 290 loss 1.1734395618500566 train RMSE 2.0129931364557607 Validation RMSE 290 : 1.9728068766299693 max of weights 3.606923718530124\n",
      "Iteration 300 loss 1.2845471783199838 train RMSE 2.1061998079228186 Validation RMSE 300 : 2.0600952948691114 max of weights 3.7473515478552555\n",
      "Iteration 310 loss 1.2184156866149454 train RMSE 2.0511679463575714 Validation RMSE 310 : 2.0114378290001036 max of weights 3.889773381324491\n",
      "Iteration 320 loss 1.224467818808392 train RMSE 2.0562340621854536 Validation RMSE 320 : 2.0180775045388515 max of weights 3.981989385618631\n",
      "Iteration 330 loss 1.2443354883203035 train RMSE 2.072845810942249 Validation RMSE 330 : 2.033736926542519 max of weights 4.12671663651713\n",
      "Iteration 340 loss 1.1289100070636562 train RMSE 1.9742215579869506 Validation RMSE 340 : 1.9343540345079395 max of weights 4.3056360616724225\n",
      "Iteration 350 loss 1.211682215242469 train RMSE 2.045368414305021 Validation RMSE 350 : 2.0055222809543007 max of weights 4.410680714734691\n",
      "Iteration 360 loss 1.21283846017886 train RMSE 2.0463115518598394 Validation RMSE 360 : 2.0066138961734827 max of weights 4.4955675766875025\n",
      "Iteration 370 loss 1.138249547487306 train RMSE 1.9822866973680369 Validation RMSE 370 : 1.9399079076214485 max of weights 4.541367874821345\n",
      "Iteration 380 loss 1.1104965732802614 train RMSE 1.957912159919342 Validation RMSE 380 : 1.915358257806364 max of weights 4.611346889972701\n",
      "Iteration 390 loss 1.029288431073319 train RMSE 1.8848432017684758 Validation RMSE 390 : 1.8415924955773846 max of weights 4.648759035549214\n",
      "Iteration 400 loss 1.2324946094796287 train RMSE 2.0627531551669347 Validation RMSE 400 : 2.022341276904195 max of weights 4.688914630761434\n",
      "Iteration 410 loss 1.137847628900714 train RMSE 1.981838857824077 Validation RMSE 410 : 1.9358401577350708 max of weights 4.77061660327651\n",
      "Iteration 420 loss 1.2199904111103619 train RMSE 2.052200246179682 Validation RMSE 420 : 2.0099568877339045 max of weights 4.80529669986739\n",
      "Iteration 430 loss 1.2368373742940173 train RMSE 2.066297179580353 Validation RMSE 430 : 2.0156968072438763 max of weights 4.855035056486319\n",
      "Iteration 440 loss 1.1689589301251164 train RMSE 2.00869006282821 Validation RMSE 440 : 1.966210428052783 max of weights 4.964805029768941\n",
      "Iteration 450 loss 1.1975152182428177 train RMSE 2.033083575714221 Validation RMSE 450 : 1.9901489104550265 max of weights 5.052784719315785\n",
      "Iteration 460 loss 1.075593624706086 train RMSE 1.926630127513264 Validation RMSE 460 : 1.8829560204743168 max of weights 5.118404158600784\n",
      "Iteration 470 loss 1.1117025961260891 train RMSE 1.9587268431982712 Validation RMSE 470 : 1.914971779504515 max of weights 5.191560020459487\n",
      "Iteration 480 loss 1.1129338636182506 train RMSE 1.9597890633474333 Validation RMSE 480 : 1.9259066908915627 max of weights 5.201152692436829\n",
      "Iteration 490 loss 1.2176687383663973 train RMSE 2.0500464016899826 Validation RMSE 490 : 2.009009067135424 max of weights 5.318184416108175\n",
      "Iteration 500 loss 1.1378516589423686 train RMSE 1.9815765869863107 Validation RMSE 500 : 1.9404905711178775 max of weights 5.428357219995966\n",
      "Iteration 510 loss 1.0214483440953028 train RMSE 1.8772589053466138 Validation RMSE 510 : 1.8406517055442424 max of weights 5.54263597776056\n",
      "Iteration 520 loss 1.1556715319699604 train RMSE 1.9969886616507164 Validation RMSE 520 : 1.9584706995096162 max of weights 5.587143299082605\n",
      "Iteration 530 loss 1.1845167043828868 train RMSE 2.0217691483318685 Validation RMSE 530 : 1.9787606651667948 max of weights 5.6494214426989835\n",
      "Iteration 540 loss 1.075807131621563 train RMSE 1.9265632259187215 Validation RMSE 540 : 1.8877891804032059 max of weights 5.663956639057183\n",
      "Iteration 550 loss 1.0652282943317972 train RMSE 1.917012229781146 Validation RMSE 550 : 1.878636508037517 max of weights 5.773484660826517\n",
      "Iteration 560 loss 1.0500522815340543 train RMSE 1.9032595643104937 Validation RMSE 560 : 1.8587780832357221 max of weights 5.77819908357013\n",
      "Iteration 570 loss 1.1030919793633496 train RMSE 1.95080136314665 Validation RMSE 570 : 1.9098325810423393 max of weights 5.821184556678558\n",
      "Iteration 580 loss 1.0637480665085106 train RMSE 1.9156065801829503 Validation RMSE 580 : 1.8675077232454538 max of weights 5.822245002770608\n",
      "Iteration 590 loss 1.0298190056219432 train RMSE 1.8847043928556249 Validation RMSE 590 : 1.8460922293155788 max of weights 5.864491616282791\n",
      "Iteration 600 loss 1.0992717095497955 train RMSE 1.9473337628228955 Validation RMSE 600 : 1.904160904370676 max of weights 5.871431935398464\n",
      "Iteration 610 loss 1.066256698977641 train RMSE 1.9177825768055405 Validation RMSE 610 : 1.876167392068508 max of weights 5.882911786237063\n",
      "Iteration 620 loss 1.059384875798588 train RMSE 1.9115460348045539 Validation RMSE 620 : 1.8737127767760116 max of weights 5.886638538763443\n",
      "Iteration 630 loss 1.1040201324813355 train RMSE 1.9514602730528874 Validation RMSE 630 : 1.9106282489530426 max of weights 5.926610847485402\n",
      "Iteration 640 loss 1.1450162855768442 train RMSE 1.9874056572599093 Validation RMSE 640 : 1.9471252582477836 max of weights 5.946761388440336\n",
      "Iteration 650 loss 1.1706115549625777 train RMSE 2.0095130461879664 Validation RMSE 650 : 1.9710604601351502 max of weights 5.957546181787038\n",
      "Iteration 660 loss 0.9736270275219521 train RMSE 1.8322529466794544 Validation RMSE 660 : 1.790149544387414 max of weights 5.996672426685471\n",
      "Iteration 670 loss 1.0447513503564085 train RMSE 1.8981275886231272 Validation RMSE 670 : 1.862114781730106 max of weights 6.000181736721282\n",
      "Iteration 680 loss 1.151570413703749 train RMSE 1.9930032662051975 Validation RMSE 680 : 1.9537606308319786 max of weights 6.004038453822434\n",
      "Iteration 690 loss 1.1712977805545033 train RMSE 2.010007956958121 Validation RMSE 690 : 1.9721748995500223 max of weights 6.010961211771352\n",
      "Iteration 700 loss 1.0460567301952755 train RMSE 1.8992215774071017 Validation RMSE 700 : 1.867839780221608 max of weights 6.0189699836095745\n",
      "Iteration 710 loss 1.1433838944177628 train RMSE 1.985800387646797 Validation RMSE 710 : 1.948237578839835 max of weights 6.031153730603368\n",
      "Iteration 720 loss 1.0819221080759331 train RMSE 1.93152906313083 Validation RMSE 720 : 1.899263248130405 max of weights 6.058914348702211\n",
      "Iteration 730 loss 1.0175256164575097 train RMSE 1.8729960085323747 Validation RMSE 730 : 1.8395856230545116 max of weights 6.087227600305985\n",
      "Iteration 740 loss 1.079856405722738 train RMSE 1.929613833694637 Validation RMSE 740 : 1.8929217047486393 max of weights 6.089886985708318\n",
      "Iteration 750 loss 1.0773231887318995 train RMSE 1.9273201069063508 Validation RMSE 750 : 1.8901009384647554 max of weights 6.09146768442929\n",
      "Iteration 760 loss 1.0443737614367137 train RMSE 1.897515360601363 Validation RMSE 760 : 1.868240563891861 max of weights 6.0936080167239135\n",
      "Iteration 770 loss 0.9623123411243663 train RMSE 1.8212018338893654 Validation RMSE 770 : 1.7857806569551258 max of weights 6.112993519412442\n",
      "Iteration 780 loss 0.9276060119010509 train RMSE 1.787941612276892 Validation RMSE 780 : 1.7488095729183646 max of weights 6.115113207068239\n",
      "Iteration 790 loss 1.1502652199332581 train RMSE 1.9915528753502396 Validation RMSE 790 : 1.9518618835615542 max of weights 6.119831421002319\n",
      "Iteration 800 loss 1.0892435721843352 train RMSE 1.9378289821123986 Validation RMSE 800 : 1.9049170735847383 max of weights 6.125408045994343\n",
      "Iteration 810 loss 1.0536494205755345 train RMSE 1.9058014221123984 Validation RMSE 810 : 1.8670154560842103 max of weights 6.126584455895959\n",
      "Iteration 820 loss 1.0922307240869498 train RMSE 1.9404375223190982 Validation RMSE 820 : 1.9001881343379825 max of weights 6.126671382540644\n",
      "Iteration 830 loss 1.0572687792967639 train RMSE 1.9090136059289164 Validation RMSE 830 : 1.8650113576425775 max of weights 6.131804036569617\n",
      "Iteration 840 loss 1.0391951952774414 train RMSE 1.8925531572832488 Validation RMSE 840 : 1.8487689951900164 max of weights 6.148617312533194\n",
      "Iteration 850 loss 0.930871174481308 train RMSE 1.7908665585608092 Validation RMSE 850 : 1.7510963920987337 max of weights 6.16406647131995\n",
      "Iteration 860 loss 1.1482415717562706 train RMSE 1.9896071320280007 Validation RMSE 860 : 1.95156564115864 max of weights 6.1658637179380325\n",
      "Iteration 870 loss 1.0745673887969522 train RMSE 1.924514978480041 Validation RMSE 870 : 1.8907027198723216 max of weights 6.1674544879779365\n",
      "Iteration 880 loss 1.1672320740057165 train RMSE 2.0059732490000104 Validation RMSE 880 : 1.9713924973924286 max of weights 6.17056221292673\n",
      "Iteration 890 loss 1.0616564507045447 train RMSE 1.9128157821474314 Validation RMSE 890 : 1.8764114668396026 max of weights 6.17378104389896\n",
      "Iteration 900 loss 1.0176470187670201 train RMSE 1.8725812545368532 Validation RMSE 900 : 1.8448694015210136 max of weights 6.195371201201186\n",
      "Iteration 910 loss 1.0481089256076086 train RMSE 1.9004629319230903 Validation RMSE 910 : 1.8703953513607898 max of weights 6.218496876769909\n",
      "Iteration 920 loss 1.0370270635113112 train RMSE 1.890318596136724 Validation RMSE 920 : 1.8562068380198078 max of weights 6.221228263568312\n",
      "Iteration 930 loss 1.0463807947492851 train RMSE 1.8988196551569798 Validation RMSE 930 : 1.875425966573225 max of weights 6.238666211705358\n",
      "Iteration 940 loss 0.9500453154780603 train RMSE 1.808961716994746 Validation RMSE 940 : 1.7694956450127894 max of weights 6.2413388377918135\n",
      "Iteration 950 loss 1.101312422174828 train RMSE 1.948127558102301 Validation RMSE 950 : 1.9151821738355772 max of weights 6.242238412371502\n",
      "Iteration 960 loss 1.0220680251006624 train RMSE 1.8764749945041184 Validation RMSE 960 : 1.8436317268946327 max of weights 6.2541518114877475\n",
      "Iteration 970 loss 0.9315828326589229 train RMSE 1.7911720764462216 Validation RMSE 970 : 1.7509881584095484 max of weights 6.256158285942123\n",
      "Iteration 980 loss 0.9872341343307524 train RMSE 1.8440533862934527 Validation RMSE 980 : 1.8121902899981728 max of weights 6.263496715653692\n",
      "Iteration 990 loss 1.1450221703479313 train RMSE 1.9864277949789828 Validation RMSE 990 : 1.952352112031443 max of weights 6.26212861535066\n",
      "Iteration 1000 loss 1.031692389523661 train RMSE 1.8852118656209498 Validation RMSE 1000 : 1.8459498815401028 max of weights 6.2630403722786605\n",
      "Iteration 1010 loss 1.0224157793703932 train RMSE 1.8766659002366164 Validation RMSE 1010 : 1.8331413545337651 max of weights 6.263620781679982\n",
      "Iteration 1020 loss 1.0310049155980443 train RMSE 1.884527993854832 Validation RMSE 1020 : 1.8455717941757621 max of weights 6.267419073823529\n",
      "Iteration 1030 loss 1.111270466509163 train RMSE 1.956730417010318 Validation RMSE 1030 : 1.9090795828975808 max of weights 6.267801019536571\n",
      "Iteration 1040 loss 1.0159308062200991 train RMSE 1.8705955996450552 Validation RMSE 1040 : 1.8370783806877364 max of weights 6.272490496469098\n",
      "Iteration 1050 loss 0.9986849111185421 train RMSE 1.8545746125103952 Validation RMSE 1050 : 1.8145688734553551 max of weights 6.277199690879587\n",
      "Iteration 1060 loss 1.0353752226819202 train RMSE 1.8884328247526938 Validation RMSE 1060 : 1.8593420813274955 max of weights 6.277914032919996\n",
      "Iteration 1070 loss 1.1725105118497239 train RMSE 2.010002509238604 Validation RMSE 1070 : 1.9776130370353884 max of weights 6.279996319892698\n",
      "Iteration 1080 loss 1.1581944806797724 train RMSE 1.9976285401360132 Validation RMSE 1080 : 1.963717091773505 max of weights 6.284583195447364\n",
      "Iteration 1090 loss 0.8702372782016433 train RMSE 1.7305854109514718 Validation RMSE 1090 : 1.702483730440656 max of weights 6.291098654722157\n",
      "Iteration 1100 loss 1.0968910626645632 train RMSE 1.9437876610437534 Validation RMSE 1100 : 1.9129808903291672 max of weights 6.302937004192906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1110 loss 1.0535888952354093 train RMSE 1.9048544625131205 Validation RMSE 1110 : 1.8751696506977953 max of weights 6.3053848996004875\n",
      "Iteration 1120 loss 1.01677021832338 train RMSE 1.8711199054174514 Validation RMSE 1120 : 1.8436427661992532 max of weights 6.318063009545165\n",
      "Iteration 1130 loss 1.0120495975958348 train RMSE 1.8667072917127387 Validation RMSE 1130 : 1.8303444116902687 max of weights 6.3211051625845345\n",
      "Iteration 1140 loss 1.044777224184134 train RMSE 1.8967574598371788 Validation RMSE 1140 : 1.8594123681283319 max of weights 6.323044733263152\n",
      "Iteration 1150 loss 0.9893220348206032 train RMSE 1.8454892643939849 Validation RMSE 1150 : 1.804971465299186 max of weights 6.324699327753526\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0814652d0502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_network_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_neural_fingerprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_network_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plotting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b948822d968b>\u001b[0m in \u001b[0;36mtrain_neural_fingerprint\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mnum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     predict_func, trained_weights, conv_training_curve =          train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n\u001b[0;32m---> 82\u001b[0;31m                  nn_train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mprint_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b948822d968b>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, validation_smiles, validation_raw_targets)\u001b[0m\n\u001b[1;32m     41\u001b[0m     trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n\u001b[1;32m     42\u001b[0m                            \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learn_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                            b1=train_params['b1'], b2=train_params['b2'])\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_smiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/optimizers.pyc\u001b[0m in \u001b[0;36madam\u001b[0;34m(grad, x, callback, num_iters, step_size, b1, b2, eps)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m      \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m  \u001b[0;31m# First  moment estimate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m  \u001b[0;31m# Second moment estimate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b948822d968b>\u001b[0m in \u001b[0;36mcallback\u001b[0;34m(weights, iter)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"max of weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mundo_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_smiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_smiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtraining_curve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/build_vanilla_net.pyc\u001b[0m in \u001b[0;36mpred_fun\u001b[0;34m(weights, smiles)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mfingerprint_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mfingerprints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfingerprint_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfingerprint_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnet_pred_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfingerprints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36moutput_layer_fun\u001b[0;34m(weights, smiles)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moutput_layer_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_layer_fun_and_atom_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36moutput_layer_fun_and_atom_activations\u001b[0;34m(weights, smiles)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mwrite_to_fingerprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             atom_features = update_layer(weights, layer, atom_features, bond_features, array_rep,\n\u001b[1;32m     94\u001b[0m                                          normalize=normalize)\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36mwrite_to_fingerprint\u001b[0;34m(atom_features, layer)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0matom_activations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# Sum over all atoms within a moleclue:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_and_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'atom_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mall_layer_fps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/for_x_in_rage/jetBrains/Molecular_props/neural-fingerprint/neuralfingerprint/build_convnet.pyc\u001b[0m in \u001b[0;36msum_and_stack\u001b[0;34m(features, idxs_list_of_lists)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msum_and_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs_list_of_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfast_array_from_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs_list_of_lists\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_network_weights = train_neural_fingerprint()\n",
    "with open('results.pkl', 'w') as f:\n",
    "    pickle.dump(trained_network_weights, f)\n",
    "\n",
    "# Plotting.\n",
    "with open('results.pkl') as f:\n",
    "    trained_weights = pickle.load(f)\n",
    "plot(trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {'target_name' : 'logP',\n",
    "               'data_file'   : 'logp_mean'}\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "normalize = 1\n",
    "dropout = 0\n",
    "activation = relu\n",
    "params = {'fp_length': 50,\n",
    "            'fp_depth': 4,\n",
    "            'init_scale':np.exp(-4),\n",
    "            'l2_penalty':np.exp(-2),\n",
    "            'conv_width':20}\n",
    "\n",
    "conv_layer_sizes = [params['conv_width']] * params['fp_depth']\n",
    "conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                    'fp_length' : params['fp_length'],\n",
    "                    'normalize' : normalize,\n",
    "                    'return_atom_activations':False}\n",
    "\n",
    "all_radii = range(params['fp_depth'] + 1)\n",
    "\n",
    "# Plotting parameters\n",
    "num_figs_per_fp = 11\n",
    "figsize = (100, 100)\n",
    "highlight_color = (30.0/255.0, 100.0/255.0, 255.0/255.0)  # A nice light blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data...\n",
      "Regression on 9629 training points.\n",
      "--------------------------------------------------------------------------------\n",
      "Mean predictor\n",
      "\n",
      "Performance (RMSE) on logP:\n",
      "Train: 1.859267165016053\n",
      "Test:  1.8550727952633121\n",
      "--------------------------------------------------------------------------------\n",
      "Task params {'fp_length': 50, 'l2_penalty': 0.1353352832366127, 'fp_depth': 4, 'conv_width': 20, 'init_scale': 0.01831563888873418}\n",
      "Convnet fingerprints with neural net\n",
      "Total number of weights in the network: 27441\n",
      "max of weights 0.08144291040373952\n",
      "Iteration 0 loss 0.9840853481222934 train RMSE 1.844335332789677 Validation RMSE 0 : 1.8476555767592138 max of weights 0.07492002162767124\n",
      "Iteration 10 loss 0.9858015229240393 train RMSE 1.8459541041866336 Validation RMSE 10 : 1.8521251328305253 max of weights 0.08025294081454813\n",
      "Iteration 20 loss 0.9844516996462558 train RMSE 1.8446898466666117 Validation RMSE 20 : 1.842170896370284 max of weights 0.0877317706072634\n",
      "Iteration 30 loss 0.9713510805581682 train RMSE 1.8323621136297816 Validation RMSE 30 : 1.8369571799996223 max of weights 0.09701140369251887\n",
      "Iteration 40 loss 0.9565941112020868 train RMSE 1.818364118162012 Validation RMSE 40 : 1.8192187288081256 max of weights 0.10812085338006447\n",
      "Iteration 50 loss 0.9339013285783027 train RMSE 1.7966231581640077 Validation RMSE 50 : 1.7996576420140051 max of weights 0.12308404979428747\n",
      "Iteration 60 loss 0.9004654118688294 train RMSE 1.7641046649043275 Validation RMSE 60 : 1.7636885921276795 max of weights 0.13884668776537817\n",
      "Iteration 70 loss 0.8552055523567317 train RMSE 1.7191130137872728 Validation RMSE 70 : 1.722901207102479 max of weights 0.15405846854740346\n",
      "Iteration 80 loss 0.7931143452769259 train RMSE 1.6554145834989626 Validation RMSE 80 : 1.6550243308046688 max of weights 0.1701708708052119\n",
      "Iteration 90 loss 0.7232990849677514 train RMSE 1.5807315489779552 Validation RMSE 90 : 1.581098985255 max of weights 0.1861152514644353\n",
      "Iteration 100 loss 0.6372017096761402 train RMSE 1.4834745935501854 Validation RMSE 100 : 1.48032329975212 max of weights 0.19765037156876836\n",
      "Iteration 110 loss 0.5656687627086071 train RMSE 1.3975022597108604 Validation RMSE 110 : 1.385833991964726 max of weights 0.20890906488835523\n",
      "Iteration 120 loss 0.5086024919787838 train RMSE 1.3248967944866783 Validation RMSE 120 : 1.3170531767632851 max of weights 0.22453782036733239\n",
      "Iteration 130 loss 0.46835854329292126 train RMSE 1.271197044794663 Validation RMSE 130 : 1.253618164706169 max of weights 0.23571878577931882\n",
      "Iteration 140 loss 0.417894058798952 train RMSE 1.2005025818248316 Validation RMSE 140 : 1.1823301345828254 max of weights 0.2440515899351929\n",
      "Iteration 150 loss 0.37961671059830826 train RMSE 1.1439680924066387 Validation RMSE 150 : 1.127156254817709 max of weights 0.2493268326034199\n",
      "Iteration 160 loss 0.35672224397612284 train RMSE 1.1087360176341987 Validation RMSE 160 : 1.0879299770171853 max of weights 0.25369730978716193\n",
      "Iteration 170 loss 0.34338669688731094 train RMSE 1.087668634596028 Validation RMSE 170 : 1.0732735269031366 max of weights 0.2617056489482029\n",
      "Iteration 180 loss 0.3593750994738561 train RMSE 1.1127291812883615 Validation RMSE 180 : 1.094694383421928 max of weights 0.27011220117190166\n",
      "Iteration 190 loss 0.3330864204449595 train RMSE 1.0710796827734599 Validation RMSE 190 : 1.049632736088907 max of weights 0.2793847577633502\n",
      "Iteration 200 loss 0.312049822178776 train RMSE 1.036526841773099 Validation RMSE 200 : 1.0096436609123094 max of weights 0.28538075452907713\n",
      "Iteration 210 loss 0.2971706362071935 train RMSE 1.0113485950863559 Validation RMSE 210 : 0.9907576016019479 max of weights 0.29287033314696254\n",
      "Iteration 220 loss 0.2978869391899238 train RMSE 1.0125399127219017 Validation RMSE 220 : 0.9928359513084399 max of weights 0.29913809004620234\n",
      "Iteration 230 loss 0.2885327430199506 train RMSE 0.9964107274882366 Validation RMSE 230 : 0.9702772903321649 max of weights 0.3062520357120233\n",
      "Iteration 240 loss 0.2801030986131916 train RMSE 0.9816359040912095 Validation RMSE 240 : 0.9515229889172601 max of weights 0.3129010232158043\n",
      "Iteration 250 loss 0.27270001481488193 train RMSE 0.968467083522331 Validation RMSE 250 : 0.9443588642600715 max of weights 0.31747504684657035\n",
      "Iteration 260 loss 0.27071358842238424 train RMSE 0.9648780568118549 Validation RMSE 260 : 0.9378790850537717 max of weights 0.3193564631926709\n",
      "Iteration 270 loss 0.2691927024576684 train RMSE 0.9621240570636409 Validation RMSE 270 : 0.9403631094212102 max of weights 0.32061616517697644\n",
      "Iteration 280 loss 0.29224043127824123 train RMSE 1.00265997100384 Validation RMSE 280 : 0.9808406272075123 max of weights 0.32402389050607\n",
      "Iteration 290 loss 0.27826507118287697 train RMSE 0.9782706331975756 Validation RMSE 290 : 0.9549121821009439 max of weights 0.3290975303686805\n",
      "Iteration 300 loss 0.2624759170429391 train RMSE 0.9499346133225869 Validation RMSE 300 : 0.9264746315489282 max of weights 0.33212733724789384\n",
      "Iteration 310 loss 0.2542293568859333 train RMSE 0.9347779154798326 Validation RMSE 310 : 0.9137174934542242 max of weights 0.33604928048335303\n",
      "Iteration 320 loss 0.2548717738190913 train RMSE 0.9359554516233894 Validation RMSE 320 : 0.9090693310407859 max of weights 0.34013735503057496\n",
      "Iteration 330 loss 0.25337882380302884 train RMSE 0.9331643465951801 Validation RMSE 330 : 0.9117039408399037 max of weights 0.34340671828354424\n",
      "Iteration 340 loss 0.25144467614276583 train RMSE 0.9295490063820482 Validation RMSE 340 : 0.9003580276220191 max of weights 0.3475929894285411\n",
      "Iteration 350 loss 0.24299592521847022 train RMSE 0.9136662829458093 Validation RMSE 350 : 0.8900420698832169 max of weights 0.34862535303489484\n",
      "Iteration 360 loss 0.24365780137429066 train RMSE 0.9148942401413218 Validation RMSE 360 : 0.889369511276521 max of weights 0.34801799303796116\n",
      "Iteration 370 loss 0.2438935072875168 train RMSE 0.9153208441186433 Validation RMSE 370 : 0.8954380205222732 max of weights 0.3475877036318742\n",
      "Iteration 380 loss 0.24850162302472734 train RMSE 0.9239842289791158 Validation RMSE 380 : 0.9097001124994902 max of weights 0.3497016410758791\n",
      "Iteration 390 loss 0.2543035315763567 train RMSE 0.9347709223178293 Validation RMSE 390 : 0.9099826063238393 max of weights 0.3534109177227111\n",
      "Iteration 400 loss 0.23998851674274044 train RMSE 0.9078882019695316 Validation RMSE 400 : 0.8892069903490785 max of weights 0.36063004582507546\n",
      "Iteration 410 loss 0.2393867451256645 train RMSE 0.9067195463278882 Validation RMSE 410 : 0.8928942531616129 max of weights 0.36663935667585484\n",
      "Iteration 420 loss 0.2616527461650686 train RMSE 0.9482072809909379 Validation RMSE 420 : 0.9211454458556672 max of weights 0.37141623755015135\n",
      "Iteration 430 loss 0.2409479124276828 train RMSE 0.9096526815955178 Validation RMSE 430 : 0.8932611772989629 max of weights 0.3773674859833205\n",
      "Iteration 440 loss 0.23272839164135994 train RMSE 0.8938768998614632 Validation RMSE 440 : 0.8688243734491199 max of weights 0.38496707867964164\n",
      "Iteration 450 loss 0.22646223233744298 train RMSE 0.8816438732860091 Validation RMSE 450 : 0.8580623066883518 max of weights 0.3916911052767766\n",
      "Iteration 460 loss 0.22614488862877186 train RMSE 0.8810028139642472 Validation RMSE 460 : 0.8605345376017828 max of weights 0.39777698306920106\n",
      "Iteration 470 loss 0.2348243983083195 train RMSE 0.8978523911331044 Validation RMSE 470 : 0.8794513937168396 max of weights 0.4025297595581533\n",
      "Iteration 480 loss 0.2390923909506886 train RMSE 0.9060411877572684 Validation RMSE 480 : 0.8965685876709601 max of weights 0.4068525953142986\n",
      "Iteration 490 loss 0.22800873769082894 train RMSE 0.8846306007942202 Validation RMSE 490 : 0.8612844873551241 max of weights 0.41136803131594163\n",
      "Iteration 500 loss 0.22175586462847785 train RMSE 0.8723012723950849 Validation RMSE 500 : 0.8586140966279587 max of weights 0.418713986448401\n",
      "Iteration 510 loss 0.22941074072647447 train RMSE 0.8873246401855168 Validation RMSE 510 : 0.876708133720297 max of weights 0.4239483039462333\n",
      "Iteration 520 loss 0.23144428888561216 train RMSE 0.8912604629403837 Validation RMSE 520 : 0.8709489546256056 max of weights 0.42842909751135283\n",
      "Iteration 530 loss 0.21721187241347484 train RMSE 0.8631952454599955 Validation RMSE 530 : 0.8453455667055592 max of weights 0.4345093772669337\n",
      "Iteration 540 loss 0.2119872873710339 train RMSE 0.8526459559461609 Validation RMSE 540 : 0.8365988699960439 max of weights 0.44169650452753906\n",
      "Iteration 550 loss 0.20966050656754534 train RMSE 0.8478876623497322 Validation RMSE 550 : 0.8300626676150689 max of weights 0.4478338065035303\n",
      "Iteration 560 loss 0.2146497847526853 train RMSE 0.8579869442923886 Validation RMSE 560 : 0.8457173556873003 max of weights 0.4538118904252959\n",
      "Iteration 570 loss 0.23999327874359494 train RMSE 0.9076012203520699 Validation RMSE 570 : 0.8951007579001207 max of weights 0.45798150792612113\n",
      "Iteration 580 loss 0.224574788094037 train RMSE 0.877755917877079 Validation RMSE 580 : 0.866750881577246 max of weights 0.4629817698757413\n",
      "Iteration 590 loss 0.21129555953033144 train RMSE 0.8511813654625188 Validation RMSE 590 : 0.8351850785152425 max of weights 0.4683971276656123\n",
      "Iteration 600 loss 0.20334265963963954 train RMSE 0.8348541375384396 Validation RMSE 600 : 0.8284114091193866 max of weights 0.4748402037814129\n",
      "Iteration 610 loss 0.20304373253484564 train RMSE 0.8342235357069228 Validation RMSE 610 : 0.8216102742653356 max of weights 0.4794923330036801\n",
      "Iteration 620 loss 0.20405414903794275 train RMSE 0.8362928129699168 Validation RMSE 620 : 0.8234571936423408 max of weights 0.484492688437495\n",
      "Iteration 630 loss 0.20333387026882235 train RMSE 0.8347861998762222 Validation RMSE 630 : 0.8149139914283406 max of weights 0.49039470828345716\n",
      "Iteration 640 loss 0.1985592293167296 train RMSE 0.8248122255450072 Validation RMSE 640 : 0.8118182725379105 max of weights 0.49762325648832184\n",
      "Iteration 650 loss 0.19804612754082057 train RMSE 0.823716150172896 Validation RMSE 650 : 0.8091460854767694 max of weights 0.5032818434517354\n",
      "Iteration 660 loss 0.19673807261740636 train RMSE 0.8209577920720379 Validation RMSE 660 : 0.8111301612197808 max of weights 0.5089393681327962\n",
      "Iteration 670 loss 0.20655916303774136 train RMSE 0.8413860560752805 Validation RMSE 670 : 0.8377238103316988 max of weights 0.5125706746013359\n",
      "Iteration 680 loss 0.20556851277665858 train RMSE 0.839345297210173 Validation RMSE 680 : 0.8272660295999527 max of weights 0.5174857557266789\n",
      "Iteration 690 loss 0.19687883584146 train RMSE 0.8212301927901866 Validation RMSE 690 : 0.8124905920272825 max of weights 0.5237318947597851\n",
      "Iteration 700 loss 0.19352757465463932 train RMSE 0.8141291150488988 Validation RMSE 700 : 0.811434474833826 max of weights 0.5292582085227092\n",
      "Iteration 710 loss 0.20887620106590102 train RMSE 0.8460763225558096 Validation RMSE 710 : 0.8305339414086227 max of weights 0.5334383267349979\n",
      "Iteration 720 loss 0.20174648757349897 train RMSE 0.8313647580065727 Validation RMSE 720 : 0.8233300520060414 max of weights 0.5411481857733481\n",
      "Iteration 730 loss 0.19200690516851948 train RMSE 0.8108475647901844 Validation RMSE 730 : 0.7964274163609997 max of weights 0.5521270665972655\n",
      "Iteration 740 loss 0.18977532256405838 train RMSE 0.8060451966907447 Validation RMSE 740 : 0.7941127553436521 max of weights 0.562332199340422\n",
      "Iteration 750 loss 0.18781709055260662 train RMSE 0.8018217620205554 Validation RMSE 750 : 0.7925688227078803 max of weights 0.570650382692967\n",
      "Iteration 760 loss 0.19266236737060588 train RMSE 0.812190982025993 Validation RMSE 760 : 0.8045378765159679 max of weights 0.5760889583126135\n",
      "Iteration 770 loss 0.1976038512757582 train RMSE 0.8226536185456721 Validation RMSE 770 : 0.8257910563225682 max of weights 0.5805625294249819\n",
      "Iteration 780 loss 0.18926823217784772 train RMSE 0.8049331385936191 Validation RMSE 780 : 0.7944083079150256 max of weights 0.5857287239253054\n",
      "Iteration 790 loss 0.18534692196717206 train RMSE 0.7964457272308707 Validation RMSE 790 : 0.7931735517697802 max of weights 0.5946357071850227\n",
      "Iteration 800 loss 0.1943944320753885 train RMSE 0.8158321893246517 Validation RMSE 800 : 0.817694535877666 max of weights 0.6007460293693944\n",
      "Iteration 810 loss 0.20682177163636398 train RMSE 0.8417334024238469 Validation RMSE 810 : 0.8290827896442076 max of weights 0.6067788735277545\n",
      "Iteration 820 loss 0.18564908803530175 train RMSE 0.7970581214003842 Validation RMSE 820 : 0.7882251632056215 max of weights 0.6150939915009177\n",
      "Iteration 830 loss 0.17953309840609177 train RMSE 0.7836619138582742 Validation RMSE 830 : 0.7779372589923226 max of weights 0.6259891637038392\n",
      "Iteration 840 loss 0.1760651819768721 train RMSE 0.775943489159776 Validation RMSE 840 : 0.7687826377080663 max of weights 0.6352436859864761\n",
      "Iteration 850 loss 0.18217290881803486 train RMSE 0.789427328902434 Validation RMSE 850 : 0.7864731106791912 max of weights 0.6428641862771552\n",
      "Iteration 860 loss 0.2039589890190025 train RMSE 0.8357643346873962 Validation RMSE 860 : 0.8306895065351505 max of weights 0.6473008789970996\n",
      "Iteration 870 loss 0.19104030842963596 train RMSE 0.8086159728838741 Validation RMSE 870 : 0.8109545855412653 max of weights 0.652189651986787\n",
      "Iteration 880 loss 0.18338372255206634 train RMSE 0.7920534057492148 Validation RMSE 880 : 0.7880004884348247 max of weights 0.658108432125758\n",
      "Iteration 890 loss 0.1726871443163133 train RMSE 0.7683428802069202 Validation RMSE 890 : 0.772181317928871 max of weights 0.6657116506177801\n",
      "Iteration 900 loss 0.17409787032101445 train RMSE 0.7714987581683928 Validation RMSE 900 : 0.7694798792873875 max of weights 0.6714673248834887\n",
      "Iteration 910 loss 0.17452475695956948 train RMSE 0.7724366536584324 Validation RMSE 910 : 0.7645608472977294 max of weights 0.6772892667313967\n",
      "Iteration 920 loss 0.17245465201245064 train RMSE 0.7677788906777665 Validation RMSE 920 : 0.7591966796673055 max of weights 0.685385882925913\n",
      "Iteration 930 loss 0.1697580954821745 train RMSE 0.7616531479210165 Validation RMSE 930 : 0.7595516432243067 max of weights 0.69599631980395\n",
      "Iteration 940 loss 0.17170025012688783 train RMSE 0.7660246227496325 Validation RMSE 940 : 0.7617253929707414 max of weights 0.7047345974534763\n",
      "Iteration 950 loss 0.16859032263851856 train RMSE 0.7589732355041654 Validation RMSE 950 : 0.7572935162896182 max of weights 0.7112750730780438\n",
      "Iteration 960 loss 0.1861515920651376 train RMSE 0.7979692008867137 Validation RMSE 960 : 0.7991676035640143 \n",
      "Performance (RMSE) on logP:\n",
      "Train: 0.7778815716425832\n",
      "Test:  0.783807089847976\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data...\n",
      "Convnet fingerprints with neural net\n",
      "FP 0 has linear regression coefficient -0.051137364053\n",
      "radius: 2 atom list: [1, 0, 2, 3, 4] activation 0.040009115369244\n",
      "radius: 2 atom list: [13, 15, 16, 14, 12] activation 0.03876628403596414\n",
      "radius: 2 atom list: [2, 1, 0] activation 0.03876431543207615\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.03875961209079962\n",
      "radius: 2 atom list: [7, 4, 5, 9, 6, 8] activation 0.03871035001621684\n",
      "radius: 2 atom list: [3, 1, 4] activation 0.038686731958544995\n",
      "radius: 2 atom list: [13, 14, 15, 16] activation 0.038650999305661356\n",
      "radius: 2 atom list: [3, 1, 0, 2] activation 0.03863924647478556\n",
      "radius: 2 atom list: [2, 1, 3] activation 0.038630931176976045\n",
      "radius: 2 atom list: [1, 3, 0, 2] activation 0.0385608969916921\n",
      "radius: 2 atom list: [8, 7, 4] activation 0.03848736059146072\n",
      "FP 1 has linear regression coefficient -0.051202276966\n",
      "radius: 1 atom list: [14, 13, 15, 16, 17, 18] activation 0.04340841427521736\n",
      "radius: 2 atom list: [2, 1, 3] activation 0.041192607894120006\n",
      "radius: 2 atom list: [8, 7, 4] activation 0.04115619455564557\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.04113257760844908\n",
      "radius: 2 atom list: [2, 1, 0] activation 0.041113191783923045\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.041017898388870454\n",
      "radius: 2 atom list: [13, 14, 15, 16] activation 0.04098741494035185\n",
      "radius: 2 atom list: [3, 1, 0, 2] activation 0.04091961051641414\n",
      "radius: 2 atom list: [3, 1, 4] activation 0.04085759883301921\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.04085337041984668\n",
      "radius: 2 atom list: [1, 3, 0, 2] activation 0.04077675833753272\n",
      "FP 2 has linear regression coefficient -0.0431157300527\n",
      "radius: 1 atom list: [14, 13, 15, 16, 17, 18] activation 0.04031669017572501\n",
      "radius: 1 atom list: [8, 7] activation 0.037725590088464336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 1 atom list: [11, 8] activation 0.03636088603222664\n",
      "radius: 1 atom list: [11, 10] activation 0.036054789817693896\n",
      "radius: 1 atom list: [3, 1] activation 0.035870824890196135\n",
      "radius: 1 atom list: [13, 16, 15, 14] activation 0.035869335916685945\n",
      "radius: 1 atom list: [11, 10] activation 0.03574187875187763\n",
      "radius: 1 atom list: [17, 18] activation 0.03572668614631112\n",
      "radius: 1 atom list: [2, 1] activation 0.035646941079506334\n",
      "radius: 1 atom list: [0, 1] activation 0.0353135504974113\n",
      "radius: 1 atom list: [7, 5] activation 0.0352996807271227\n",
      "FP 3 has linear regression coefficient 0.0547802044756\n",
      "radius: 2 atom list: [12, 4, 5, 16, 0, 2, 3, 11, 1, 6, 10] activation 0.05200975607028246\n",
      "radius: 2 atom list: [9, 13, 23, 10, 20, 11, 12, 21, 22, 7, 8] activation 0.05171720722861676\n",
      "radius: 2 atom list: [3, 7, 18, 11, 13, 4, 19, 6, 12, 5] activation 0.05109597873925983\n",
      "radius: 2 atom list: [6, 7, 8, 15, 21, 5, 9, 14, 16, 1] activation 0.050881224175346816\n",
      "radius: 2 atom list: [5, 10, 11, 13, 14, 8, 7, 6, 9] activation 0.05072431959957953\n",
      "radius: 2 atom list: [12, 13, 14, 4, 7, 15, 5, 6, 8, 11] activation 0.050613726530763244\n",
      "radius: 2 atom list: [11, 16, 3, 10, 12, 17, 0, 2, 18, 1] activation 0.05045268886621954\n",
      "radius: 2 atom list: [13, 19, 7, 9, 8, 12, 14, 11, 10] activation 0.050398199656119046\n",
      "radius: 2 atom list: [7, 8, 14, 6, 9, 19, 13, 15, 4] activation 0.05035546488519815\n",
      "radius: 2 atom list: [9, 7, 8, 10, 15, 11, 5] activation 0.050321523492946364\n",
      "radius: 2 atom list: [1, 4, 3, 6, 14, 13, 5, 15, 24] activation 0.05028266824718712\n",
      "FP 4 has linear regression coefficient -0.0672102504816\n",
      "radius: 3 atom list: [0, 2, 3, 6, 4, 5, 1] activation 0.06868283337987005\n",
      "radius: 3 atom list: [4, 7, 5, 1, 0, 2, 3, 6] activation 0.06867819976474164\n",
      "radius: 3 atom list: [23, 5, 4, 6, 0, 2, 3, 1] activation 0.06865116835774865\n",
      "radius: 3 atom list: [4, 1, 0, 2, 3, 6, 5, 7] activation 0.068643007165466\n",
      "radius: 3 atom list: [4, 5, 0, 2, 3, 6, 8, 9, 1] activation 0.06862260390361466\n",
      "radius: 3 atom list: [19, 20, 22, 23, 18, 21, 17, 14, 15, 16] activation 0.06860155578007164\n",
      "radius: 3 atom list: [4, 5, 6, 0, 2, 3, 1] activation 0.06835717145524861\n",
      "radius: 3 atom list: [5, 10, 7, 6, 9, 4, 8, 3] activation 0.06832791142347272\n",
      "radius: 3 atom list: [6, 4, 3, 7, 8, 5, 9] activation 0.06828366441629652\n",
      "radius: 3 atom list: [3, 6, 7, 23, 24, 4, 0, 2, 5, 1, 8] activation 0.06791753953401895\n",
      "radius: 3 atom list: [2, 0, 4, 6, 8, 1, 3, 5, 7] activation 0.0679053016989202\n",
      "FP 5 has linear regression coefficient 0.0279092130517\n",
      "radius: 1 atom list: [8, 9, 10, 7, 11] activation 0.03222587637863353\n",
      "radius: 1 atom list: [0, 1, 2, 3, 4] activation 0.032067144726317484\n",
      "radius: 1 atom list: [9, 7, 10, 6, 8] activation 0.032010618128604514\n",
      "radius: 1 atom list: [9, 10, 8, 11, 12] activation 0.03193943372634635\n",
      "radius: 1 atom list: [10, 6, 7, 8, 9] activation 0.03175358601463976\n",
      "radius: 1 atom list: [24, 25, 21, 26, 23] activation 0.03134752264177701\n",
      "radius: 1 atom list: [23, 26, 24, 25, 22] activation 0.03128645361450033\n",
      "radius: 1 atom list: [6, 4, 10, 5, 3] activation 0.031100257564004523\n",
      "radius: 1 atom list: [13, 14, 9, 12, 15] activation 0.031055448072461835\n",
      "radius: 1 atom list: [22, 25, 20, 24, 23] activation 0.03091704907776876\n",
      "radius: 1 atom list: [0, 2, 12, 1, 3] activation 0.030913055574665987\n",
      "FP 6 has linear regression coefficient 0.0879860285369\n",
      "radius: 4 atom list: [24, 1, 2, 3, 4, 6, 8, 9, 11, 12, 13, 14, 16, 5, 7, 15] activation 0.11264294258898166\n",
      "radius: 3 atom list: [10, 16, 19, 20, 8, 9, 18, 4, 6, 5, 7, 17] activation 0.11239144110289027\n",
      "radius: 4 atom list: [16, 17, 18, 7, 13, 3, 4, 5, 6, 8, 9, 10, 11, 14, 15] activation 0.11123554185233556\n",
      "radius: 3 atom list: [9, 11, 12, 14, 15, 17, 21, 3, 6, 8, 10, 16, 7] activation 0.11114157677069217\n",
      "radius: 4 atom list: [5, 10, 1, 2, 3, 4, 6, 7, 8, 11, 12, 13, 14, 15, 9] activation 0.11096556329643921\n",
      "radius: 4 atom list: [1, 3, 5, 7, 8, 9, 11, 14, 4, 6, 10, 12, 2, 13, 19, 0, 15, 16, 17, 18] activation 0.11067357970293444\n",
      "radius: 4 atom list: [0, 3, 5, 9, 11, 15, 17, 19, 21, 1, 2, 4, 6, 7, 8, 10, 12, 14, 16, 18, 20] activation 0.11056678544051841\n",
      "radius: 4 atom list: [0, 4, 6, 10, 12, 14, 16, 18, 20, 1, 3, 5, 7, 8, 9, 11, 13, 15, 17, 19, 2] activation 0.11046350794486828\n",
      "radius: 4 atom list: [0, 4, 6, 10, 12, 16, 18, 20, 1, 3, 5, 7, 8, 9, 11, 13, 15, 17, 19, 2] activation 0.11009892822053274\n",
      "radius: 4 atom list: [0, 4, 1, 2, 5, 7, 8, 6, 10, 11, 12, 14, 15, 17, 18, 13, 3] activation 0.10957497609119687\n",
      "radius: 4 atom list: [3, 1, 2, 5, 8, 10, 13, 0, 4, 7, 9, 12, 14, 6, 11] activation 0.1095616829587443\n",
      "FP 7 has linear regression coefficient 0.0364906363386\n",
      "radius: 1 atom list: [8, 9, 10, 7, 11] activation 0.03326655216650756\n",
      "radius: 1 atom list: [6, 4, 10, 5, 3] activation 0.03308420956861827\n",
      "radius: 1 atom list: [9, 10, 8, 11, 12] activation 0.03288554460348672\n",
      "radius: 1 atom list: [5, 9, 7, 6, 8] activation 0.03285802247013538\n",
      "radius: 1 atom list: [9, 7, 10, 6, 8] activation 0.032588667642363796\n",
      "radius: 1 atom list: [0, 1, 2, 3, 4] activation 0.03235958457800292\n",
      "radius: 1 atom list: [22, 25, 20, 24, 23] activation 0.03234918593969338\n",
      "radius: 1 atom list: [13, 14, 9, 12, 15] activation 0.03234611000679855\n",
      "radius: 1 atom list: [15, 16, 17, 13, 14] activation 0.03230225027895484\n",
      "radius: 1 atom list: [10, 6, 7, 8, 9] activation 0.032207276801074845\n",
      "radius: 1 atom list: [24, 25, 21, 26, 23] activation 0.03187207925865864\n",
      "FP 8 has linear regression coefficient -0.0560280276197\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.05264812726394212\n",
      "radius: 2 atom list: [14, 12, 13, 15, 16, 17, 18] activation 0.05215699774933493\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.05202467881152801\n",
      "radius: 2 atom list: [8, 7, 4] activation 0.05133896505019929\n",
      "radius: 2 atom list: [2, 1, 3] activation 0.05085133074562583\n",
      "radius: 2 atom list: [13, 14, 15, 16] activation 0.05017386525751806\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.049818209334025164\n",
      "radius: 2 atom list: [2, 1, 0] activation 0.04969039202850424\n",
      "radius: 2 atom list: [3, 1, 0, 2] activation 0.04917797920851902\n",
      "radius: 2 atom list: [3, 1, 4] activation 0.048889487402996916\n",
      "radius: 2 atom list: [1, 3, 0, 2] activation 0.048681177924479985\n",
      "FP 9 has linear regression coefficient -0.019144233476\n",
      "radius: 0 atom list: [11] activation 0.026737107104615507\n",
      "radius: 0 atom list: [0] activation 0.02608980802452138\n",
      "radius: 0 atom list: [10] activation 0.0250751588158977\n",
      "radius: 0 atom list: [13] activation 0.024652839366854664\n",
      "radius: 0 atom list: [6] activation 0.023866775050533674\n",
      "radius: 0 atom list: [3] activation 0.02354182771815475\n",
      "radius: 0 atom list: [7] activation 0.02337014122005892\n",
      "radius: 0 atom list: [9] activation 0.022475629768951882\n",
      "radius: 0 atom list: [6] activation 0.022455412093935725\n",
      "radius: 0 atom list: [7] activation 0.02208144667160695\n",
      "radius: 0 atom list: [0] activation 0.021788977173853287\n",
      "FP 10 has linear regression coefficient -0.0764885063118\n",
      "radius: 3 atom list: [1, 0, 2, 3, 4] activation 0.09249116871329888\n",
      "radius: 3 atom list: [6, 4, 3, 7, 8, 5, 9] activation 0.091953652761847\n",
      "radius: 3 atom list: [2, 5, 1, 4, 0, 3, 9] activation 0.09064178647015017\n",
      "radius: 3 atom list: [14, 1, 0, 15, 3, 2] activation 0.0906090545800066\n",
      "radius: 3 atom list: [18, 1, 2, 3, 19, 0] activation 0.09060563649177882\n",
      "radius: 3 atom list: [31, 33, 29, 32, 30, 27] activation 0.09058446218291959\n",
      "radius: 3 atom list: [18, 1, 19, 3, 0, 2] activation 0.09058069197174898\n",
      "radius: 3 atom list: [5, 25, 21, 22, 23, 24, 26] activation 0.09055215090588337\n",
      "radius: 3 atom list: [1, 33, 30, 31, 32, 35, 34] activation 0.09054670742043322\n",
      "radius: 3 atom list: [0, 2, 3, 6, 4, 5, 1] activation 0.09046786401952847\n",
      "radius: 3 atom list: [2, 1, 4, 6, 5, 0, 3] activation 0.09033159197159371\n",
      "FP 11 has linear regression coefficient 0.0332303344824\n",
      "radius: 1 atom list: [8, 9, 10, 7, 11] activation 0.03272201971572316\n",
      "radius: 1 atom list: [6, 4, 10, 5, 3] activation 0.032658499434555774\n",
      "radius: 1 atom list: [9, 7, 10, 6, 8] activation 0.03229937389150345\n",
      "radius: 1 atom list: [5, 9, 7, 6, 8] activation 0.032274256557747974\n",
      "radius: 1 atom list: [9, 10, 8, 11, 12] activation 0.03224596911632249\n",
      "radius: 1 atom list: [0, 1, 2, 3, 4] activation 0.032228471341192756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 1 atom list: [6, 7, 8, 15] activation 0.032058640503369595\n",
      "radius: 1 atom list: [10, 6, 7, 8, 9] activation 0.03194752631083712\n",
      "radius: 1 atom list: [13, 14, 9, 12, 15] activation 0.031921970451364266\n",
      "radius: 1 atom list: [15, 16, 17, 13, 14] activation 0.03179513638155196\n",
      "radius: 1 atom list: [22, 25, 20, 24, 23] activation 0.03179322759153739\n",
      "FP 12 has linear regression coefficient -0.0399822244796\n",
      "radius: 1 atom list: [14, 13, 15, 16, 17, 18] activation 0.0402598257267124\n",
      "radius: 1 atom list: [8, 7] activation 0.03774257342327456\n",
      "radius: 1 atom list: [11, 8] activation 0.0365743155280539\n",
      "radius: 1 atom list: [11, 10] activation 0.036254390201922566\n",
      "radius: 1 atom list: [13, 16, 15, 14] activation 0.036118974081954844\n",
      "radius: 1 atom list: [3, 1] activation 0.03607925342742003\n",
      "radius: 1 atom list: [2, 1] activation 0.035900605952832156\n",
      "radius: 1 atom list: [14, 13] activation 0.035709072937530284\n",
      "radius: 1 atom list: [11, 10] activation 0.03568792963449312\n",
      "radius: 1 atom list: [17, 18] activation 0.03566043793395518\n",
      "radius: 1 atom list: [8, 4, 7] activation 0.03564994756335746\n",
      "FP 13 has linear regression coefficient 0.038885900888\n",
      "radius: 1 atom list: [8, 9, 10, 7, 11] activation 0.0320176643796316\n",
      "radius: 1 atom list: [0, 1, 2, 3, 4] activation 0.03201235856878591\n",
      "radius: 1 atom list: [9, 7, 10, 6, 8] activation 0.03191369536490375\n",
      "radius: 1 atom list: [9, 10, 8, 11, 12] activation 0.03176995809206908\n",
      "radius: 1 atom list: [10, 6, 7, 8, 9] activation 0.03170707415694572\n",
      "radius: 1 atom list: [6, 4, 10, 5, 3] activation 0.03168681096830267\n",
      "radius: 1 atom list: [24, 25, 21, 26, 23] activation 0.03139179413083785\n",
      "radius: 1 atom list: [23, 26, 24, 25, 22] activation 0.03134743595417343\n",
      "radius: 1 atom list: [13, 14, 9, 12, 15] activation 0.031193257215501036\n",
      "radius: 1 atom list: [5, 9, 7, 6, 8] activation 0.031158578306041344\n",
      "radius: 1 atom list: [5, 7, 9, 8, 13] activation 0.03084716851126793\n",
      "FP 14 has linear regression coefficient 0.0575703680327\n",
      "radius: 2 atom list: [9, 13, 23, 10, 20, 11, 12, 21, 22, 7, 8] activation 0.05056686546555089\n",
      "radius: 2 atom list: [12, 4, 5, 16, 0, 2, 3, 11, 1, 6, 10] activation 0.05037610292550138\n",
      "radius: 2 atom list: [3, 7, 18, 11, 13, 4, 19, 6, 12, 5] activation 0.04973296801447419\n",
      "radius: 2 atom list: [7, 12, 13, 11, 4, 5, 14, 3, 6, 17] activation 0.04972884403907439\n",
      "radius: 2 atom list: [12, 13, 14, 4, 7, 15, 5, 6, 8, 11] activation 0.04965392838216362\n",
      "radius: 2 atom list: [6, 7, 8, 15, 21, 5, 9, 14, 16, 1] activation 0.04961880645293649\n",
      "radius: 2 atom list: [7, 12, 13, 6, 4, 3, 5] activation 0.04957913096206039\n",
      "radius: 2 atom list: [5, 10, 11, 13, 14, 8, 7, 6, 9] activation 0.04950814655263624\n",
      "radius: 2 atom list: [6, 8, 9, 13, 4, 7, 5] activation 0.049470942165104975\n",
      "radius: 2 atom list: [13, 19, 7, 9, 8, 12, 14, 11, 10] activation 0.04942523975569057\n",
      "radius: 2 atom list: [11, 16, 3, 10, 12, 17, 0, 2, 18, 1] activation 0.04941346062566719\n",
      "FP 15 has linear regression coefficient -0.0212275892141\n",
      "radius: 0 atom list: [11] activation 0.024944688582271418\n",
      "radius: 0 atom list: [0] activation 0.024877252019098416\n",
      "radius: 0 atom list: [13] activation 0.02370208142240204\n",
      "radius: 0 atom list: [10] activation 0.02361527013827435\n",
      "radius: 0 atom list: [3] activation 0.02320596380261627\n",
      "radius: 0 atom list: [0] activation 0.022619933084951447\n",
      "radius: 0 atom list: [7] activation 0.02193303963428671\n",
      "radius: 0 atom list: [6] activation 0.021916105118086757\n",
      "radius: 0 atom list: [6] activation 0.021862416613086647\n",
      "radius: 0 atom list: [9] activation 0.02146657780682299\n",
      "radius: 0 atom list: [7] activation 0.020945254084287736\n",
      "FP 16 has linear regression coefficient 0.0323416390483\n",
      "radius: 1 atom list: [5, 9, 7, 6, 8] activation 0.034647202643140154\n",
      "radius: 1 atom list: [8, 9, 10, 7, 11] activation 0.03415405904815869\n",
      "radius: 1 atom list: [0, 1, 2, 3, 4] activation 0.033802857921532745\n",
      "radius: 1 atom list: [6, 4, 10, 5, 3] activation 0.03370176990766032\n",
      "radius: 1 atom list: [9, 10, 8, 11, 12] activation 0.03352939098408158\n",
      "radius: 1 atom list: [9, 7, 10, 6, 8] activation 0.033522813163924706\n",
      "radius: 1 atom list: [13, 14, 9, 12, 15] activation 0.03338237780046079\n",
      "radius: 1 atom list: [22, 25, 20, 24, 23] activation 0.033288113158551826\n",
      "radius: 1 atom list: [10, 6, 7, 8, 9] activation 0.033266991600314454\n",
      "radius: 1 atom list: [15, 16, 17, 13, 14] activation 0.033090307995786016\n",
      "radius: 1 atom list: [24, 25, 21, 26, 23] activation 0.03304747993670922\n",
      "FP 17 has linear regression coefficient 0.0253523738298\n",
      "radius: 1 atom list: [8, 9, 10, 7, 11] activation 0.033843384771235156\n",
      "radius: 1 atom list: [0, 1, 2, 3, 4] activation 0.033719549949795175\n",
      "radius: 1 atom list: [9, 10, 8, 11, 12] activation 0.03357562255006635\n",
      "radius: 1 atom list: [9, 7, 10, 6, 8] activation 0.03355036307394666\n",
      "radius: 1 atom list: [10, 6, 7, 8, 9] activation 0.03329204114263036\n",
      "radius: 1 atom list: [6, 4, 10, 5, 3] activation 0.033267311105709796\n",
      "radius: 1 atom list: [24, 25, 21, 26, 23] activation 0.03301824303428097\n",
      "radius: 1 atom list: [23, 26, 24, 25, 22] activation 0.03295063221854621\n",
      "radius: 1 atom list: [13, 14, 9, 12, 15] activation 0.03271628891596223\n",
      "radius: 1 atom list: [5, 9, 7, 6, 8] activation 0.03268000553235118\n",
      "radius: 1 atom list: [22, 25, 20, 24, 23] activation 0.03252336719231937\n",
      "FP 18 has linear regression coefficient 0.0480343467958\n",
      "radius: 2 atom list: [7, 12, 13, 11, 4, 5, 14, 3, 6, 17] activation 0.04128632181397082\n",
      "radius: 2 atom list: [7, 12, 13, 6, 4, 3, 5] activation 0.04119618316869383\n",
      "radius: 2 atom list: [5, 7, 3, 4, 6, 13, 14] activation 0.04112592303357942\n",
      "radius: 2 atom list: [9, 13, 23, 10, 20, 11, 12, 21, 22, 7, 8] activation 0.041060222666112714\n",
      "radius: 2 atom list: [6, 8, 9, 13, 4, 7, 5] activation 0.04101321627267829\n",
      "radius: 2 atom list: [10, 16, 18, 17, 6, 8, 7, 9] activation 0.04092860206770849\n",
      "radius: 2 atom list: [5, 12, 4, 6, 11, 2, 3] activation 0.04088173291616889\n",
      "radius: 2 atom list: [4, 6, 7, 3, 5, 17, 16] activation 0.04088173291616887\n",
      "radius: 2 atom list: [12, 13, 14, 4, 7, 15, 5, 6, 8, 11] activation 0.040879865125206874\n",
      "radius: 2 atom list: [9, 10, 11, 4, 8, 2, 5, 6, 7, 12] activation 0.04086408651312983\n",
      "radius: 2 atom list: [9, 7, 8, 10, 15, 11, 5] activation 0.04085086496599003\n",
      "FP 19 has linear regression coefficient -0.122634694938\n",
      "radius: 4 atom list: [14, 11, 12, 13, 15, 16, 17, 18] activation 0.9981867086794007\n",
      "radius: 4 atom list: [14, 11, 12, 13, 15, 16, 17, 18] activation 0.9980553920009568\n",
      "radius: 4 atom list: [14, 10, 11, 12, 13, 15, 16, 17, 18] activation 0.9968561981366786\n",
      "radius: 4 atom list: [14, 9, 10, 11, 12, 13, 15, 16, 17, 18] activation 0.9866250533886056\n",
      "radius: 4 atom list: [5, 6, 8, 1, 3, 9, 15, 4, 2, 7, 10] activation 0.9844019841153776\n",
      "radius: 4 atom list: [11, 7, 10, 16, 12, 13, 15, 8, 9, 14, 17] activation 0.9843652047042545\n",
      "radius: 4 atom list: [20, 27, 23, 26, 19, 21, 25, 30, 28, 29, 31] activation 0.9842705636912388\n",
      "radius: 4 atom list: [15, 13, 17, 16, 21, 23, 19, 20, 22, 18] activation 0.9838846467353167\n",
      "radius: 4 atom list: [15, 13, 14, 19, 21, 17, 18, 20, 22, 16] activation 0.983846606754164\n",
      "radius: 4 atom list: [9, 10, 11, 15, 13, 14, 16, 12] activation 0.9836104522293924\n",
      "radius: 4 atom list: [10, 11, 15, 13, 14, 16, 12] activation 0.9739933129304107\n",
      "FP 20 has linear regression coefficient -0.06535480278\n",
      "radius: 2 atom list: [14, 12, 13, 15, 16, 17, 18] activation 0.06932073031901576\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.06810864621775291\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.06614848386295452\n",
      "radius: 2 atom list: [8, 7, 4] activation 0.06492034146944876\n",
      "radius: 2 atom list: [2, 1, 3] activation 0.06370983480511103\n",
      "radius: 3 atom list: [4, 1, 0, 2, 3, 6, 5, 7] activation 0.06318982707583577\n",
      "radius: 3 atom list: [4, 7, 5, 1, 0, 2, 3, 6] activation 0.06310708724241709\n",
      "radius: 3 atom list: [19, 20, 22, 23, 18, 21, 17, 14, 15, 16] activation 0.06310185860371072\n",
      "radius: 3 atom list: [4, 5, 6, 0, 2, 3, 1] activation 0.06301799733015512\n",
      "radius: 3 atom list: [23, 5, 4, 6, 0, 2, 3, 1] activation 0.06296639236953552\n",
      "radius: 3 atom list: [5, 10, 7, 6, 9, 4, 8, 3] activation 0.06295289833303559\n",
      "FP 21 has linear regression coefficient 0.0687183487708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 3 atom list: [10, 16, 19, 20, 8, 9, 18, 4, 6, 5, 7, 17] activation 0.0849411727786653\n",
      "radius: 3 atom list: [9, 11, 12, 14, 15, 17, 21, 3, 6, 8, 10, 16, 7] activation 0.08435447671686198\n",
      "radius: 3 atom list: [7, 14, 20, 3, 6, 8, 9, 11, 12, 13, 4, 5] activation 0.08361949514410132\n",
      "radius: 3 atom list: [2, 4, 6, 7, 9, 10, 11, 12, 3, 5, 1] activation 0.08354818134658865\n",
      "radius: 3 atom list: [0, 4, 6, 8, 9, 10, 2, 1, 3, 5, 11, 12, 7] activation 0.08191483180159329\n",
      "radius: 3 atom list: [7, 9, 11, 12, 15, 16, 5, 6, 8, 10, 17, 18] activation 0.08157555730714765\n",
      "radius: 3 atom list: [19, 20, 21, 4, 6, 8, 14, 2, 3, 5, 7] activation 0.0812399727353034\n",
      "radius: 3 atom list: [3, 5, 1, 2, 4, 6, 7, 9, 10, 11, 12] activation 0.08091872902924693\n",
      "radius: 3 atom list: [1, 3, 9, 11, 8, 10, 12, 13, 16, 17, 18] activation 0.08025159260604021\n",
      "radius: 3 atom list: [11, 3, 5, 6, 0, 8, 9, 10, 1, 2, 4, 12] activation 0.07991429444545813\n",
      "radius: 3 atom list: [9, 11, 15, 17, 19, 21, 5, 10, 7, 18, 20, 2, 4, 6, 8] activation 0.0799064673752011\n",
      "FP 22 has linear regression coefficient 0.0426026528745\n",
      "radius: 2 atom list: [6, 9, 11, 7, 3, 5, 13, 8] activation 0.035384901776178694\n",
      "radius: 2 atom list: [8, 10, 3, 7, 6, 4, 2, 5] activation 0.035284672945278914\n",
      "radius: 2 atom list: [1, 2, 3, 4, 5] activation 0.03497198460007913\n",
      "radius: 2 atom list: [8, 7, 1, 4, 6, 11, 9, 10] activation 0.03495914713644013\n",
      "radius: 2 atom list: [1, 2, 3, 4, 0] activation 0.03479754155114809\n",
      "radius: 2 atom list: [20, 3, 5, 6, 7, 8, 12, 21, 23] activation 0.034752570415312885\n",
      "radius: 2 atom list: [8, 10, 12, 18, 17, 19, 9, 11] activation 0.03475241945672638\n",
      "radius: 2 atom list: [16, 23, 13, 17, 9, 11, 12, 14, 15] activation 0.03474590034167461\n",
      "radius: 2 atom list: [4, 5, 3, 8, 6, 2, 14, 23, 24, 25, 7] activation 0.0347430592245696\n",
      "radius: 2 atom list: [6, 3, 5, 8, 7, 9, 10, 4] activation 0.034742254843115064\n",
      "radius: 2 atom list: [3, 5, 7, 21, 20, 4, 6] activation 0.034740988726560555\n",
      "FP 23 has linear regression coefficient 0.0649403969277\n",
      "radius: 3 atom list: [10, 16, 19, 20, 8, 9, 18, 4, 6, 5, 7, 17] activation 0.06309851211881888\n",
      "radius: 3 atom list: [9, 11, 12, 14, 15, 17, 21, 3, 6, 8, 10, 16, 7] activation 0.06309243346156015\n",
      "radius: 3 atom list: [7, 14, 20, 3, 6, 8, 9, 11, 12, 13, 4, 5] activation 0.06302995015821325\n",
      "radius: 3 atom list: [2, 4, 6, 7, 9, 10, 11, 12, 3, 5, 1] activation 0.06292648531544894\n",
      "radius: 3 atom list: [7, 9, 11, 12, 15, 16, 5, 6, 8, 10, 17, 18] activation 0.06266187542850378\n",
      "radius: 3 atom list: [0, 4, 6, 8, 9, 10, 2, 1, 3, 5, 11, 12, 7] activation 0.06260236637297618\n",
      "radius: 3 atom list: [19, 20, 21, 4, 6, 8, 14, 2, 3, 5, 7] activation 0.06257978259755928\n",
      "radius: 3 atom list: [3, 5, 1, 2, 4, 6, 7, 9, 10, 11, 12] activation 0.06244567284096406\n",
      "radius: 3 atom list: [2, 6, 9, 11, 14, 18, 3, 5, 13, 10, 12, 7, 8, 4] activation 0.062443184172459455\n",
      "radius: 3 atom list: [6, 7, 10, 12, 14, 17, 20, 9, 11, 13, 19, 21] activation 0.062310197899430125\n",
      "radius: 3 atom list: [1, 3, 9, 11, 8, 10, 12, 13, 16, 17, 18] activation 0.06230978711761698\n",
      "FP 24 has linear regression coefficient -0.0625328350693\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.05448389895981286\n",
      "radius: 2 atom list: [14, 12, 13, 15, 16, 17, 18] activation 0.05411203324622559\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.05375671647780904\n",
      "radius: 2 atom list: [8, 7, 4] activation 0.05300770193251749\n",
      "radius: 3 atom list: [0, 2, 3, 4, 1] activation 0.05265178755513369\n",
      "radius: 3 atom list: [3, 1, 6, 0, 2, 4, 5, 7, 8] activation 0.0526314429673\n",
      "radius: 3 atom list: [3, 5, 2, 8, 6, 0, 1, 4, 7] activation 0.052565235213101205\n",
      "radius: 3 atom list: [4, 1, 0, 2, 3] activation 0.05253582060769745\n",
      "radius: 3 atom list: [45, 46, 47, 48, 41, 43, 44] activation 0.05252654293947903\n",
      "radius: 3 atom list: [8, 1, 5, 0, 2, 3, 6, 7, 4] activation 0.052498537008773716\n",
      "radius: 3 atom list: [17, 13, 14, 18, 10, 12, 19, 20, 15, 16] activation 0.05249754575747592\n",
      "FP 25 has linear regression coefficient 0.0232453194626\n",
      "radius: 1 atom list: [8, 9, 10, 7, 11] activation 0.0323755351728413\n",
      "radius: 1 atom list: [0, 1, 2, 3, 4] activation 0.03205263617113562\n",
      "radius: 1 atom list: [9, 10, 8, 11, 12] activation 0.031843293505424426\n",
      "radius: 1 atom list: [9, 7, 10, 6, 8] activation 0.031832218327736264\n",
      "radius: 1 atom list: [10, 6, 7, 8, 9] activation 0.03162698316794895\n",
      "radius: 1 atom list: [5, 9, 7, 6, 8] activation 0.03158377831321064\n",
      "radius: 1 atom list: [24, 25, 21, 26, 23] activation 0.03138696744740163\n",
      "radius: 1 atom list: [13, 14, 9, 12, 15] activation 0.03128389706153796\n",
      "radius: 1 atom list: [23, 26, 24, 25, 22] activation 0.031254982418152825\n",
      "radius: 1 atom list: [6, 4, 10, 5, 3] activation 0.03123468724793883\n",
      "radius: 1 atom list: [22, 25, 20, 24, 23] activation 0.031222065576346426\n",
      "FP 26 has linear regression coefficient 0.0831949096743\n",
      "radius: 3 atom list: [10, 16, 19, 20, 8, 9, 18, 4, 6, 5, 7, 17] activation 0.12606651915815548\n",
      "radius: 3 atom list: [9, 11, 12, 14, 15, 17, 21, 3, 6, 8, 10, 16, 7] activation 0.12455071282727634\n",
      "radius: 3 atom list: [7, 14, 20, 3, 6, 8, 9, 11, 12, 13, 4, 5] activation 0.12232593947469546\n",
      "radius: 3 atom list: [2, 4, 6, 7, 9, 10, 11, 12, 3, 5, 1] activation 0.12176072294982758\n",
      "radius: 3 atom list: [0, 4, 6, 8, 9, 10, 2, 1, 3, 5, 11, 12, 7] activation 0.11763665310381201\n",
      "radius: 3 atom list: [7, 9, 11, 12, 15, 16, 5, 6, 8, 10, 17, 18] activation 0.11670014668672613\n",
      "radius: 3 atom list: [19, 20, 21, 4, 6, 8, 14, 2, 3, 5, 7] activation 0.11581777192148217\n",
      "radius: 3 atom list: [3, 5, 1, 2, 4, 6, 7, 9, 10, 11, 12] activation 0.11495206241091224\n",
      "radius: 3 atom list: [2, 6, 9, 11, 14, 18, 3, 5, 13, 10, 12, 7, 8, 4] activation 0.11374109508953303\n",
      "radius: 3 atom list: [1, 3, 9, 11, 8, 10, 12, 13, 16, 17, 18] activation 0.11328566302798361\n",
      "radius: 3 atom list: [11, 3, 5, 6, 0, 8, 9, 10, 1, 2, 4, 12] activation 0.11282415392518867\n",
      "FP 27 has linear regression coefficient 0.0578790324636\n",
      "radius: 2 atom list: [9, 13, 23, 10, 20, 11, 12, 21, 22, 7, 8] activation 0.053627088916444236\n",
      "radius: 2 atom list: [12, 4, 5, 16, 0, 2, 3, 11, 1, 6, 10] activation 0.053560624808497395\n",
      "radius: 2 atom list: [7, 12, 13, 11, 4, 5, 14, 3, 6, 17] activation 0.052720904530834924\n",
      "radius: 2 atom list: [3, 7, 18, 11, 13, 4, 19, 6, 12, 5] activation 0.05272021713299783\n",
      "radius: 2 atom list: [6, 7, 8, 15, 21, 5, 9, 14, 16, 1] activation 0.052550270001531146\n",
      "radius: 2 atom list: [7, 12, 13, 6, 4, 3, 5] activation 0.052542690037946614\n",
      "radius: 2 atom list: [12, 13, 14, 4, 7, 15, 5, 6, 8, 11] activation 0.05253951277745779\n",
      "radius: 2 atom list: [5, 10, 11, 13, 14, 8, 7, 6, 9] activation 0.052350359530720675\n",
      "radius: 2 atom list: [6, 8, 9, 13, 4, 7, 5] activation 0.05232854687556819\n",
      "radius: 2 atom list: [11, 16, 3, 10, 12, 17, 0, 2, 18, 1] activation 0.0523037593145325\n",
      "radius: 2 atom list: [13, 19, 7, 9, 8, 12, 14, 11, 10] activation 0.05228444483167306\n",
      "FP 28 has linear regression coefficient -0.0556452745775\n",
      "radius: 2 atom list: [14, 12, 13, 15, 16, 17, 18] activation 0.05586012865456181\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.055097318656126305\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.05414711745754479\n",
      "radius: 2 atom list: [8, 7, 4] activation 0.05370667344326604\n",
      "radius: 2 atom list: [2, 1, 3] activation 0.0530826627067632\n",
      "radius: 2 atom list: [13, 14, 15, 16] activation 0.05191079090707128\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.05180573082194431\n",
      "radius: 2 atom list: [2, 1, 0] activation 0.05165449750238446\n",
      "radius: 2 atom list: [3, 1, 0, 2] activation 0.05080741561777926\n",
      "radius: 2 atom list: [3, 1, 4] activation 0.050551902393211494\n",
      "radius: 2 atom list: [1, 3, 0, 2] activation 0.0502232644679713\n",
      "FP 29 has linear regression coefficient 0.0575822195239\n",
      "radius: 2 atom list: [9, 13, 23, 10, 20, 11, 12, 21, 22, 7, 8] activation 0.05450172093957621\n",
      "radius: 2 atom list: [12, 4, 5, 16, 0, 2, 3, 11, 1, 6, 10] activation 0.05442342056572922\n",
      "radius: 2 atom list: [3, 7, 18, 11, 13, 4, 19, 6, 12, 5] activation 0.053498626970629\n",
      "radius: 2 atom list: [7, 12, 13, 11, 4, 5, 14, 3, 6, 17] activation 0.05347217556727336\n",
      "radius: 2 atom list: [12, 13, 14, 4, 7, 15, 5, 6, 8, 11] activation 0.05334446087294537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 2 atom list: [6, 7, 8, 15, 21, 5, 9, 14, 16, 1] activation 0.053324467842339025\n",
      "radius: 2 atom list: [7, 12, 13, 6, 4, 3, 5] activation 0.05328110202248333\n",
      "radius: 2 atom list: [5, 10, 11, 13, 14, 8, 7, 6, 9] activation 0.053106212552841384\n",
      "radius: 2 atom list: [6, 8, 9, 13, 4, 7, 5] activation 0.053084980724179595\n",
      "radius: 2 atom list: [11, 16, 3, 10, 12, 17, 0, 2, 18, 1] activation 0.05305192895630075\n",
      "radius: 2 atom list: [13, 19, 7, 9, 8, 12, 14, 11, 10] activation 0.05304968345808279\n",
      "FP 30 has linear regression coefficient -0.0718351623914\n",
      "radius: 3 atom list: [14, 10, 11, 12, 13, 15, 16, 17, 18] activation 0.07732746726833285\n",
      "radius: 3 atom list: [3, 6, 1, 10, 0, 2, 4, 5] activation 0.0773269579582674\n",
      "radius: 3 atom list: [3, 0, 1, 2, 4] activation 0.07705080249806932\n",
      "radius: 3 atom list: [1, 2, 0, 3, 4] activation 0.07701022751699867\n",
      "radius: 3 atom list: [4, 1, 3, 0, 2] activation 0.07698327360118123\n",
      "radius: 3 atom list: [0, 3, 5, 1, 4, 2] activation 0.0769558945201774\n",
      "radius: 3 atom list: [4, 7, 6, 8, 5, 3] activation 0.07694405997269081\n",
      "radius: 3 atom list: [2, 1, 4, 5, 0, 3] activation 0.07692760112866202\n",
      "radius: 3 atom list: [1, 4, 5, 0, 2, 3] activation 0.0768914474046658\n",
      "radius: 3 atom list: [0, 3, 2, 5, 1, 4] activation 0.0768791292541618\n",
      "radius: 3 atom list: [2, 0, 1, 4, 3, 5] activation 0.0768584081132434\n",
      "FP 31 has linear regression coefficient -0.0357492922983\n",
      "radius: 1 atom list: [14, 13, 15, 16, 17, 18] activation 0.04277378687678375\n",
      "radius: 1 atom list: [8, 7] activation 0.03985804719931326\n",
      "radius: 1 atom list: [11, 8] activation 0.03854179383320732\n",
      "radius: 1 atom list: [13, 16, 15, 14] activation 0.03842822627248792\n",
      "radius: 1 atom list: [11, 10] activation 0.0383083156656297\n",
      "radius: 1 atom list: [3, 1] activation 0.037984608000562664\n",
      "radius: 1 atom list: [11, 10] activation 0.03780687736099313\n",
      "radius: 1 atom list: [2, 1] activation 0.03779928960279418\n",
      "radius: 1 atom list: [17, 18] activation 0.03756269183227453\n",
      "radius: 1 atom list: [0, 1] activation 0.03730370459420029\n",
      "radius: 1 atom list: [17, 16] activation 0.037299876175672014\n",
      "FP 32 has linear regression coefficient 0.118184429721\n",
      "radius: 4 atom list: [4, 5, 6, 7, 8, 9, 10, 11, 12] activation 0.8073856117972321\n",
      "radius: 4 atom list: [1, 2, 3, 4, 5, 6, 7, 8, 9] activation 0.8027103610853352\n",
      "radius: 4 atom list: [7, 8, 0, 1, 2, 3, 4, 5, 6] activation 0.8001745692480174\n",
      "radius: 4 atom list: [9, 10, 11, 12, 13, 14, 15, 16, 17] activation 0.798730316184796\n",
      "radius: 4 atom list: [5, 6, 7, 8, 9, 1, 2, 3, 4] activation 0.7983028693969767\n",
      "radius: 4 atom list: [0, 8, 1, 2, 3, 4, 5, 6, 7] activation 0.7976080669209621\n",
      "radius: 4 atom list: [1, 2, 3, 4, 5, 6, 7, 8, 0] activation 0.7961463584122259\n",
      "radius: 4 atom list: [1, 2, 3, 4, 5, 0, 7, 6, 8] activation 0.7957137394250918\n",
      "radius: 4 atom list: [2, 3, 4, 5, 6, 7, 8, 9, 10] activation 0.789564226037721\n",
      "radius: 4 atom list: [1, 2, 3, 4, 5, 6, 7, 8, 9] activation 0.7887356527081403\n",
      "radius: 4 atom list: [5, 6, 7, 8, 0, 1, 2, 3, 4] activation 0.7868702935325693\n",
      "FP 33 has linear regression coefficient 0.0492385676828\n",
      "radius: 2 atom list: [9, 13, 23, 10, 20, 11, 12, 21, 22, 7, 8] activation 0.04541452513604053\n",
      "radius: 2 atom list: [7, 12, 13, 11, 4, 5, 14, 3, 6, 17] activation 0.04521732013748105\n",
      "radius: 2 atom list: [12, 4, 5, 16, 0, 2, 3, 11, 1, 6, 10] activation 0.045121047881272636\n",
      "radius: 2 atom list: [7, 12, 13, 6, 4, 3, 5] activation 0.04505893625096244\n",
      "radius: 2 atom list: [6, 8, 9, 13, 4, 7, 5] activation 0.04491225196732037\n",
      "radius: 2 atom list: [12, 13, 14, 4, 7, 15, 5, 6, 8, 11] activation 0.04488910704938596\n",
      "radius: 2 atom list: [13, 19, 7, 9, 8, 12, 14, 11, 10] activation 0.044884126320277704\n",
      "radius: 2 atom list: [11, 16, 3, 10, 12, 17, 0, 2, 18, 1] activation 0.04485381894215379\n",
      "radius: 2 atom list: [3, 7, 18, 11, 13, 4, 19, 6, 12, 5] activation 0.044822884729214876\n",
      "radius: 2 atom list: [9, 10, 11, 4, 8, 2, 5, 6, 7, 12] activation 0.04481421979844984\n",
      "radius: 2 atom list: [9, 7, 8, 10, 15, 11, 5] activation 0.04479718351081145\n",
      "FP 34 has linear regression coefficient -0.0668469805566\n",
      "radius: 3 atom list: [1, 3, 0, 2, 4] activation 0.06561657359203793\n",
      "radius: 3 atom list: [0, 2, 3, 4, 1] activation 0.06561571392509243\n",
      "radius: 3 atom list: [1, 2, 0, 3, 4] activation 0.06561535033932105\n",
      "radius: 3 atom list: [19, 20, 21, 17] activation 0.06561342209389562\n",
      "radius: 3 atom list: [0, 1, 2, 3] activation 0.06561090687580151\n",
      "radius: 3 atom list: [1, 4, 3, 0, 2] activation 0.06560962783800575\n",
      "radius: 3 atom list: [5, 4, 6, 7, 2, 3] activation 0.06560836252618929\n",
      "radius: 3 atom list: [9, 11, 8, 10, 6] activation 0.0656066774857457\n",
      "radius: 3 atom list: [2, 4, 1, 3, 0] activation 0.06560526518784328\n",
      "radius: 3 atom list: [11, 9, 10, 12, 13] activation 0.06560116942693052\n",
      "radius: 3 atom list: [3, 7, 6, 8, 5] activation 0.06559982414497408\n",
      "FP 35 has linear regression coefficient 0.0690414516902\n",
      "radius: 3 atom list: [10, 16, 19, 20, 8, 9, 18, 4, 6, 5, 7, 17] activation 0.07537413198479909\n",
      "radius: 3 atom list: [9, 11, 12, 14, 15, 17, 21, 3, 6, 8, 10, 16, 7] activation 0.07500961987688809\n",
      "radius: 3 atom list: [7, 14, 20, 3, 6, 8, 9, 11, 12, 13, 4, 5] activation 0.07456329217902535\n",
      "radius: 3 atom list: [2, 4, 6, 7, 9, 10, 11, 12, 3, 5, 1] activation 0.07451999760442166\n",
      "radius: 3 atom list: [0, 4, 6, 8, 9, 10, 2, 1, 3, 5, 11, 12, 7] activation 0.07342102050663774\n",
      "radius: 3 atom list: [7, 9, 11, 12, 15, 16, 5, 6, 8, 10, 17, 18] activation 0.07323035556578807\n",
      "radius: 3 atom list: [19, 20, 21, 4, 6, 8, 14, 2, 3, 5, 7] activation 0.07300536005125928\n",
      "radius: 3 atom list: [3, 5, 1, 2, 4, 6, 7, 9, 10, 11, 12] activation 0.07278030116337282\n",
      "radius: 3 atom list: [1, 3, 9, 11, 8, 10, 12, 13, 16, 17, 18] activation 0.07232448068990255\n",
      "radius: 3 atom list: [9, 11, 15, 17, 19, 21, 5, 10, 7, 18, 20, 2, 4, 6, 8] activation 0.0721287139243172\n",
      "radius: 3 atom list: [11, 3, 5, 6, 0, 8, 9, 10, 1, 2, 4, 12] activation 0.07208257083353203\n",
      "FP 36 has linear regression coefficient 0.0555859224761\n",
      "radius: 2 atom list: [12, 4, 5, 16, 0, 2, 3, 11, 1, 6, 10] activation 0.05223930267981932\n",
      "radius: 2 atom list: [9, 13, 23, 10, 20, 11, 12, 21, 22, 7, 8] activation 0.05205133355498664\n",
      "radius: 2 atom list: [3, 7, 18, 11, 13, 4, 19, 6, 12, 5] activation 0.051519357482182164\n",
      "radius: 2 atom list: [6, 7, 8, 15, 21, 5, 9, 14, 16, 1] activation 0.05133576461728671\n",
      "radius: 2 atom list: [12, 13, 14, 4, 7, 15, 5, 6, 8, 11] activation 0.05108725199493274\n",
      "radius: 2 atom list: [5, 10, 11, 13, 14, 8, 7, 6, 9] activation 0.0510334575230244\n",
      "radius: 2 atom list: [7, 8, 14, 6, 9, 19, 13, 15, 4] activation 0.050901896964889025\n",
      "radius: 2 atom list: [1, 4, 3, 6, 14, 13, 5, 15, 24] activation 0.05084946031412774\n",
      "radius: 2 atom list: [11, 16, 3, 10, 12, 17, 0, 2, 18, 1] activation 0.050826037955641515\n",
      "radius: 2 atom list: [7, 12, 13, 11, 4, 5, 14, 3, 6, 17] activation 0.050819756984111335\n",
      "radius: 2 atom list: [13, 19, 7, 9, 8, 12, 14, 11, 10] activation 0.05076911152821743\n",
      "FP 37 has linear regression coefficient 0.0194585160086\n",
      "radius: 1 atom list: [8, 9, 10, 7, 11] activation 0.030275908128097762\n",
      "radius: 1 atom list: [0, 1, 2, 3, 4] activation 0.029869616491136362\n",
      "radius: 1 atom list: [5, 9, 7, 6, 8] activation 0.0298529816082596\n",
      "radius: 1 atom list: [9, 10, 8, 11, 12] activation 0.02984232728492575\n",
      "radius: 1 atom list: [9, 7, 10, 6, 8] activation 0.029803068979033252\n",
      "radius: 1 atom list: [6, 4, 10, 5, 3] activation 0.02966708502450598\n",
      "radius: 1 atom list: [10, 6, 7, 8, 9] activation 0.029655551299484183\n",
      "radius: 1 atom list: [24, 25, 21, 26, 23] activation 0.029448045512559144\n",
      "radius: 1 atom list: [23, 26, 24, 25, 22] activation 0.029379209869906643\n",
      "radius: 1 atom list: [13, 14, 9, 12, 15] activation 0.029180495911927264\n",
      "radius: 1 atom list: [10, 3, 6, 8, 9] activation 0.02902298783512451\n",
      "FP 38 has linear regression coefficient -0.057581851093\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.04583571046251802\n",
      "radius: 2 atom list: [14, 13, 15, 16, 17, 18] activation 0.04566841823195026\n",
      "radius: 2 atom list: [13, 14, 15, 16] activation 0.04513272321604517\n",
      "radius: 2 atom list: [8, 7, 4] activation 0.0450183587621007\n",
      "radius: 2 atom list: [2, 1, 3] activation 0.04490772730208873\n",
      "radius: 2 atom list: [3, 1, 0, 2] activation 0.04458436164829741\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.04457021354075628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 2 atom list: [2, 1, 0] activation 0.04451628933102522\n",
      "radius: 2 atom list: [1, 3, 0, 2] activation 0.044374992641212724\n",
      "radius: 2 atom list: [3, 1, 4] activation 0.04435625112774728\n",
      "radius: 2 atom list: [16, 4, 18, 17] activation 0.044189732745716935\n",
      "FP 39 has linear regression coefficient -0.0307688154402\n",
      "radius: 1 atom list: [8, 7] activation 0.030352463152853632\n",
      "radius: 1 atom list: [11, 10] activation 0.030099614390976505\n",
      "radius: 1 atom list: [11, 8] activation 0.030065558026274655\n",
      "radius: 1 atom list: [17, 18] activation 0.029909483444331118\n",
      "radius: 1 atom list: [11, 10] activation 0.02990192857032803\n",
      "radius: 1 atom list: [3, 1] activation 0.029901403433909617\n",
      "radius: 1 atom list: [7, 5] activation 0.029884273806504588\n",
      "radius: 1 atom list: [0, 1] activation 0.029812183476763877\n",
      "radius: 1 atom list: [25, 26] activation 0.029784587275333223\n",
      "radius: 1 atom list: [2, 1] activation 0.029716495042535176\n",
      "radius: 1 atom list: [6, 5] activation 0.029704991267430897\n",
      "FP 40 has linear regression coefficient -0.0424875152327\n",
      "radius: 1 atom list: [14, 13, 15, 16, 17, 18] activation 0.04312041273008159\n",
      "radius: 1 atom list: [8, 7] activation 0.03940480615130064\n",
      "radius: 1 atom list: [13, 16, 15, 14] activation 0.0381773413110797\n",
      "radius: 1 atom list: [11, 8] activation 0.038124714902070915\n",
      "radius: 1 atom list: [11, 10] activation 0.037700623823593044\n",
      "radius: 1 atom list: [3, 1] activation 0.03750135645615197\n",
      "radius: 1 atom list: [2, 1] activation 0.037268361049833554\n",
      "radius: 1 atom list: [11, 10] activation 0.036984528740472755\n",
      "radius: 1 atom list: [17, 18] activation 0.03686329375575145\n",
      "radius: 1 atom list: [17, 16] activation 0.036823520213803376\n",
      "radius: 1 atom list: [14, 13] activation 0.036650481240307904\n",
      "FP 41 has linear regression coefficient -0.0398772772661\n",
      "radius: 1 atom list: [14, 13, 15, 16, 17, 18] activation 0.03925218940584191\n",
      "radius: 1 atom list: [8, 7] activation 0.03747903778801386\n",
      "radius: 1 atom list: [11, 8] activation 0.036358169542159896\n",
      "radius: 1 atom list: [11, 10] activation 0.03611988555634732\n",
      "radius: 1 atom list: [3, 1] activation 0.03592739560511773\n",
      "radius: 1 atom list: [11, 10] activation 0.035853791080469676\n",
      "radius: 1 atom list: [17, 18] activation 0.03573370858431115\n",
      "radius: 1 atom list: [2, 1] activation 0.03570157810182723\n",
      "radius: 1 atom list: [13, 16, 15, 14] activation 0.03554605896143687\n",
      "radius: 1 atom list: [0, 1] activation 0.03546756682614961\n",
      "radius: 1 atom list: [7, 5] activation 0.035426966342482366\n",
      "FP 42 has linear regression coefficient 0.0630924609542\n",
      "radius: 3 atom list: [9, 11, 12, 14, 15, 17, 21, 3, 6, 8, 10, 16, 7] activation 0.062066480316858984\n",
      "radius: 3 atom list: [10, 16, 19, 20, 8, 9, 18, 4, 6, 5, 7, 17] activation 0.062019672039439505\n",
      "radius: 3 atom list: [7, 14, 20, 3, 6, 8, 9, 11, 12, 13, 4, 5] activation 0.061952061832201505\n",
      "radius: 3 atom list: [2, 4, 6, 7, 9, 10, 11, 12, 3, 5, 1] activation 0.06179060669838716\n",
      "radius: 3 atom list: [2, 6, 9, 11, 14, 18, 3, 5, 13, 10, 12, 7, 8, 4] activation 0.06176724487764521\n",
      "radius: 3 atom list: [0, 4, 6, 8, 9, 10, 2, 1, 3, 5, 11, 12, 7] activation 0.061709549706041034\n",
      "radius: 3 atom list: [7, 9, 11, 12, 15, 16, 5, 6, 8, 10, 17, 18] activation 0.061529561984759396\n",
      "radius: 3 atom list: [2, 3, 4, 5, 6, 7, 1] activation 0.06149882307538006\n",
      "radius: 3 atom list: [4, 8, 10, 1, 2, 5, 7, 3, 6, 13, 11, 12] activation 0.06146582858840693\n",
      "radius: 3 atom list: [19, 20, 21, 4, 6, 8, 14, 2, 3, 5, 7] activation 0.06143691754026209\n",
      "radius: 3 atom list: [3, 5, 1, 2, 4, 6, 7, 9, 10, 11, 12] activation 0.06130998105062139\n",
      "FP 43 has linear regression coefficient -0.045484089696\n",
      "radius: 1 atom list: [14, 13, 15, 16, 17, 18] activation 0.042051551367489955\n",
      "radius: 2 atom list: [2, 1, 0] activation 0.040550699218873346\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.040541981091002245\n",
      "radius: 2 atom list: [2, 1, 3] activation 0.04039649498614913\n",
      "radius: 2 atom list: [13, 15, 16, 14, 12] activation 0.040280786301638054\n",
      "radius: 2 atom list: [3, 1, 4] activation 0.04026923615754385\n",
      "radius: 2 atom list: [8, 7, 4] activation 0.040244067243655965\n",
      "radius: 2 atom list: [3, 1, 0, 2] activation 0.04015561842611698\n",
      "radius: 2 atom list: [11, 15, 13, 14, 16, 12] activation 0.04011200206477177\n",
      "radius: 2 atom list: [1, 3, 0, 2] activation 0.04006408287723723\n",
      "radius: 2 atom list: [0, 2, 1] activation 0.040021780810972456\n",
      "FP 44 has linear regression coefficient 0.0373661644101\n",
      "radius: 1 atom list: [8, 9, 10, 7, 11] activation 0.031109350646430674\n",
      "radius: 1 atom list: [0, 1, 2, 3, 4] activation 0.030684080171166327\n",
      "radius: 1 atom list: [9, 10, 8, 11, 12] activation 0.030630831240395968\n",
      "radius: 1 atom list: [9, 7, 10, 6, 8] activation 0.03059797640369428\n",
      "radius: 1 atom list: [6, 4, 10, 5, 3] activation 0.03050421073636129\n",
      "radius: 1 atom list: [10, 6, 7, 8, 9] activation 0.030370370269279048\n",
      "radius: 1 atom list: [13, 14, 9, 12, 15] activation 0.030320054674056643\n",
      "radius: 1 atom list: [22, 25, 20, 24, 23] activation 0.03025140691536291\n",
      "radius: 1 atom list: [15, 16, 17, 13, 14] activation 0.03020428975610264\n",
      "radius: 1 atom list: [5, 9, 7, 6, 8] activation 0.03020386361284876\n",
      "radius: 1 atom list: [24, 25, 21, 26, 23] activation 0.030074922807158122\n",
      "FP 45 has linear regression coefficient 0.00521736596067\n",
      "radius: 0 atom list: [5] activation 0.019288899198717253\n",
      "radius: 0 atom list: [5] activation 0.018453813634836745\n",
      "radius: 4 atom list: [4, 8, 18, 0, 6, 5, 7, 1, 2, 3] activation 0.018448493811538808\n",
      "radius: 4 atom list: [4, 5, 10, 11, 12, 13, 0, 2, 9, 1, 3, 6, 7, 8] activation 0.01844845305564061\n",
      "radius: 4 atom list: [5, 4, 10, 13, 8, 9, 12, 14, 15, 17, 18, 6, 7, 11] activation 0.018448437024829154\n",
      "radius: 4 atom list: [7, 9, 11, 12, 15, 10, 13, 14, 16, 8, 17] activation 0.018448338962964914\n",
      "radius: 4 atom list: [0, 3, 5, 2, 1, 4] activation 0.01844833480330968\n",
      "radius: 4 atom list: [0, 5, 7, 2, 3, 1, 4, 6] activation 0.01844831250922069\n",
      "radius: 4 atom list: [6, 3, 0, 4, 9, 7, 12, 1, 2, 5, 8, 10, 11] activation 0.01844829225078514\n",
      "radius: 4 atom list: [6, 13, 18, 4, 5, 0, 2, 14, 16, 17, 1, 3] activation 0.018448287508034737\n",
      "radius: 4 atom list: [3, 6, 7, 9, 19, 29, 0, 4, 8, 30, 1, 2, 5] activation 0.018448277041287823\n",
      "FP 46 has linear regression coefficient -0.0845090556195\n",
      "radius: 3 atom list: [14, 12, 13, 15, 16, 17, 18] activation 0.1544494293530951\n",
      "radius: 3 atom list: [14, 12, 13, 15, 16, 17, 18] activation 0.1539812201294694\n",
      "radius: 3 atom list: [11, 15, 13, 14, 16, 12] activation 0.14646042106780927\n",
      "radius: 3 atom list: [3, 5, 6, 8, 4, 7] activation 0.14578098923915025\n",
      "radius: 3 atom list: [0, 3, 1, 2, 4] activation 0.14457934728274235\n",
      "radius: 3 atom list: [18, 1, 0, 19, 2] activation 0.14241604663343324\n",
      "radius: 3 atom list: [33, 35, 31, 32, 34] activation 0.1419607904999176\n",
      "radius: 3 atom list: [13, 15, 16, 14, 12] activation 0.14092047447966716\n",
      "radius: 3 atom list: [12, 13, 11, 9, 10] activation 0.14070111456865195\n",
      "radius: 3 atom list: [2, 3, 1, 4, 0] activation 0.14049599741797844\n",
      "radius: 3 atom list: [4, 1, 2, 0, 3] activation 0.1403195141578824\n",
      "FP 47 has linear regression coefficient 0.0435979757816\n",
      "radius: 2 atom list: [5, 7, 3, 4, 6, 13, 14] activation 0.03580103150686422\n",
      "radius: 2 atom list: [1, 3, 5, 7, 8, 9, 16, 6, 19, 18] activation 0.03575549050151121\n",
      "radius: 2 atom list: [10, 7, 9, 12, 5, 8, 11, 16] activation 0.03575036046794938\n",
      "radius: 2 atom list: [5, 19, 2, 4, 6, 7, 8, 16, 18, 20] activation 0.0357412699412543\n",
      "radius: 2 atom list: [3, 5, 13, 2, 6, 14, 7, 4, 18] activation 0.035732982742380154\n",
      "radius: 2 atom list: [7, 12, 13, 6, 4, 3, 5] activation 0.03572669984348763\n",
      "radius: 2 atom list: [20, 3, 5, 6, 7, 8, 12, 21, 23] activation 0.035724471763195514\n",
      "radius: 2 atom list: [5, 8, 18, 3, 4, 6, 7] activation 0.03572078639623808\n",
      "radius: 2 atom list: [5, 12, 4, 6, 11, 2, 3] activation 0.03572077072028173\n",
      "radius: 2 atom list: [4, 6, 7, 3, 5, 17, 16] activation 0.0357207707202817\n",
      "radius: 2 atom list: [3, 5, 7, 21, 20, 4, 6] activation 0.03571363707875629\n",
      "FP 48 has linear regression coefficient -0.0436513216797\n",
      "radius: 1 atom list: [14, 13, 15, 16, 17, 18] activation 0.04033543177658441\n",
      "radius: 1 atom list: [8, 7] activation 0.03803571481278783\n",
      "radius: 1 atom list: [11, 8] activation 0.0367732426523091\n",
      "radius: 1 atom list: [11, 10] activation 0.036706371113418186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 1 atom list: [3, 1] activation 0.03634896268803238\n",
      "radius: 1 atom list: [2, 1] activation 0.03626771141871885\n",
      "radius: 1 atom list: [8, 4, 7] activation 0.036238852455860816\n",
      "radius: 1 atom list: [11, 10] activation 0.03619062596314381\n",
      "radius: 1 atom list: [13, 16, 15, 14] activation 0.036113356725398255\n",
      "radius: 1 atom list: [17, 18] activation 0.03603567356321768\n",
      "radius: 1 atom list: [17, 16] activation 0.03587322405789431\n",
      "FP 49 has linear regression coefficient -0.0612226765284\n",
      "radius: 3 atom list: [0, 2, 3, 4, 1] activation 0.05284660798086984\n",
      "radius: 3 atom list: [3, 5, 2, 8, 6, 0, 1, 4, 7] activation 0.052768338472256884\n",
      "radius: 3 atom list: [4, 5, 6, 10, 11, 12, 7, 8, 9] activation 0.05275906538455532\n",
      "radius: 3 atom list: [1, 5, 7, 2, 6, 13, 0, 3, 4] activation 0.052755561830026367\n",
      "radius: 3 atom list: [4, 1, 0, 2, 3] activation 0.052744443893458004\n",
      "radius: 3 atom list: [3, 1, 6, 0, 2, 4, 5, 7, 8] activation 0.05273728827442454\n",
      "radius: 3 atom list: [0, 2, 3, 4, 1] activation 0.05273125460782265\n",
      "radius: 3 atom list: [9, 10, 11, 15, 13, 14, 16, 12] activation 0.052718502554905795\n",
      "radius: 3 atom list: [12, 5, 0, 2, 3, 10, 11, 4, 1] activation 0.052712481448965016\n",
      "radius: 3 atom list: [2, 3, 1, 4, 5, 8, 0, 11, 10, 9, 12] activation 0.052707985938296564\n",
      "radius: 3 atom list: [1, 4, 0, 2, 3, 5, 8, 9, 6] activation 0.05270306152903501\n"
     ]
    }
   ],
   "source": [
    "trained_network_weights = train_neural_fingerprint()\n",
    "with open('results.pkl', 'w') as f:\n",
    "    pickle.dump(trained_network_weights, f)\n",
    "\n",
    "# Plotting.\n",
    "with open('results.pkl') as f:\n",
    "    trained_weights = pickle.load(f)\n",
    "plot(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {'target_name' : 'logP',\n",
    "               'data_file'   : 'logp_mean'}\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "normalize = 1\n",
    "dropout = 0\n",
    "activation = relu\n",
    "params = {'fp_length': 50,\n",
    "            'fp_depth': 4,\n",
    "            'init_scale':np.exp(-4),\n",
    "            'l2_penalty':np.exp(-2),\n",
    "            'conv_width':20}\n",
    "\n",
    "conv_layer_sizes = [params['conv_width']] * params['fp_depth']\n",
    "conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                    'fp_length' : params['fp_length'],\n",
    "                    'normalize' : normalize,\n",
    "                    'return_atom_activations':False}\n",
    "\n",
    "all_radii = range(params['fp_depth'] + 1)\n",
    "\n",
    "# Plotting parameters\n",
    "num_figs_per_fp = 11\n",
    "figsize = (100, 100)\n",
    "highlight_color = (30.0/255.0, 100.0/255.0, 255.0/255.0)  # A nice light blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Regression on 9631 training points.\n",
      "--------------------------------------------------------------------------------\n",
      "Mean predictor\n",
      "\n",
      "Performance (RMSE) on logP:\n",
      "Train: 1.8591219922763682\n",
      "Test:  1.9087278157747403\n",
      "\n",
      "Performance (R2) on logP:\n",
      "Train: -1.7525698430687504e+31\n",
      "Test:  -4.618357749255554e+30\n",
      "--------------------------------------------------------------------------------\n",
      "Task params {'fp_length': 50, 'l2_penalty': 0.1353352832366127, 'fp_depth': 4, 'conv_width': 20, 'init_scale': 0.01831563888873418}\n",
      "Convnet fingerprints with neural net\n",
      "Total number of weights in the network: 30571\n",
      "max of weights 0.08535001578936458\n",
      "Iteration 0 loss 1.0050158468991277 train RMSE 1.86369714187141 Train R2 0 : -18760.506803882163 Validation RMSE 0 : 1.8582204651665832 Validation R2 0 : -18395.132611209214 max of weights 0.08088414585016009\n",
      "Iteration 10 loss 0.9832034478799879 train RMSE 1.8433730638836396 Train R2 10 : -527.3224731482445 Validation RMSE 10 : 1.8445310207053718 Validation R2 10 : -519.5894124521668 max of weights 0.08097803158741446\n",
      "Iteration 20 loss 0.9792270683989526 train RMSE 1.8396421860533554 Train R2 20 : -601.3129468375679 Validation RMSE 20 : 1.8401827527733539 Validation R2 20 : -592.8265740946874 max of weights 0.09250232458934804\n",
      "Iteration 30 loss 0.970529987467982 train RMSE 1.83144422929922 Train R2 30 : -278.06884728915213 Validation RMSE 30 : 1.8342118336102584 Validation R2 30 : -274.67051014732476 max of weights 0.10554503205364685\n",
      "Iteration 40 loss 0.9563374148428666 train RMSE 1.8179808725306814 Train R2 40 : -253.2193652704912 Validation RMSE 40 : 1.8201231908847804 Validation R2 40 : -248.75854142042542 max of weights 0.11966832055902425\n",
      "Iteration 50 loss 0.9333407092081334 train RMSE 1.795951727037214 Train R2 50 : -137.6353117370424 Validation RMSE 50 : 1.7988537398708644 Validation R2 50 : -134.19421889673285 max of weights 0.1338800464154578\n",
      "Iteration 60 loss 0.898569396332682 train RMSE 1.7621225833239067 Train R2 60 : -82.03937540006613 Validation RMSE 60 : 1.762434717871822 Validation R2 60 : -78.40491993845123 max of weights 0.14734666192275753\n",
      "Iteration 70 loss 0.8517543021904963 train RMSE 1.7155266288864888 Train R2 70 : -32.99581875124341 Validation RMSE 70 : 1.7194723456922094 Validation R2 70 : -31.962935847502997 max of weights 0.1613753896583816\n",
      "Iteration 80 loss 0.7885499135594283 train RMSE 1.6505424868327818 Train R2 80 : -16.37977447143375 Validation RMSE 80 : 1.6508108199092357 Validation R2 80 : -15.593128476084487 max of weights 0.17495086289342845\n",
      "Iteration 90 loss 0.7172103227430968 train RMSE 1.5739764933504683 Train R2 90 : -7.773317201635939 Validation RMSE 90 : 1.5757159855211937 Validation R2 90 : -7.438044231210863 max of weights 0.18516109782578435\n",
      "Iteration 100 loss 0.6322762648566744 train RMSE 1.4776641902498586 Train R2 100 : -3.9135883423914803 Validation RMSE 100 : 1.4766531124001265 Validation R2 100 : -3.7994159033019583 max of weights 0.19430944087330398\n",
      "Iteration 110 loss 0.5637864410450388 train RMSE 1.3951391007588319 Train R2 110 : -1.9023918871308063 Validation RMSE 110 : 1.3875275019419921 Validation R2 110 : -1.801648228432767 max of weights 0.2079403247480054\n",
      "Iteration 120 loss 0.5078780440268286 train RMSE 1.323945351113149 Train R2 120 : -0.7295145584906673 Validation RMSE 120 : 1.3203782871632748 Validation R2 120 : -0.6912504074628627 max of weights 0.2219877189155204\n",
      "Iteration 130 loss 0.47153975935467507 train RMSE 1.2755304650608736 Train R2 130 : -0.36643679228031845 Validation RMSE 130 : 1.261060734498097 Validation R2 130 : -0.2988124566542869 max of weights 0.23407602652270326\n",
      "Iteration 140 loss 0.42184116564354535 train RMSE 1.2062155687679916 Train R2 140 : -0.04816198033939134 Validation RMSE 140 : 1.191076813203858 Validation R2 140 : -0.015468100675238627 max of weights 0.24487526577811883\n",
      "Iteration 150 loss 0.3831328033006515 train RMSE 1.1493334571801272 Train R2 150 : 0.09234719887944387 Validation RMSE 150 : 1.1327518305650501 Validation R2 150 : 0.13099564342418402 max of weights 0.2555202298117708\n",
      "Iteration 160 loss 0.3597470971022976 train RMSE 1.1135185017371172 Train R2 160 : 0.23033949140876753 Validation RMSE 160 : 1.0948900450329788 Validation R2 160 : 0.2633446085812804 max of weights 0.2607273725770529\n",
      "Iteration 170 loss 0.3441883868782591 train RMSE 1.0890253045975304 Train R2 170 : 0.3502174651953217 Validation RMSE 170 : 1.076358266703478 Validation R2 170 : 0.37389243820912443 max of weights 0.26676411947154144\n",
      "Iteration 180 loss 0.3530390543974726 train RMSE 1.102925823300129 Train R2 180 : 0.398144835896974 Validation RMSE 180 : 1.08379493912777 Validation R2 180 : 0.4328433075529493 max of weights 0.273513915907372\n",
      "Iteration 190 loss 0.3313263000816821 train RMSE 1.068322219617702 Train R2 190 : 0.4040054107933394 Validation RMSE 190 : 1.0487730666463468 Validation R2 190 : 0.4320260665585639 max of weights 0.2780457434304624\n",
      "Iteration 200 loss 0.30804625736770375 train RMSE 1.0299201151137745 Train R2 200 : 0.4757530160247676 Validation RMSE 200 : 1.0038810375170724 Validation R2 200 : 0.5129904972427746 max of weights 0.2829595682033187\n",
      "Iteration 210 loss 0.29412048273647967 train RMSE 1.006220878983406 Train R2 210 : 0.5432735432667649 Validation RMSE 210 : 0.9843608570630082 Validation R2 210 : 0.5718862703285527 max of weights 0.2885504096047201\n",
      "Iteration 220 loss 0.29463825869693466 train RMSE 1.007074342178846 Train R2 220 : 0.5353351522657652 Validation RMSE 220 : 0.9883369304089601 Validation R2 220 : 0.5581309599468095 max of weights 0.29103515911362754\n",
      "Iteration 230 loss 0.28433220861767006 train RMSE 0.9891969811969716 Train R2 230 : 0.5595363647243052 Validation RMSE 230 : 0.9633519131175551 Validation R2 230 : 0.5914003381505856 max of weights 0.29749678175595445\n",
      "Iteration 240 loss 0.2774407589945365 train RMSE 0.9770436078076397 Train R2 240 : 0.5729380509248236 Validation RMSE 240 : 0.9487292666450118 Validation R2 240 : 0.6037209857015322 max of weights 0.2941208442460174\n",
      "Iteration 250 loss 0.2696441881635616 train RMSE 0.9631115038813429 Train R2 250 : 0.600654269702663 Validation RMSE 250 : 0.9402835282852398 Validation R2 250 : 0.625420285999176 max of weights 0.29657450368057015\n",
      "Iteration 260 loss 0.26747631087446244 train RMSE 0.9591797750341415 Train R2 260 : 0.6063511266133095 Validation RMSE 260 : 0.9336943313812901 Validation R2 260 : 0.6305358290543182 max of weights 0.29927506688579214\n",
      "Iteration 270 loss 0.26568467064283613 train RMSE 0.9559227677005688 Train R2 270 : 0.6197213286460324 Validation RMSE 270 : 0.9363802107760398 Validation R2 270 : 0.6401227768541106 max of weights 0.3018394760726798\n",
      "Iteration 280 loss 0.28399003584228455 train RMSE 0.9884545701280941 Train R2 280 : 0.5917346457656142 Validation RMSE 280 : 0.9648218433438742 Validation R2 280 : 0.6185028128463346 max of weights 0.3073006314963646\n",
      "Iteration 290 loss 0.27585253021441697 train RMSE 0.9741154116441575 Train R2 290 : 0.5899401927375232 Validation RMSE 290 : 0.9551084258347455 Validation R2 290 : 0.61100058225193 max of weights 0.3054279282524049\n",
      "Iteration 300 loss 0.260474757711736 train RMSE 0.9464095642047868 Train R2 300 : 0.6362154273877683 Validation RMSE 300 : 0.9276043804339977 Validation R2 300 : 0.6573875618220638 max of weights 0.30916519068822024\n",
      "Iteration 310 loss 0.25125390612138016 train RMSE 0.9293914123935518 Train R2 310 : 0.6536161670803646 Validation RMSE 310 : 0.9128266725028187 Validation R2 310 : 0.6703507105245072 max of weights 0.31298073217365996\n",
      "Iteration 320 loss 0.25130470167251084 train RMSE 0.9294765637286002 Train R2 320 : 0.6384032928630429 Validation RMSE 320 : 0.9084592287270498 Validation R2 320 : 0.659428275082412 max of weights 0.31668167960715465\n",
      "Iteration 330 loss 0.2511181524911176 train RMSE 0.9291056626732319 Train R2 330 : 0.6555777059513663 Validation RMSE 330 : 0.912014846503786 Validation R2 330 : 0.672199375298701 max of weights 0.319084487098534\n",
      "Iteration 340 loss 0.2512384002956891 train RMSE 0.9293095059687652 Train R2 340 : 0.6411737276844883 Validation RMSE 340 : 0.9040271369500502 Validation R2 340 : 0.6628072086810092 max of weights 0.3293832319376924\n",
      "Iteration 350 loss 0.24121105803098528 train RMSE 0.9104399300699412 Train R2 350 : 0.6659935183365799 Validation RMSE 350 : 0.889468115210159 Validation R2 350 : 0.6846879857235749 max of weights 0.34106163087915947\n",
      "Iteration 360 loss 0.2423597570147222 train RMSE 0.9125997092408873 Train R2 360 : 0.6662772484390291 Validation RMSE 360 : 0.8918281553898553 Validation R2 360 : 0.6824203635441016 max of weights 0.3496077168894739\n",
      "Iteration 370 loss 0.24430136281135742 train RMSE 0.9162561753213236 Train R2 370 : 0.6716360272202984 Validation RMSE 370 : 0.8981108279795329 Validation R2 370 : 0.6870111278090404 max of weights 0.35572380687543254\n",
      "Iteration 380 loss 0.24474061159760724 train RMSE 0.9170905157307484 Train R2 380 : 0.6543028733351741 Validation RMSE 380 : 0.9037924016422332 Validation R2 380 : 0.6678387504172341 max of weights 0.3598992212747966\n",
      "Iteration 390 loss 0.250750915677492 train RMSE 0.9283363215379332 Train R2 390 : 0.6466708021898739 Validation RMSE 390 : 0.9102279963611478 Validation R2 390 : 0.6638209170216849 max of weights 0.3655074035219695\n",
      "Iteration 400 loss 0.24223048569603917 train RMSE 0.9123112585530626 Train R2 400 : 0.6777457771008872 Validation RMSE 400 : 0.8984805568763662 Validation R2 400 : 0.6920971165356424 max of weights 0.37462331453488257\n",
      "Iteration 410 loss 0.24159367938224152 train RMSE 0.911084340439918 Train R2 410 : 0.6778379405445978 Validation RMSE 410 : 0.9006957862110234 Validation R2 410 : 0.6860801426465895 max of weights 0.3814255279256887\n",
      "Iteration 420 loss 0.2612173508155676 train RMSE 0.9475690768729709 Train R2 420 : 0.6303878213681059 Validation RMSE 420 : 0.9253272431726194 Validation R2 420 : 0.6527086015112499 max of weights 0.3877104355066425\n",
      "Iteration 430 loss 0.23972311516125314 train RMSE 0.9074977314119295 Train R2 430 : 0.6858522052757788 Validation RMSE 430 : 0.8952604808214263 Validation R2 430 : 0.6959294942651507 max of weights 0.39543516212113045\n",
      "Iteration 440 loss 0.2313174082450086 train RMSE 0.8913287867948537 Train R2 440 : 0.6790270856288805 Validation RMSE 440 : 0.8708867569968041 Validation R2 440 : 0.694323408425088 max of weights 0.40652003497778366\n",
      "Iteration 450 loss 0.22596899818761104 train RMSE 0.8808693968163703 Train R2 450 : 0.6921974359419054 Validation RMSE 450 : 0.8613405972750434 Validation R2 450 : 0.7074613099169446 max of weights 0.4157012795057302\n",
      "Iteration 460 loss 0.2265647468665972 train RMSE 0.8820225280821798 Train R2 460 : 0.6962398921153168 Validation RMSE 460 : 0.8671375600088983 Validation R2 460 : 0.7061931665358248 max of weights 0.4236124543174444\n",
      "Iteration 470 loss 0.2370403902498385 train RMSE 0.9023024756142716 Train R2 470 : 0.6868965610684589 Validation RMSE 470 : 0.8856770433582959 Validation R2 470 : 0.6999592118551055 max of weights 0.4283745987810567\n",
      "Iteration 480 loss 0.2413768944912123 train RMSE 0.9105787607718993 Train R2 480 : 0.6691613691197378 Validation RMSE 480 : 0.9056541758452497 Validation R2 480 : 0.67378202632556 max of weights 0.43291865919283906\n",
      "Iteration 490 loss 0.228245837652587 train RMSE 0.885289298593834 Train R2 490 : 0.6910903004670402 Validation RMSE 490 : 0.8688651761370673 Validation R2 490 : 0.7051359700823407 max of weights 0.4385290825124467\n",
      "Iteration 500 loss 0.22411315307536558 train RMSE 0.8771620874987398 Train R2 500 : 0.7092716545235558 Validation RMSE 500 : 0.8669251162333547 Validation R2 500 : 0.7189106806034558 max of weights 0.4473461460038527\n",
      "Iteration 510 loss 0.23158632963424794 train RMSE 0.8917510757387325 Train R2 510 : 0.6957759577387148 Validation RMSE 510 : 0.8857110084542418 Validation R2 510 : 0.6986034270416734 max of weights 0.45372295902568005\n",
      "Iteration 520 loss 0.2317315054459512 train RMSE 0.8920167099339904 Train R2 520 : 0.6866380252721955 Validation RMSE 520 : 0.8775431165510709 Validation R2 520 : 0.7001181442429751 max of weights 0.45945971279033293\n",
      "Iteration 530 loss 0.21707746920913099 train RMSE 0.8631395681343323 Train R2 530 : 0.7147714512955785 Validation RMSE 530 : 0.849097615489675 Validation R2 530 : 0.7242927052945063 max of weights 0.4677747985272244\n",
      "Iteration 540 loss 0.21198525610220154 train RMSE 0.8528588384135107 Train R2 540 : 0.7190545204184623 Validation RMSE 540 : 0.8405882557242137 Validation R2 540 : 0.7272970681504021 max of weights 0.4785786232994842\n",
      "Iteration 550 loss 0.20970625988633176 train RMSE 0.8482027061545362 Train R2 550 : 0.7273126857099342 Validation RMSE 550 : 0.8351799332473211 Validation R2 550 : 0.735988050325186 max of weights 0.4871635788996014\n",
      "Iteration 560 loss 0.2164531624826285 train RMSE 0.8618291724648429 Train R2 560 : 0.7202270109971629 Validation RMSE 560 : 0.8545624807070253 Validation R2 560 : 0.7240956178823893 max of weights 0.494007346282\n",
      "Iteration 570 loss 0.24119251652680881 train RMSE 0.9100821734473811 Train R2 570 : 0.6840714190365391 Validation RMSE 570 : 0.8993934723258812 Validation R2 570 : 0.6934360931633274 max of weights 0.4979616905773452\n",
      "Iteration 580 loss 0.22604204444573392 train RMSE 0.8808466272830497 Train R2 580 : 0.6999921143849839 Validation RMSE 580 : 0.8760100070456781 Validation R2 580 : 0.7036521307173704 max of weights 0.5030454227373692\n",
      "Iteration 590 loss 0.21361112727906836 train RMSE 0.8560862133369879 Train R2 590 : 0.7226594253427299 Validation RMSE 590 : 0.8464079270196323 Validation R2 590 : 0.7307487696890012 max of weights 0.5097507936657254\n",
      "Iteration 600 loss 0.20566940341657175 train RMSE 0.839881845853067 Train R2 600 : 0.73727476378443 Validation RMSE 600 : 0.8380449239053537 Validation R2 600 : 0.7395495069570754 max of weights 0.5171311285984411\n",
      "Iteration 610 loss 0.20404604390444617 train RMSE 0.8365224490941852 Train R2 610 : 0.7299205474197288 Validation RMSE 610 : 0.83068894480246 Validation R2 610 : 0.7334700327075521 max of weights 0.522927793528201\n",
      "Iteration 620 loss 0.20388281126332902 train RMSE 0.83616376952675 Train R2 620 : 0.7383921498921436 Validation RMSE 620 : 0.8311639632990213 Validation R2 620 : 0.742510269611415 max of weights 0.5288864699331114\n",
      "Iteration 630 loss 0.20572388990426307 train RMSE 0.8399418582196736 Train R2 630 : 0.7272452845634645 Validation RMSE 630 : 0.825490503671645 Validation R2 630 : 0.7355200946522396 max of weights 0.5372530911315129\n",
      "Iteration 640 loss 0.19839026636580062 train RMSE 0.8246853854734374 Train R2 640 : 0.742852637939104 Validation RMSE 640 : 0.8168488934647355 Validation R2 640 : 0.7473969019825594 max of weights 0.5481378223507618\n",
      "Iteration 650 loss 0.20103441335124045 train RMSE 0.8301899339885936 Train R2 650 : 0.7419495294331422 Validation RMSE 650 : 0.8216941097824655 Validation R2 650 : 0.7452987909643212 max of weights 0.55631246301098\n",
      "Iteration 660 loss 0.2000988582337365 train RMSE 0.8282296410092064 Train R2 660 : 0.7437985166171208 Validation RMSE 660 : 0.8220893174834336 Validation R2 660 : 0.7470492717921399 max of weights 0.5622183649531768\n",
      "Iteration 670 loss 0.20835754341069118 train RMSE 0.8452887350074151 Train R2 670 : 0.7246818190042017 Validation RMSE 670 : 0.844359915470481 Validation R2 670 : 0.7258293358260447 max of weights 0.5659030832702687\n",
      "Iteration 680 loss 0.20577456673983957 train RMSE 0.839982665291688 Train R2 680 : 0.7337632368575724 Validation RMSE 680 : 0.8363786923979111 Validation R2 680 : 0.7362209995448199 max of weights 0.5710558279926038\n",
      "Iteration 690 loss 0.19932264900415167 train RMSE 0.8265765535626673 Train R2 690 : 0.7481729495796391 Validation RMSE 690 : 0.8237267530862092 Validation R2 690 : 0.7506660430585368 max of weights 0.5785411972181235\n",
      "Iteration 700 loss 0.19548018925106164 train RMSE 0.8184845404496413 Train R2 700 : 0.7527938688671967 Validation RMSE 700 : 0.8206168733338909 Validation R2 700 : 0.751075467824636 max of weights 0.584779521224975\n",
      "Iteration 710 loss 0.20856056870661424 train RMSE 0.8456371996730713 Train R2 710 : 0.7234875915282674 Validation RMSE 710 : 0.8387137901909895 Validation R2 710 : 0.7290942074548071 max of weights 0.5900546903448992\n",
      "Iteration 720 loss 0.20097281283091453 train RMSE 0.8299649460861499 Train R2 720 : 0.7506189240142507 Validation RMSE 720 : 0.8306277651269074 Validation R2 720 : 0.7491837287279852 max of weights 0.5965678399589048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 730 loss 0.1919403881859394 train RMSE 0.8109195859849205 Train R2 730 : 0.7486684639154484 Validation RMSE 730 : 0.804109695999905 Validation R2 730 : 0.750998620941955 max of weights 0.6060825920279131\n",
      "Iteration 740 loss 0.18819764125643684 train RMSE 0.8028730618944706 Train R2 740 : 0.7585724226143383 Validation RMSE 740 : 0.7982059717991685 Validation R2 740 : 0.7605420847625309 max of weights 0.6157255259703738\n",
      "Iteration 750 loss 0.1906207058902415 train RMSE 0.8080577099901245 Train R2 750 : 0.7588021175827063 Validation RMSE 750 : 0.8051934720349959 Validation R2 750 : 0.7580791448050538 max of weights 0.6236328126830659\n",
      "Iteration 760 loss 0.1962478680757335 train RMSE 0.8199917369860722 Train R2 760 : 0.7506340768615449 Validation RMSE 760 : 0.8143920385282418 Validation R2 760 : 0.7533460987384756 max of weights 0.6287980581243671\n",
      "Iteration 770 loss 0.20214912491905182 train RMSE 0.8323412795146214 Train R2 770 : 0.7392666208798098 Validation RMSE 770 : 0.8380894101631589 Validation R2 770 : 0.734654042313974 max of weights 0.6329806077154114\n",
      "Iteration 780 loss 0.1890848626454975 train RMSE 0.8047376678500747 Train R2 780 : 0.7592608105205857 Validation RMSE 780 : 0.8009448511335561 Validation R2 780 : 0.7615934698902068 max of weights 0.637907726923525\n",
      "Iteration 790 loss 0.18697081976475133 train RMSE 0.8001611183886345 Train R2 790 : 0.7673531567204983 Validation RMSE 790 : 0.8000838997695796 Validation R2 790 : 0.7681516513666091 max of weights 0.6458367726174288\n",
      "Iteration 800 loss 0.1947426809834366 train RMSE 0.8167569972951974 Train R2 800 : 0.7593741404763557 Validation RMSE 800 : 0.8241317141736668 Validation R2 800 : 0.7530104723599627 max of weights 0.6516409637032692\n",
      "Iteration 810 loss 0.20383298227010246 train RMSE 0.8357493639092829 Train R2 810 : 0.7366372666594241 Validation RMSE 810 : 0.831501219261203 Validation R2 810 : 0.7406542480411351 max of weights 0.6568153001941496\n",
      "Iteration 820 loss 0.18351354926530977 train RMSE 0.7926012126397628 Train R2 820 : 0.7701226711034929 Validation RMSE 820 : 0.7900800272432975 Validation R2 820 : 0.7692661793441878 max of weights 0.6640406285123123\n",
      "Iteration 830 loss 0.1780462022999094 train RMSE 0.7805640150303221 Train R2 830 : 0.7731351193898613 Validation RMSE 830 : 0.7808646553332937 Validation R2 830 : 0.7707397976617277 max of weights 0.6746066161574047\n",
      "Iteration 840 loss 0.17433328748506505 train RMSE 0.7722696417303314 Train R2 840 : 0.7840404916156718 Validation RMSE 840 : 0.7717304147238745 Validation R2 840 : 0.7831863261059361 max of weights 0.68411623579687\n",
      "Iteration 850 loss 0.1829552345353946 train RMSE 0.7913184538981405 Train R2 850 : 0.7733917936730104 Validation RMSE 850 : 0.7944807684098022 Validation R2 850 : 0.7691847567499032 max of weights 0.6915076854024611\n",
      "Iteration 860 loss 0.20378595822870735 train RMSE 0.8355640869314847 Train R2 860 : 0.7429128508776984 Validation RMSE 860 : 0.8349709618946005 Validation R2 860 : 0.7433162101545461 max of weights 0.6958412143625726\n",
      "Iteration 870 loss 0.19373414381079568 train RMSE 0.8145103814971583 Train R2 870 : 0.7582977315082583 Validation RMSE 870 : 0.82081614730394 Validation R2 870 : 0.7531007925353244 max of weights 0.7008202547604903\n",
      "Iteration 880 loss 0.18327101464452739 train RMSE 0.7919708622343831 Train R2 880 : 0.774131703559585 Validation RMSE 880 : 0.7903703358074106 Validation R2 880 : 0.7747699482164411 max of weights 0.7068399833926751\n",
      "Iteration 890 loss 0.1718560695244113 train RMSE 0.7666407448823951 Train R2 890 : 0.7900481639443052 Validation RMSE 890 : 0.772101231036756 Validation R2 890 : 0.7871308598065597 max of weights 0.7144312686057658\n",
      "Iteration 900 loss 0.17200880145583317 train RMSE 0.7669658898286997 Train R2 900 : 0.7863360561006942 Validation RMSE 900 : 0.7722579396453416 Validation R2 900 : 0.7816835543953573 max of weights 0.7202144026794743\n",
      "Iteration 910 loss 0.17171744427402677 train RMSE 0.7662862947200507 Train R2 910 : 0.7868962264780992 Validation RMSE 910 : 0.7672175431843676 Validation R2 910 : 0.7856041131602693 max of weights 0.7258340101398747\n",
      "Iteration 920 loss 0.172060182823171 train RMSE 0.767041940440404 Train R2 920 : 0.7818290097109724 Validation RMSE 920 : 0.7627128705714462 Validation R2 920 : 0.7816133098659739 max of weights 0.7337031242432736\n",
      "Iteration 930 loss 0.16786950910374693 train RMSE 0.7575108023518211 Train R2 930 : 0.7928887165479614 Validation RMSE 930 : 0.7622118081050158 Validation R2 930 : 0.7884566553185897 max of weights 0.7443510838775786\n",
      "Iteration 940 loss 0.1704816932763465 train RMSE 0.7634255284722737 Train R2 940 : 0.794026433247683 Validation RMSE 940 : 0.7655619888062029 Validation R2 940 : 0.7902991554841632 max of weights 0.7527945642706956\n",
      "Iteration 950 loss 0.1679688302202743 train RMSE 0.7577072030104887 Train R2 950 : 0.7944500324310777 Validation RMSE 950 : 0.7610547953511408 Validation R2 950 : 0.7910239726738093 max of weights 0.7591734997906805\n",
      "Iteration 960 loss 0.1842098669672063 train RMSE 0.7938841551194904 Train R2 960 : 0.7673304104025511 Validation RMSE 960 : 0.8002793489522714 Validation R2 960 : 0.7629603461084582 \n",
      "Performance (RMSE) on logP:\n",
      "Train: 0.7838066353271441\n",
      "Test:  0.8021059553436842\n",
      "\n",
      "Performance (R2) on logP:\n",
      "Train: 0.7747382212530106\n",
      "Test:  0.7693503717279297\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_network_weights = train_neural_fingerprint()\n",
    "with open('results'+task_params['data_file']+'.pkl', 'w') as f:\n",
    "    pickle.dump(trained_network_weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logP_wo_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {'target_name' : 'logP',\n",
    "               'data_file'   : 'logP_wo_parameters'}\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "normalize = 1\n",
    "dropout = 0\n",
    "activation = relu\n",
    "params = {'fp_length': 50,\n",
    "            'fp_depth': 4,\n",
    "            'init_scale':np.exp(-4),\n",
    "            'l2_penalty':np.exp(-2),\n",
    "            'conv_width':20}\n",
    "\n",
    "conv_layer_sizes = [params['conv_width']] * params['fp_depth']\n",
    "conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                    'fp_length' : params['fp_length'],\n",
    "                    'normalize' : normalize,\n",
    "                    'return_atom_activations':False}\n",
    "\n",
    "all_radii = range(params['fp_depth'] + 1)\n",
    "\n",
    "# Plotting parameters\n",
    "num_figs_per_fp = 11\n",
    "figsize = (100, 100)\n",
    "highlight_color = (30.0/255.0, 100.0/255.0, 255.0/255.0)  # A nice light blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Regression on 8837 training points.\n",
      "--------------------------------------------------------------------------------\n",
      "Mean predictor\n",
      "\n",
      "Performance (RMSE) on logP:\n",
      "Train: 1.8146549349304573\n",
      "Test:  1.786769730059924\n",
      "--------------------------------------------------------------------------------\n",
      "Task params {'fp_length': 50, 'l2_penalty': 0.1353352832366127, 'fp_depth': 4, 'conv_width': 20, 'init_scale': 0.01831563888873418}\n",
      "Convnet fingerprints with neural net\n",
      "Total number of weights in the network: 27441\n",
      "max of weights 0.08144291040373952\n",
      "Iteration 0 loss 0.9886515906588851 train RMSE 1.8042531705773819 Validation RMSE 0 : 1.782187447872895 max of weights 0.08015057691461697\n",
      "Iteration 10 loss 1.00953598902406 train RMSE 1.8232229054122153 Validation RMSE 10 : 1.8078542461999854 max of weights 0.07794223909831255\n",
      "Iteration 20 loss 0.9836639417604071 train RMSE 1.799710182979298 Validation RMSE 20 : 1.775675355283461 max of weights 0.08258433888104313\n",
      "Iteration 30 loss 0.980730155071616 train RMSE 1.7970174747296634 Validation RMSE 30 : 1.7749043443184047 max of weights 0.09535365755818245\n",
      "Iteration 40 loss 0.9752057670870755 train RMSE 1.7919308455291594 Validation RMSE 40 : 1.76377179958263 max of weights 0.10892321272264756\n",
      "Iteration 50 loss 0.9517019099917721 train RMSE 1.7701665307544392 Validation RMSE 50 : 1.7416248932132516 max of weights 0.12406368897200784\n",
      "Iteration 60 loss 0.9233258595955445 train RMSE 1.7435134388054596 Validation RMSE 60 : 1.7196131294234551 max of weights 0.13926371274561158\n",
      "Iteration 70 loss 0.8677641882172851 train RMSE 1.6901495792516903 Validation RMSE 70 : 1.6596553591962981 max of weights 0.15397439512711636\n",
      "Iteration 80 loss 0.819755212457701 train RMSE 1.642611949500761 Validation RMSE 80 : 1.6040391182386369 max of weights 0.16945826989619553\n",
      "Iteration 90 loss 0.7155684616482497 train RMSE 1.5344996001397144 Validation RMSE 90 : 1.5074002090353347 max of weights 0.18210761954573862\n",
      "Iteration 100 loss 0.6313438256616936 train RMSE 1.4411577198679217 Validation RMSE 100 : 1.4176635557803394 max of weights 0.19276453864158272\n",
      "Iteration 110 loss 0.5464608347303815 train RMSE 1.3405211838628202 Validation RMSE 110 : 1.3168022372422168 max of weights 0.20800725374618623\n",
      "Iteration 120 loss 0.4714889352498817 train RMSE 1.2448721503263809 Validation RMSE 120 : 1.224764697240589 max of weights 0.222367699109025\n",
      "Iteration 130 loss 0.4246062834158083 train RMSE 1.1810935916021972 Validation RMSE 130 : 1.1735625087282393 max of weights 0.23677670358037647\n",
      "Iteration 140 loss 0.4113069943140268 train RMSE 1.1622707941365322 Validation RMSE 140 : 1.139842835891494 max of weights 0.24724718554950456\n",
      "Iteration 150 loss 0.3774627074204838 train RMSE 1.1132031461010439 Validation RMSE 150 : 1.1301523097427326 max of weights 0.2568952100632645\n",
      "Iteration 160 loss 0.3336754962932576 train RMSE 1.0463584690195078 Validation RMSE 160 : 1.036087989502687 max of weights 0.26057356386772723\n",
      "Iteration 170 loss 0.3307833518614886 train RMSE 1.0417327057653263 Validation RMSE 170 : 1.0310427244854878 max of weights 0.26097023931279134\n",
      "Iteration 180 loss 0.30888805228117133 train RMSE 1.006483497440283 Validation RMSE 180 : 1.0119300545164236 max of weights 0.2674579372594642\n",
      "Iteration 190 loss 0.31565976316732164 train RMSE 1.0174683626317425 Validation RMSE 190 : 1.0316149117458104 max of weights 0.27190118810003155\n",
      "Iteration 200 loss 0.28987897292891196 train RMSE 0.9748158487819334 Validation RMSE 200 : 0.9700392774059361 max of weights 0.27669495817676826\n",
      "Iteration 210 loss 0.2887164578871113 train RMSE 0.9727928458863051 Validation RMSE 210 : 0.9575244722305697 max of weights 0.28669443903881614\n",
      "Iteration 220 loss 0.2820566549600728 train RMSE 0.9614120793127247 Validation RMSE 220 : 0.9685109528685777 max of weights 0.2972709966214828\n",
      "Iteration 230 loss 0.28492937436728655 train RMSE 0.9662621516175763 Validation RMSE 230 : 0.9472236481006286 max of weights 0.30567311362590355\n",
      "Iteration 240 loss 0.2820630397596456 train RMSE 0.9613256782348543 Validation RMSE 240 : 0.975094415712936 max of weights 0.311076488424914\n",
      "Iteration 250 loss 0.2515306785156916 train RMSE 0.9074906757514294 Validation RMSE 250 : 0.8957876774368058 max of weights 0.31322106848684017\n",
      "Iteration 260 loss 0.25430705797501263 train RMSE 0.9124878219281287 Validation RMSE 260 : 0.9061979585627271 max of weights 0.31731327902980716\n",
      "Iteration 270 loss 0.24011918477440383 train RMSE 0.8865024488888222 Validation RMSE 270 : 0.8894041033725606 max of weights 0.3225615542344627\n",
      "Iteration 280 loss 0.2539834741238322 train RMSE 0.9118729338205082 Validation RMSE 280 : 0.9262094317191185 max of weights 0.32414102635089814\n",
      "Iteration 290 loss 0.23842333494810342 train RMSE 0.8833068386427406 Validation RMSE 290 : 0.8821497036720928 max of weights 0.3263332378044161\n",
      "Iteration 300 loss 0.24374891312135616 train RMSE 0.8931394081166103 Validation RMSE 300 : 0.8786047521083796 max of weights 0.33196230439357755\n",
      "Iteration 310 loss 0.24053213044435515 train RMSE 0.8871617620712479 Validation RMSE 310 : 0.8970216310569331 max of weights 0.3364539713128002\n",
      "Iteration 320 loss 0.24035626889927353 train RMSE 0.8867914866021951 Validation RMSE 320 : 0.873897223715584 max of weights 0.3391071072282025\n",
      "Iteration 330 loss 0.23975666399246123 train RMSE 0.8856542635868522 Validation RMSE 330 : 0.9009195835819762 max of weights 0.34035553936578494\n",
      "Iteration 340 loss 0.21694129510317994 train RMSE 0.8421507354380522 Validation RMSE 340 : 0.8363522629728668 max of weights 0.339018258982426\n",
      "Iteration 350 loss 0.21981038397346658 train RMSE 0.8477288034157853 Validation RMSE 350 : 0.8502653353412093 max of weights 0.34014046213345983\n",
      "Iteration 360 loss 0.20929036925092645 train RMSE 0.8270395401351278 Validation RMSE 360 : 0.832418470641264 max of weights 0.34195623998132296\n",
      "Iteration 370 loss 0.2184095026365332 train RMSE 0.8449888693588874 Validation RMSE 370 : 0.85891557381093 max of weights 0.34530627880333015\n",
      "Iteration 380 loss 0.2115419987233416 train RMSE 0.8314809179927529 Validation RMSE 380 : 0.8368751226643343 max of weights 0.34937891465608506\n",
      "Iteration 390 loss 0.2153955542732428 train RMSE 0.8390463435827573 Validation RMSE 390 : 0.8272049823745635 max of weights 0.35524236888010313\n",
      "Iteration 400 loss 0.2104639345266104 train RMSE 0.829288923389001 Validation RMSE 400 : 0.838506083269002 max of weights 0.36101426408117976\n",
      "Iteration 410 loss 0.2089378878723376 train RMSE 0.8262152480512677 Validation RMSE 410 : 0.8186906126551391 max of weights 0.3674144871351724\n",
      "Iteration 420 loss 0.20582778076425853 train RMSE 0.819977196046869 Validation RMSE 420 : 0.8292198505182673 max of weights 0.3684941344419801\n",
      "Iteration 430 loss 0.19132622556565104 train RMSE 0.7903045352842899 Validation RMSE 430 : 0.7907561499050478 max of weights 0.3699988059679822\n",
      "Iteration 440 loss 0.19354603596731101 train RMSE 0.7949074992712333 Validation RMSE 440 : 0.803642867350061 max of weights 0.37482813962802686\n",
      "Iteration 450 loss 0.1870295681442455 train RMSE 0.7812880949889034 Validation RMSE 450 : 0.785374829281073 max of weights 0.3831619519767138\n",
      "Iteration 460 loss 0.18906308358643167 train RMSE 0.7855520351527964 Validation RMSE 460 : 0.7952686262477935 max of weights 0.39244049781176543\n",
      "Iteration 470 loss 0.19289717511954987 train RMSE 0.7935284180260656 Validation RMSE 470 : 0.8020163164738987 max of weights 0.397839312925886\n",
      "Iteration 480 loss 0.19117608917812587 train RMSE 0.7899230210886133 Validation RMSE 480 : 0.7787795487764334 max of weights 0.4038331638560887\n",
      "Iteration 490 loss 0.18514111418762078 train RMSE 0.7772208308529966 Validation RMSE 490 : 0.7864733286139433 max of weights 0.41129991990400017\n",
      "Iteration 500 loss 0.18061879710025197 train RMSE 0.767544383479566 Validation RMSE 500 : 0.7687460922378871 max of weights 0.41833199042874575\n",
      "Iteration 510 loss 0.18033679722757073 train RMSE 0.7669244236472273 Validation RMSE 510 : 0.7709889068923248 max of weights 0.4234652756880004\n",
      "Iteration 520 loss 0.1720929738729323 train RMSE 0.7490019658028776 Validation RMSE 520 : 0.7569185941695856 max of weights 0.431227050167259\n",
      "Iteration 530 loss 0.17307200421048377 train RMSE 0.7511451403952943 Validation RMSE 530 : 0.7633709262756488 max of weights 0.44126538333258\n",
      "Iteration 540 loss 0.16953580780823352 train RMSE 0.743345884823936 Validation RMSE 540 : 0.7493224251820495 max of weights 0.4520931661473538\n",
      "Iteration 550 loss 0.16564146625905302 train RMSE 0.734659208353818 Validation RMSE 550 : 0.7403410072783637 max of weights 0.46117991403229064\n",
      "Iteration 560 loss 0.17318905531346768 train RMSE 0.7513618291358691 Validation RMSE 560 : 0.7603006778935659 max of weights 0.46657479176250644\n",
      "Iteration 570 loss 0.17106114110941445 train RMSE 0.746660416365363 Validation RMSE 570 : 0.7379352023811308 max of weights 0.47241273749480184\n",
      "Iteration 580 loss 0.16327170332764043 train RMSE 0.7292612082524393 Validation RMSE 580 : 0.7361781612560132 max of weights 0.47998701042417047\n",
      "Iteration 590 loss 0.16355482237727462 train RMSE 0.7298647402368978 Validation RMSE 590 : 0.7392125468555499 max of weights 0.48705422693396916\n",
      "Iteration 600 loss 0.16339861189054064 train RMSE 0.7294952978438181 Validation RMSE 600 : 0.7329099544772317 max of weights 0.4924083706358052\n",
      "Iteration 610 loss 0.15705235331989936 train RMSE 0.7150152667733186 Validation RMSE 610 : 0.7292493636453906 max of weights 0.5000711950714953\n",
      "Iteration 620 loss 0.15675239977633643 train RMSE 0.7143203384770092 Validation RMSE 620 : 0.7319178566987395 max of weights 0.5101694880076421\n",
      "Iteration 630 loss 0.15556586540322623 train RMSE 0.7115737179112728 Validation RMSE 630 : 0.723725077416468 max of weights 0.5209120790039014\n",
      "Iteration 640 loss 0.15072961707435653 train RMSE 0.7002800685601692 Validation RMSE 640 : 0.7053189398725985 max of weights 0.5293174497233871\n",
      "Iteration 650 loss 0.1575936082050969 train RMSE 0.7162125977707832 Validation RMSE 650 : 0.7259568856458835 max of weights 0.5342475297132394\n",
      "Iteration 660 loss 0.15586931493410663 train RMSE 0.7122207923155296 Validation RMSE 660 : 0.7087788802358488 max of weights 0.5398155373051177\n",
      "Iteration 670 loss 0.15108208471233892 train RMSE 0.7010450932909446 Validation RMSE 670 : 0.706690535871255 max of weights 0.5473221066198206\n",
      "Iteration 680 loss 0.1596253325448749 train RMSE 0.7207965302425062 Validation RMSE 680 : 0.7378505238105663 max of weights 0.554480839547002\n",
      "Iteration 690 loss 0.15396444525480435 train RMSE 0.707729105821579 Validation RMSE 690 : 0.7138594067839751 max of weights 0.5602876052009057\n",
      "Iteration 700 loss 0.14550271659829583 train RMSE 0.6877506244062931 Validation RMSE 700 : 0.703475735057519 max of weights 0.5677714186359191\n",
      "Iteration 710 loss 0.14650970153693318 train RMSE 0.6901569883529693 Validation RMSE 710 : 0.7122496190088146 max of weights 0.5779991748352059\n",
      "Iteration 720 loss 0.14738276435341055 train RMSE 0.6922307632548945 Validation RMSE 720 : 0.706473757704384 max of weights 0.5884688371623433\n",
      "Iteration 730 loss 0.14117431922265788 train RMSE 0.6772873443353064 Validation RMSE 730 : 0.6827121268168705 max of weights 0.596085256341671\n",
      "Iteration 740 loss 0.14769061947957923 train RMSE 0.6929240639324551 Validation RMSE 740 : 0.703620756068951 max of weights 0.6007978347762678\n",
      "Iteration 750 loss 0.14644731444734985 train RMSE 0.6899539470539231 Validation RMSE 750 : 0.690477082386343 max of weights 0.6060971325812757\n",
      "Iteration 760 loss 0.14663487409685455 train RMSE 0.6903781886299895 Validation RMSE 760 : 0.6956266938920755 max of weights 0.6132247207894472\n",
      "Iteration 770 loss 0.1580212982365483 train RMSE 0.7169890896967439 Validation RMSE 770 : 0.7384071439979539 max of weights 0.620203855048959\n",
      "Iteration 780 loss 0.14419529208214996 train RMSE 0.6844856490146494 Validation RMSE 780 : 0.6928495460476459 max of weights 0.6264006300290731\n",
      "Iteration 790 loss 0.13880162842655744 train RMSE 0.6713790833777031 Validation RMSE 790 : 0.6863164803257485 max of weights 0.6335581424786835\n",
      "Iteration 800 loss 0.13923516133887492 train RMSE 0.6724448011144519 Validation RMSE 800 : 0.6967804842214089 max of weights 0.6436968245037321\n",
      "Iteration 810 loss 0.14188663175579563 train RMSE 0.6788980995384547 Validation RMSE 810 : 0.6954771234247271 max of weights 0.6536547546397834\n",
      "Iteration 820 loss 0.13439875739475338 train RMSE 0.660472294808044 Validation RMSE 820 : 0.6688753650974341 max of weights 0.6604126174016651\n",
      "Iteration 830 loss 0.1373609541210181 train RMSE 0.6677946766053201 Validation RMSE 830 : 0.6799614961854501 max of weights 0.6648560539512947\n",
      "Iteration 840 loss 0.14084352871397296 train RMSE 0.6763215740440726 Validation RMSE 840 : 0.6794226450947249 max of weights 0.6700400468303076\n",
      "Iteration 850 loss 0.1432612550072346 train RMSE 0.6821575118560167 Validation RMSE 850 : 0.6866598561942726 max of weights 0.6768088695240269\n",
      "Iteration 860 loss 0.15457728435331833 train RMSE 0.7089195051701392 Validation RMSE 860 : 0.7343508494524205 max of weights 0.6834548759819665\n",
      "Iteration 870 loss 0.13466655087232834 train RMSE 0.6610440829719242 Validation RMSE 870 : 0.6706988082927471 max of weights 0.6897013862740465\n",
      "Iteration 880 loss 0.13789666608151374 train RMSE 0.6690342991718793 Validation RMSE 880 : 0.6817370377746961 \n",
      "Performance (RMSE) on logP:\n",
      "Train: 0.6680607872810364\n",
      "Test:  0.6825474758763097\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data...\n",
      "Convnet fingerprints with neural net\n",
      "FP 0 has linear regression coefficient -0.0490426064538\n",
      "radius: 1 atom list: [15, 16, 14, 18, 17, 13] activation 0.03840076220142145\n",
      "radius: 1 atom list: [9, 8] activation 0.038176989533725836\n",
      "radius: 1 atom list: [0, 1] activation 0.03656567717393078\n",
      "radius: 1 atom list: [7, 8] activation 0.03629790245965815\n",
      "radius: 1 atom list: [3, 4] activation 0.03625934610664531\n",
      "radius: 2 atom list: [4, 8, 5, 7, 6, 9] activation 0.036194711288342736\n",
      "radius: 2 atom list: [0, 2, 3, 4, 5, 1] activation 0.036190582594676494\n",
      "radius: 1 atom list: [1, 0] activation 0.03618587934188338\n",
      "radius: 2 atom list: [7, 5, 9, 4, 6, 8] activation 0.03616873542185512\n",
      "radius: 2 atom list: [0, 2, 3, 5, 1, 4] activation 0.03615283707448545\n",
      "radius: 1 atom list: [18, 17] activation 0.03612997560031009\n",
      "FP 1 has linear regression coefficient -0.0475459570895\n",
      "radius: 1 atom list: [15, 16, 14, 18, 17, 13] activation 0.044756047324066114\n",
      "radius: 1 atom list: [9, 8] activation 0.04319705709236875\n",
      "radius: 1 atom list: [0, 1] activation 0.04025084987794295\n",
      "radius: 1 atom list: [7, 8] activation 0.040064793341522596\n",
      "radius: 1 atom list: [13, 14] activation 0.03985779444657144\n",
      "radius: 1 atom list: [1, 0] activation 0.039752351644029085\n",
      "radius: 1 atom list: [3, 4] activation 0.03974687233547064\n",
      "radius: 1 atom list: [18, 17] activation 0.039512373072613835\n",
      "radius: 1 atom list: [6, 9] activation 0.03950586341787496\n",
      "radius: 1 atom list: [9, 6] activation 0.039319017161489805\n",
      "radius: 1 atom list: [4, 3] activation 0.038690811680871996\n",
      "FP 2 has linear regression coefficient -0.0349214116669\n",
      "radius: 1 atom list: [18, 17] activation 0.030189508380682652\n",
      "radius: 1 atom list: [9, 8] activation 0.030119412693169874\n",
      "radius: 1 atom list: [7, 8] activation 0.030051600385009956\n",
      "radius: 1 atom list: [0, 1] activation 0.03001469649964413\n",
      "radius: 0 atom list: [9] activation 0.029960622371634677\n",
      "radius: 1 atom list: [3, 4] activation 0.029953112464144904\n",
      "radius: 1 atom list: [6, 9] activation 0.029929856473074834\n",
      "radius: 1 atom list: [9, 8, 5] activation 0.029877658597409682\n",
      "radius: 1 atom list: [3, 2, 1] activation 0.029761645159461763\n",
      "radius: 1 atom list: [1, 0] activation 0.029707587764019733\n",
      "radius: 1 atom list: [1, 0] activation 0.029696173707824302\n",
      "FP 3 has linear regression coefficient 0.0558917972311\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.03724615351038397\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.03722372170624322\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.03702242042170903\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03695782218968556\n",
      "radius: 3 atom list: [2, 3, 1, 10, 9, 12, 7, 13] activation 0.036888299775026126\n",
      "radius: 3 atom list: [4, 2, 1, 3, 5, 6, 15] activation 0.03688390962280478\n",
      "radius: 3 atom list: [5, 6, 7, 14, 13, 11, 12] activation 0.03688271606162086\n",
      "radius: 3 atom list: [15, 16, 7, 8, 9, 14, 5] activation 0.03688249724170459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 3 atom list: [9, 1, 4, 5, 8, 3, 10] activation 0.03688111516210626\n",
      "radius: 3 atom list: [7, 8, 10, 12, 14, 9, 11, 13] activation 0.03688006013900692\n",
      "radius: 3 atom list: [11, 13, 15, 7, 10, 12, 14, 9] activation 0.03687983483591856\n",
      "FP 4 has linear regression coefficient -0.0674564992073\n",
      "radius: 2 atom list: [15, 14, 17, 18, 16, 12, 13] activation 0.07263332693084341\n",
      "radius: 2 atom list: [9, 8, 5] activation 0.07235106461077892\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.07233746009289359\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.07227312238876252\n",
      "radius: 2 atom list: [2, 0, 1] activation 0.07172148223888844\n",
      "radius: 2 atom list: [10, 12, 11] activation 0.07161859330708381\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.07125508433223503\n",
      "radius: 2 atom list: [6, 5, 7, 8, 9] activation 0.06784753921174579\n",
      "radius: 2 atom list: [3, 4, 1] activation 0.06761490266998506\n",
      "radius: 2 atom list: [0, 3, 4, 1, 2] activation 0.06663839644539474\n",
      "radius: 2 atom list: [1, 0, 2] activation 0.06657268372169167\n",
      "FP 5 has linear regression coefficient 0.0398680582128\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.03263796357505918\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.03262406300904489\n",
      "radius: 2 atom list: [15, 1, 4, 5, 14, 3, 6, 24, 13] activation 0.032405658942154025\n",
      "radius: 2 atom list: [2, 4, 3, 5, 11, 6, 17, 10, 12, 16, 18, 22] activation 0.032379745495107956\n",
      "radius: 2 atom list: [4, 5, 6, 13, 3, 7, 12, 14, 18, 19] activation 0.03233427152446348\n",
      "radius: 2 atom list: [3, 4, 5, 6, 12, 13, 17, 18, 14, 7] activation 0.03230978779047426\n",
      "radius: 2 atom list: [19, 1, 3, 7, 12, 14, 2, 5, 6, 13] activation 0.03220511489800181\n",
      "radius: 2 atom list: [17, 18, 21, 26, 19, 20, 27, 28, 33] activation 0.03217472904463735\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03205989725462933\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.03205764052377728\n",
      "radius: 2 atom list: [3, 7, 12, 14, 4, 5, 6, 20, 19, 13] activation 0.03203798465833234\n",
      "FP 6 has linear regression coefficient 0.0886934393464\n",
      "radius: 4 atom list: [4, 5, 6, 7, 9, 10, 11, 12, 15, 16, 17, 8, 19, 14, 18] activation 0.08417022008935614\n",
      "radius: 4 atom list: [5, 10, 1, 2, 3, 4, 6, 7, 8, 11, 12, 13, 14, 15, 9] activation 0.08395448159071332\n",
      "radius: 4 atom list: [11, 10, 7, 3, 4, 5, 6, 8, 9, 12, 13, 14, 15, 16] activation 0.08273550484085729\n",
      "radius: 4 atom list: [6, 12, 7, 3, 8, 9, 10, 11] activation 0.08074027275272087\n",
      "radius: 4 atom list: [5, 1, 2, 7, 3, 0, 4, 6, 9, 11, 12, 8] activation 0.080728304437695\n",
      "radius: 4 atom list: [8, 9, 10, 11, 12, 13, 14] activation 0.08070210517678335\n",
      "radius: 4 atom list: [5, 4, 6, 24, 0, 2, 3, 19, 1] activation 0.08069737030797494\n",
      "radius: 4 atom list: [10, 11, 4, 6, 3, 8, 5, 7, 9] activation 0.08069552598788574\n",
      "radius: 4 atom list: [3, 7, 2, 1, 0, 8, 9, 10] activation 0.08069251863908065\n",
      "radius: 4 atom list: [7, 11, 9, 10, 16, 12, 13, 14, 15] activation 0.08069174716016343\n",
      "radius: 4 atom list: [5, 0, 2, 3, 4, 1, 20, 17, 6] activation 0.08068832022284488\n",
      "FP 7 has linear regression coefficient 0.0438692368533\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.03222252787728625\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.03221327797776223\n",
      "radius: 2 atom list: [15, 1, 4, 5, 14, 3, 6, 24, 13] activation 0.03195234846137008\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.03192162710498195\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03190065934829778\n",
      "radius: 2 atom list: [4, 5, 6, 13, 3, 7, 12, 14, 18, 19] activation 0.03189521119613011\n",
      "radius: 2 atom list: [3, 4, 5, 6, 12, 13, 17, 18, 14, 7] activation 0.03185907607613114\n",
      "radius: 2 atom list: [2, 18, 3, 5, 6, 7, 14, 16, 17, 4] activation 0.03177761925281173\n",
      "radius: 2 atom list: [15, 9, 3, 5, 6, 7, 12, 14, 16, 8] activation 0.03176876658345624\n",
      "radius: 2 atom list: [19, 1, 3, 7, 12, 14, 2, 5, 6, 13] activation 0.0317633140930598\n",
      "radius: 2 atom list: [16, 2, 4, 6, 7, 8, 5, 18, 20, 19] activation 0.03175986091305838\n",
      "FP 8 has linear regression coefficient -0.0565395450744\n",
      "radius: 1 atom list: [15, 16, 14, 18, 17, 13] activation 0.051211863178076605\n",
      "radius: 2 atom list: [0, 3, 4, 1, 2] activation 0.05018655615113038\n",
      "radius: 2 atom list: [0, 2, 4, 3, 1] activation 0.049993014577013675\n",
      "radius: 2 atom list: [1, 0, 2, 4, 3] activation 0.04989611036312591\n",
      "radius: 2 atom list: [10, 11, 7, 8, 9] activation 0.04986413553202999\n",
      "radius: 2 atom list: [2, 1, 4, 3, 0] activation 0.04984221527537309\n",
      "radius: 2 atom list: [6, 5, 7, 8, 9] activation 0.04983412143834484\n",
      "radius: 2 atom list: [3, 4, 1] activation 0.04973640017698562\n",
      "radius: 2 atom list: [3, 4, 5, 1, 2] activation 0.049718474468038745\n",
      "radius: 2 atom list: [24, 26, 25, 23, 22] activation 0.049696532800091404\n",
      "radius: 2 atom list: [7, 5, 9, 8, 6] activation 0.04969534724648221\n",
      "FP 9 has linear regression coefficient -0.010895303823\n",
      "radius: 0 atom list: [0] activation 0.025454548084372072\n",
      "radius: 0 atom list: [8] activation 0.02544942170831244\n",
      "radius: 0 atom list: [0] activation 0.024827648772156742\n",
      "radius: 0 atom list: [0] activation 0.023440410963998466\n",
      "radius: 0 atom list: [8] activation 0.0233743109960947\n",
      "radius: 0 atom list: [16] activation 0.02322642007606634\n",
      "radius: 0 atom list: [9] activation 0.023119249023164496\n",
      "radius: 0 atom list: [6] activation 0.022775747279696713\n",
      "radius: 0 atom list: [1] activation 0.022707734965642917\n",
      "radius: 0 atom list: [1] activation 0.022466213870042016\n",
      "radius: 0 atom list: [12] activation 0.021828459451967527\n",
      "FP 10 has linear regression coefficient -0.0819562125285\n",
      "radius: 2 atom list: [15, 14, 17, 18, 16, 12, 13] activation 0.10660671304200318\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.09678191676586109\n",
      "radius: 2 atom list: [9, 8, 5] activation 0.09644496317323402\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.09640084503785772\n",
      "radius: 2 atom list: [2, 0, 1] activation 0.09441291413008837\n",
      "radius: 2 atom list: [10, 12, 11] activation 0.09404152077282582\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.09382160772593487\n",
      "radius: 3 atom list: [3, 0, 4, 5, 1, 2] activation 0.09298515767146624\n",
      "radius: 3 atom list: [6, 4, 7, 10, 9, 5, 8] activation 0.09298442365237357\n",
      "radius: 3 atom list: [7, 10, 11, 6, 9, 5, 8] activation 0.09293249805134929\n",
      "radius: 3 atom list: [2, 5, 6, 3, 1, 4, 0] activation 0.09290659763679715\n",
      "FP 11 has linear regression coefficient 0.0412300905398\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.031453616601140204\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.03143960450009189\n",
      "radius: 2 atom list: [15, 1, 4, 5, 14, 3, 6, 24, 13] activation 0.03123470937272249\n",
      "radius: 2 atom list: [4, 5, 6, 13, 3, 7, 12, 14, 18, 19] activation 0.031159756722323385\n",
      "radius: 2 atom list: [3, 4, 5, 6, 12, 13, 17, 18, 14, 7] activation 0.031137780295500732\n",
      "radius: 2 atom list: [19, 1, 3, 7, 12, 14, 2, 5, 6, 13] activation 0.031077520809734336\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03101789900672603\n",
      "radius: 2 atom list: [17, 18, 21, 26, 19, 20, 27, 28, 33] activation 0.031017712210965925\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.030997207823870717\n",
      "radius: 2 atom list: [3, 7, 12, 14, 4, 5, 6, 20, 19, 13] activation 0.0309054120235464\n",
      "radius: 2 atom list: [16, 4, 7, 17, 5, 22, 23, 6, 8] activation 0.03089419186853213\n",
      "FP 12 has linear regression coefficient -0.034234432593\n",
      "radius: 1 atom list: [9, 8] activation 0.03261736848821935\n",
      "radius: 1 atom list: [3, 4] activation 0.03217246836886359\n",
      "radius: 1 atom list: [7, 8] activation 0.03216817147123545\n",
      "radius: 1 atom list: [0, 1] activation 0.03214377449736292\n",
      "radius: 1 atom list: [6, 9] activation 0.031978004024419034\n",
      "radius: 1 atom list: [18, 17] activation 0.031956386022404526\n",
      "radius: 1 atom list: [1, 0] activation 0.031900807469736066\n",
      "radius: 1 atom list: [15, 16, 14, 18, 17, 13] activation 0.03172416822779701\n",
      "radius: 1 atom list: [13, 14] activation 0.03156005939815804\n",
      "radius: 1 atom list: [9, 8, 5] activation 0.03153708651410553\n",
      "radius: 1 atom list: [1, 0] activation 0.031499932090890956\n",
      "FP 13 has linear regression coefficient 0.045803692394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.032855760202695324\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.032843916431971286\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.03260556641020427\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03253848986335458\n",
      "radius: 2 atom list: [4, 5, 6, 13, 3, 7, 12, 14, 18, 19] activation 0.03249104972292756\n",
      "radius: 2 atom list: [3, 4, 5, 6, 12, 13, 17, 18, 14, 7] activation 0.032456611251177575\n",
      "radius: 2 atom list: [15, 1, 4, 5, 14, 3, 6, 24, 13] activation 0.03244203223324115\n",
      "radius: 2 atom list: [2, 18, 3, 5, 6, 7, 14, 16, 17, 4] activation 0.032408596541668194\n",
      "radius: 2 atom list: [15, 9, 3, 5, 6, 7, 12, 14, 16, 8] activation 0.03239373144916207\n",
      "radius: 2 atom list: [16, 2, 4, 6, 7, 8, 5, 18, 20, 19] activation 0.03237881662672333\n",
      "radius: 2 atom list: [16, 3, 5, 6, 13, 15, 2, 7, 17, 4] activation 0.03236442825825944\n",
      "FP 14 has linear regression coefficient 0.060683824797\n",
      "radius: 3 atom list: [0, 4, 19, 1, 3, 7, 8, 11, 12, 14, 15, 18, 2, 5, 6, 13] activation 0.043755156603515546\n",
      "radius: 3 atom list: [2, 7, 8, 9, 16, 1, 3, 5, 6, 10, 11, 14, 15, 17, 18, 21, 22] activation 0.04353479194998744\n",
      "radius: 3 atom list: [22, 4, 5, 1, 2, 6, 7, 9, 19, 21, 3, 24, 25, 10, 11, 18, 23] activation 0.043497338675829784\n",
      "radius: 3 atom list: [8, 14, 9, 4, 2, 1, 3, 5, 6, 7, 13, 15, 16, 17, 12] activation 0.04321497556641404\n",
      "radius: 3 atom list: [2, 0, 4, 6, 10, 20, 18, 1, 3, 5, 7, 8, 9, 11, 15, 17, 19] activation 0.04313838620641524\n",
      "radius: 3 atom list: [18, 3, 5, 1, 2, 4, 6, 7, 8, 10, 14, 16, 9, 20, 17, 19, 21] activation 0.04311841808785596\n",
      "radius: 3 atom list: [5, 10, 2, 3, 4, 6, 7, 8, 11, 12, 14, 15] activation 0.043087438415241375\n",
      "radius: 3 atom list: [4, 5, 6, 12, 2, 3, 7, 8, 10, 11, 13, 14, 17, 18, 19, 20] activation 0.04303531624666114\n",
      "radius: 3 atom list: [13, 15, 2, 9, 18, 3, 5, 6, 7, 10, 12, 14, 16, 17, 4, 8] activation 0.04297551816229623\n",
      "radius: 3 atom list: [15, 2, 6, 8, 9, 11, 3, 4, 7, 10, 14, 19, 20, 21, 22, 23, 13] activation 0.04292504184550895\n",
      "radius: 3 atom list: [2, 1, 3, 4, 5, 7, 9, 11, 13, 15, 17, 6, 8, 12, 14, 16] activation 0.04286690096653463\n",
      "FP 15 has linear regression coefficient -0.0124062727262\n",
      "radius: 0 atom list: [0] activation 0.02414387198874524\n",
      "radius: 0 atom list: [8] activation 0.023740628867054206\n",
      "radius: 0 atom list: [0] activation 0.023260219827425585\n",
      "radius: 0 atom list: [9] activation 0.02268543713240953\n",
      "radius: 0 atom list: [0] activation 0.02189913830627014\n",
      "radius: 0 atom list: [1] activation 0.0217740898839587\n",
      "radius: 0 atom list: [6] activation 0.021739536198796084\n",
      "radius: 0 atom list: [16] activation 0.021694652795314677\n",
      "radius: 0 atom list: [8] activation 0.021476989001454432\n",
      "radius: 0 atom list: [1] activation 0.0214562292580807\n",
      "radius: 0 atom list: [7] activation 0.02055348789059429\n",
      "FP 16 has linear regression coefficient 0.0415257725012\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.03312356295023652\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.03311225807078521\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.03287611351512546\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03281680449233803\n",
      "radius: 2 atom list: [15, 1, 4, 5, 14, 3, 6, 24, 13] activation 0.032798883336433035\n",
      "radius: 2 atom list: [2, 4, 3, 5, 11, 6, 17, 10, 12, 16, 18, 22] activation 0.03277542613422737\n",
      "radius: 2 atom list: [4, 5, 6, 13, 3, 7, 12, 14, 18, 19] activation 0.032769118799102666\n",
      "radius: 2 atom list: [3, 4, 5, 6, 12, 13, 17, 18, 14, 7] activation 0.03273450382569074\n",
      "radius: 2 atom list: [2, 18, 3, 5, 6, 7, 14, 16, 17, 4] activation 0.03268565258325011\n",
      "radius: 2 atom list: [15, 9, 3, 5, 6, 7, 12, 14, 16, 8] activation 0.03266983493664482\n",
      "radius: 2 atom list: [16, 2, 4, 6, 7, 8, 5, 18, 20, 19] activation 0.03265396791595607\n",
      "FP 17 has linear regression coefficient 0.0391178021433\n",
      "radius: 2 atom list: [15, 14, 16, 18, 19, 20] activation 0.02969808986797569\n",
      "radius: 2 atom list: [4, 28, 1, 2, 3, 5] activation 0.029683314701701873\n",
      "radius: 2 atom list: [4, 1, 2, 3, 5, 7] activation 0.029656876143347628\n",
      "radius: 2 atom list: [0, 1, 2, 4, 9, 3] activation 0.029643150055440687\n",
      "radius: 2 atom list: [7, 11, 4, 5, 6, 8] activation 0.02962140456259884\n",
      "radius: 2 atom list: [3, 6, 1, 4, 0, 2, 5] activation 0.029498657138370826\n",
      "radius: 2 atom list: [18, 20, 21, 22, 13, 19] activation 0.02947333904189588\n",
      "radius: 2 atom list: [16, 17, 18, 19, 20, 27] activation 0.029460277392351426\n",
      "radius: 2 atom list: [1, 2, 3, 5, 4, 6] activation 0.029438363956262505\n",
      "radius: 2 atom list: [10, 12, 11, 7, 8, 9] activation 0.029388002492070662\n",
      "radius: 2 atom list: [2, 4, 3, 5, 11, 6, 17, 10, 12, 16, 18, 22] activation 0.029378715828732818\n",
      "FP 18 has linear regression coefficient 0.0523121157183\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.03735459443079455\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.03733388681160251\n",
      "radius: 2 atom list: [2, 4, 3, 5, 11, 6, 17, 10, 12, 16, 18, 22] activation 0.03708184365638437\n",
      "radius: 2 atom list: [15, 1, 4, 5, 14, 3, 6, 24, 13] activation 0.03695929498761445\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03694259191760866\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.03692739471054288\n",
      "radius: 2 atom list: [4, 5, 6, 13, 3, 7, 12, 14, 18, 19] activation 0.03678067444292891\n",
      "radius: 2 atom list: [2, 18, 3, 5, 6, 7, 14, 16, 17, 4] activation 0.03675431599075034\n",
      "radius: 2 atom list: [15, 9, 3, 5, 6, 7, 12, 14, 16, 8] activation 0.03674031389293392\n",
      "radius: 2 atom list: [16, 2, 4, 6, 7, 8, 5, 18, 20, 19] activation 0.036726252988596234\n",
      "radius: 2 atom list: [3, 4, 5, 6, 12, 13, 17, 18, 14, 7] activation 0.03671646271210133\n",
      "FP 19 has linear regression coefficient -0.129445799907\n",
      "radius: 4 atom list: [14, 15, 16, 17, 18, 11, 12, 13] activation 0.9972350376758342\n",
      "radius: 4 atom list: [14, 15, 16, 17, 18, 11, 12, 13] activation 0.9970107775913206\n",
      "radius: 4 atom list: [14, 15, 16, 17, 18, 10, 11, 12, 13] activation 0.991506348391126\n",
      "radius: 4 atom list: [4, 9, 0, 3, 5, 2, 1] activation 0.9730581178304027\n",
      "radius: 4 atom list: [6, 7, 9, 8, 5, 10, 4, 3] activation 0.9730264920850495\n",
      "radius: 4 atom list: [7, 6, 11, 8, 13, 9, 10, 12] activation 0.9724965478336836\n",
      "radius: 4 atom list: [14, 15, 16, 17, 18, 9, 10, 11, 12, 13] activation 0.9682482011199778\n",
      "radius: 4 atom list: [44, 14, 16, 15, 18, 39, 38, 43, 40, 41, 42] activation 0.9607941255364243\n",
      "radius: 4 atom list: [6, 7, 3, 8, 11, 5, 10, 2, 4, 9, 16] activation 0.9607267284991029\n",
      "radius: 4 atom list: [12, 8, 9, 14, 17, 7, 10, 16, 11, 13, 15] activation 0.9605375136562984\n",
      "radius: 4 atom list: [6, 7, 9, 2, 3, 8, 11, 4, 10, 15, 5] activation 0.9605308082430439\n",
      "FP 20 has linear regression coefficient -0.0680302252512\n",
      "radius: 2 atom list: [15, 14, 17, 18, 16, 12, 13] activation 0.09266273894553725\n",
      "radius: 2 atom list: [9, 8, 5] activation 0.08633135018503003\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.08631723022113952\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.08592888281644356\n",
      "radius: 2 atom list: [2, 0, 1] activation 0.08506754392125701\n",
      "radius: 2 atom list: [10, 12, 11] activation 0.08480007813795755\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.08391020309273246\n",
      "radius: 2 atom list: [3, 4, 1] activation 0.07808354098411192\n",
      "radius: 2 atom list: [6, 5, 7, 8, 9] activation 0.07752197559689958\n",
      "radius: 2 atom list: [0, 3, 4, 1, 2] activation 0.07714462920737436\n",
      "radius: 2 atom list: [1, 0, 2] activation 0.07654016384348639\n",
      "FP 21 has linear regression coefficient 0.0719918470898\n",
      "radius: 3 atom list: [11, 7, 3, 4, 5, 6, 8, 9, 12, 16] activation 0.06943645061868882\n",
      "radius: 3 atom list: [4, 5, 6, 7, 8, 9, 10] activation 0.06885390852757622\n",
      "radius: 3 atom list: [12, 6, 7, 8, 9, 11, 14, 18, 10, 13] activation 0.06858282116005987\n",
      "radius: 3 atom list: [10, 11, 12, 13, 14, 15, 16] activation 0.06854184044687325\n",
      "radius: 3 atom list: [6, 7, 8, 9, 3, 4, 5] activation 0.06812417670290453\n",
      "radius: 3 atom list: [11, 7, 2, 3, 4, 5, 6, 8] activation 0.06783815102544688\n",
      "radius: 3 atom list: [6, 7, 8, 2, 3, 4, 5] activation 0.06772437043377856\n",
      "radius: 3 atom list: [11, 10, 12, 5, 6, 7, 8, 9] activation 0.06754906884582576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 3 atom list: [8, 9, 10, 11, 12, 13, 14] activation 0.06745212504911544\n",
      "radius: 3 atom list: [1, 2, 3, 4, 5, 6, 0] activation 0.06736070466556014\n",
      "radius: 3 atom list: [21, 15, 16, 17, 18, 19, 20] activation 0.06734872400582491\n",
      "FP 22 has linear regression coefficient 0.0485123786553\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.03891463956639332\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.03888369810899686\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03848435799501331\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.03846792247654398\n",
      "radius: 2 atom list: [15, 1, 4, 5, 14, 3, 6, 24, 13] activation 0.038354018799339154\n",
      "radius: 2 atom list: [2, 18, 3, 5, 6, 7, 14, 16, 17, 4] activation 0.03824897640751383\n",
      "radius: 2 atom list: [15, 9, 3, 5, 6, 7, 12, 14, 16, 8] activation 0.03823540972349227\n",
      "radius: 2 atom list: [16, 2, 4, 6, 7, 8, 5, 18, 20, 19] activation 0.03822178109879029\n",
      "radius: 2 atom list: [4, 5, 6, 13, 3, 7, 12, 14, 18, 19] activation 0.038194974058310745\n",
      "radius: 2 atom list: [16, 3, 5, 6, 13, 15, 2, 7, 17, 4] activation 0.038162855579044155\n",
      "radius: 2 atom list: [5, 7, 8, 9, 13, 15, 10, 16, 11, 17] activation 0.038149059076788235\n",
      "FP 23 has linear regression coefficient 0.0674724243033\n",
      "radius: 3 atom list: [11, 7, 3, 4, 5, 6, 8, 9, 12, 16] activation 0.05632577435074329\n",
      "radius: 3 atom list: [4, 5, 6, 7, 8, 9, 10] activation 0.05611756398958598\n",
      "radius: 3 atom list: [10, 11, 12, 13, 14, 15, 16] activation 0.056037159317846794\n",
      "radius: 3 atom list: [12, 6, 7, 8, 9, 11, 14, 18, 10, 13] activation 0.055930874673080826\n",
      "radius: 3 atom list: [6, 7, 8, 9, 3, 4, 5] activation 0.05573651701607486\n",
      "radius: 3 atom list: [2, 3, 1, 10, 9, 12, 7, 13] activation 0.05569551628048287\n",
      "radius: 3 atom list: [11, 7, 2, 3, 4, 5, 6, 8] activation 0.05564902877740639\n",
      "radius: 3 atom list: [6, 7, 8, 2, 3, 4, 5] activation 0.05560231336958956\n",
      "radius: 3 atom list: [11, 10, 12, 5, 6, 7, 8, 9] activation 0.05552412915781222\n",
      "radius: 3 atom list: [4, 2, 1, 3, 5, 6, 15] activation 0.05550294848465685\n",
      "radius: 3 atom list: [8, 9, 10, 11, 12, 13, 14] activation 0.055488718968467216\n",
      "FP 24 has linear regression coefficient -0.0635567231973\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.059932016846211265\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.05986181554716783\n",
      "radius: 2 atom list: [10, 12, 11] activation 0.05986066667736992\n",
      "radius: 2 atom list: [2, 0, 1] activation 0.059851975384704395\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.059733006843097104\n",
      "radius: 2 atom list: [9, 8, 5] activation 0.059730576585402394\n",
      "radius: 2 atom list: [0, 3, 4, 1, 2] activation 0.05896039130429629\n",
      "radius: 2 atom list: [3, 4, 1] activation 0.058841757146679156\n",
      "radius: 2 atom list: [6, 5, 7, 8, 9] activation 0.05876801774151603\n",
      "radius: 2 atom list: [0, 2, 4, 3, 1] activation 0.05847344139643084\n",
      "radius: 2 atom list: [1, 0, 2] activation 0.058372400365664816\n",
      "FP 25 has linear regression coefficient 0.0360940602434\n",
      "radius: 1 atom list: [4, 11, 5, 3, 17] activation 0.027947173019515142\n",
      "radius: 2 atom list: [2, 4, 3, 5, 11, 6, 17, 10, 12, 16, 18, 22] activation 0.027150761287793125\n",
      "radius: 2 atom list: [15, 14, 16, 18, 19, 20] activation 0.027110344771923253\n",
      "radius: 2 atom list: [4, 28, 1, 2, 3, 5] activation 0.02710174872014404\n",
      "radius: 2 atom list: [0, 1, 2, 4, 9, 3] activation 0.027083271163066792\n",
      "radius: 2 atom list: [4, 1, 2, 3, 5, 7] activation 0.027078388924438687\n",
      "radius: 2 atom list: [7, 11, 4, 5, 6, 8] activation 0.02706591460137024\n",
      "radius: 2 atom list: [3, 6, 1, 4, 0, 2, 5] activation 0.02701571130586994\n",
      "radius: 2 atom list: [18, 20, 21, 22, 13, 19] activation 0.027008572842254372\n",
      "radius: 2 atom list: [16, 17, 18, 19, 20, 27] activation 0.02698880473892343\n",
      "radius: 2 atom list: [1, 2, 3, 5, 4, 6] activation 0.026968785598869893\n",
      "FP 26 has linear regression coefficient 0.0851678763516\n",
      "radius: 3 atom list: [11, 7, 3, 4, 5, 6, 8, 9, 12, 16] activation 0.08622867384424932\n",
      "radius: 3 atom list: [4, 5, 6, 7, 8, 9, 10] activation 0.08576148603565067\n",
      "radius: 3 atom list: [10, 11, 12, 13, 14, 15, 16] activation 0.08539525009997173\n",
      "radius: 3 atom list: [12, 6, 7, 8, 9, 11, 14, 18, 10, 13] activation 0.08482070089081709\n",
      "radius: 3 atom list: [6, 7, 8, 9, 3, 4, 5] activation 0.08445303153426725\n",
      "radius: 3 atom list: [11, 7, 2, 3, 4, 5, 6, 8] activation 0.08419160056997889\n",
      "radius: 3 atom list: [6, 7, 8, 2, 3, 4, 5] activation 0.08395603157755335\n",
      "radius: 3 atom list: [11, 10, 12, 5, 6, 7, 8, 9] activation 0.08372264996833031\n",
      "radius: 3 atom list: [8, 9, 10, 11, 12, 13, 14] activation 0.08349493686483894\n",
      "radius: 3 atom list: [21, 15, 16, 17, 18, 19, 20] activation 0.08337155827930166\n",
      "radius: 3 atom list: [1, 2, 3, 4, 5, 6, 0] activation 0.08324283473500406\n",
      "FP 27 has linear regression coefficient 0.061111558075\n",
      "radius: 3 atom list: [11, 7, 3, 4, 5, 6, 8, 9, 12, 16] activation 0.04531122337587561\n",
      "radius: 3 atom list: [4, 5, 6, 7, 8, 9, 10] activation 0.045218925233093024\n",
      "radius: 3 atom list: [10, 11, 12, 13, 14, 15, 16] activation 0.04519148484069965\n",
      "radius: 3 atom list: [12, 6, 7, 8, 9, 11, 14, 18, 10, 13] activation 0.045157266829318234\n",
      "radius: 3 atom list: [6, 7, 8, 9, 3, 4, 5] activation 0.0450770828972726\n",
      "radius: 3 atom list: [11, 7, 2, 3, 4, 5, 6, 8] activation 0.04507031216225141\n",
      "radius: 3 atom list: [11, 10, 12, 5, 6, 7, 8, 9] activation 0.04500338634786251\n",
      "radius: 3 atom list: [6, 7, 8, 2, 3, 4, 5] activation 0.045001114595746676\n",
      "radius: 3 atom list: [8, 9, 10, 11, 12, 13, 14] activation 0.0449657148764454\n",
      "radius: 3 atom list: [21, 15, 16, 17, 18, 19, 20] activation 0.044926851757267086\n",
      "radius: 3 atom list: [1, 2, 3, 4, 5, 6, 0] activation 0.04491901017419601\n",
      "FP 28 has linear regression coefficient -0.055209399149\n",
      "radius: 2 atom list: [6, 4, 9, 8, 5, 7] activation 0.04935498622794843\n",
      "radius: 2 atom list: [4, 3, 0, 2, 1] activation 0.04920103064728898\n",
      "radius: 2 atom list: [0, 3, 4, 1, 2] activation 0.04851740975937377\n",
      "radius: 2 atom list: [3, 4, 1] activation 0.04843403125548784\n",
      "radius: 2 atom list: [0, 2, 4, 3, 1] activation 0.04840406782755723\n",
      "radius: 2 atom list: [1, 0, 2] activation 0.04839783084479994\n",
      "radius: 2 atom list: [13, 12, 11] activation 0.04838496893300725\n",
      "radius: 2 atom list: [1, 0, 2] activation 0.04837505019744468\n",
      "radius: 2 atom list: [1, 2, 0] activation 0.04835909543094587\n",
      "radius: 2 atom list: [6, 5, 7, 8, 9] activation 0.04834614449154478\n",
      "radius: 2 atom list: [1, 0, 2, 4, 3] activation 0.04832777252872715\n",
      "FP 29 has linear regression coefficient 0.0615067317657\n",
      "radius: 3 atom list: [0, 4, 19, 1, 3, 7, 8, 11, 12, 14, 15, 18, 2, 5, 6, 13] activation 0.04877911424643036\n",
      "radius: 3 atom list: [22, 4, 5, 1, 2, 6, 7, 9, 19, 21, 3, 24, 25, 10, 11, 18, 23] activation 0.0485796491199201\n",
      "radius: 3 atom list: [2, 7, 8, 9, 16, 1, 3, 5, 6, 10, 11, 14, 15, 17, 18, 21, 22] activation 0.0485393844034095\n",
      "radius: 3 atom list: [8, 14, 9, 4, 2, 1, 3, 5, 6, 7, 13, 15, 16, 17, 12] activation 0.04821364146621763\n",
      "radius: 3 atom list: [2, 0, 4, 6, 10, 20, 18, 1, 3, 5, 7, 8, 9, 11, 15, 17, 19] activation 0.048115368286066426\n",
      "radius: 3 atom list: [18, 3, 5, 1, 2, 4, 6, 7, 8, 10, 14, 16, 9, 20, 17, 19, 21] activation 0.04809055586896973\n",
      "radius: 3 atom list: [2, 3, 1, 10, 9, 12, 7, 13] activation 0.04808044753621309\n",
      "radius: 3 atom list: [4, 2, 1, 3, 5, 6, 15] activation 0.04797801517679601\n",
      "radius: 3 atom list: [5, 6, 7, 14, 13, 11, 12] activation 0.0479418963301421\n",
      "radius: 3 atom list: [15, 16, 7, 8, 9, 14, 5] activation 0.04793735346896058\n",
      "radius: 3 atom list: [6, 8, 9, 10, 11, 7] activation 0.047930320615240024\n",
      "FP 30 has linear regression coefficient -0.0719693884057\n",
      "radius: 2 atom list: [15, 14, 17, 18, 16, 12, 13] activation 0.07430106231173304\n",
      "radius: 2 atom list: [6, 4, 9, 8, 5, 7] activation 0.07336845773520717\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.07333608282854766\n",
      "radius: 2 atom list: [9, 8, 5] activation 0.0733281120191207\n",
      "radius: 2 atom list: [2, 0, 1] activation 0.07289356511051213\n",
      "radius: 2 atom list: [10, 12, 11] activation 0.07277403556982126\n",
      "radius: 2 atom list: [4, 3, 0, 2, 1] activation 0.07205649471829394\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.07195940071901141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.07077890973992583\n",
      "radius: 2 atom list: [3, 4, 1] activation 0.06879605989088831\n",
      "radius: 2 atom list: [6, 5, 7, 8, 9] activation 0.06860070598033363\n",
      "FP 31 has linear regression coefficient -0.0274620576738\n",
      "radius: 1 atom list: [1, 0] activation 0.02801805741838737\n",
      "radius: 1 atom list: [3, 2] activation 0.0280004733610676\n",
      "radius: 1 atom list: [3, 5] activation 0.027892955209480402\n",
      "radius: 1 atom list: [0, 1] activation 0.027759402721880416\n",
      "radius: 1 atom list: [18, 17] activation 0.027666407481306495\n",
      "radius: 1 atom list: [18, 21] activation 0.02756696982815133\n",
      "radius: 1 atom list: [14, 13] activation 0.027562056498218693\n",
      "radius: 1 atom list: [7, 8] activation 0.02755966037600879\n",
      "radius: 1 atom list: [15, 16] activation 0.027554806874065023\n",
      "radius: 1 atom list: [15, 16] activation 0.027518627262030027\n",
      "radius: 1 atom list: [13, 14] activation 0.027499315917395517\n",
      "FP 32 has linear regression coefficient 0.12921246698\n",
      "radius: 4 atom list: [3, 4, 5, 6, 7, 8, 9, 10, 11] activation 0.8443292263240186\n",
      "radius: 4 atom list: [1, 2, 3, 4, 5, 6, 7, 8, 9] activation 0.834354954074278\n",
      "radius: 4 atom list: [2, 3, 4, 5, 6, 7, 8, 9, 10] activation 0.8324249263137398\n",
      "radius: 4 atom list: [1, 2, 3, 4, 5, 6, 7, 8, 9] activation 0.831099537929897\n",
      "radius: 4 atom list: [10, 2, 3, 4, 5, 6, 7, 8, 9] activation 0.8246029438851868\n",
      "radius: 4 atom list: [11, 12, 13, 14, 15, 16, 17, 9, 10] activation 0.8245691637296293\n",
      "radius: 4 atom list: [19, 11, 12, 13, 14, 15, 16, 17, 18] activation 0.8230000656205715\n",
      "radius: 4 atom list: [11, 7, 1, 2, 3, 4, 5, 6, 8, 9, 12, 16] activation 0.8228736076780018\n",
      "radius: 4 atom list: [2, 3, 4, 5, 6, 7, 8, 9, 1] activation 0.8227600203156952\n",
      "radius: 4 atom list: [1, 2, 3, 4, 5, 6, 7, 8, 9] activation 0.8217469681157732\n",
      "radius: 4 atom list: [11, 15, 16, 19, 8, 14, 10, 3, 4, 5, 6, 7, 9, 18] activation 0.8211529467676678\n",
      "FP 33 has linear regression coefficient 0.0547353414791\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.040357022565202384\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.040327871352225826\n",
      "radius: 3 atom list: [2, 3, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 22, 4, 1, 5, 11, 17] activation 0.04018480818936357\n",
      "radius: 3 atom list: [0, 4, 19, 1, 3, 7, 8, 11, 12, 14, 15, 18, 2, 5, 6, 13] activation 0.040112518559228344\n",
      "radius: 3 atom list: [2, 7, 8, 9, 16, 1, 3, 5, 6, 10, 11, 14, 15, 17, 18, 21, 22] activation 0.03997116664657761\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.03989203256680399\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.039869786417852764\n",
      "radius: 3 atom list: [22, 4, 5, 1, 2, 6, 7, 9, 19, 21, 3, 24, 25, 10, 11, 18, 23] activation 0.03985167546180922\n",
      "radius: 3 atom list: [5, 10, 2, 3, 4, 6, 7, 8, 11, 12, 14, 15] activation 0.039832592018311776\n",
      "radius: 3 atom list: [8, 14, 9, 4, 2, 1, 3, 5, 6, 7, 13, 15, 16, 17, 12] activation 0.03972377765791826\n",
      "radius: 3 atom list: [4, 5, 6, 12, 2, 3, 7, 8, 10, 11, 13, 14, 17, 18, 19, 20] activation 0.03972376612622313\n",
      "FP 34 has linear regression coefficient -0.0659876373988\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.06582759086680147\n",
      "radius: 2 atom list: [9, 8, 5] activation 0.06574566380527554\n",
      "radius: 2 atom list: [2, 3, 1] activation 0.06573458033705479\n",
      "radius: 2 atom list: [2, 0, 1] activation 0.0655560513472681\n",
      "radius: 2 atom list: [10, 12, 11] activation 0.0655021732950096\n",
      "radius: 2 atom list: [15, 16, 14, 18, 17, 13] activation 0.06543993186122135\n",
      "radius: 2 atom list: [15, 14, 17, 18, 16, 12, 13] activation 0.06509863007404398\n",
      "radius: 2 atom list: [0, 3, 4, 1, 2] activation 0.06360433701821532\n",
      "radius: 2 atom list: [3, 4, 1] activation 0.06318806767098681\n",
      "radius: 2 atom list: [0, 2, 4, 3, 1] activation 0.06293851278406136\n",
      "radius: 2 atom list: [6, 5, 7, 8, 9] activation 0.0629180272503073\n",
      "FP 35 has linear regression coefficient 0.0733478706155\n",
      "radius: 3 atom list: [11, 7, 3, 4, 5, 6, 8, 9, 12, 16] activation 0.06510626147260169\n",
      "radius: 3 atom list: [4, 5, 6, 7, 8, 9, 10] activation 0.06476458074605478\n",
      "radius: 3 atom list: [10, 11, 12, 13, 14, 15, 16] activation 0.06451606867910761\n",
      "radius: 3 atom list: [12, 6, 7, 8, 9, 11, 14, 18, 10, 13] activation 0.06439945623947738\n",
      "radius: 3 atom list: [6, 7, 8, 9, 3, 4, 5] activation 0.06413667457246691\n",
      "radius: 3 atom list: [11, 7, 2, 3, 4, 5, 6, 8] activation 0.06393267890003658\n",
      "radius: 3 atom list: [6, 7, 8, 2, 3, 4, 5] activation 0.06383694129509877\n",
      "radius: 3 atom list: [11, 10, 12, 5, 6, 7, 8, 9] activation 0.06368676278958738\n",
      "radius: 3 atom list: [8, 9, 10, 11, 12, 13, 14] activation 0.06357679535314913\n",
      "radius: 3 atom list: [21, 15, 16, 17, 18, 19, 20] activation 0.06353158393304881\n",
      "radius: 3 atom list: [1, 2, 3, 4, 5, 6, 0] activation 0.063505308366533\n",
      "FP 36 has linear regression coefficient 0.0580516290708\n",
      "radius: 2 atom list: [2, 4, 3, 5, 11, 6, 17, 10, 12, 16, 18, 22] activation 0.03898260214415595\n",
      "radius: 3 atom list: [2, 7, 9, 3, 5, 6, 8, 10, 11, 13, 15, 1, 14] activation 0.03848548278823857\n",
      "radius: 3 atom list: [10, 4, 6, 8, 17, 5, 7, 9, 18, 16, 19, 20] activation 0.03847108778380056\n",
      "radius: 3 atom list: [1, 3, 5, 6, 11, 13, 2, 4, 8, 10, 15, 7, 12, 14] activation 0.03841655190826691\n",
      "radius: 3 atom list: [1, 3, 5, 7, 11, 13, 12, 14, 2, 4, 6, 10, 15] activation 0.03837880596951279\n",
      "radius: 3 atom list: [7, 9, 10, 11, 12, 3, 5, 1, 2, 4, 6] activation 0.038353466139961226\n",
      "radius: 3 atom list: [1, 6, 8, 9, 14, 16, 4, 17, 7, 11, 15, 13, 5, 10] activation 0.03834060231207884\n",
      "radius: 3 atom list: [7, 18, 4, 8, 10, 12, 3, 5, 13, 2, 6, 9, 11, 14] activation 0.03834039819586707\n",
      "radius: 3 atom list: [9, 5, 10, 3, 4, 6, 7, 8, 11, 15] activation 0.03830849400016663\n",
      "radius: 3 atom list: [12, 6, 7, 8, 9, 11, 14, 18, 10, 13] activation 0.038289516331276224\n",
      "radius: 3 atom list: [11, 7, 3, 4, 5, 6, 8, 9, 12, 16] activation 0.038258207940596735\n",
      "FP 37 has linear regression coefficient 0.0296425297005\n",
      "radius: 1 atom list: [4, 11, 5, 3, 17] activation 0.026708109030858695\n",
      "radius: 1 atom list: [5, 11, 1, 4] activation 0.025369138890860145\n",
      "radius: 1 atom list: [7, 15, 5, 6] activation 0.025356822119857323\n",
      "radius: 1 atom list: [1, 4, 5, 8] activation 0.025315001993802665\n",
      "radius: 1 atom list: [11, 7, 3, 6] activation 0.02524555149842077\n",
      "radius: 1 atom list: [4, 9, 12, 13] activation 0.025182051422724373\n",
      "radius: 1 atom list: [7, 9, 10, 2] activation 0.025177362474997463\n",
      "radius: 1 atom list: [5, 7, 9, 6] activation 0.025155617043161545\n",
      "radius: 1 atom list: [11, 13, 25, 14] activation 0.025116773748492603\n",
      "radius: 1 atom list: [7, 11, 6, 8] activation 0.025109355123477416\n",
      "radius: 1 atom list: [1, 3, 5, 2] activation 0.02508805239168209\n",
      "FP 38 has linear regression coefficient -0.0594409624502\n",
      "radius: 2 atom list: [0, 3, 4, 1, 2] activation 0.04971512207569099\n",
      "radius: 2 atom list: [0, 2, 4, 3, 1] activation 0.049660345824805614\n",
      "radius: 2 atom list: [1, 0, 2, 4, 3] activation 0.049606163055214765\n",
      "radius: 2 atom list: [2, 1, 4, 3, 0] activation 0.049575688522132386\n",
      "radius: 2 atom list: [10, 11, 7, 8, 9] activation 0.04950607731229031\n",
      "radius: 2 atom list: [24, 26, 25, 23, 22] activation 0.04947722538767603\n",
      "radius: 2 atom list: [3, 1, 0, 2, 4] activation 0.04945074832262508\n",
      "radius: 2 atom list: [1, 4, 3, 0, 2] activation 0.04944471836275645\n",
      "radius: 2 atom list: [10, 11, 9, 7, 8] activation 0.04943099730263908\n",
      "radius: 2 atom list: [3, 4, 5, 1, 2] activation 0.04943071597560254\n",
      "radius: 2 atom list: [1, 4, 3, 0, 2] activation 0.04943037838048687\n",
      "FP 39 has linear regression coefficient -0.0230001489591\n",
      "radius: 0 atom list: [0] activation 0.027216290036901153\n",
      "radius: 0 atom list: [0] activation 0.02710590218421074\n",
      "radius: 0 atom list: [16] activation 0.025992440608035162\n",
      "radius: 0 atom list: [8] activation 0.02543061405820047\n",
      "radius: 0 atom list: [0] activation 0.024574557343206384\n",
      "radius: 0 atom list: [1] activation 0.024522560852196684\n",
      "radius: 0 atom list: [9] activation 0.02451403484732012\n",
      "radius: 0 atom list: [6] activation 0.024429021358733413\n",
      "radius: 0 atom list: [1] activation 0.02361469562684683\n",
      "radius: 0 atom list: [12] activation 0.023362269031222325\n",
      "radius: 0 atom list: [7] activation 0.023342463965032756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP 40 has linear regression coefficient -0.0354881487666\n",
      "radius: 1 atom list: [9, 8] activation 0.03427144674794161\n",
      "radius: 1 atom list: [15, 16, 14, 18, 17, 13] activation 0.03358054834053237\n",
      "radius: 1 atom list: [3, 4] activation 0.03299661337020233\n",
      "radius: 1 atom list: [6, 9] activation 0.03297020209571208\n",
      "radius: 1 atom list: [18, 17] activation 0.032934216589622076\n",
      "radius: 1 atom list: [7, 8] activation 0.0329180104602864\n",
      "radius: 1 atom list: [9, 6] activation 0.032898419487616654\n",
      "radius: 1 atom list: [13, 14] activation 0.032894218591640434\n",
      "radius: 1 atom list: [0, 1] activation 0.032818986985712305\n",
      "radius: 1 atom list: [1, 0] activation 0.03272626430368111\n",
      "radius: 1 atom list: [3, 2] activation 0.03267710603359108\n",
      "FP 41 has linear regression coefficient -0.0320808855754\n",
      "radius: 1 atom list: [1, 0] activation 0.02993885778813455\n",
      "radius: 1 atom list: [3, 2] activation 0.029813399975502447\n",
      "radius: 1 atom list: [3, 5] activation 0.029752788204433903\n",
      "radius: 1 atom list: [0, 1] activation 0.029728080375964076\n",
      "radius: 1 atom list: [18, 17] activation 0.029683346485040254\n",
      "radius: 1 atom list: [11, 12] activation 0.0295601381106493\n",
      "radius: 1 atom list: [3, 4] activation 0.029547649596168415\n",
      "radius: 1 atom list: [0, 1] activation 0.02954027129248143\n",
      "radius: 1 atom list: [8, 7] activation 0.029479129961513174\n",
      "radius: 1 atom list: [13, 14] activation 0.029449136596191763\n",
      "radius: 1 atom list: [1, 0] activation 0.029414111593165374\n",
      "FP 42 has linear regression coefficient 0.0649172218032\n",
      "radius: 3 atom list: [11, 7, 3, 4, 5, 6, 8, 9, 12, 16] activation 0.054022360120382536\n",
      "radius: 3 atom list: [4, 5, 6, 7, 8, 9, 10] activation 0.05390997249294986\n",
      "radius: 3 atom list: [10, 11, 12, 13, 14, 15, 16] activation 0.053777345242792006\n",
      "radius: 3 atom list: [12, 6, 7, 8, 9, 11, 14, 18, 10, 13] activation 0.053641877835635635\n",
      "radius: 3 atom list: [6, 7, 8, 9, 3, 4, 5] activation 0.053561801197410074\n",
      "radius: 3 atom list: [11, 7, 2, 3, 4, 5, 6, 8] activation 0.053458164635042514\n",
      "radius: 3 atom list: [6, 7, 8, 2, 3, 4, 5] activation 0.05340430562060902\n",
      "radius: 3 atom list: [11, 10, 12, 5, 6, 7, 8, 9] activation 0.05332226459764302\n",
      "radius: 3 atom list: [8, 9, 10, 11, 12, 13, 14] activation 0.05325418807182294\n",
      "radius: 3 atom list: [21, 15, 16, 17, 18, 19, 20] activation 0.053235002314902656\n",
      "radius: 3 atom list: [1, 2, 3, 4, 5, 6, 0] activation 0.05321218846522428\n",
      "FP 43 has linear regression coefficient -0.0389675641411\n",
      "radius: 1 atom list: [9, 8] activation 0.037933035076816994\n",
      "radius: 1 atom list: [15, 16, 14, 18, 17, 13] activation 0.037798256290875666\n",
      "radius: 1 atom list: [3, 4] activation 0.036268834934381\n",
      "radius: 1 atom list: [18, 17] activation 0.03615525667873694\n",
      "radius: 1 atom list: [7, 8] activation 0.036150663419879235\n",
      "radius: 1 atom list: [0, 1] activation 0.036025391909003855\n",
      "radius: 1 atom list: [1, 0] activation 0.03585995833865838\n",
      "radius: 1 atom list: [6, 9] activation 0.03580201773685958\n",
      "radius: 1 atom list: [13, 14] activation 0.03565403175159783\n",
      "radius: 1 atom list: [28, 27] activation 0.03558496792747024\n",
      "radius: 1 atom list: [1, 0] activation 0.035452863953708336\n",
      "FP 44 has linear regression coefficient 0.0432332365603\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.03559610781024322\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.03557625647877003\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03526295311576646\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.035258336080683905\n",
      "radius: 2 atom list: [15, 1, 4, 5, 14, 3, 6, 24, 13] activation 0.03514512604694785\n",
      "radius: 2 atom list: [2, 18, 3, 5, 6, 7, 14, 16, 17, 4] activation 0.035084455140760105\n",
      "radius: 2 atom list: [15, 9, 3, 5, 6, 7, 12, 14, 16, 8] activation 0.035074851189724345\n",
      "radius: 2 atom list: [16, 2, 4, 6, 7, 8, 5, 18, 20, 19] activation 0.03506518862780669\n",
      "radius: 2 atom list: [4, 5, 6, 13, 3, 7, 12, 14, 18, 19] activation 0.035041016562215414\n",
      "radius: 2 atom list: [16, 3, 5, 6, 13, 15, 2, 7, 17, 4] activation 0.03501752251287611\n",
      "radius: 2 atom list: [5, 7, 8, 9, 13, 15, 10, 16, 11, 17] activation 0.03500769795864943\n",
      "FP 45 has linear regression coefficient 0.0152833407468\n",
      "radius: 1 atom list: [2, 1, 0] activation 0.01993367917691927\n",
      "radius: 1 atom list: [10, 11, 9] activation 0.019792330710057076\n",
      "radius: 1 atom list: [1, 3, 2, 4] activation 0.01976722291889126\n",
      "radius: 1 atom list: [13, 15, 14, 16] activation 0.01966768258248861\n",
      "radius: 1 atom list: [10, 8, 9] activation 0.019638975259619773\n",
      "radius: 1 atom list: [5, 7, 9, 6] activation 0.019629375861022712\n",
      "radius: 1 atom list: [5, 13, 6, 3] activation 0.019585255274071078\n",
      "radius: 1 atom list: [6, 7, 8] activation 0.0195657613868109\n",
      "radius: 1 atom list: [1, 4, 5, 8] activation 0.019565229356754612\n",
      "radius: 1 atom list: [3, 5, 4] activation 0.019562070890596358\n",
      "radius: 1 atom list: [2, 4, 5, 7] activation 0.019541855719255492\n",
      "FP 46 has linear regression coefficient -0.0902863890704\n",
      "radius: 3 atom list: [3, 4, 6, 7, 5, 9, 8] activation 0.15130807318439676\n",
      "radius: 3 atom list: [0, 2, 3, 6, 4, 5, 1] activation 0.148681187704041\n",
      "radius: 3 atom list: [0, 3, 4, 1, 2] activation 0.14600478950834134\n",
      "radius: 3 atom list: [4, 6, 1, 0, 2, 3, 5, 23] activation 0.14506936344988333\n",
      "radius: 3 atom list: [3, 7, 0, 2, 1, 6, 5, 4] activation 0.14463593275480152\n",
      "radius: 3 atom list: [10, 11, 7, 8, 9] activation 0.14453615428125396\n",
      "radius: 3 atom list: [3, 4, 5, 1, 2] activation 0.14450841140050474\n",
      "radius: 3 atom list: [10, 11, 9, 7, 8] activation 0.14449191433934405\n",
      "radius: 3 atom list: [9, 8, 10, 11, 6] activation 0.14447149084198474\n",
      "radius: 3 atom list: [0, 2, 4, 3, 1] activation 0.1443778893795986\n",
      "radius: 3 atom list: [4, 10, 7, 6, 9, 8, 3, 5] activation 0.14427860514397683\n",
      "FP 47 has linear regression coefficient 0.0483172117449\n",
      "radius: 2 atom list: [2, 4, 3, 5, 11, 6, 17, 10, 12, 16, 18, 22] activation 0.034805770461919606\n",
      "radius: 2 atom list: [2, 7, 8, 9, 16, 6, 10, 15, 17, 22] activation 0.03455110774240839\n",
      "radius: 2 atom list: [4, 5, 6, 12, 3, 7, 11, 13, 18, 19] activation 0.034529245982582356\n",
      "radius: 2 atom list: [15, 1, 4, 5, 14, 3, 6, 24, 13] activation 0.03424478161278737\n",
      "radius: 2 atom list: [4, 5, 6, 13, 3, 7, 12, 14, 18, 19] activation 0.03410034432625583\n",
      "radius: 2 atom list: [3, 4, 5, 6, 12, 13, 17, 18, 14, 7] activation 0.034061932172344274\n",
      "radius: 2 atom list: [17, 18, 21, 26, 19, 20, 27, 28, 33] activation 0.03396712698249898\n",
      "radius: 2 atom list: [4, 2, 3, 5, 6, 7, 13, 15, 16, 17] activation 0.03396105689357828\n",
      "radius: 2 atom list: [19, 1, 3, 7, 12, 14, 2, 5, 6, 13] activation 0.03396010991475019\n",
      "radius: 2 atom list: [22, 2, 6, 4, 3, 24, 5, 10, 18, 23] activation 0.03394760274485386\n",
      "radius: 2 atom list: [2, 18, 3, 5, 6, 7, 14, 16, 17, 4] activation 0.03379325680524073\n",
      "FP 48 has linear regression coefficient -0.0357322640512\n",
      "radius: 1 atom list: [9, 8] activation 0.031913662458153164\n",
      "radius: 1 atom list: [0, 1] activation 0.0318006673733016\n",
      "radius: 1 atom list: [18, 17] activation 0.0316692495534142\n",
      "radius: 1 atom list: [1, 0] activation 0.03157539500458591\n",
      "radius: 1 atom list: [3, 4] activation 0.03155128500593217\n",
      "radius: 1 atom list: [7, 8] activation 0.03154965899700452\n",
      "radius: 1 atom list: [6, 9] activation 0.031256599449854434\n",
      "radius: 1 atom list: [13, 14] activation 0.031115735793811036\n",
      "radius: 1 atom list: [13, 14] activation 0.031012699110292304\n",
      "radius: 1 atom list: [10, 11, 8, 9] activation 0.031007509899837075\n",
      "radius: 1 atom list: [1, 0] activation 0.031004476856638617\n",
      "FP 49 has linear regression coefficient -0.0613273087789\n",
      "radius: 1 atom list: [15, 16, 14, 18, 17, 13] activation 0.055873627272315214\n",
      "radius: 1 atom list: [9, 8] activation 0.05095652632154932\n",
      "radius: 2 atom list: [6, 4, 9, 8, 5, 7] activation 0.048857477608425584\n",
      "radius: 2 atom list: [4, 3, 0, 2, 1] activation 0.048703546707572064\n",
      "radius: 2 atom list: [7, 5, 9, 4, 6, 8] activation 0.048293922117132994\n",
      "radius: 2 atom list: [6, 5, 7, 8, 9] activation 0.04804842839654459\n",
      "radius: 2 atom list: [7, 5, 9, 8, 6] activation 0.04801400672623441\n",
      "radius: 2 atom list: [3, 4, 1] activation 0.04801059444274578\n",
      "radius: 2 atom list: [0, 2, 3, 4, 5, 1] activation 0.048000395809905505\n",
      "radius: 2 atom list: [1, 0, 2] activation 0.04799266597695721\n",
      "radius: 2 atom list: [1, 0, 2] activation 0.047987094508354915\n"
     ]
    }
   ],
   "source": [
    "trained_network_weights = train_neural_fingerprint()\n",
    "with open('results.pkl', 'w') as f:\n",
    "    pickle.dump(trained_network_weights, f)\n",
    "\n",
    "# Plotting.\n",
    "with open('results.pkl') as f:\n",
    "    trained_weights = pickle.load(f)\n",
    "plot(trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {'target_name' : 'logP',\n",
    "               'data_file'   : 'logP_wo_parameters'}\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "normalize = 1\n",
    "dropout = 0\n",
    "activation = relu\n",
    "params = {'fp_length': 50,\n",
    "            'fp_depth': 4,\n",
    "            'init_scale':np.exp(-4),\n",
    "            'l2_penalty':np.exp(-2),\n",
    "            'conv_width':20}\n",
    "\n",
    "conv_layer_sizes = [params['conv_width']] * params['fp_depth']\n",
    "conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                    'fp_length' : params['fp_length'],\n",
    "                    'normalize' : normalize,\n",
    "                    'return_atom_activations':False}\n",
    "\n",
    "all_radii = range(params['fp_depth'] + 1)\n",
    "\n",
    "# Plotting parameters\n",
    "num_figs_per_fp = 11\n",
    "figsize = (100, 100)\n",
    "highlight_color = (30.0/255.0, 100.0/255.0, 255.0/255.0)  # A nice light blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Regression on 8838 training points.\n",
      "--------------------------------------------------------------------------------\n",
      "Mean predictor\n",
      "\n",
      "Performance (RMSE) on logP:\n",
      "Train: 1.8146042407352028\n",
      "Test:  1.8141849992522239\n",
      "\n",
      "Performance (R2) on logP:\n",
      "Train: 0.0\n",
      "Test:  0.0\n",
      "--------------------------------------------------------------------------------\n",
      "Task params {'fp_length': 50, 'l2_penalty': 0.1353352832366127, 'fp_depth': 4, 'conv_width': 20, 'init_scale': 0.01831563888873418}\n",
      "Convnet fingerprints with neural net\n",
      "Total number of weights in the network: 30571\n",
      "max of weights 0.08535001578936458\n",
      "Iteration 0 loss 1.0041956952405227 train RMSE 1.8183273856100233 Train R2 0 : -22107.556204434153 Validation RMSE 0 : 1.789294284026002 Validation R2 0 : -21766.52263076145 max of weights 0.08585993000392719\n",
      "Iteration 10 loss 1.00273735062751 train RMSE 1.8170183194398009 Train R2 10 : -111.97485486306299 Validation RMSE 10 : 1.8001877783747764 Validation R2 10 : -111.66544845927847 max of weights 0.08939940050440684\n",
      "Iteration 20 loss 0.9836876077117314 train RMSE 1.799677497854971 Train R2 20 : -609.0845138806596 Validation RMSE 20 : 1.7752101542533445 Validation R2 20 : -607.8478618192668 max of weights 0.09489799237897517\n",
      "Iteration 30 loss 0.9833846078515265 train RMSE 1.7993935252022293 Train R2 30 : -177.3370802019511 Validation RMSE 30 : 1.7788326857296788 Validation R2 30 : -177.79787501101208 max of weights 0.10255027161161659\n",
      "Iteration 40 loss 0.9789600105920917 train RMSE 1.7953256460413733 Train R2 40 : -2648.9443911294406 Validation RMSE 40 : 1.765948562157426 Validation R2 40 : -2527.8658864127883 max of weights 0.11554277664331668\n",
      "Iteration 50 loss 0.9508568890834527 train RMSE 1.76933497000723 Train R2 50 : -369.0412971170286 Validation RMSE 50 : 1.7416254132670173 Validation R2 50 : -363.22133942187736 max of weights 0.13074227017093978\n",
      "Iteration 60 loss 0.924175327734089 train RMSE 1.744278738253183 Train R2 60 : -87.37365158244556 Validation RMSE 60 : 1.7200251146880923 Validation R2 60 : -86.02415256757894 max of weights 0.14503248426568785\n",
      "Iteration 70 loss 0.869150199791663 train RMSE 1.6914757460429384 Train R2 70 : -58.247623219668846 Validation RMSE 70 : 1.6604506914191162 Validation R2 70 : -54.34475834046219 max of weights 0.15898135367213925\n",
      "Iteration 80 loss 0.8180032623484047 train RMSE 1.6408456424470546 Train R2 80 : -26.581700300721682 Validation RMSE 80 : 1.6011223620712074 Validation R2 80 : -23.848358850720732 max of weights 0.17504794359463996\n",
      "Iteration 90 loss 0.7120476975992351 train RMSE 1.5307241577409374 Train R2 90 : -9.889102642787622 Validation RMSE 90 : 1.5023118843579728 Validation R2 90 : -8.985288874328045 max of weights 0.18707570746613555\n",
      "Iteration 100 loss 0.6251057482994511 train RMSE 1.4340403512257014 Train R2 100 : -4.46336465082318 Validation RMSE 100 : 1.4080530137643505 Validation R2 100 : -4.04426637006862 max of weights 0.20358407219331232\n",
      "Iteration 110 loss 0.5386698465809319 train RMSE 1.3309672148258058 Train R2 110 : -1.9712568714214456 Validation RMSE 110 : 1.3075959530434766 Validation R2 110 : -1.7381785375795578 max of weights 0.2195889182002014\n",
      "Iteration 120 loss 0.4604752464477912 train RMSE 1.2302874517792128 Train R2 120 : -0.6654299562291695 Validation RMSE 120 : 1.2121252348622888 Validation R2 120 : -0.5629864609393933 max of weights 0.2349046598036898\n",
      "Iteration 130 loss 0.41039478743725516 train RMSE 1.1611944240679455 Train R2 130 : -0.16759694799999147 Validation RMSE 130 : 1.1496296142952154 Validation R2 130 : -0.11962582116570153 max of weights 0.24487250191473459\n",
      "Iteration 140 loss 0.38976787445143163 train RMSE 1.1314390167928026 Train R2 140 : 0.16158304679440472 Validation RMSE 140 : 1.1122532486327161 Validation R2 140 : 0.21144325999690916 max of weights 0.2535262606711354\n",
      "Iteration 150 loss 0.3578480425779544 train RMSE 1.0838994532275772 Train R2 150 : 0.3076583657252945 Validation RMSE 150 : 1.096007059724523 Validation R2 150 : 0.30555486427410594 max of weights 0.2595232220865467\n",
      "Iteration 160 loss 0.32092511391232303 train RMSE 1.0262125280352403 Train R2 160 : 0.3974711491398605 Validation RMSE 160 : 1.018607041064921 Validation R2 160 : 0.40321408820802673 max of weights 0.26001546542985143\n",
      "Iteration 170 loss 0.3165150847682941 train RMSE 1.0190501561248095 Train R2 170 : 0.46436477859794234 Validation RMSE 170 : 1.0095585841240486 Validation R2 170 : 0.47768668430712347 max of weights 0.2625035174762853\n",
      "Iteration 180 loss 0.2938717927255363 train RMSE 0.9817368597744867 Train R2 180 : 0.5235682683006427 Validation RMSE 180 : 0.9886039228923427 Validation R2 180 : 0.5263342502977058 max of weights 0.26316719466813576\n",
      "Iteration 190 loss 0.2957111993350621 train RMSE 0.9847832120223481 Train R2 190 : 0.5054606900218694 Validation RMSE 190 : 0.9938182025630725 Validation R2 190 : 0.5032568851399045 max of weights 0.2705223700351065\n",
      "Iteration 200 loss 0.27382960379776883 train RMSE 0.947455534704637 Train R2 200 : 0.5522432044074694 Validation RMSE 200 : 0.9395396473745845 Validation R2 200 : 0.5642132059389707 max of weights 0.2839390721679027\n",
      "Iteration 210 loss 0.26910066111933695 train RMSE 0.9391457416936162 Train R2 210 : 0.5874503973709047 Validation RMSE 210 : 0.9252831050278234 Validation R2 210 : 0.6034231264394023 max of weights 0.29924127979947723\n",
      "Iteration 220 loss 0.2662970245563597 train RMSE 0.9341719544410656 Train R2 220 : 0.5889321797746943 Validation RMSE 220 : 0.9348240407195697 Validation R2 220 : 0.5895464339549967 max of weights 0.30709707324042174\n",
      "Iteration 230 loss 0.2731290863017289 train RMSE 0.9460847815034842 Train R2 230 : 0.6057963517995648 Validation RMSE 230 : 0.9307927540351654 Validation R2 230 : 0.624645358786553 max of weights 0.314969642136341\n",
      "Iteration 240 loss 0.2667066548971162 train RMSE 0.9348061533052286 Train R2 240 : 0.6280454017047454 Validation RMSE 240 : 0.9483458709557352 Validation R2 240 : 0.6204488354114301 max of weights 0.32396726117407243\n",
      "Iteration 250 loss 0.24394067559276 train RMSE 0.893790703508227 Train R2 250 : 0.6500503072782224 Validation RMSE 250 : 0.8832530301541519 Validation R2 250 : 0.6549876821334524 max of weights 0.33063368089879314\n",
      "Iteration 260 loss 0.24634935723946702 train RMSE 0.8982002501707018 Train R2 260 : 0.6555882910207851 Validation RMSE 260 : 0.8951941558131146 Validation R2 260 : 0.6603094961072336 max of weights 0.3370957531698449\n",
      "Iteration 270 loss 0.23366623143711118 train RMSE 0.8746329696285184 Train R2 270 : 0.6706290486598437 Validation RMSE 270 : 0.8792717277772888 Validation R2 270 : 0.6708268039044856 max of weights 0.3431538100544856\n",
      "Iteration 280 loss 0.24606905230933024 train RMSE 0.8976608910110601 Train R2 280 : 0.6478968385881259 Validation RMSE 280 : 0.9133575415427928 Validation R2 280 : 0.6403381475383176 max of weights 0.3524384964089902\n",
      "Iteration 290 loss 0.23178332264288826 train RMSE 0.8710482673560098 Train R2 290 : 0.675653645854327 Validation RMSE 290 : 0.8717011509434118 Validation R2 290 : 0.6789321261651784 max of weights 0.3574220637109623\n",
      "Iteration 300 loss 0.23503016092073503 train RMSE 0.8771368810174313 Train R2 300 : 0.680613249084067 Validation RMSE 300 : 0.8632912822962208 Validation R2 300 : 0.6926815546855081 max of weights 0.3660653193770359\n",
      "Iteration 310 loss 0.229808142201943 train RMSE 0.8672587436139955 Train R2 310 : 0.6788482865979292 Validation RMSE 310 : 0.8721544433309709 Validation R2 310 : 0.6757372015350497 max of weights 0.3648093610016024\n",
      "Iteration 320 loss 0.23637423191422063 train RMSE 0.879599658829229 Train R2 320 : 0.6790805619793212 Validation RMSE 320 : 0.8671635119963099 Validation R2 320 : 0.6918484826836115 max of weights 0.3671449211217113\n",
      "Iteration 330 loss 0.23337906946155945 train RMSE 0.8739508376380435 Train R2 330 : 0.697799039530568 Validation RMSE 330 : 0.8847283843662398 Validation R2 330 : 0.6913710567937131 max of weights 0.37134458687307076\n",
      "Iteration 340 loss 0.21293096304399745 train RMSE 0.8345295126759653 Train R2 340 : 0.7149939375156695 Validation RMSE 340 : 0.8314472139169565 Validation R2 340 : 0.7156286874625974 max of weights 0.37316234731692516\n",
      "Iteration 350 loss 0.21738989405024894 train RMSE 0.8432744256988914 Train R2 350 : 0.7106834228329371 Validation RMSE 350 : 0.8456020792513916 Validation R2 350 : 0.7115795325372389 max of weights 0.3757497777034769\n",
      "Iteration 360 loss 0.20686844862505638 train RMSE 0.8224715767692723 Train R2 360 : 0.7179664396393585 Validation RMSE 360 : 0.8257627675174528 Validation R2 360 : 0.7185027635072535 max of weights 0.3783184049102351\n",
      "Iteration 370 loss 0.21681249115862355 train RMSE 0.8421268214882998 Train R2 370 : 0.7054935363268917 Validation RMSE 370 : 0.8594421323090154 Validation R2 370 : 0.6969632297603033 max of weights 0.3837437388708765\n",
      "Iteration 380 loss 0.20932382238447955 train RMSE 0.827340189664821 Train R2 380 : 0.7262024479775571 Validation RMSE 380 : 0.833189499398382 Validation R2 380 : 0.7255206744944274 max of weights 0.38464359573320206\n",
      "Iteration 390 loss 0.21219093952962292 train RMSE 0.8330069451502313 Train R2 390 : 0.7218276338589071 Validation RMSE 390 : 0.8198045906138018 Validation R2 390 : 0.7320723847904094 max of weights 0.3893164477507604\n",
      "Iteration 400 loss 0.20465803707754263 train RMSE 0.817965332402832 Train R2 400 : 0.7257482833341646 Validation RMSE 400 : 0.8270373982180852 Validation R2 400 : 0.720317168802876 max of weights 0.3839547066104063\n",
      "Iteration 410 loss 0.20756155247529565 train RMSE 0.8237534758114682 Train R2 410 : 0.7288934281910305 Validation RMSE 410 : 0.8163934739657904 Validation R2 410 : 0.7367705079298035 max of weights 0.3838823439922559\n",
      "Iteration 420 loss 0.20520146793341962 train RMSE 0.8190019575910579 Train R2 420 : 0.7434219236104835 Validation RMSE 420 : 0.8254284061255313 Validation R2 420 : 0.7400994720818068 max of weights 0.38561314676670166\n",
      "Iteration 430 loss 0.19036692978301092 train RMSE 0.7886065171428359 Train R2 430 : 0.7548313091552132 Validation RMSE 430 : 0.7927506391818682 Validation R2 430 : 0.7521140270350629 max of weights 0.3858943170010474\n",
      "Iteration 440 loss 0.1918334863355846 train RMSE 0.7916566735776115 Train R2 440 : 0.752198892407836 Validation RMSE 440 : 0.7999606324212283 Validation R2 440 : 0.7494302089824327 max of weights 0.38636674663407156\n",
      "Iteration 450 loss 0.18743212851198196 train RMSE 0.7824414986754789 Train R2 450 : 0.7495319785612258 Validation RMSE 450 : 0.7862934469172072 Validation R2 450 : 0.7494357673173299 max of weights 0.3875078976523085\n",
      "Iteration 460 loss 0.1894212792087862 train RMSE 0.7865983654868431 Train R2 460 : 0.7502527036262019 Validation RMSE 460 : 0.800410535561184 Validation R2 460 : 0.7436116725957366 max of weights 0.3945000384204245\n",
      "Iteration 470 loss 0.1912349666159161 train RMSE 0.7903652398234452 Train R2 470 : 0.7610940581807042 Validation RMSE 470 : 0.7983455379580107 Validation R2 470 : 0.7595108602427446 max of weights 0.40377354540096866\n",
      "Iteration 480 loss 0.1903577854177013 train RMSE 0.7885186947966953 Train R2 480 : 0.7542392969416076 Validation RMSE 480 : 0.7753804025137859 Validation R2 480 : 0.7635406777235155 max of weights 0.4118961333237849\n",
      "Iteration 490 loss 0.18366544940173443 train RMSE 0.7743999731140534 Train R2 490 : 0.7600290846541102 Validation RMSE 490 : 0.7827374438254648 Validation R2 490 : 0.7548645399318664 max of weights 0.41789672222000385\n",
      "Iteration 500 loss 0.18074711390068546 train RMSE 0.7681313288410041 Train R2 500 : 0.7721518256468397 Validation RMSE 500 : 0.7665031154035452 Validation R2 500 : 0.7756958453777877 max of weights 0.42342352537263067\n",
      "Iteration 510 loss 0.18165333897493124 train RMSE 0.7700495353954526 Train R2 510 : 0.776677216287916 Validation RMSE 510 : 0.7712538834897263 Validation R2 510 : 0.7763106500710333 max of weights 0.42859801432506434\n",
      "Iteration 520 loss 0.17343306234365055 train RMSE 0.7522570663037056 Train R2 520 : 0.7824603788712066 Validation RMSE 520 : 0.761129242886727 Validation R2 520 : 0.7775839104008816 max of weights 0.43403798566480195\n",
      "Iteration 530 loss 0.17350673941175726 train RMSE 0.7524114682510966 Train R2 530 : 0.7820396228864459 Validation RMSE 530 : 0.7635791868281926 Validation R2 530 : 0.7781357059978826 max of weights 0.44111060221020326\n",
      "Iteration 540 loss 0.17048076106083082 train RMSE 0.7457486984655387 Train R2 540 : 0.7778792467555031 Validation RMSE 540 : 0.7526503008584164 Validation R2 540 : 0.7765778205746818 max of weights 0.44992028788644883\n",
      "Iteration 550 loss 0.1667368688717704 train RMSE 0.737418763612612 Train R2 550 : 0.7856582424006412 Validation RMSE 550 : 0.7452986691290473 Validation R2 550 : 0.782823149335788 max of weights 0.4571612456405718\n",
      "Iteration 560 loss 0.17416142990302808 train RMSE 0.7537902136788905 Train R2 560 : 0.7892605857948707 Validation RMSE 560 : 0.761435642541555 Validation R2 560 : 0.7883057269382545 max of weights 0.46248834191572774\n",
      "Iteration 570 loss 0.17347519624930755 train RMSE 0.7522730343177506 Train R2 570 : 0.7766389906203222 Validation RMSE 570 : 0.7404032428856792 Validation R2 570 : 0.7843013635925336 max of weights 0.4679926821690365\n",
      "Iteration 580 loss 0.16405579140507773 train RMSE 0.7313439644202433 Train R2 580 : 0.7907711547040241 Validation RMSE 580 : 0.7393432705225756 Validation R2 580 : 0.7862540962408315 max of weights 0.47314661851599576\n",
      "Iteration 590 loss 0.1638014250916596 train RMSE 0.7307304581011965 Train R2 590 : 0.802664677957242 Validation RMSE 590 : 0.7375493185272914 Validation R2 590 : 0.8015904164740386 max of weights 0.47991055474299865\n",
      "Iteration 600 loss 0.16281743978184343 train RMSE 0.7284903483094674 Train R2 600 : 0.8026663928748008 Validation RMSE 600 : 0.7283820211091085 Validation R2 600 : 0.8028181183606338 max of weights 0.4850118403756113\n",
      "Iteration 610 loss 0.15808689437107806 train RMSE 0.7177088437861234 Train R2 610 : 0.8059736508756257 Validation RMSE 610 : 0.7300247273809989 Validation R2 610 : 0.8000117445417274 max of weights 0.4923220079512641\n",
      "Iteration 620 loss 0.15889906351858293 train RMSE 0.7195604218336166 Train R2 620 : 0.8053197159557595 Validation RMSE 620 : 0.7357519449407147 Validation R2 620 : 0.7995512963157588 max of weights 0.5037674847400406\n",
      "Iteration 630 loss 0.15672951535733026 train RMSE 0.7145642677643907 Train R2 630 : 0.8028756940587349 Validation RMSE 630 : 0.7293271867089351 Validation R2 630 : 0.7973793429449698 max of weights 0.5147896436531199\n",
      "Iteration 640 loss 0.15164226913680787 train RMSE 0.7027257952180345 Train R2 640 : 0.8116492922799055 Validation RMSE 640 : 0.7064290612739075 Validation R2 640 : 0.8110441155593093 max of weights 0.5232945371412857\n",
      "Iteration 650 loss 0.1590990978781878 train RMSE 0.7199626281069446 Train R2 650 : 0.8121965490497645 Validation RMSE 650 : 0.7274120792295248 Validation R2 650 : 0.8115994882067591 max of weights 0.5280438206451762\n",
      "Iteration 660 loss 0.15956820602711355 train RMSE 0.7210211643496179 Train R2 660 : 0.795421111450995 Validation RMSE 660 : 0.7123300744650796 Validation R2 660 : 0.800702935870046 max of weights 0.5332688487331645\n",
      "Iteration 670 loss 0.15193895413091746 train RMSE 0.7033578923833331 Train R2 670 : 0.8103423852845065 Validation RMSE 670 : 0.7104799751213785 Validation R2 670 : 0.8069930618514777 max of weights 0.5398545314573001\n",
      "Iteration 680 loss 0.15656750410172376 train RMSE 0.7140701167718863 Train R2 680 : 0.8184633675569503 Validation RMSE 680 : 0.7277521642898613 Validation R2 680 : 0.8136822649772495 max of weights 0.5463924315349814\n",
      "Iteration 690 loss 0.15117839190865862 train RMSE 0.701517766579271 Train R2 690 : 0.8191632281413094 Validation RMSE 690 : 0.7038648952080009 Validation R2 690 : 0.8174930416331345 max of weights 0.5517309089898056\n",
      "Iteration 700 loss 0.14598082539976862 train RMSE 0.689201619267457 Train R2 700 : 0.8247533619225392 Validation RMSE 700 : 0.702629656797535 Validation R2 700 : 0.8187307999261239 max of weights 0.5585587733929976\n",
      "Iteration 710 loss 0.14879755370886738 train RMSE 0.6958914665190273 Train R2 710 : 0.8223333380703155 Validation RMSE 710 : 0.7165550533628823 Validation R2 710 : 0.8143553632630062 max of weights 0.5691066136228924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 720 loss 0.14835179807202856 train RMSE 0.6948236128476037 Train R2 720 : 0.8180406522189897 Validation RMSE 720 : 0.711683412537445 Validation R2 720 : 0.810815808051538 max of weights 0.5791162844924415\n",
      "Iteration 730 loss 0.1419068642314763 train RMSE 0.6793615546571781 Train R2 730 : 0.8279495965605145 Validation RMSE 730 : 0.6815060914339749 Validation R2 730 : 0.8279013989733897 max of weights 0.5865479412308696\n",
      "Iteration 740 loss 0.1483888952611154 train RMSE 0.6948737901107499 Train R2 740 : 0.8286902974892062 Validation RMSE 740 : 0.7058477111189645 Validation R2 740 : 0.8262762951799777 max of weights 0.5909151346221024\n",
      "Iteration 750 loss 0.15051951212114395 train RMSE 0.6998953641200406 Train R2 750 : 0.8101860863143744 Validation RMSE 750 : 0.6968952312262747 Validation R2 750 : 0.8113070473969448 max of weights 0.59574089942562\n",
      "Iteration 760 loss 0.14492985529687083 train RMSE 0.6865924143023695 Train R2 760 : 0.8232727352512635 Validation RMSE 760 : 0.6933115827887004 Validation R2 760 : 0.8204451557284249 max of weights 0.6017266798935669\n",
      "Iteration 770 loss 0.15369212017435155 train RMSE 0.7072569689393109 Train R2 770 : 0.8266058959717341 Validation RMSE 770 : 0.7269341737380602 Validation R2 770 : 0.8182627304804037 max of weights 0.6079313986723843\n",
      "Iteration 780 loss 0.14289999879545323 train RMSE 0.6816579448253013 Train R2 780 : 0.831978380789968 Validation RMSE 780 : 0.6867800081383234 Validation R2 780 : 0.8286280391442469 max of weights 0.6135033404864967\n",
      "Iteration 790 loss 0.138599987326635 train RMSE 0.6711845745557111 Train R2 790 : 0.8364949779193492 Validation RMSE 790 : 0.6849592759863172 Validation R2 790 : 0.8307946090856837 max of weights 0.6197927942380305\n",
      "Iteration 800 loss 0.1392708289004137 train RMSE 0.6728235500146934 Train R2 800 : 0.8362842440088044 Validation RMSE 800 : 0.6966663608153931 Validation R2 800 : 0.8265061359604682 max of weights 0.6287540565270289\n",
      "Iteration 810 loss 0.1414847570764892 train RMSE 0.6782054758322162 Train R2 810 : 0.8302942188535823 Validation RMSE 810 : 0.6970138524649329 Validation R2 810 : 0.8215509311281702 max of weights 0.6375056392850627\n",
      "Iteration 820 loss 0.13365165016196773 train RMSE 0.6588962710436315 Train R2 820 : 0.8414308177781038 Validation RMSE 820 : 0.6657758810481125 Validation R2 820 : 0.8388590541907842 max of weights 0.6438313471733332\n",
      "Iteration 830 loss 0.13821474230581557 train RMSE 0.6701833817020377 Train R2 830 : 0.8426436513075408 Validation RMSE 830 : 0.6846985407260519 Validation R2 830 : 0.8381741607090734 max of weights 0.6478428601138427\n",
      "Iteration 840 loss 0.14524201411673215 train RMSE 0.6872249187135921 Train R2 840 : 0.8208667452975369 Validation RMSE 840 : 0.6896831049211447 Validation R2 840 : 0.8181390829421428 max of weights 0.6524915607598643\n",
      "Iteration 850 loss 0.14132507127844168 train RMSE 0.6777478264505911 Train R2 850 : 0.8313864008200431 Validation RMSE 850 : 0.6831514829063404 Validation R2 850 : 0.8294554700794662 max of weights 0.6582490350851955\n",
      "Iteration 860 loss 0.1518674583903104 train RMSE 0.7028636128479948 Train R2 860 : 0.8324125926475705 Validation RMSE 860 : 0.7273331265194032 Validation R2 860 : 0.8212799760414891 max of weights 0.6641605291344712\n",
      "Iteration 870 loss 0.1346131529610469 train RMSE 0.6612031382741532 Train R2 870 : 0.8437417877047301 Validation RMSE 870 : 0.6694427846591418 Validation R2 870 : 0.8388503120702344 max of weights 0.669758393727691\n",
      "Iteration 880 loss 0.13674295659623 train RMSE 0.6664802580094167 Train R2 880 : 0.8407484758593675 Validation RMSE 880 : 0.6790655042704553 Validation R2 880 : 0.8362408571204938 \n",
      "Performance (RMSE) on logP:\n",
      "Train: 0.6671997360781458\n",
      "Test:  0.7693981652457923\n",
      "\n",
      "Performance (R2) on logP:\n",
      "Train: 0.8407282474126163\n",
      "Test:  0.8015678595817767\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_network_weights = train_neural_fingerprint()\n",
    "with open('results'+task_params['data_file']+'.pkl', 'w') as f:\n",
    "    pickle.dump(trained_network_weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different errors for molecules with duplicates and unique measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_params = {'target_name' : 'logP',\n",
    "               'data_file'   : 'logP_wo_parameters'}\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "normalize = 1\n",
    "dropout = 0\n",
    "activation = relu\n",
    "params = {'fp_length': 50,\n",
    "            'fp_depth': 4,\n",
    "            'init_scale':np.exp(-4),\n",
    "            'l2_penalty':np.exp(-2),\n",
    "            'conv_width':20}\n",
    "\n",
    "conv_layer_sizes = [params['conv_width']] * params['fp_depth']\n",
    "conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                    'fp_length' : params['fp_length'],\n",
    "                    'normalize' : normalize,\n",
    "                    'return_atom_activations':False}\n",
    "\n",
    "all_radii = range(params['fp_depth'] + 1)\n",
    "\n",
    "# Plotting parameters\n",
    "num_figs_per_fp = 11\n",
    "figsize = (100, 100)\n",
    "highlight_color = (30.0/255.0, 100.0/255.0, 255.0/255.0)  # A nice light blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Regression on 8838 training points.\n",
      "--------------------------------------------------------------------------------\n",
      "Mean predictor\n",
      "\n",
      "Performance (RMSE) on logP:\n",
      "Train: 1.8146042407352028\n",
      "Test:  1.8141849992522239\n",
      "Dub: 2.1832152392353295\n",
      "Unique:  1.8014880653973313\n",
      "\n",
      "Performance (R2) on logP:\n",
      "Train: 0.0\n",
      "Test:  0.0\n",
      "Dub: 0.0\n",
      "Unique:  0.0\n",
      "--------------------------------------------------------------------------------\n",
      "Task params {'fp_length': 50, 'l2_penalty': 0.1353352832366127, 'fp_depth': 4, 'conv_width': 20, 'init_scale': 0.01831563888873418}\n",
      "Convnet fingerprints with neural net\n",
      "Total number of weights in the network: 30571\n",
      "max of weights 0.08535001578936458\n",
      "Iteration 0 loss 1.0041956952405227 train RMSE 1.8183273856100233 Train R2 0 : -22107.556204434153 Validation RMSE 0 : 1.789294284026002 Validation R2 0 : -21766.52263076145 Dub RMSE 0 : 2.1911715822316413\n",
      "Unique RMSE 0 : 1.8048158473406302\n",
      "Dub R2 0 : -34402.64057304353\n",
      "Unique R2 0 : -22017.617080364544\n",
      "max of weights 0.08585993000392719\n",
      "Iteration 10 loss 1.00273735062751 train RMSE 1.8170183194398009 Train R2 10 : -111.97485486306299 Validation RMSE 10 : 1.8001877783747764 Validation R2 10 : -111.66544845927847 Dub RMSE 10 : 2.1497013873740247\n",
      "Unique RMSE 10 : 1.806769606205297\n",
      "Dub R2 10 : -167.94516062236858\n",
      "Unique R2 10 : -111.88253482843905\n",
      "max of weights 0.08939940050440684\n",
      "Iteration 20 loss 0.9836876077117314 train RMSE 1.799677497854971 Train R2 20 : -609.0845138806596 Validation RMSE 20 : 1.7752101542533445 Validation R2 20 : -607.8478618192668 Dub RMSE 20 : 2.1562788382822995\n",
      "Unique RMSE 20 : 1.7874589806602554\n",
      "Dub R2 20 : -928.4895452670255\n",
      "Unique R2 20 : -607.6414473659906\n",
      "max of weights 0.09489799237897517\n",
      "Iteration 30 loss 0.9833846078515265 train RMSE 1.7993935252022293 Train R2 30 : -177.3370802019511 Validation RMSE 30 : 1.7788326857296788 Validation R2 30 : -177.79787501101208 Dub RMSE 30 : 2.141176321309471\n",
      "Unique RMSE 30 : 1.7882021488472368\n",
      "Dub R2 30 : -265.7877774423833\n",
      "Unique R2 30 : -177.0673573099392\n",
      "max of weights 0.10255027161161659\n",
      "Iteration 40 loss 0.9789600105920917 train RMSE 1.7953256460413733 Train R2 40 : -2648.9443911294406 Validation RMSE 40 : 1.765948562157426 Validation R2 40 : -2527.8658864127883 Dub RMSE 40 : 2.162003930488877\n",
      "Unique RMSE 40 : 1.7817401067834637\n",
      "Dub R2 40 : -2874.2634511352544\n",
      "Unique R2 40 : -2614.329315784935\n",
      "max of weights 0.11554277664331668\n",
      "Iteration 50 loss 0.9508568890834527 train RMSE 1.76933497000723 Train R2 50 : -369.0412971170286 Validation RMSE 50 : 1.7416254132670173 Validation R2 50 : -363.22133942187736 Dub RMSE 50 : 2.1207435048630936\n",
      "Unique RMSE 50 : 1.7563279685845943\n",
      "Dub R2 50 : -439.58705065670864\n",
      "Unique R2 50 : -365.0622735848975\n",
      "max of weights 0.13074227017093978\n",
      "Iteration 60 loss 0.924175327734089 train RMSE 1.744278738253183 Train R2 60 : -87.37365158244556 Validation RMSE 60 : 1.7200251146880923 Validation R2 60 : -86.02415256757894 Dub RMSE 60 : 2.0677879773317787\n",
      "Unique RMSE 60 : 1.7323635107324011\n",
      "Dub R2 60 : -106.4739696987601\n",
      "Unique R2 60 : -86.37121802752715\n",
      "max of weights 0.14503248426568785\n",
      "Iteration 70 loss 0.869150199791663 train RMSE 1.6914757460429384 Train R2 70 : -58.247623219668846 Validation RMSE 70 : 1.6604506914191162 Validation R2 70 : -54.34475834046219 Dub RMSE 70 : 2.014490266393595\n",
      "Unique RMSE 70 : 1.6780407737928973\n",
      "Dub R2 70 : -63.18482389277959\n",
      "Unique R2 70 : -56.92614596340744\n",
      "max of weights 0.15898135367213925\n",
      "Iteration 80 loss 0.8180032623484047 train RMSE 1.6408456424470546 Train R2 80 : -26.581700300721682 Validation RMSE 80 : 1.6011223620712074 Validation R2 80 : -23.848358850720732 Dub RMSE 80 : 1.9610569181803537\n",
      "Unique RMSE 80 : 1.6253864243187444\n",
      "Dub R2 80 : -29.090918247581072\n",
      "Unique R2 80 : -25.807874037031834\n",
      "max of weights 0.17504794359463996\n",
      "Iteration 90 loss 0.7120476975992351 train RMSE 1.5307241577409374 Train R2 90 : -9.889102642787622 Validation RMSE 90 : 1.5023118843579728 Validation R2 90 : -8.985288874328045 Dub RMSE 90 : 1.7811071628487498\n",
      "Unique RMSE 90 : 1.5195840481418976\n",
      "Dub R2 90 : -10.790999343198495\n",
      "Unique R2 90 : -9.57970988354449\n",
      "max of weights 0.18707570746613555\n",
      "Iteration 100 loss 0.6251057482994511 train RMSE 1.4340403512257014 Train R2 100 : -4.46336465082318 Validation RMSE 100 : 1.4080530137643505 Validation R2 100 : -4.04426637006862 Dub RMSE 100 : 1.6378394090047492\n",
      "Unique RMSE 100 : 1.4245341847131099\n",
      "Dub R2 100 : -4.32073677960984\n",
      "Unique R2 100 : -4.3411513435580265\n",
      "max of weights 0.20358407219331232\n",
      "Iteration 110 loss 0.5386698465809319 train RMSE 1.3309672148258058 Train R2 110 : -1.9712568714214456 Validation RMSE 110 : 1.3075959530434766 Validation R2 110 : -1.7381785375795578 Dub RMSE 110 : 1.4824238723763856\n",
      "Unique RMSE 110 : 1.3235858344056441\n",
      "Dub R2 110 : -1.5642019399169977\n",
      "Unique R2 110 : -1.919986660855705\n",
      "max of weights 0.2195889182002014\n",
      "Iteration 120 loss 0.4604752464477912 train RMSE 1.2302874517792128 Train R2 120 : -0.6654299562291695 Validation RMSE 120 : 1.2121252348622888 Validation R2 120 : -0.5629864609393933 Dub RMSE 120 : 1.3454422452311519\n",
      "Unique RMSE 120 : 1.224677351228461\n",
      "Dub R2 120 : -0.3662183566919013\n",
      "Unique R2 120 : -0.647423708245016\n",
      "max of weights 0.2349046598036898\n",
      "Iteration 130 loss 0.41039478743725516 train RMSE 1.1611944240679455 Train R2 130 : -0.16759694799999147 Validation RMSE 130 : 1.1496296142952154 Validation R2 130 : -0.11962582116570153 Dub RMSE 130 : 1.279565775692149\n",
      "Unique RMSE 130 : 1.157315332354393\n",
      "Dub R2 130 : 0.05368533443494716\n",
      "Unique R2 130 : -0.16363350971730628\n",
      "max of weights 0.24487250191473459\n",
      "Iteration 140 loss 0.38976787445143163 train RMSE 1.1314390167928026 Train R2 140 : 0.16158304679440472 Validation RMSE 140 : 1.1122532486327161 Validation R2 140 : 0.21144325999690916 Dub RMSE 140 : 1.2240656293969119\n",
      "Unique RMSE 140 : 1.1244538293968893\n",
      "Dub R2 140 : 0.33156978405983195\n",
      "Unique R2 140 : 0.16984250602411377\n",
      "max of weights 0.2535262606711354\n",
      "Iteration 150 loss 0.3578480425779544 train RMSE 1.0838994532275772 Train R2 150 : 0.3076583657252945 Validation RMSE 150 : 1.096007059724523 Validation R2 150 : 0.30555486427410594 Dub RMSE 150 : 1.1244872938227994\n",
      "Unique RMSE 150 : 1.086671931683169\n",
      "Dub R2 150 : 0.4902196844556985\n",
      "Unique R2 150 : 0.30313623224607567\n",
      "max of weights 0.2595232220865467\n",
      "Iteration 160 loss 0.32092511391232303 train RMSE 1.0262125280352403 Train R2 160 : 0.3974711491398605 Validation RMSE 160 : 1.018607041064921 Validation R2 160 : 0.40321408820802673 Dub RMSE 160 : 1.1331337791388658\n",
      "Unique RMSE 160 : 1.0237052590127\n",
      "Dub R2 160 : 0.5138035009534476\n",
      "Unique R2 160 : 0.39463997859845457\n",
      "max of weights 0.26001546542985143\n",
      "Iteration 170 loss 0.3165150847682941 train RMSE 1.0190501561248095 Train R2 170 : 0.46436477859794234 Validation RMSE 170 : 1.0095585841240486 Validation R2 170 : 0.47768668430712347 Dub RMSE 170 : 1.1140431857449737\n",
      "Unique RMSE 170 : 1.0147777463472443\n",
      "Dub R2 170 : 0.5749425851627037\n",
      "Unique R2 170 : 0.4649401353018643\n",
      "max of weights 0.2625035174762853\n",
      "Iteration 180 loss 0.2938717927255363 train RMSE 0.9817368597744867 Train R2 180 : 0.5235682683006427 Validation RMSE 180 : 0.9886039228923427 Validation R2 180 : 0.5263342502977058 Dub RMSE 180 : 1.040227052870118\n",
      "Unique RMSE 180 : 0.9835092236934255\n",
      "Dub R2 180 : 0.6380987131280675\n",
      "Unique R2 180 : 0.5210361358209458\n",
      "max of weights 0.26316719466813576\n",
      "Iteration 190 loss 0.2957111993350621 train RMSE 0.9847832120223481 Train R2 190 : 0.5054606900218694 Validation RMSE 190 : 0.9938182025630725 Validation R2 190 : 0.5032568851399045 Dub RMSE 190 : 1.0634677807566315\n",
      "Unique RMSE 190 : 0.9862444663171609\n",
      "Dub R2 190 : 0.5974087684506202\n",
      "Unique R2 190 : 0.5028490979851279\n",
      "max of weights 0.2705223700351065\n",
      "Iteration 200 loss 0.27382960379776883 train RMSE 0.947455534704637 Train R2 200 : 0.5522432044074694 Validation RMSE 200 : 0.9395396473745845 Validation R2 200 : 0.5642132059389707 Dub RMSE 200 : 1.0327866419596492\n",
      "Unique RMSE 200 : 0.9465660182628091\n",
      "Dub R2 200 : 0.6415614260909577\n",
      "Unique R2 200 : 0.5515769021807793\n",
      "max of weights 0.2839390721679027\n",
      "Iteration 210 loss 0.26910066111933695 train RMSE 0.9391457416936162 Train R2 210 : 0.5874503973709047 Validation RMSE 210 : 0.9252831050278234 Validation R2 210 : 0.6034231264394023 Dub RMSE 210 : 1.0276134919551276\n",
      "Unique RMSE 210 : 0.9355832468004047\n",
      "Dub R2 210 : 0.6720044979509939\n",
      "Unique R2 210 : 0.5886961887518507\n",
      "max of weights 0.29924127979947723\n",
      "Iteration 220 loss 0.2662970245563597 train RMSE 0.9341719544410656 Train R2 220 : 0.5889321797746943 Validation RMSE 220 : 0.9348240407195697 Validation R2 220 : 0.5895464339549967 Dub RMSE 220 : 1.045236524229758\n",
      "Unique RMSE 220 : 0.9354413136187543\n",
      "Dub R2 220 : 0.6496898457906327\n",
      "Unique R2 220 : 0.5861736376913363\n",
      "max of weights 0.30709707324042174\n",
      "Iteration 230 loss 0.2731290863017289 train RMSE 0.9460847815034842 Train R2 230 : 0.6057963517995648 Validation RMSE 230 : 0.9307927540351654 Validation R2 230 : 0.624645358786553 Dub RMSE 230 : 1.0728119665611775\n",
      "Unique RMSE 230 : 0.9412803276043998\n",
      "Dub R2 230 : 0.6581572409205869\n",
      "Unique R2 230 : 0.6089941630958776\n",
      "max of weights 0.314969642136341\n",
      "Iteration 240 loss 0.2667066548971162 train RMSE 0.9348061533052286 Train R2 240 : 0.6280454017047454 Validation RMSE 240 : 0.9483458709557352 Validation R2 240 : 0.6204488354114301 Dub RMSE 240 : 0.9914430276737133\n",
      "Unique RMSE 240 : 0.9406095391417323\n",
      "Dub R2 240 : 0.7065519279775805\n",
      "Unique R2 240 : 0.6232360287174177\n",
      "max of weights 0.32396726117407243\n",
      "Iteration 250 loss 0.24394067559276 train RMSE 0.893790703508227 Train R2 250 : 0.6500503072782224 Validation RMSE 250 : 0.8832530301541519 Validation R2 250 : 0.6549876821334524 Dub RMSE 250 : 1.0235588577553894\n",
      "Unique RMSE 250 : 0.8927398862324074\n",
      "Dub R2 250 : 0.6886089733852561\n",
      "Unique R2 250 : 0.6484724828048967\n",
      "max of weights 0.33063368089879314\n",
      "Iteration 260 loss 0.24634935723946702 train RMSE 0.8982002501707018 Train R2 260 : 0.6555882910207851 Validation RMSE 260 : 0.8951941558131146 Validation R2 260 : 0.6603094961072336 Dub RMSE 260 : 1.0115680091863115\n",
      "Unique RMSE 260 : 0.8982035755664133\n",
      "Dub R2 260 : 0.7040973117069085\n",
      "Unique R2 260 : 0.654481631543146\n",
      "max of weights 0.3370957531698449\n",
      "Iteration 270 loss 0.23366623143711118 train RMSE 0.8746329696285184 Train R2 270 : 0.6706290486598437 Validation RMSE 270 : 0.8792717277772888 Validation R2 270 : 0.6708268039044856 Dub RMSE 270 : 0.965024202770755\n",
      "Unique RMSE 270 : 0.878290587413206\n",
      "Dub R2 270 : 0.7260939371122408\n",
      "Unique R2 270 : 0.6675833000558017\n",
      "max of weights 0.3431538100544856\n",
      "Iteration 280 loss 0.24606905230933024 train RMSE 0.8976608910110601 Train R2 280 : 0.6478968385881259 Validation RMSE 280 : 0.9133575415427928 Validation R2 280 : 0.6403381475383176 Dub RMSE 280 : 0.9732000946492155\n",
      "Unique RMSE 280 : 0.9036606311826125\n",
      "Dub R2 280 : 0.7048397697360489\n",
      "Unique R2 280 : 0.6435976518363231\n",
      "max of weights 0.3524384964089902\n",
      "Iteration 290 loss 0.23178332264288826 train RMSE 0.8710482673560098 Train R2 290 : 0.675653645854327 Validation RMSE 290 : 0.8717011509434118 Validation R2 290 : 0.6789321261651784 Dub RMSE 290 : 0.9589867112567105\n",
      "Unique RMSE 290 : 0.8748589158811384\n",
      "Dub R2 290 : 0.7302195458141754\n",
      "Unique R2 290 : 0.6726276798528208\n",
      "max of weights 0.3574220637109623\n",
      "Iteration 300 loss 0.23503016092073503 train RMSE 0.8771368810174313 Train R2 300 : 0.680613249084067 Validation RMSE 300 : 0.8632912822962208 Validation R2 300 : 0.6926815546855081 Dub RMSE 300 : 0.9875224631140465\n",
      "Unique RMSE 300 : 0.8749644173899738\n",
      "Dub R2 300 : 0.7294213969894663\n",
      "Unique R2 300 : 0.6808968882818633\n",
      "max of weights 0.3660653193770359\n",
      "Iteration 310 loss 0.229808142201943 train RMSE 0.8672587436139955 Train R2 310 : 0.6788482865979292 Validation RMSE 310 : 0.8721544433309709 Validation R2 310 : 0.6757372015350497 Dub RMSE 310 : 0.9823327298436727\n",
      "Unique RMSE 310 : 0.872048334616082\n",
      "Dub R2 310 : 0.7139629981032632\n",
      "Unique R2 310 : 0.6746282736883249\n",
      "max of weights 0.3648093610016024\n",
      "Iteration 320 loss 0.23637423191422063 train RMSE 0.879599658829229 Train R2 320 : 0.6790805619793212 Validation RMSE 320 : 0.8671635119963099 Validation R2 320 : 0.6918484826836115 Dub RMSE 320 : 1.019190677943093\n",
      "Unique RMSE 320 : 0.8773968720832209\n",
      "Dub R2 320 : 0.7032704667057768\n",
      "Unique R2 320 : 0.6804394322459921\n",
      "max of weights 0.3671449211217113\n",
      "Iteration 330 loss 0.23337906946155945 train RMSE 0.8739508376380435 Train R2 330 : 0.697799039530568 Validation RMSE 330 : 0.8847283843662398 Validation R2 330 : 0.6913710567937131 Dub RMSE 330 : 0.928954567599862\n",
      "Unique RMSE 330 : 0.8817070557102747\n",
      "Dub R2 330 : 0.7567457105592158\n",
      "Unique R2 330 : 0.69242114299119\n",
      "max of weights 0.37134458687307076\n",
      "Iteration 340 loss 0.21293096304399745 train RMSE 0.8345295126759653 Train R2 340 : 0.7149939375156695 Validation RMSE 340 : 0.8314472139169565 Validation R2 340 : 0.7156286874625974 Dub RMSE 340 : 0.9565430855554108\n",
      "Unique RMSE 340 : 0.8366464784920957\n",
      "Dub R2 340 : 0.7386356034259097\n",
      "Unique R2 340 : 0.712319098519566\n",
      "max of weights 0.37316234731692516\n",
      "Iteration 350 loss 0.21738989405024894 train RMSE 0.8432744256988914 Train R2 350 : 0.7106834228329371 Validation RMSE 350 : 0.8456020792513916 Validation R2 350 : 0.7115795325372389 Dub RMSE 350 : 0.964623126845349\n",
      "Unique RMSE 350 : 0.8459950072985605\n",
      "Dub R2 350 : 0.7397982620484906\n",
      "Unique R2 350 : 0.7083538908182649\n",
      "max of weights 0.3757497777034769\n",
      "Iteration 360 loss 0.20686844862505638 train RMSE 0.8224715767692723 Train R2 360 : 0.7179664396393585 Validation RMSE 360 : 0.8257627675174528 Validation R2 360 : 0.7185027635072535 Dub RMSE 360 : 0.9397330724088484\n",
      "Unique RMSE 360 : 0.8270591773302826\n",
      "Dub R2 360 : 0.7451937304359042\n",
      "Unique R2 360 : 0.7147972193673765\n",
      "max of weights 0.3783184049102351\n",
      "Iteration 370 loss 0.21681249115862355 train RMSE 0.8421268214882998 Train R2 370 : 0.7054935363268917 Validation RMSE 370 : 0.8594421323090154 Validation R2 370 : 0.6969632297603033 Dub RMSE 370 : 0.9297800664455705\n",
      "Unique RMSE 370 : 0.8499651227013649\n",
      "Dub R2 370 : 0.7402449047032\n",
      "Unique R2 370 : 0.7005397117994615\n",
      "max of weights 0.3837437388708765\n",
      "Iteration 380 loss 0.20932382238447955 train RMSE 0.827340189664821 Train R2 380 : 0.7262024479775571 Validation RMSE 380 : 0.833189499398382 Validation R2 380 : 0.7255206744944274 Dub RMSE 380 : 0.9127829943150112\n",
      "Unique RMSE 380 : 0.8345039613173445\n",
      "Dub R2 380 : 0.7681079546921338\n",
      "Unique R2 380 : 0.721607895461218\n",
      "max of weights 0.38464359573320206\n",
      "Iteration 390 loss 0.21219093952962292 train RMSE 0.8330069451502313 Train R2 390 : 0.7218276338589071 Validation RMSE 390 : 0.8198045906138018 Validation R2 390 : 0.7320723847904094 Dub RMSE 390 : 0.961058667775122\n",
      "Unique RMSE 390 : 0.8320669176680773\n",
      "Dub R2 390 : 0.7500728777787894\n",
      "Unique R2 390 : 0.7214688602491997\n",
      "max of weights 0.3893164477507604\n",
      "Iteration 400 loss 0.20465803707754263 train RMSE 0.817965332402832 Train R2 400 : 0.7257482833341646 Validation RMSE 400 : 0.8270373982180852 Validation R2 400 : 0.720317168802876 Dub RMSE 400 : 0.929613029336716\n",
      "Unique RMSE 400 : 0.8256835908722406\n",
      "Dub R2 400 : 0.7487600997089909\n",
      "Unique R2 400 : 0.7203729008932642\n",
      "max of weights 0.3839547066104063\n",
      "Iteration 410 loss 0.20756155247529565 train RMSE 0.8237534758114682 Train R2 410 : 0.7288934281910305 Validation RMSE 410 : 0.8163934739657904 Validation R2 410 : 0.7367705079298035 Dub RMSE 410 : 0.9677737756662765\n",
      "Unique RMSE 410 : 0.8246020066032552\n",
      "Dub R2 410 : 0.7376416727087203\n",
      "Unique R2 410 : 0.7284442395359538\n",
      "max of weights 0.3838823439922559\n",
      "Iteration 420 loss 0.20520146793341962 train RMSE 0.8190019575910579 Train R2 420 : 0.7434219236104835 Validation RMSE 420 : 0.8254284061255313 Validation R2 420 : 0.7400994720818068 Dub RMSE 420 : 0.8853046460110988\n",
      "Unique RMSE 420 : 0.8280429793390944\n",
      "Dub R2 420 : 0.7855092366183766\n",
      "Unique R2 420 : 0.7378701237459917\n",
      "max of weights 0.38561314676670166\n",
      "Iteration 430 loss 0.19036692978301092 train RMSE 0.7886065171428359 Train R2 430 : 0.7548313091552132 Validation RMSE 430 : 0.7927506391818682 Validation R2 430 : 0.7521140270350629 Dub RMSE 430 : 0.908456218894913\n",
      "Unique RMSE 430 : 0.7943374116619756\n",
      "Dub R2 430 : 0.7696206826738923\n",
      "Unique R2 430 : 0.7507468993534532\n",
      "max of weights 0.3858943170010474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 440 loss 0.1918334863355846 train RMSE 0.7916566735776115 Train R2 440 : 0.752198892407836 Validation RMSE 440 : 0.7999606324212283 Validation R2 440 : 0.7494302089824327 Dub RMSE 440 : 0.9136218989966348\n",
      "Unique RMSE 440 : 0.7981911872944559\n",
      "Dub R2 440 : 0.7710663726642895\n",
      "Unique R2 440 : 0.7480747181107089\n",
      "max of weights 0.38636674663407156\n",
      "Iteration 450 loss 0.18743212851198196 train RMSE 0.7824414986754789 Train R2 450 : 0.7495319785612258 Validation RMSE 450 : 0.7862934469172072 Validation R2 450 : 0.7494357673173299 Dub RMSE 450 : 0.9187986328408969\n",
      "Unique RMSE 450 : 0.7879588840602818\n",
      "Dub R2 450 : 0.7586989657751735\n",
      "Unique R2 450 : 0.7461076792908057\n",
      "max of weights 0.3875078976523085\n",
      "Iteration 460 loss 0.1894212792087862 train RMSE 0.7865983654868431 Train R2 460 : 0.7502527036262019 Validation RMSE 460 : 0.800410535561184 Validation R2 460 : 0.7436116725957366 Dub RMSE 460 : 0.8863908440072729\n",
      "Unique RMSE 460 : 0.7950435800800327\n",
      "Dub R2 460 : 0.770986848421243\n",
      "Unique R2 460 : 0.7451816370589569\n",
      "max of weights 0.3945000384204245\n",
      "Iteration 470 loss 0.1912349666159161 train RMSE 0.7903652398234452 Train R2 470 : 0.7610940581807042 Validation RMSE 470 : 0.7983455379580107 Validation R2 470 : 0.7595108602427446 Dub RMSE 470 : 0.8746874514432914\n",
      "Unique RMSE 470 : 0.8000028429005719\n",
      "Dub R2 470 : 0.7952191179641377\n",
      "Unique R2 470 : 0.755675303254917\n",
      "max of weights 0.40377354540096866\n",
      "Iteration 480 loss 0.1903577854177013 train RMSE 0.7885186947966953 Train R2 480 : 0.7542392969416076 Validation RMSE 480 : 0.7753804025137859 Validation R2 480 : 0.7635406777235155 Dub RMSE 480 : 0.9271863667060971\n",
      "Unique RMSE 480 : 0.7895495606955909\n",
      "Dub R2 480 : 0.7679183915097921\n",
      "Unique R2 480 : 0.7529838525221046\n",
      "max of weights 0.4118961333237849\n",
      "Iteration 490 loss 0.18366544940173443 train RMSE 0.7743999731140534 Train R2 490 : 0.7600290846541102 Validation RMSE 490 : 0.7827374438254648 Validation R2 490 : 0.7548645399318664 Dub RMSE 490 : 0.8879281857871432\n",
      "Unique RMSE 490 : 0.7840997412537568\n",
      "Dub R2 490 : 0.7735214251058624\n",
      "Unique R2 490 : 0.7539817227503139\n",
      "max of weights 0.41789672222000385\n",
      "Iteration 500 loss 0.18074711390068546 train RMSE 0.7681313288410041 Train R2 500 : 0.7721518256468397 Validation RMSE 500 : 0.7665031154035452 Validation R2 500 : 0.7756958453777877 Dub RMSE 500 : 0.9064604309994441\n",
      "Unique RMSE 500 : 0.773725390595543\n",
      "Dub R2 500 : 0.7746851196736044\n",
      "Unique R2 500 : 0.7692876772950832\n",
      "max of weights 0.42342352537263067\n",
      "Iteration 510 loss 0.18165333897493124 train RMSE 0.7700495353954526 Train R2 510 : 0.776677216287916 Validation RMSE 510 : 0.7712538834897263 Validation R2 510 : 0.7763106500710333 Dub RMSE 510 : 0.8525489546103691\n",
      "Unique RMSE 510 : 0.7801024090967961\n",
      "Dub R2 510 : 0.8047262662196304\n",
      "Unique R2 510 : 0.7709618231542178\n",
      "max of weights 0.42859801432506434\n",
      "Iteration 520 loss 0.17343306234365055 train RMSE 0.7522570663037056 Train R2 520 : 0.7824603788712066 Validation RMSE 520 : 0.761129242886727 Validation R2 520 : 0.7775839104008816 Dub RMSE 520 : 0.8741126750901679\n",
      "Unique RMSE 520 : 0.7612867899234167\n",
      "Dub R2 520 : 0.790104250169529\n",
      "Unique R2 520 : 0.7770791086448656\n",
      "max of weights 0.43403798566480195\n",
      "Iteration 530 loss 0.17350673941175726 train RMSE 0.7524114682510966 Train R2 530 : 0.7820396228864459 Validation RMSE 530 : 0.7635791868281926 Validation R2 530 : 0.7781357059978826 Dub RMSE 530 : 0.8730376717216572\n",
      "Unique RMSE 530 : 0.7619778639770243\n",
      "Dub R2 530 : 0.7954499910497906\n",
      "Unique R2 530 : 0.7767186605419696\n",
      "max of weights 0.44111060221020326\n",
      "Iteration 540 loss 0.17048076106083082 train RMSE 0.7457486984655387 Train R2 540 : 0.7778792467555031 Validation RMSE 540 : 0.7526503008584164 Validation R2 540 : 0.7765778205746818 Dub RMSE 540 : 0.886420186241221\n",
      "Unique RMSE 540 : 0.753328729317554\n",
      "Dub R2 540 : 0.777606759440315\n",
      "Unique R2 540 : 0.7737480903367504\n",
      "max of weights 0.44992028788644883\n",
      "Iteration 550 loss 0.1667368688717704 train RMSE 0.737418763612612 Train R2 550 : 0.7856582424006412 Validation RMSE 550 : 0.7452986691290473 Validation R2 550 : 0.782823149335788 Dub RMSE 550 : 0.8540636269826263\n",
      "Unique RMSE 550 : 0.7462312341349034\n",
      "Dub R2 550 : 0.7932568251332799\n",
      "Unique R2 550 : 0.7807089740577284\n",
      "max of weights 0.4571612456405718\n",
      "Iteration 560 loss 0.17416142990302808 train RMSE 0.7537902136788905 Train R2 560 : 0.7892605857948707 Validation RMSE 560 : 0.761435642541555 Validation R2 560 : 0.7883057269382545 Dub RMSE 560 : 0.8418501398521723\n",
      "Unique RMSE 560 : 0.765305698239033\n",
      "Dub R2 560 : 0.8152844563559529\n",
      "Unique R2 560 : 0.7833889364486205\n",
      "max of weights 0.46248834191572774\n",
      "Iteration 570 loss 0.17347519624930755 train RMSE 0.7522730343177506 Train R2 570 : 0.7766389906203222 Validation RMSE 570 : 0.7404032428856792 Validation R2 570 : 0.7843013635925336 Dub RMSE 570 : 0.9050684963166069\n",
      "Unique RMSE 570 : 0.7561607279358143\n",
      "Dub R2 570 : 0.7754356306238306\n",
      "Unique R2 570 : 0.7740284331032466\n",
      "max of weights 0.4679926821690365\n",
      "Iteration 580 loss 0.16405579140507773 train RMSE 0.7313439644202433 Train R2 580 : 0.7907711547040241 Validation RMSE 580 : 0.7393432705225756 Validation R2 580 : 0.7862540962408315 Dub RMSE 580 : 0.8521410437368474\n",
      "Unique RMSE 580 : 0.7429760808553364\n",
      "Dub R2 580 : 0.7940941195424531\n",
      "Unique R2 580 : 0.7842802590975719\n",
      "max of weights 0.47314661851599576\n",
      "Iteration 590 loss 0.1638014250916596 train RMSE 0.7307304581011965 Train R2 590 : 0.802664677957242 Validation RMSE 590 : 0.7375493185272914 Validation R2 590 : 0.8015904164740386 Dub RMSE 590 : 0.8490239405117381\n",
      "Unique RMSE 590 : 0.7424337026014095\n",
      "Dub R2 590 : 0.8082331430073793\n",
      "Unique R2 590 : 0.7971241482883726\n",
      "max of weights 0.47991055474299865\n",
      "Iteration 600 loss 0.16281743978184343 train RMSE 0.7284903483094674 Train R2 600 : 0.8026663928748008 Validation RMSE 600 : 0.7283820211091085 Validation R2 600 : 0.8028181183606338 Dub RMSE 600 : 0.8332237968977662\n",
      "Unique RMSE 600 : 0.7393111398247046\n",
      "Dub R2 600 : 0.8153072932991936\n",
      "Unique R2 600 : 0.7968694314767046\n",
      "max of weights 0.4850118403756113\n",
      "Iteration 610 loss 0.15808689437107806 train RMSE 0.7177088437861234 Train R2 610 : 0.8059736508756257 Validation RMSE 610 : 0.7300247273809989 Validation R2 610 : 0.8000117445417274 Dub RMSE 610 : 0.842730808022963\n",
      "Unique RMSE 610 : 0.7301641435501909\n",
      "Dub R2 610 : 0.807192973140838\n",
      "Unique R2 610 : 0.7994205847380937\n",
      "max of weights 0.4923220079512641\n",
      "Iteration 620 loss 0.15889906351858293 train RMSE 0.7195604218336166 Train R2 620 : 0.8053197159557595 Validation RMSE 620 : 0.7357519449407147 Validation R2 620 : 0.7995512963157588 Dub RMSE 620 : 0.8333783220493999\n",
      "Unique RMSE 620 : 0.733500630156152\n",
      "Dub R2 620 : 0.8155916477973801\n",
      "Unique R2 620 : 0.7983866722906239\n",
      "max of weights 0.5037674847400406\n",
      "Iteration 630 loss 0.15672951535733026 train RMSE 0.7145642677643907 Train R2 630 : 0.8028756940587349 Validation RMSE 630 : 0.7293271867089351 Validation R2 630 : 0.7973793429449698 Dub RMSE 630 : 0.8385630328990902\n",
      "Unique RMSE 630 : 0.7267030451107965\n",
      "Dub R2 630 : 0.802461904434999\n",
      "Unique R2 630 : 0.7968023296241427\n",
      "max of weights 0.5147896436531199\n",
      "Iteration 640 loss 0.15164226913680787 train RMSE 0.7027257952180345 Train R2 640 : 0.8116492922799055 Validation RMSE 640 : 0.7064290612739075 Validation R2 640 : 0.8110441155593093 Dub RMSE 640 : 0.8234983098561633\n",
      "Unique RMSE 640 : 0.7135836241992338\n",
      "Dub R2 640 : 0.8136035025608251\n",
      "Unique R2 640 : 0.8060582758388342\n",
      "max of weights 0.5232945371412857\n",
      "Iteration 650 loss 0.1590990978781878 train RMSE 0.7199626281069446 Train R2 650 : 0.8121965490497645 Validation RMSE 650 : 0.7274120792295248 Validation R2 650 : 0.8115994882067591 Dub RMSE 650 : 0.818854220905371\n",
      "Unique RMSE 650 : 0.7326700035789471\n",
      "Dub R2 650 : 0.8278234847776261\n",
      "Unique R2 650 : 0.8062443637036778\n",
      "max of weights 0.5280438206451762\n",
      "Iteration 660 loss 0.15956820602711355 train RMSE 0.7210211643496179 Train R2 660 : 0.795421111450995 Validation RMSE 660 : 0.7123300744650796 Validation R2 660 : 0.800702935870046 Dub RMSE 660 : 0.8805414485509476\n",
      "Unique RMSE 660 : 0.7288849678418421\n",
      "Dub R2 660 : 0.7827296112442611\n",
      "Unique R2 660 : 0.7910640685773571\n",
      "max of weights 0.5332688487331645\n",
      "Iteration 670 loss 0.15193895413091746 train RMSE 0.7033578923833331 Train R2 670 : 0.8103423852845065 Validation RMSE 670 : 0.7104799751213785 Validation R2 670 : 0.8069930618514777 Dub RMSE 670 : 0.8407660030673456\n",
      "Unique RMSE 670 : 0.7153145441916633\n",
      "Dub R2 670 : 0.8021783539144327\n",
      "Unique R2 670 : 0.8042247360670152\n",
      "max of weights 0.5398545314573001\n",
      "Iteration 680 loss 0.15656750410172376 train RMSE 0.7140701167718863 Train R2 680 : 0.8184633675569503 Validation RMSE 680 : 0.7277521642898613 Validation R2 680 : 0.8136822649772495 Dub RMSE 680 : 0.7999626549807484\n",
      "Unique RMSE 680 : 0.7315255385286875\n",
      "Dub R2 680 : 0.8341268881956811\n",
      "Unique R2 680 : 0.8104478797741728\n",
      "max of weights 0.5463924315349814\n",
      "Iteration 690 loss 0.15117839190865862 train RMSE 0.701517766579271 Train R2 690 : 0.8191632281413094 Validation RMSE 690 : 0.7038648952080009 Validation R2 690 : 0.8174930416331345 Dub RMSE 690 : 0.8301365640080249\n",
      "Unique RMSE 690 : 0.7129298118698298\n",
      "Dub R2 690 : 0.8175045775426408\n",
      "Unique R2 690 : 0.8132218112509295\n",
      "max of weights 0.5517309089898056\n",
      "Iteration 700 loss 0.14598082539976862 train RMSE 0.689201619267457 Train R2 700 : 0.8247533619225392 Validation RMSE 700 : 0.702629656797535 Validation R2 700 : 0.8187307999261239 Dub RMSE 700 : 0.8244551584143437\n",
      "Unique RMSE 700 : 0.7045861612936855\n",
      "Dub R2 700 : 0.8190243001856881\n",
      "Unique R2 700 : 0.8172912716730518\n",
      "max of weights 0.5585587733929976\n",
      "Iteration 710 loss 0.14879755370886738 train RMSE 0.6958914665190273 Train R2 710 : 0.8223333380703155 Validation RMSE 710 : 0.7165550533628823 Validation R2 710 : 0.8143553632630062 Dub RMSE 710 : 0.803847131201136\n",
      "Unique RMSE 710 : 0.7134702291583404\n",
      "Dub R2 710 : 0.830452189269983\n",
      "Unique R2 710 : 0.8140987404573241\n",
      "max of weights 0.5691066136228924\n",
      "Iteration 720 loss 0.14835179807202856 train RMSE 0.6948236128476037 Train R2 720 : 0.8180406522189897 Validation RMSE 720 : 0.711683412537445 Validation R2 720 : 0.810815808051538 Dub RMSE 720 : 0.8087410738540882\n",
      "Unique RMSE 720 : 0.7092750882311912\n",
      "Dub R2 720 : 0.818078622271972\n",
      "Unique R2 720 : 0.8109770435902904\n",
      "max of weights 0.5791162844924415\n",
      "Iteration 730 loss 0.1419068642314763 train RMSE 0.6793615546571781 Train R2 730 : 0.8279495965605145 Validation RMSE 730 : 0.6815060914339749 Validation R2 730 : 0.8279013989733897 Dub RMSE 730 : 0.80532682145971\n",
      "Unique RMSE 730 : 0.6924310153135416\n",
      "Dub R2 730 : 0.8253111201914997\n",
      "Unique R2 730 : 0.8215981628403222\n",
      "max of weights 0.5865479412308696\n",
      "Iteration 740 loss 0.1483888952611154 train RMSE 0.6948737901107499 Train R2 740 : 0.8286902974892062 Validation RMSE 740 : 0.7058477111189645 Validation R2 740 : 0.8262762951799777 Dub RMSE 740 : 0.8020976592568995\n",
      "Unique RMSE 740 : 0.7097196259284629\n",
      "Dub R2 740 : 0.8377440568167338\n",
      "Unique R2 740 : 0.8220530898519894\n",
      "max of weights 0.5909151346221024\n",
      "Iteration 750 loss 0.15051951212114395 train RMSE 0.6998953641200406 Train R2 750 : 0.8101860863143744 Validation RMSE 750 : 0.6968952312262747 Validation R2 750 : 0.8113070473969448 Dub RMSE 750 : 0.8690601912900426\n",
      "Unique RMSE 750 : 0.7122486549102586\n",
      "Dub R2 750 : 0.7872887452426499\n",
      "Unique R2 750 : 0.8037158472587544\n",
      "max of weights 0.59574089942562\n",
      "Iteration 760 loss 0.14492985529687083 train RMSE 0.6865924143023695 Train R2 760 : 0.8232727352512635 Validation RMSE 760 : 0.6933115827887004 Validation R2 760 : 0.8204451557284249 Dub RMSE 760 : 0.8431516286963217\n",
      "Unique RMSE 760 : 0.6989858867630702\n",
      "Dub R2 760 : 0.8059215897683413\n",
      "Unique R2 760 : 0.8172824379055599\n",
      "max of weights 0.6017266798935669\n",
      "Iteration 770 loss 0.15369212017435155 train RMSE 0.7072569689393109 Train R2 770 : 0.8266058959717341 Validation RMSE 770 : 0.7269341737380602 Validation R2 770 : 0.8182627304804037 Dub RMSE 770 : 0.765909440820956\n",
      "Unique RMSE 770 : 0.7292553637668355\n",
      "Dub R2 770 : 0.8510241883332881\n",
      "Unique R2 770 : 0.8166063348178703\n",
      "max of weights 0.6079313986723843\n",
      "Iteration 780 loss 0.14289999879545323 train RMSE 0.6816579448253013 Train R2 780 : 0.831978380789968 Validation RMSE 780 : 0.6867800081383234 Validation R2 780 : 0.8286280391442469 Dub RMSE 780 : 0.8303913601779398\n",
      "Unique RMSE 780 : 0.6944886033391301\n",
      "Dub R2 780 : 0.8197509072895497\n",
      "Unique R2 780 : 0.8255517623196311\n",
      "max of weights 0.6135033404864967\n",
      "Iteration 790 loss 0.138599987326635 train RMSE 0.6711845745557111 Train R2 790 : 0.8364949779193492 Validation RMSE 790 : 0.6849592759863172 Validation R2 790 : 0.8307946090856837 Dub RMSE 790 : 0.8239779423308972\n",
      "Unique RMSE 790 : 0.6877230593634014\n",
      "Dub R2 790 : 0.822997173469936\n",
      "Unique R2 790 : 0.8288561830951179\n",
      "max of weights 0.6197927942380305\n",
      "Iteration 800 loss 0.1392708289004137 train RMSE 0.6728235500146934 Train R2 800 : 0.8362842440088044 Validation RMSE 800 : 0.6966663608153931 Validation R2 800 : 0.8265061359604682 Dub RMSE 800 : 0.7755449310963413\n",
      "Unique RMSE 800 : 0.6935933960243537\n",
      "Dub R2 800 : 0.8426503477361399\n",
      "Unique R2 800 : 0.8268581956879347\n",
      "max of weights 0.6287540565270289\n",
      "Iteration 810 loss 0.1414847570764892 train RMSE 0.6782054758322162 Train R2 810 : 0.8302942188535823 Validation RMSE 810 : 0.6970138524649329 Validation R2 810 : 0.8215509311281702 Dub RMSE 810 : 0.7785888928987221\n",
      "Unique RMSE 810 : 0.6948966079805674\n",
      "Dub R2 810 : 0.8343179718950807\n",
      "Unique R2 810 : 0.8222992886195845\n",
      "max of weights 0.6375056392850627\n",
      "Iteration 820 loss 0.13365165016196773 train RMSE 0.6588962710436315 Train R2 820 : 0.8414308177781038 Validation RMSE 820 : 0.6657758810481125 Validation R2 820 : 0.8388590541907842 Dub RMSE 820 : 0.7807988188316736\n",
      "Unique RMSE 820 : 0.6754940063570074\n",
      "Dub R2 820 : 0.8384804075527641\n",
      "Unique R2 820 : 0.8337221879871912\n",
      "max of weights 0.6438313471733332\n",
      "Iteration 830 loss 0.13821474230581557 train RMSE 0.6701833817020377 Train R2 830 : 0.8426436513075408 Validation RMSE 830 : 0.6846985407260519 Validation R2 830 : 0.8381741607090734 Dub RMSE 830 : 0.784784472625548\n",
      "Unique RMSE 830 : 0.6876616814229152\n",
      "Dub R2 830 : 0.8460934154323854\n",
      "Unique R2 830 : 0.8350490739934493\n",
      "max of weights 0.6478428601138427\n",
      "Iteration 840 loss 0.14524201411673215 train RMSE 0.6872249187135921 Train R2 840 : 0.8208667452975369 Validation RMSE 840 : 0.6896831049211447 Validation R2 840 : 0.8181390829421428 Dub RMSE 840 : 0.8610345171089808\n",
      "Unique RMSE 840 : 0.7030467360388112\n",
      "Dub R2 840 : 0.7933171253338076\n",
      "Unique R2 840 : 0.8127725707602196\n",
      "max of weights 0.6524915607598643\n",
      "Iteration 850 loss 0.14132507127844168 train RMSE 0.6777478264505911 Train R2 850 : 0.8313864008200431 Validation RMSE 850 : 0.6831514829063404 Validation R2 850 : 0.8294554700794662 Dub RMSE 850 : 0.8536350309594759\n",
      "Unique RMSE 850 : 0.6901077878683484\n",
      "Dub R2 850 : 0.8064132697089086\n",
      "Unique R2 850 : 0.8256239852383084\n",
      "max of weights 0.6582490350851955\n",
      "Iteration 860 loss 0.1518674583903104 train RMSE 0.7028636128479948 Train R2 860 : 0.8324125926475705 Validation RMSE 860 : 0.7273331265194032 Validation R2 860 : 0.8212799760414891 Dub RMSE 860 : 0.7411019707017906\n",
      "Unique RMSE 860 : 0.7278930780792046\n",
      "Dub R2 860 : 0.8630496189532498\n",
      "Unique R2 860 : 0.8211483033257824\n",
      "max of weights 0.6641605291344712\n",
      "Iteration 870 loss 0.1346131529610469 train RMSE 0.6612031382741532 Train R2 870 : 0.8437417877047301 Validation RMSE 870 : 0.6694427846591418 Validation R2 870 : 0.8388503120702344 Dub RMSE 870 : 0.8224928432377581\n",
      "Unique RMSE 870 : 0.6764398074328244\n",
      "Dub R2 870 : 0.8248678917627936\n",
      "Unique R2 870 : 0.8364964919412821\n",
      "max of weights 0.669758393727691\n",
      "Iteration 880 loss 0.13674295659623 train RMSE 0.6664802580094167 Train R2 880 : 0.8407484758593675 Validation RMSE 880 : 0.6790655042704553 Validation R2 880 : 0.8362408571204938 Dub RMSE 880 : 0.8439030235878425\n",
      "Unique RMSE 880 : 0.681823729511212\n",
      "Dub R2 880 : 0.8188758220489275\n",
      "Unique R2 880 : 0.8338416377947084\n",
      "\n",
      "Performance (RMSE) on logP:\n",
      "Train: 0.6671997360781458\n",
      "Test:  0.7693981652457923\n",
      "Dub: 0.8406820692658812\n",
      "Unique:  0.6830023013309037\n",
      "\n",
      "Performance (R2) on logP:\n",
      "Train: 0.8407282474126163\n",
      "Test:  0.8015678595817767\n",
      "Dub: 0.8212605266560888\n",
      "Unique:  0.8337038340329004\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_network_weights = train_neural_fingerprint()\n",
    "with open('results'+task_params['data_file']+'.pkl', 'w') as f:\n",
    "    pickle.dump(trained_network_weights, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
