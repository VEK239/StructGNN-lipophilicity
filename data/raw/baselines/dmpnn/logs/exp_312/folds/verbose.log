Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_312/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['LogD'],
 'task_names': ['LogD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_312/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['LogD'],
 'task_names': ['LogD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_312/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD'],
 'task_names': ['logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 3,541 | train size = 2,655 | val size = 886 | test size = 625
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=895, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,222,401
Moving model to cuda
Epoch 0
Train function
Loss = 2.2886e-02, PNorm = 52.4962, GNorm = 8.3944, lr_0 = 1.9340e-04
Loss = 1.8225e-02, PNorm = 52.5030, GNorm = 6.0000, lr_0 = 2.7830e-04
Loss = 1.7960e-02, PNorm = 52.5150, GNorm = 1.2125, lr_0 = 3.6321e-04
Loss = 1.6288e-02, PNorm = 52.5334, GNorm = 2.7616, lr_0 = 4.4811e-04
Loss = 1.5772e-02, PNorm = 52.5546, GNorm = 6.9812, lr_0 = 5.3302e-04
Validation rmse logD = 0.991863
Validation R2 logD = 0.241330
Epoch 1
Train function
Loss = 1.3596e-02, PNorm = 52.5900, GNorm = 1.4338, lr_0 = 6.2642e-04
Loss = 1.4687e-02, PNorm = 52.6450, GNorm = 2.5504, lr_0 = 7.1132e-04
Loss = 1.3155e-02, PNorm = 52.7187, GNorm = 5.2467, lr_0 = 7.9623e-04
Loss = 1.3600e-02, PNorm = 52.7861, GNorm = 3.4418, lr_0 = 8.8113e-04
Loss = 1.7432e-02, PNorm = 52.8692, GNorm = 4.8435, lr_0 = 9.6604e-04
Validation rmse logD = 0.935260
Validation R2 logD = 0.325450
Epoch 2
Train function
Loss = 1.3729e-02, PNorm = 52.9923, GNorm = 2.7197, lr_0 = 9.9690e-04
Loss = 1.2645e-02, PNorm = 53.0893, GNorm = 1.0543, lr_0 = 9.9249e-04
Loss = 1.1681e-02, PNorm = 53.1749, GNorm = 2.0974, lr_0 = 9.8810e-04
Loss = 1.0389e-02, PNorm = 53.2666, GNorm = 1.0106, lr_0 = 9.8373e-04
Loss = 1.2302e-02, PNorm = 53.3594, GNorm = 2.9242, lr_0 = 9.7938e-04
Validation rmse logD = 0.851017
Validation R2 logD = 0.441497
Epoch 3
Train function
Loss = 7.1966e-03, PNorm = 53.4754, GNorm = 1.2416, lr_0 = 9.7462e-04
Loss = 8.1374e-03, PNorm = 53.5437, GNorm = 2.0216, lr_0 = 9.7030e-04
Loss = 9.7411e-03, PNorm = 53.6350, GNorm = 1.2662, lr_0 = 9.6601e-04
Loss = 9.7733e-03, PNorm = 53.7366, GNorm = 2.7294, lr_0 = 9.6174e-04
Loss = 9.4644e-03, PNorm = 53.8468, GNorm = 2.2668, lr_0 = 9.5749e-04
Loss = 8.7574e-03, PNorm = 53.9711, GNorm = 2.4396, lr_0 = 9.5325e-04
Validation rmse logD = 0.787827
Validation R2 logD = 0.521358
Epoch 4
Train function
Loss = 7.6515e-03, PNorm = 54.0529, GNorm = 1.4347, lr_0 = 9.4861e-04
Loss = 7.3134e-03, PNorm = 54.1491, GNorm = 2.5844, lr_0 = 9.4442e-04
Loss = 6.5591e-03, PNorm = 54.2334, GNorm = 1.0765, lr_0 = 9.4024e-04
Loss = 7.0101e-03, PNorm = 54.3309, GNorm = 2.8235, lr_0 = 9.3608e-04
Loss = 7.2218e-03, PNorm = 54.4440, GNorm = 1.0683, lr_0 = 9.3194e-04
Validation rmse logD = 0.770383
Validation R2 logD = 0.542319
Epoch 5
Train function
Loss = 6.4842e-03, PNorm = 54.5669, GNorm = 1.1924, lr_0 = 9.2741e-04
Loss = 5.8750e-03, PNorm = 54.6833, GNorm = 1.7093, lr_0 = 9.2330e-04
Loss = 6.9400e-03, PNorm = 54.8008, GNorm = 4.3867, lr_0 = 9.1922e-04
Loss = 7.2483e-03, PNorm = 54.9089, GNorm = 0.9897, lr_0 = 9.1515e-04
Loss = 6.3393e-03, PNorm = 54.9932, GNorm = 1.6175, lr_0 = 9.1111e-04
Validation rmse logD = 0.767082
Validation R2 logD = 0.546233
Epoch 6
Train function
Loss = 5.3038e-03, PNorm = 55.0883, GNorm = 1.2979, lr_0 = 9.0667e-04
Loss = 5.6748e-03, PNorm = 55.1840, GNorm = 3.8927, lr_0 = 9.0266e-04
Loss = 5.7810e-03, PNorm = 55.2920, GNorm = 1.4652, lr_0 = 8.9867e-04
Loss = 5.3489e-03, PNorm = 55.3966, GNorm = 1.0403, lr_0 = 8.9469e-04
Loss = 5.7541e-03, PNorm = 55.4862, GNorm = 0.8999, lr_0 = 8.9074e-04
Loss = 5.4273e-03, PNorm = 55.5862, GNorm = 1.7944, lr_0 = 8.8680e-04
Validation rmse logD = 0.719195
Validation R2 logD = 0.601119
Epoch 7
Train function
Loss = 4.6640e-03, PNorm = 55.6814, GNorm = 1.8081, lr_0 = 8.8248e-04
Loss = 4.8795e-03, PNorm = 55.7755, GNorm = 0.7730, lr_0 = 8.7858e-04
Loss = 4.8456e-03, PNorm = 55.8770, GNorm = 4.1141, lr_0 = 8.7469e-04
Loss = 5.4526e-03, PNorm = 55.9453, GNorm = 3.9767, lr_0 = 8.7082e-04
Loss = 6.7292e-03, PNorm = 56.0439, GNorm = 0.9219, lr_0 = 8.6697e-04
Validation rmse logD = 0.739294
Validation R2 logD = 0.578514
Epoch 8
Train function
Loss = 5.0467e-03, PNorm = 56.1661, GNorm = 2.0042, lr_0 = 8.6276e-04
Loss = 3.9159e-03, PNorm = 56.2631, GNorm = 0.9812, lr_0 = 8.5894e-04
Loss = 3.9391e-03, PNorm = 56.3594, GNorm = 0.9549, lr_0 = 8.5514e-04
Loss = 4.6381e-03, PNorm = 56.4244, GNorm = 1.6233, lr_0 = 8.5136e-04
Loss = 3.8760e-03, PNorm = 56.5141, GNorm = 2.0983, lr_0 = 8.4759e-04
Validation rmse logD = 0.786673
Validation R2 logD = 0.522759
Epoch 9
Train function
Loss = 5.2850e-03, PNorm = 56.6062, GNorm = 1.6351, lr_0 = 8.4347e-04
Loss = 5.1330e-03, PNorm = 56.7035, GNorm = 1.8461, lr_0 = 8.3974e-04
Loss = 4.6166e-03, PNorm = 56.8065, GNorm = 1.0829, lr_0 = 8.3602e-04
Loss = 3.4168e-03, PNorm = 56.8937, GNorm = 0.6970, lr_0 = 8.3232e-04
Loss = 4.5457e-03, PNorm = 56.9845, GNorm = 1.9027, lr_0 = 8.2864e-04
Loss = 3.3377e-03, PNorm = 57.0402, GNorm = 0.8200, lr_0 = 8.2498e-04
Validation rmse logD = 0.662169
Validation R2 logD = 0.661867
Epoch 10
Train function
Loss = 3.2801e-03, PNorm = 57.1118, GNorm = 1.1098, lr_0 = 8.2133e-04
Loss = 3.5406e-03, PNorm = 57.1983, GNorm = 0.9669, lr_0 = 8.1770e-04
Loss = 3.7621e-03, PNorm = 57.2800, GNorm = 1.1357, lr_0 = 8.1408e-04
Loss = 3.0505e-03, PNorm = 57.3510, GNorm = 0.9428, lr_0 = 8.1048e-04
Loss = 2.9206e-03, PNorm = 57.4075, GNorm = 2.1344, lr_0 = 8.0689e-04
Validation rmse logD = 0.669280
Validation R2 logD = 0.654566
Epoch 11
Train function
Loss = 3.1143e-03, PNorm = 57.4652, GNorm = 1.6351, lr_0 = 8.0297e-04
Loss = 3.0819e-03, PNorm = 57.5428, GNorm = 2.2911, lr_0 = 7.9942e-04
Loss = 3.2317e-03, PNorm = 57.6116, GNorm = 1.0932, lr_0 = 7.9588e-04
Loss = 3.2460e-03, PNorm = 57.6893, GNorm = 2.8288, lr_0 = 7.9236e-04
Loss = 4.2213e-03, PNorm = 57.7753, GNorm = 2.5645, lr_0 = 7.8885e-04
Validation rmse logD = 0.644493
Validation R2 logD = 0.679679
Epoch 12
Train function
Loss = 3.5140e-03, PNorm = 57.8489, GNorm = 1.4134, lr_0 = 7.8502e-04
Loss = 2.5884e-03, PNorm = 57.9318, GNorm = 2.1298, lr_0 = 7.8154e-04
Loss = 2.4527e-03, PNorm = 58.0032, GNorm = 0.9611, lr_0 = 7.7809e-04
Loss = 2.3363e-03, PNorm = 58.0669, GNorm = 1.2237, lr_0 = 7.7465e-04
Loss = 2.3178e-03, PNorm = 58.1202, GNorm = 1.3049, lr_0 = 7.7122e-04
Loss = 2.4749e-03, PNorm = 58.1662, GNorm = 1.1082, lr_0 = 7.6781e-04
Loss = 8.7542e-03, PNorm = 58.1716, GNorm = 1.2008, lr_0 = 7.6747e-04
Validation rmse logD = 0.643409
Validation R2 logD = 0.680755
Epoch 13
Train function
Loss = 2.2607e-03, PNorm = 58.2263, GNorm = 1.1060, lr_0 = 7.6407e-04
Loss = 2.0631e-03, PNorm = 58.3024, GNorm = 1.3919, lr_0 = 7.6069e-04
Loss = 2.4994e-03, PNorm = 58.3569, GNorm = 1.3320, lr_0 = 7.5733e-04
Loss = 2.0024e-03, PNorm = 58.4026, GNorm = 0.7109, lr_0 = 7.5398e-04
Loss = 2.1177e-03, PNorm = 58.4707, GNorm = 1.2034, lr_0 = 7.5064e-04
Validation rmse logD = 0.728121
Validation R2 logD = 0.591157
Epoch 14
Train function
Loss = 4.3052e-03, PNorm = 58.5582, GNorm = 4.3812, lr_0 = 7.4699e-04
Loss = 2.6989e-03, PNorm = 58.6707, GNorm = 0.9746, lr_0 = 7.4369e-04
Loss = 2.2799e-03, PNorm = 58.7571, GNorm = 1.5459, lr_0 = 7.4040e-04
Loss = 2.6223e-03, PNorm = 58.8213, GNorm = 2.2963, lr_0 = 7.3712e-04
Loss = 1.9570e-03, PNorm = 58.8728, GNorm = 0.9382, lr_0 = 7.3386e-04
Validation rmse logD = 0.644811
Validation R2 logD = 0.679363
Epoch 15
Train function
Loss = 1.9304e-03, PNorm = 58.9377, GNorm = 0.6935, lr_0 = 7.3029e-04
Loss = 2.0391e-03, PNorm = 59.0045, GNorm = 1.5726, lr_0 = 7.2706e-04
Loss = 2.1057e-03, PNorm = 59.0767, GNorm = 0.5975, lr_0 = 7.2385e-04
Loss = 2.1455e-03, PNorm = 59.1380, GNorm = 2.0632, lr_0 = 7.2064e-04
Loss = 2.0531e-03, PNorm = 59.1834, GNorm = 3.0149, lr_0 = 7.1746e-04
Validation rmse logD = 0.624135
Validation R2 logD = 0.699596
Epoch 16
Train function
Loss = 2.0187e-03, PNorm = 59.2291, GNorm = 1.0386, lr_0 = 7.1397e-04
Loss = 1.5094e-03, PNorm = 59.2861, GNorm = 0.5282, lr_0 = 7.1081e-04
Loss = 1.3405e-03, PNorm = 59.3376, GNorm = 0.5149, lr_0 = 7.0766e-04
Loss = 1.7534e-03, PNorm = 59.3868, GNorm = 1.5412, lr_0 = 7.0453e-04
Loss = 1.5538e-03, PNorm = 59.4293, GNorm = 0.8937, lr_0 = 7.0142e-04
Loss = 1.9449e-03, PNorm = 59.4788, GNorm = 3.2751, lr_0 = 6.9831e-04
Validation rmse logD = 0.656419
Validation R2 logD = 0.667714
Epoch 17
Train function
Loss = 1.8010e-03, PNorm = 59.5328, GNorm = 2.2780, lr_0 = 6.9492e-04
Loss = 1.5972e-03, PNorm = 59.5805, GNorm = 0.7474, lr_0 = 6.9184e-04
Loss = 1.4710e-03, PNorm = 59.6330, GNorm = 0.5434, lr_0 = 6.8878e-04
Loss = 1.5205e-03, PNorm = 59.6820, GNorm = 0.8310, lr_0 = 6.8574e-04
Loss = 1.3681e-03, PNorm = 59.7167, GNorm = 0.5649, lr_0 = 6.8270e-04
Validation rmse logD = 0.603135
Validation R2 logD = 0.719470
Epoch 18
Train function
Loss = 1.0001e-03, PNorm = 59.7579, GNorm = 0.6789, lr_0 = 6.7938e-04
Loss = 1.5127e-03, PNorm = 59.8119, GNorm = 1.4390, lr_0 = 6.7638e-04
Loss = 1.4191e-03, PNorm = 59.8542, GNorm = 0.9979, lr_0 = 6.7338e-04
Loss = 1.3753e-03, PNorm = 59.8905, GNorm = 1.0119, lr_0 = 6.7041e-04
Loss = 1.3818e-03, PNorm = 59.9297, GNorm = 1.3712, lr_0 = 6.6744e-04
Validation rmse logD = 0.656189
Validation R2 logD = 0.667948
Epoch 19
Train function
Loss = 1.8251e-03, PNorm = 59.9765, GNorm = 1.4883, lr_0 = 6.6419e-04
Loss = 1.6276e-03, PNorm = 60.0257, GNorm = 1.5599, lr_0 = 6.6126e-04
Loss = 1.5970e-03, PNorm = 60.0791, GNorm = 1.1968, lr_0 = 6.5833e-04
Loss = 1.4719e-03, PNorm = 60.1192, GNorm = 0.9840, lr_0 = 6.5542e-04
Loss = 1.1392e-03, PNorm = 60.1731, GNorm = 0.5509, lr_0 = 6.5252e-04
Loss = 1.1892e-03, PNorm = 60.2214, GNorm = 0.5973, lr_0 = 6.4963e-04
Validation rmse logD = 0.625923
Validation R2 logD = 0.697872
Epoch 20
Train function
Loss = 1.1775e-03, PNorm = 60.2706, GNorm = 1.3252, lr_0 = 6.4676e-04
Loss = 1.1006e-03, PNorm = 60.3070, GNorm = 0.6171, lr_0 = 6.4390e-04
Loss = 1.0900e-03, PNorm = 60.3402, GNorm = 0.8952, lr_0 = 6.4105e-04
Loss = 1.2073e-03, PNorm = 60.3768, GNorm = 0.6944, lr_0 = 6.3822e-04
Loss = 1.3402e-03, PNorm = 60.4071, GNorm = 1.0803, lr_0 = 6.3539e-04
Validation rmse logD = 0.597948
Validation R2 logD = 0.724275
Epoch 21
Train function
Loss = 8.2899e-04, PNorm = 60.4469, GNorm = 0.9537, lr_0 = 6.3230e-04
Loss = 8.8800e-04, PNorm = 60.4837, GNorm = 1.0420, lr_0 = 6.2950e-04
Loss = 7.8494e-04, PNorm = 60.5185, GNorm = 0.3938, lr_0 = 6.2672e-04
Loss = 9.2663e-04, PNorm = 60.5463, GNorm = 0.4837, lr_0 = 6.2395e-04
Loss = 8.5330e-04, PNorm = 60.5758, GNorm = 0.6560, lr_0 = 6.2119e-04
Validation rmse logD = 0.591651
Validation R2 logD = 0.730052
Epoch 22
Train function
Loss = 7.3301e-04, PNorm = 60.5968, GNorm = 0.4316, lr_0 = 6.1817e-04
Loss = 1.0438e-03, PNorm = 60.6292, GNorm = 3.1609, lr_0 = 6.1543e-04
Loss = 1.1481e-03, PNorm = 60.6615, GNorm = 2.4409, lr_0 = 6.1271e-04
Loss = 1.0556e-03, PNorm = 60.7034, GNorm = 0.8012, lr_0 = 6.1000e-04
Loss = 8.6489e-04, PNorm = 60.7392, GNorm = 0.8709, lr_0 = 6.0730e-04
Loss = 8.7743e-04, PNorm = 60.7699, GNorm = 0.4578, lr_0 = 6.0461e-04
Validation rmse logD = 0.600825
Validation R2 logD = 0.721616
Epoch 23
Train function
Loss = 8.4311e-04, PNorm = 60.8062, GNorm = 0.8397, lr_0 = 6.0167e-04
Loss = 8.4246e-04, PNorm = 60.8451, GNorm = 0.5523, lr_0 = 5.9901e-04
Loss = 7.6480e-04, PNorm = 60.8719, GNorm = 1.0161, lr_0 = 5.9636e-04
Loss = 7.8140e-04, PNorm = 60.9016, GNorm = 0.5020, lr_0 = 5.9372e-04
Loss = 7.1361e-04, PNorm = 60.9217, GNorm = 0.3333, lr_0 = 5.9110e-04
Validation rmse logD = 0.606298
Validation R2 logD = 0.716521
Epoch 24
Train function
Loss = 9.6997e-04, PNorm = 60.9425, GNorm = 1.0687, lr_0 = 5.8822e-04
Loss = 8.9026e-04, PNorm = 60.9769, GNorm = 0.7275, lr_0 = 5.8562e-04
Loss = 9.2734e-04, PNorm = 61.0133, GNorm = 0.5055, lr_0 = 5.8303e-04
Loss = 6.6380e-04, PNorm = 61.0475, GNorm = 0.5978, lr_0 = 5.8045e-04
Loss = 8.2421e-04, PNorm = 61.0696, GNorm = 1.5057, lr_0 = 5.7788e-04
Validation rmse logD = 0.628523
Validation R2 logD = 0.695357
Epoch 25
Train function
Loss = 8.8259e-04, PNorm = 61.1108, GNorm = 1.5404, lr_0 = 5.7507e-04
Loss = 7.7156e-04, PNorm = 61.1489, GNorm = 1.1205, lr_0 = 5.7253e-04
Loss = 7.2495e-04, PNorm = 61.1714, GNorm = 0.4085, lr_0 = 5.7000e-04
Loss = 6.6925e-04, PNorm = 61.1966, GNorm = 0.9125, lr_0 = 5.6748e-04
Loss = 6.6683e-04, PNorm = 61.2218, GNorm = 0.5233, lr_0 = 5.6497e-04
Loss = 7.6358e-04, PNorm = 61.2490, GNorm = 0.5412, lr_0 = 5.6247e-04
Loss = 3.4151e-03, PNorm = 61.2508, GNorm = 0.8800, lr_0 = 5.6222e-04
Validation rmse logD = 0.599816
Validation R2 logD = 0.722550
Epoch 26
Train function
Loss = 5.9137e-04, PNorm = 61.2632, GNorm = 1.0551, lr_0 = 5.5973e-04
Loss = 6.9698e-04, PNorm = 61.2863, GNorm = 0.8318, lr_0 = 5.5725e-04
Loss = 6.3563e-04, PNorm = 61.3191, GNorm = 0.6719, lr_0 = 5.5479e-04
Loss = 6.4403e-04, PNorm = 61.3411, GNorm = 0.6656, lr_0 = 5.5233e-04
Loss = 6.6579e-04, PNorm = 61.3685, GNorm = 0.7695, lr_0 = 5.4989e-04
Validation rmse logD = 0.605892
Validation R2 logD = 0.716900
Epoch 27
Train function
Loss = 7.4673e-04, PNorm = 61.3945, GNorm = 1.1670, lr_0 = 5.4722e-04
Loss = 7.5654e-04, PNorm = 61.4362, GNorm = 0.4636, lr_0 = 5.4480e-04
Loss = 6.7032e-04, PNorm = 61.4687, GNorm = 0.5077, lr_0 = 5.4239e-04
Loss = 6.8973e-04, PNorm = 61.4952, GNorm = 0.6276, lr_0 = 5.3999e-04
Loss = 6.1417e-04, PNorm = 61.5189, GNorm = 0.4603, lr_0 = 5.3760e-04
Validation rmse logD = 0.598667
Validation R2 logD = 0.723612
Epoch 28
Train function
Loss = 5.7951e-04, PNorm = 61.5454, GNorm = 0.3330, lr_0 = 5.3498e-04
Loss = 6.0103e-04, PNorm = 61.5729, GNorm = 0.8672, lr_0 = 5.3262e-04
Loss = 5.0790e-04, PNorm = 61.5947, GNorm = 0.3522, lr_0 = 5.3026e-04
Loss = 5.5817e-04, PNorm = 61.6119, GNorm = 1.0550, lr_0 = 5.2792e-04
Loss = 5.0061e-04, PNorm = 61.6304, GNorm = 0.8837, lr_0 = 5.2558e-04
Validation rmse logD = 0.601969
Validation R2 logD = 0.720554
Epoch 29
Train function
Loss = 4.4272e-04, PNorm = 61.6485, GNorm = 0.8511, lr_0 = 5.2302e-04
Loss = 6.3542e-04, PNorm = 61.6769, GNorm = 0.7204, lr_0 = 5.2071e-04
Loss = 4.7075e-04, PNorm = 61.7034, GNorm = 0.2445, lr_0 = 5.1841e-04
Loss = 4.6159e-04, PNorm = 61.7212, GNorm = 0.6422, lr_0 = 5.1611e-04
Loss = 4.8268e-04, PNorm = 61.7375, GNorm = 1.1672, lr_0 = 5.1383e-04
Loss = 5.0526e-04, PNorm = 61.7544, GNorm = 0.7109, lr_0 = 5.1156e-04
Validation rmse logD = 0.591554
Validation R2 logD = 0.730140
Epoch 30
Train function
Loss = 3.8081e-04, PNorm = 61.7672, GNorm = 0.3914, lr_0 = 5.0930e-04
Loss = 3.8814e-04, PNorm = 61.7831, GNorm = 0.5394, lr_0 = 5.0704e-04
Loss = 4.5663e-04, PNorm = 61.8012, GNorm = 1.0913, lr_0 = 5.0480e-04
Loss = 3.4194e-04, PNorm = 61.8171, GNorm = 0.6605, lr_0 = 5.0257e-04
Loss = 3.8875e-04, PNorm = 61.8290, GNorm = 0.3800, lr_0 = 5.0034e-04
Validation rmse logD = 0.615078
Validation R2 logD = 0.708251
Epoch 31
Train function
Loss = 4.5609e-04, PNorm = 61.8450, GNorm = 1.0638, lr_0 = 4.9791e-04
Loss = 4.2905e-04, PNorm = 61.8633, GNorm = 0.7549, lr_0 = 4.9571e-04
Loss = 3.5159e-04, PNorm = 61.8801, GNorm = 0.3045, lr_0 = 4.9351e-04
Loss = 3.7714e-04, PNorm = 61.8946, GNorm = 0.5236, lr_0 = 4.9133e-04
Loss = 5.0145e-04, PNorm = 61.9126, GNorm = 0.6862, lr_0 = 4.8916e-04
Validation rmse logD = 0.626274
Validation R2 logD = 0.697533
Epoch 32
Train function
Loss = 8.0839e-04, PNorm = 61.9362, GNorm = 1.9022, lr_0 = 4.8678e-04
Loss = 7.8058e-04, PNorm = 61.9632, GNorm = 1.2201, lr_0 = 4.8463e-04
Loss = 9.2646e-04, PNorm = 62.0023, GNorm = 0.7968, lr_0 = 4.8248e-04
Loss = 5.7313e-04, PNorm = 62.0359, GNorm = 0.4637, lr_0 = 4.8035e-04
Loss = 4.7807e-04, PNorm = 62.0580, GNorm = 0.7682, lr_0 = 4.7822e-04
Loss = 3.8231e-04, PNorm = 62.0719, GNorm = 0.5014, lr_0 = 4.7611e-04
Validation rmse logD = 0.585829
Validation R2 logD = 0.735338
Epoch 33
Train function
Loss = 3.9849e-04, PNorm = 62.0923, GNorm = 0.5521, lr_0 = 4.7379e-04
Loss = 3.0686e-04, PNorm = 62.1076, GNorm = 0.3652, lr_0 = 4.7170e-04
Loss = 4.3175e-04, PNorm = 62.1184, GNorm = 0.9290, lr_0 = 4.6961e-04
Loss = 3.3577e-04, PNorm = 62.1332, GNorm = 0.7442, lr_0 = 4.6753e-04
Loss = 3.9236e-04, PNorm = 62.1476, GNorm = 0.8279, lr_0 = 4.6546e-04
Validation rmse logD = 0.596712
Validation R2 logD = 0.725413
Epoch 34
Train function
Loss = 2.8812e-04, PNorm = 62.1705, GNorm = 0.3148, lr_0 = 4.6320e-04
Loss = 3.2847e-04, PNorm = 62.1826, GNorm = 0.3195, lr_0 = 4.6115e-04
Loss = 3.3917e-04, PNorm = 62.1905, GNorm = 0.5880, lr_0 = 4.5911e-04
Loss = 3.4442e-04, PNorm = 62.2021, GNorm = 0.4445, lr_0 = 4.5708e-04
Loss = 3.6129e-04, PNorm = 62.2155, GNorm = 0.5426, lr_0 = 4.5506e-04
Validation rmse logD = 0.591706
Validation R2 logD = 0.730002
Epoch 35
Train function
Loss = 2.8467e-04, PNorm = 62.2338, GNorm = 0.3429, lr_0 = 4.5284e-04
Loss = 2.9090e-04, PNorm = 62.2461, GNorm = 0.3560, lr_0 = 4.5084e-04
Loss = 2.5850e-04, PNorm = 62.2603, GNorm = 0.4883, lr_0 = 4.4885e-04
Loss = 2.7942e-04, PNorm = 62.2690, GNorm = 0.3625, lr_0 = 4.4686e-04
Loss = 2.9649e-04, PNorm = 62.2765, GNorm = 0.4096, lr_0 = 4.4489e-04
Loss = 3.1622e-04, PNorm = 62.2895, GNorm = 0.2977, lr_0 = 4.4292e-04
Validation rmse logD = 0.585047
Validation R2 logD = 0.736045
Epoch 36
Train function
Loss = 3.0357e-04, PNorm = 62.3109, GNorm = 0.7557, lr_0 = 4.4076e-04
Loss = 2.9945e-04, PNorm = 62.3255, GNorm = 0.3364, lr_0 = 4.3881e-04
Loss = 2.5657e-04, PNorm = 62.3380, GNorm = 0.4661, lr_0 = 4.3687e-04
Loss = 2.7125e-04, PNorm = 62.3531, GNorm = 0.7722, lr_0 = 4.3494e-04
Loss = 3.6882e-04, PNorm = 62.3666, GNorm = 0.5480, lr_0 = 4.3302e-04
Validation rmse logD = 0.582175
Validation R2 logD = 0.738629
Epoch 37
Train function
Loss = 2.0563e-04, PNorm = 62.3711, GNorm = 0.4268, lr_0 = 4.3091e-04
Loss = 2.3219e-04, PNorm = 62.3851, GNorm = 0.3431, lr_0 = 4.2900e-04
Loss = 2.1304e-04, PNorm = 62.3963, GNorm = 0.3420, lr_0 = 4.2711e-04
Loss = 2.3294e-04, PNorm = 62.4096, GNorm = 0.3959, lr_0 = 4.2522e-04
Loss = 2.5128e-04, PNorm = 62.4184, GNorm = 0.3926, lr_0 = 4.2334e-04
Validation rmse logD = 0.593787
Validation R2 logD = 0.728100
Epoch 38
Train function
Loss = 3.4405e-04, PNorm = 62.4272, GNorm = 0.9814, lr_0 = 4.2128e-04
Loss = 4.2073e-04, PNorm = 62.4478, GNorm = 0.9595, lr_0 = 4.1941e-04
Loss = 2.7961e-04, PNorm = 62.4646, GNorm = 0.3189, lr_0 = 4.1756e-04
Loss = 2.5771e-04, PNorm = 62.4771, GNorm = 0.4540, lr_0 = 4.1571e-04
Loss = 2.9542e-04, PNorm = 62.4882, GNorm = 0.6207, lr_0 = 4.1387e-04
Loss = 2.4924e-04, PNorm = 62.5041, GNorm = 0.2412, lr_0 = 4.1204e-04
Loss = 5.0151e-03, PNorm = 62.5047, GNorm = 2.0480, lr_0 = 4.1186e-04
Validation rmse logD = 0.591650
Validation R2 logD = 0.730053
Epoch 39
Train function
Loss = 2.6209e-04, PNorm = 62.5205, GNorm = 1.0518, lr_0 = 4.1004e-04
Loss = 3.1467e-04, PNorm = 62.5408, GNorm = 0.4094, lr_0 = 4.0822e-04
Loss = 2.7663e-04, PNorm = 62.5537, GNorm = 0.3539, lr_0 = 4.0642e-04
Loss = 2.2625e-04, PNorm = 62.5648, GNorm = 0.2346, lr_0 = 4.0462e-04
Loss = 3.0287e-04, PNorm = 62.5748, GNorm = 0.3530, lr_0 = 4.0283e-04
Validation rmse logD = 0.588003
Validation R2 logD = 0.733371
Epoch 40
Train function
Loss = 2.3834e-04, PNorm = 62.5843, GNorm = 0.7677, lr_0 = 4.0105e-04
Loss = 2.7432e-04, PNorm = 62.5923, GNorm = 0.3592, lr_0 = 3.9927e-04
Loss = 2.3957e-04, PNorm = 62.6028, GNorm = 0.5711, lr_0 = 3.9751e-04
Loss = 2.3767e-04, PNorm = 62.6126, GNorm = 0.6635, lr_0 = 3.9575e-04
Loss = 2.3091e-04, PNorm = 62.6212, GNorm = 0.2163, lr_0 = 3.9400e-04
Validation rmse logD = 0.594357
Validation R2 logD = 0.727577
Epoch 41
Train function
Loss = 2.3960e-04, PNorm = 62.6325, GNorm = 0.2803, lr_0 = 3.9208e-04
Loss = 2.5666e-04, PNorm = 62.6447, GNorm = 0.6696, lr_0 = 3.9035e-04
Loss = 2.3203e-04, PNorm = 62.6550, GNorm = 0.4186, lr_0 = 3.8862e-04
Loss = 2.2103e-04, PNorm = 62.6655, GNorm = 0.2371, lr_0 = 3.8690e-04
Loss = 2.1065e-04, PNorm = 62.6741, GNorm = 0.2304, lr_0 = 3.8519e-04
Loss = 1.7815e-04, PNorm = 62.6843, GNorm = 0.3103, lr_0 = 3.8349e-04
Loss = 9.1249e-04, PNorm = 62.6854, GNorm = 0.5288, lr_0 = 3.8332e-04
Validation rmse logD = 0.588028
Validation R2 logD = 0.733348
Epoch 42
Train function
Loss = 1.3827e-04, PNorm = 62.6942, GNorm = 0.1700, lr_0 = 3.8162e-04
Loss = 1.7392e-04, PNorm = 62.7049, GNorm = 0.3340, lr_0 = 3.7993e-04
Loss = 1.6159e-04, PNorm = 62.7142, GNorm = 0.2144, lr_0 = 3.7825e-04
Loss = 2.2388e-04, PNorm = 62.7210, GNorm = 0.4049, lr_0 = 3.7658e-04
Loss = 1.2800e-04, PNorm = 62.7276, GNorm = 0.2028, lr_0 = 3.7491e-04
Validation rmse logD = 0.589460
Validation R2 logD = 0.732048
Epoch 43
Train function
Loss = 1.1316e-04, PNorm = 62.7343, GNorm = 0.1785, lr_0 = 3.7309e-04
Loss = 1.3446e-04, PNorm = 62.7435, GNorm = 0.3197, lr_0 = 3.7144e-04
Loss = 1.1988e-04, PNorm = 62.7518, GNorm = 0.2856, lr_0 = 3.6980e-04
Loss = 1.1575e-04, PNorm = 62.7586, GNorm = 0.2113, lr_0 = 3.6816e-04
Loss = 1.7059e-04, PNorm = 62.7674, GNorm = 0.3566, lr_0 = 3.6653e-04
Validation rmse logD = 0.593834
Validation R2 logD = 0.728056
Epoch 44
Train function
Loss = 1.6644e-04, PNorm = 62.7761, GNorm = 0.4981, lr_0 = 3.6475e-04
Loss = 1.7198e-04, PNorm = 62.7843, GNorm = 0.4598, lr_0 = 3.6314e-04
Loss = 1.9125e-04, PNorm = 62.7922, GNorm = 0.5289, lr_0 = 3.6153e-04
Loss = 1.6872e-04, PNorm = 62.8025, GNorm = 0.5303, lr_0 = 3.5993e-04
Loss = 2.0602e-04, PNorm = 62.8096, GNorm = 0.5169, lr_0 = 3.5834e-04
Validation rmse logD = 0.584837
Validation R2 logD = 0.736234
Epoch 45
Train function
Loss = 1.4385e-04, PNorm = 62.8179, GNorm = 0.5086, lr_0 = 3.5660e-04
Loss = 1.4449e-04, PNorm = 62.8245, GNorm = 0.2110, lr_0 = 3.5502e-04
Loss = 1.3833e-04, PNorm = 62.8325, GNorm = 0.3985, lr_0 = 3.5345e-04
Loss = 1.7256e-04, PNorm = 62.8399, GNorm = 0.3021, lr_0 = 3.5188e-04
Loss = 1.7467e-04, PNorm = 62.8480, GNorm = 0.3199, lr_0 = 3.5033e-04
Loss = 1.4414e-04, PNorm = 62.8574, GNorm = 0.2595, lr_0 = 3.4878e-04
Validation rmse logD = 0.589786
Validation R2 logD = 0.731751
Epoch 46
Train function
Loss = 1.1730e-04, PNorm = 62.8664, GNorm = 0.2027, lr_0 = 3.4708e-04
Loss = 1.5718e-04, PNorm = 62.8732, GNorm = 0.3994, lr_0 = 3.4555e-04
Loss = 1.3721e-04, PNorm = 62.8813, GNorm = 0.3521, lr_0 = 3.4402e-04
Loss = 1.3046e-04, PNorm = 62.8885, GNorm = 0.2001, lr_0 = 3.4250e-04
Loss = 1.6258e-04, PNorm = 62.9014, GNorm = 0.6617, lr_0 = 3.4098e-04
Validation rmse logD = 0.590005
Validation R2 logD = 0.731552
Epoch 47
Train function
Loss = 1.4508e-04, PNorm = 62.9069, GNorm = 0.1670, lr_0 = 3.3932e-04
Loss = 1.4888e-04, PNorm = 62.9158, GNorm = 0.3484, lr_0 = 3.3782e-04
Loss = 1.1597e-04, PNorm = 62.9224, GNorm = 0.2316, lr_0 = 3.3633e-04
Loss = 1.1263e-04, PNorm = 62.9288, GNorm = 0.2036, lr_0 = 3.3484e-04
Loss = 1.1628e-04, PNorm = 62.9332, GNorm = 0.2159, lr_0 = 3.3336e-04
Validation rmse logD = 0.607266
Validation R2 logD = 0.715615
Epoch 48
Train function
Loss = 3.6137e-04, PNorm = 62.9416, GNorm = 1.6792, lr_0 = 3.3174e-04
Loss = 3.3806e-04, PNorm = 62.9516, GNorm = 0.4871, lr_0 = 3.3027e-04
Loss = 2.0497e-04, PNorm = 62.9676, GNorm = 0.4634, lr_0 = 3.2881e-04
Loss = 1.4670e-04, PNorm = 62.9787, GNorm = 0.2347, lr_0 = 3.2735e-04
Loss = 1.4118e-04, PNorm = 62.9860, GNorm = 0.2949, lr_0 = 3.2591e-04
Loss = 1.4272e-04, PNorm = 62.9941, GNorm = 0.2705, lr_0 = 3.2446e-04
Validation rmse logD = 0.586365
Validation R2 logD = 0.734854
Epoch 49
Train function
Loss = 1.3910e-04, PNorm = 63.0040, GNorm = 0.5042, lr_0 = 3.2289e-04
Loss = 1.2400e-04, PNorm = 63.0155, GNorm = 0.3300, lr_0 = 3.2146e-04
Loss = 1.5637e-04, PNorm = 63.0212, GNorm = 0.5345, lr_0 = 3.2004e-04
Loss = 1.2032e-04, PNorm = 63.0259, GNorm = 0.3151, lr_0 = 3.1862e-04
Loss = 1.0959e-04, PNorm = 63.0337, GNorm = 0.2498, lr_0 = 3.1721e-04
Validation rmse logD = 0.593211
Validation R2 logD = 0.728627
Epoch 50
Train function
Loss = 9.6127e-05, PNorm = 63.0395, GNorm = 0.3368, lr_0 = 3.1581e-04
Loss = 7.6043e-05, PNorm = 63.0419, GNorm = 0.1432, lr_0 = 3.1441e-04
Loss = 8.8908e-05, PNorm = 63.0478, GNorm = 0.3535, lr_0 = 3.1302e-04
Loss = 1.3069e-04, PNorm = 63.0543, GNorm = 0.6363, lr_0 = 3.1164e-04
Loss = 1.0088e-04, PNorm = 63.0613, GNorm = 0.1902, lr_0 = 3.1026e-04
Validation rmse logD = 0.589408
Validation R2 logD = 0.732095
Epoch 51
Train function
Loss = 8.3693e-05, PNorm = 63.0657, GNorm = 0.4102, lr_0 = 3.0875e-04
Loss = 1.0380e-04, PNorm = 63.0726, GNorm = 0.2701, lr_0 = 3.0738e-04
Loss = 1.3720e-04, PNorm = 63.0804, GNorm = 0.2636, lr_0 = 3.0602e-04
Loss = 1.1340e-04, PNorm = 63.0862, GNorm = 0.2696, lr_0 = 3.0467e-04
Loss = 9.3576e-05, PNorm = 63.0934, GNorm = 0.1461, lr_0 = 3.0332e-04
Loss = 9.5438e-05, PNorm = 63.0996, GNorm = 0.2591, lr_0 = 3.0198e-04
Validation rmse logD = 0.583556
Validation R2 logD = 0.737388
Epoch 52
Train function
Loss = 9.9213e-05, PNorm = 63.1061, GNorm = 0.2439, lr_0 = 3.0051e-04
Loss = 1.1688e-04, PNorm = 63.1140, GNorm = 0.1891, lr_0 = 2.9918e-04
Loss = 1.2998e-04, PNorm = 63.1194, GNorm = 0.1518, lr_0 = 2.9786e-04
Loss = 1.4234e-04, PNorm = 63.1272, GNorm = 0.1455, lr_0 = 2.9654e-04
Loss = 1.4636e-04, PNorm = 63.1336, GNorm = 0.1919, lr_0 = 2.9523e-04
Validation rmse logD = 0.590180
Validation R2 logD = 0.731393
Epoch 53
Train function
Loss = 7.4979e-05, PNorm = 63.1405, GNorm = 0.2700, lr_0 = 2.9379e-04
Loss = 1.2043e-04, PNorm = 63.1506, GNorm = 0.1509, lr_0 = 2.9249e-04
Loss = 1.1247e-04, PNorm = 63.1596, GNorm = 0.2414, lr_0 = 2.9120e-04
Loss = 9.9208e-05, PNorm = 63.1653, GNorm = 0.1571, lr_0 = 2.8991e-04
Loss = 1.1233e-04, PNorm = 63.1692, GNorm = 0.1621, lr_0 = 2.8863e-04
Validation rmse logD = 0.587230
Validation R2 logD = 0.734071
Epoch 54
Train function
Loss = 1.1159e-04, PNorm = 63.1743, GNorm = 0.5906, lr_0 = 2.8722e-04
Loss = 1.3781e-04, PNorm = 63.1794, GNorm = 0.3567, lr_0 = 2.8595e-04
Loss = 1.0627e-04, PNorm = 63.1828, GNorm = 0.2224, lr_0 = 2.8469e-04
Loss = 7.6953e-05, PNorm = 63.1871, GNorm = 0.1251, lr_0 = 2.8343e-04
Loss = 7.5975e-05, PNorm = 63.1931, GNorm = 0.1474, lr_0 = 2.8218e-04
Loss = 8.3039e-05, PNorm = 63.1984, GNorm = 0.2643, lr_0 = 2.8093e-04
Loss = 1.6085e-03, PNorm = 63.1996, GNorm = 1.0854, lr_0 = 2.8080e-04
Validation rmse logD = 0.586549
Validation R2 logD = 0.734687
Epoch 55
Train function
Loss = 1.0461e-04, PNorm = 63.2080, GNorm = 0.3760, lr_0 = 2.7956e-04
Loss = 9.2439e-05, PNorm = 63.2111, GNorm = 0.5814, lr_0 = 2.7832e-04
Loss = 7.4108e-05, PNorm = 63.2163, GNorm = 0.1556, lr_0 = 2.7709e-04
Loss = 7.0567e-05, PNorm = 63.2206, GNorm = 0.1323, lr_0 = 2.7587e-04
Loss = 6.4475e-05, PNorm = 63.2257, GNorm = 0.2501, lr_0 = 2.7465e-04
Validation rmse logD = 0.589283
Validation R2 logD = 0.732208
Epoch 56
Train function
Loss = 8.8967e-05, PNorm = 63.2299, GNorm = 0.1475, lr_0 = 2.7331e-04
Loss = 6.3326e-05, PNorm = 63.2322, GNorm = 0.1027, lr_0 = 2.7210e-04
Loss = 5.4457e-05, PNorm = 63.2375, GNorm = 0.1881, lr_0 = 2.7090e-04
Loss = 7.2574e-05, PNorm = 63.2418, GNorm = 0.3275, lr_0 = 2.6970e-04
Loss = 6.4921e-05, PNorm = 63.2452, GNorm = 0.1494, lr_0 = 2.6851e-04
Validation rmse logD = 0.597183
Validation R2 logD = 0.724980
Epoch 57
Train function
Loss = 1.2662e-04, PNorm = 63.2504, GNorm = 0.4575, lr_0 = 2.6720e-04
Loss = 1.2115e-04, PNorm = 63.2566, GNorm = 0.3969, lr_0 = 2.6602e-04
Loss = 9.8224e-05, PNorm = 63.2614, GNorm = 0.2126, lr_0 = 2.6484e-04
Loss = 7.8174e-05, PNorm = 63.2654, GNorm = 0.3186, lr_0 = 2.6367e-04
Loss = 8.5904e-05, PNorm = 63.2694, GNorm = 0.3054, lr_0 = 2.6250e-04
Validation rmse logD = 0.588785
Validation R2 logD = 0.732661
Epoch 58
Train function
Loss = 5.6153e-05, PNorm = 63.2738, GNorm = 0.2065, lr_0 = 2.6123e-04
Loss = 8.1661e-05, PNorm = 63.2777, GNorm = 0.3611, lr_0 = 2.6007e-04
Loss = 5.7575e-05, PNorm = 63.2813, GNorm = 0.1213, lr_0 = 2.5892e-04
Loss = 5.6126e-05, PNorm = 63.2855, GNorm = 0.1225, lr_0 = 2.5778e-04
Loss = 6.3597e-05, PNorm = 63.2899, GNorm = 0.3831, lr_0 = 2.5664e-04
Loss = 8.2747e-05, PNorm = 63.2956, GNorm = 0.3092, lr_0 = 2.5550e-04
Validation rmse logD = 0.590501
Validation R2 logD = 0.731100
Epoch 59
Train function
Loss = 7.4961e-05, PNorm = 63.2980, GNorm = 0.4342, lr_0 = 2.5426e-04
Loss = 6.9092e-05, PNorm = 63.3000, GNorm = 0.4500, lr_0 = 2.5313e-04
Loss = 6.4887e-05, PNorm = 63.3032, GNorm = 0.3191, lr_0 = 2.5201e-04
Loss = 6.9867e-05, PNorm = 63.3089, GNorm = 0.3136, lr_0 = 2.5090e-04
Loss = 6.3308e-05, PNorm = 63.3141, GNorm = 0.1856, lr_0 = 2.4979e-04
Validation rmse logD = 0.585992
Validation R2 logD = 0.735191
Epoch 60
Train function
Loss = 8.5745e-05, PNorm = 63.3176, GNorm = 0.3029, lr_0 = 2.4868e-04
Loss = 1.1197e-04, PNorm = 63.3244, GNorm = 0.2072, lr_0 = 2.4758e-04
Loss = 8.6578e-05, PNorm = 63.3260, GNorm = 0.3303, lr_0 = 2.4649e-04
Loss = 6.5906e-05, PNorm = 63.3319, GNorm = 0.1446, lr_0 = 2.4540e-04
Loss = 6.5287e-05, PNorm = 63.3390, GNorm = 0.3366, lr_0 = 2.4431e-04
Validation rmse logD = 0.592540
Validation R2 logD = 0.729240
Epoch 61
Train function
Loss = 8.0504e-05, PNorm = 63.3416, GNorm = 0.5221, lr_0 = 2.4313e-04
Loss = 7.1734e-05, PNorm = 63.3460, GNorm = 0.4290, lr_0 = 2.4205e-04
Loss = 7.0321e-05, PNorm = 63.3499, GNorm = 0.6572, lr_0 = 2.4098e-04
Loss = 5.1119e-05, PNorm = 63.3520, GNorm = 0.2696, lr_0 = 2.3991e-04
Loss = 4.2162e-05, PNorm = 63.3540, GNorm = 0.1729, lr_0 = 2.3885e-04
Loss = 6.2005e-05, PNorm = 63.3576, GNorm = 0.1569, lr_0 = 2.3780e-04
Validation rmse logD = 0.589193
Validation R2 logD = 0.732290
Epoch 62
Train function
Loss = 5.7330e-05, PNorm = 63.3612, GNorm = 0.2400, lr_0 = 2.3664e-04
Loss = 6.0088e-05, PNorm = 63.3664, GNorm = 0.1839, lr_0 = 2.3559e-04
Loss = 4.9132e-05, PNorm = 63.3711, GNorm = 0.2227, lr_0 = 2.3455e-04
Loss = 5.6172e-05, PNorm = 63.3744, GNorm = 0.1248, lr_0 = 2.3351e-04
Loss = 3.6282e-05, PNorm = 63.3767, GNorm = 0.1092, lr_0 = 2.3248e-04
Validation rmse logD = 0.588240
Validation R2 logD = 0.733155
Epoch 63
Train function
Loss = 4.6315e-05, PNorm = 63.3799, GNorm = 0.2774, lr_0 = 2.3135e-04
Loss = 4.6463e-05, PNorm = 63.3837, GNorm = 0.2495, lr_0 = 2.3033e-04
Loss = 3.8598e-05, PNorm = 63.3861, GNorm = 0.1373, lr_0 = 2.2931e-04
Loss = 4.2561e-05, PNorm = 63.3888, GNorm = 0.0928, lr_0 = 2.2829e-04
Loss = 5.0888e-05, PNorm = 63.3922, GNorm = 0.1646, lr_0 = 2.2728e-04
Validation rmse logD = 0.587546
Validation R2 logD = 0.733784
Epoch 64
Train function
Loss = 2.3120e-05, PNorm = 63.3931, GNorm = 0.2034, lr_0 = 2.2618e-04
Loss = 4.1168e-05, PNorm = 63.3948, GNorm = 0.2142, lr_0 = 2.2518e-04
Loss = 4.3796e-05, PNorm = 63.3989, GNorm = 0.1014, lr_0 = 2.2418e-04
Loss = 4.9353e-05, PNorm = 63.4030, GNorm = 0.3873, lr_0 = 2.2319e-04
Loss = 5.9304e-05, PNorm = 63.4049, GNorm = 0.5785, lr_0 = 2.2220e-04
Loss = 5.5596e-05, PNorm = 63.4076, GNorm = 0.4646, lr_0 = 2.2122e-04
Validation rmse logD = 0.588974
Validation R2 logD = 0.732489
Epoch 65
Train function
Loss = 6.2310e-05, PNorm = 63.4115, GNorm = 0.4316, lr_0 = 2.2014e-04
Loss = 5.6298e-05, PNorm = 63.4161, GNorm = 0.3584, lr_0 = 2.1917e-04
Loss = 4.6973e-05, PNorm = 63.4197, GNorm = 0.1729, lr_0 = 2.1820e-04
Loss = 5.3616e-05, PNorm = 63.4239, GNorm = 0.3285, lr_0 = 2.1723e-04
Loss = 4.2382e-05, PNorm = 63.4248, GNorm = 0.1945, lr_0 = 2.1627e-04
Validation rmse logD = 0.587325
Validation R2 logD = 0.733985
Epoch 66
Train function
Loss = 4.2023e-05, PNorm = 63.4272, GNorm = 0.1856, lr_0 = 2.1522e-04
Loss = 4.8788e-05, PNorm = 63.4295, GNorm = 0.1056, lr_0 = 2.1427e-04
Loss = 4.5844e-05, PNorm = 63.4334, GNorm = 0.1264, lr_0 = 2.1332e-04
Loss = 3.9878e-05, PNorm = 63.4360, GNorm = 0.1041, lr_0 = 2.1238e-04
Loss = 4.6695e-05, PNorm = 63.4378, GNorm = 0.4589, lr_0 = 2.1144e-04
Validation rmse logD = 0.589668
Validation R2 logD = 0.731858
Epoch 67
Train function
Loss = 4.4958e-05, PNorm = 63.4413, GNorm = 0.1254, lr_0 = 2.1041e-04
Loss = 3.5892e-05, PNorm = 63.4443, GNorm = 0.0904, lr_0 = 2.0948e-04
Loss = 2.9011e-05, PNorm = 63.4456, GNorm = 0.1353, lr_0 = 2.0855e-04
Loss = 2.6481e-05, PNorm = 63.4478, GNorm = 0.1925, lr_0 = 2.0763e-04
Loss = 3.1705e-05, PNorm = 63.4500, GNorm = 0.1930, lr_0 = 2.0671e-04
Loss = 3.9767e-05, PNorm = 63.4528, GNorm = 0.1730, lr_0 = 2.0580e-04
Loss = 1.3159e-04, PNorm = 63.4529, GNorm = 0.3074, lr_0 = 2.0571e-04
Validation rmse logD = 0.590981
Validation R2 logD = 0.730663
Epoch 68
Train function
Loss = 2.9052e-05, PNorm = 63.4559, GNorm = 0.0956, lr_0 = 2.0480e-04
Loss = 3.5177e-05, PNorm = 63.4577, GNorm = 0.1203, lr_0 = 2.0389e-04
Loss = 3.8364e-05, PNorm = 63.4597, GNorm = 0.1139, lr_0 = 2.0299e-04
Loss = 3.2333e-05, PNorm = 63.4617, GNorm = 0.1400, lr_0 = 2.0209e-04
Loss = 3.5150e-05, PNorm = 63.4637, GNorm = 0.0939, lr_0 = 2.0120e-04
Validation rmse logD = 0.590386
Validation R2 logD = 0.731205
Epoch 69
Train function
Loss = 2.2508e-05, PNorm = 63.4673, GNorm = 0.0952, lr_0 = 2.0022e-04
Loss = 2.7248e-05, PNorm = 63.4687, GNorm = 0.1198, lr_0 = 1.9933e-04
Loss = 2.7476e-05, PNorm = 63.4712, GNorm = 0.1122, lr_0 = 1.9845e-04
Loss = 2.9990e-05, PNorm = 63.4746, GNorm = 0.1575, lr_0 = 1.9757e-04
Loss = 2.8731e-05, PNorm = 63.4769, GNorm = 0.2156, lr_0 = 1.9670e-04
Validation rmse logD = 0.591546
Validation R2 logD = 0.730148
Epoch 70
Train function
Loss = 4.3022e-05, PNorm = 63.4784, GNorm = 0.1771, lr_0 = 1.9583e-04
Loss = 4.0177e-05, PNorm = 63.4805, GNorm = 0.1183, lr_0 = 1.9496e-04
Loss = 4.3372e-05, PNorm = 63.4842, GNorm = 0.2821, lr_0 = 1.9410e-04
Loss = 4.3838e-05, PNorm = 63.4871, GNorm = 0.2311, lr_0 = 1.9324e-04
Loss = 3.6157e-05, PNorm = 63.4907, GNorm = 0.0819, lr_0 = 1.9239e-04
Loss = 3.5378e-05, PNorm = 63.4939, GNorm = 0.2161, lr_0 = 1.9154e-04
Loss = 6.3396e-05, PNorm = 63.4938, GNorm = 0.1719, lr_0 = 1.9145e-04
Validation rmse logD = 0.592491
Validation R2 logD = 0.729284
Epoch 71
Train function
Loss = 3.9045e-05, PNorm = 63.4954, GNorm = 0.1233, lr_0 = 1.9060e-04
Loss = 2.9716e-05, PNorm = 63.4970, GNorm = 0.2174, lr_0 = 1.8976e-04
Loss = 3.8203e-05, PNorm = 63.4992, GNorm = 0.1997, lr_0 = 1.8892e-04
Loss = 2.6839e-05, PNorm = 63.5030, GNorm = 0.2107, lr_0 = 1.8809e-04
Loss = 2.8849e-05, PNorm = 63.5058, GNorm = 0.0948, lr_0 = 1.8725e-04
Validation rmse logD = 0.592997
Validation R2 logD = 0.728822
Epoch 72
Train function
Loss = 2.6715e-05, PNorm = 63.5066, GNorm = 0.1284, lr_0 = 1.8634e-04
Loss = 3.3192e-05, PNorm = 63.5077, GNorm = 0.1835, lr_0 = 1.8552e-04
Loss = 3.5024e-05, PNorm = 63.5103, GNorm = 0.2403, lr_0 = 1.8470e-04
Loss = 2.9032e-05, PNorm = 63.5134, GNorm = 0.1216, lr_0 = 1.8388e-04
Loss = 2.4006e-05, PNorm = 63.5156, GNorm = 0.1054, lr_0 = 1.8307e-04
Validation rmse logD = 0.591868
Validation R2 logD = 0.729854
Epoch 73
Train function
Loss = 1.6565e-05, PNorm = 63.5175, GNorm = 0.1295, lr_0 = 1.8218e-04
Loss = 1.7648e-05, PNorm = 63.5180, GNorm = 0.0742, lr_0 = 1.8137e-04
Loss = 1.8527e-05, PNorm = 63.5205, GNorm = 0.1165, lr_0 = 1.8057e-04
Loss = 3.2924e-05, PNorm = 63.5235, GNorm = 0.0782, lr_0 = 1.7977e-04
Loss = 2.4380e-05, PNorm = 63.5251, GNorm = 0.1999, lr_0 = 1.7897e-04
Validation rmse logD = 0.588844
Validation R2 logD = 0.732607
Epoch 74
Train function
Loss = 2.3690e-05, PNorm = 63.5269, GNorm = 0.0954, lr_0 = 1.7810e-04
Loss = 2.1557e-05, PNorm = 63.5292, GNorm = 0.2235, lr_0 = 1.7732e-04
Loss = 2.0801e-05, PNorm = 63.5302, GNorm = 0.2383, lr_0 = 1.7653e-04
Loss = 2.6240e-05, PNorm = 63.5320, GNorm = 0.3199, lr_0 = 1.7575e-04
Loss = 2.2638e-05, PNorm = 63.5338, GNorm = 0.2260, lr_0 = 1.7497e-04
Loss = 2.6710e-05, PNorm = 63.5352, GNorm = 0.2431, lr_0 = 1.7420e-04
Validation rmse logD = 0.590538
Validation R2 logD = 0.731066
Epoch 75
Train function
Loss = 3.0130e-05, PNorm = 63.5379, GNorm = 0.1180, lr_0 = 1.7335e-04
Loss = 2.1133e-05, PNorm = 63.5398, GNorm = 0.1819, lr_0 = 1.7259e-04
Loss = 2.3071e-05, PNorm = 63.5429, GNorm = 0.0859, lr_0 = 1.7182e-04
Loss = 2.8478e-05, PNorm = 63.5439, GNorm = 0.2610, lr_0 = 1.7106e-04
Loss = 2.5047e-05, PNorm = 63.5458, GNorm = 0.2125, lr_0 = 1.7031e-04
Validation rmse logD = 0.590446
Validation R2 logD = 0.731151
Epoch 76
Train function
Loss = 2.5727e-05, PNorm = 63.5487, GNorm = 0.1135, lr_0 = 1.6948e-04
Loss = 2.0480e-05, PNorm = 63.5506, GNorm = 0.1947, lr_0 = 1.6873e-04
Loss = 1.7805e-05, PNorm = 63.5523, GNorm = 0.0736, lr_0 = 1.6798e-04
Loss = 1.7792e-05, PNorm = 63.5534, GNorm = 0.2407, lr_0 = 1.6724e-04
Loss = 2.5397e-05, PNorm = 63.5540, GNorm = 0.3313, lr_0 = 1.6650e-04
Validation rmse logD = 0.590452
Validation R2 logD = 0.731145
Epoch 77
Train function
Loss = 2.2662e-05, PNorm = 63.5564, GNorm = 0.2717, lr_0 = 1.6569e-04
Loss = 3.3558e-05, PNorm = 63.5576, GNorm = 0.1823, lr_0 = 1.6496e-04
Loss = 3.1272e-05, PNorm = 63.5605, GNorm = 0.1542, lr_0 = 1.6423e-04
Loss = 2.8631e-05, PNorm = 63.5627, GNorm = 0.1365, lr_0 = 1.6350e-04
Loss = 2.4632e-05, PNorm = 63.5655, GNorm = 0.0804, lr_0 = 1.6278e-04
Loss = 2.6229e-05, PNorm = 63.5678, GNorm = 0.1246, lr_0 = 1.6206e-04
Validation rmse logD = 0.589499
Validation R2 logD = 0.732012
Epoch 78
Train function
Loss = 2.0759e-05, PNorm = 63.5685, GNorm = 0.0441, lr_0 = 1.6127e-04
Loss = 1.8796e-05, PNorm = 63.5690, GNorm = 0.1796, lr_0 = 1.6055e-04
Loss = 2.2280e-05, PNorm = 63.5704, GNorm = 0.1443, lr_0 = 1.5984e-04
Loss = 2.5980e-05, PNorm = 63.5729, GNorm = 0.1018, lr_0 = 1.5914e-04
Loss = 1.8078e-05, PNorm = 63.5754, GNorm = 0.1423, lr_0 = 1.5843e-04
Validation rmse logD = 0.591156
Validation R2 logD = 0.730503
Epoch 79
Train function
Loss = 1.5873e-05, PNorm = 63.5779, GNorm = 0.1803, lr_0 = 1.5766e-04
Loss = 2.3161e-05, PNorm = 63.5786, GNorm = 0.1232, lr_0 = 1.5697e-04
Loss = 2.3871e-05, PNorm = 63.5784, GNorm = 0.0640, lr_0 = 1.5627e-04
Loss = 1.6954e-05, PNorm = 63.5809, GNorm = 0.0947, lr_0 = 1.5558e-04
Loss = 2.5344e-05, PNorm = 63.5820, GNorm = 0.1290, lr_0 = 1.5489e-04
Validation rmse logD = 0.592054
Validation R2 logD = 0.729684
Epoch 80
Train function
Loss = 2.1920e-05, PNorm = 63.5845, GNorm = 0.1595, lr_0 = 1.5421e-04
Loss = 2.2971e-05, PNorm = 63.5860, GNorm = 0.0827, lr_0 = 1.5352e-04
Loss = 1.3844e-05, PNorm = 63.5866, GNorm = 0.1450, lr_0 = 1.5284e-04
Loss = 2.0048e-05, PNorm = 63.5885, GNorm = 0.1478, lr_0 = 1.5217e-04
Loss = 1.8271e-05, PNorm = 63.5913, GNorm = 0.1649, lr_0 = 1.5150e-04
Loss = 1.9083e-05, PNorm = 63.5933, GNorm = 0.0947, lr_0 = 1.5083e-04
Validation rmse logD = 0.591589
Validation R2 logD = 0.730108
Epoch 81
Train function
Loss = 2.3988e-05, PNorm = 63.5949, GNorm = 0.2894, lr_0 = 1.5009e-04
Loss = 2.1671e-05, PNorm = 63.5958, GNorm = 0.0846, lr_0 = 1.4943e-04
Loss = 1.5565e-05, PNorm = 63.5966, GNorm = 0.0587, lr_0 = 1.4877e-04
Loss = 1.7588e-05, PNorm = 63.5979, GNorm = 0.1841, lr_0 = 1.4811e-04
Loss = 1.9209e-05, PNorm = 63.5989, GNorm = 0.1863, lr_0 = 1.4745e-04
Validation rmse logD = 0.591889
Validation R2 logD = 0.729835
Epoch 82
Train function
Loss = 2.6027e-05, PNorm = 63.6005, GNorm = 0.2823, lr_0 = 1.4674e-04
Loss = 2.4258e-05, PNorm = 63.6017, GNorm = 0.1149, lr_0 = 1.4609e-04
Loss = 2.3967e-05, PNorm = 63.6022, GNorm = 0.2249, lr_0 = 1.4544e-04
Loss = 3.3069e-05, PNorm = 63.6043, GNorm = 0.0904, lr_0 = 1.4480e-04
Loss = 1.5701e-05, PNorm = 63.6061, GNorm = 0.1407, lr_0 = 1.4416e-04
Validation rmse logD = 0.591074
Validation R2 logD = 0.730578
Epoch 83
Train function
Loss = 1.2044e-05, PNorm = 63.6090, GNorm = 0.1056, lr_0 = 1.4346e-04
Loss = 2.0615e-05, PNorm = 63.6103, GNorm = 0.2800, lr_0 = 1.4282e-04
Loss = 1.9193e-05, PNorm = 63.6119, GNorm = 0.1586, lr_0 = 1.4219e-04
Loss = 1.9096e-05, PNorm = 63.6135, GNorm = 0.2898, lr_0 = 1.4156e-04
Loss = 1.3951e-05, PNorm = 63.6151, GNorm = 0.0559, lr_0 = 1.4093e-04
Loss = 2.0849e-05, PNorm = 63.6164, GNorm = 0.1154, lr_0 = 1.4031e-04
Loss = 9.7256e-05, PNorm = 63.6166, GNorm = 0.1955, lr_0 = 1.4025e-04
Validation rmse logD = 0.590494
Validation R2 logD = 0.731106
Epoch 84
Train function
Loss = 1.7053e-05, PNorm = 63.6184, GNorm = 0.0578, lr_0 = 1.3963e-04
Loss = 1.4227e-05, PNorm = 63.6190, GNorm = 0.0761, lr_0 = 1.3901e-04
Loss = 1.5506e-05, PNorm = 63.6205, GNorm = 0.1193, lr_0 = 1.3840e-04
Loss = 2.1334e-05, PNorm = 63.6218, GNorm = 0.1610, lr_0 = 1.3778e-04
Loss = 1.8103e-05, PNorm = 63.6232, GNorm = 0.1155, lr_0 = 1.3717e-04
Validation rmse logD = 0.593739
Validation R2 logD = 0.728143
Epoch 85
Train function
Loss = 1.9280e-05, PNorm = 63.6246, GNorm = 0.2389, lr_0 = 1.3651e-04
Loss = 1.9934e-05, PNorm = 63.6262, GNorm = 0.1297, lr_0 = 1.3590e-04
Loss = 1.4733e-05, PNorm = 63.6282, GNorm = 0.0657, lr_0 = 1.3530e-04
Loss = 1.8599e-05, PNorm = 63.6289, GNorm = 0.2577, lr_0 = 1.3470e-04
Loss = 1.9172e-05, PNorm = 63.6308, GNorm = 0.0921, lr_0 = 1.3411e-04
Validation rmse logD = 0.592397
Validation R2 logD = 0.729371
Epoch 86
Train function
Loss = 1.3843e-05, PNorm = 63.6318, GNorm = 0.0862, lr_0 = 1.3346e-04
Loss = 2.3809e-05, PNorm = 63.6331, GNorm = 0.0952, lr_0 = 1.3287e-04
Loss = 2.2584e-05, PNorm = 63.6355, GNorm = 0.1544, lr_0 = 1.3228e-04
Loss = 1.7197e-05, PNorm = 63.6365, GNorm = 0.0899, lr_0 = 1.3169e-04
Loss = 1.5401e-05, PNorm = 63.6378, GNorm = 0.1060, lr_0 = 1.3111e-04
Validation rmse logD = 0.591720
Validation R2 logD = 0.729989
Epoch 87
Train function
Loss = 7.3216e-06, PNorm = 63.6395, GNorm = 0.0821, lr_0 = 1.3047e-04
Loss = 1.5299e-05, PNorm = 63.6406, GNorm = 0.2286, lr_0 = 1.2990e-04
Loss = 1.6236e-05, PNorm = 63.6427, GNorm = 0.0975, lr_0 = 1.2932e-04
Loss = 1.7425e-05, PNorm = 63.6433, GNorm = 0.0712, lr_0 = 1.2875e-04
Loss = 1.3729e-05, PNorm = 63.6449, GNorm = 0.0898, lr_0 = 1.2818e-04
Loss = 2.1062e-05, PNorm = 63.6460, GNorm = 0.0696, lr_0 = 1.2761e-04
Validation rmse logD = 0.592094
Validation R2 logD = 0.729648
Epoch 88
Train function
Loss = 1.9441e-05, PNorm = 63.6463, GNorm = 0.2497, lr_0 = 1.2699e-04
Loss = 2.2575e-05, PNorm = 63.6482, GNorm = 0.1428, lr_0 = 1.2643e-04
Loss = 1.7760e-05, PNorm = 63.6492, GNorm = 0.0882, lr_0 = 1.2587e-04
Loss = 1.4955e-05, PNorm = 63.6505, GNorm = 0.1030, lr_0 = 1.2531e-04
Loss = 1.9691e-05, PNorm = 63.6525, GNorm = 0.1249, lr_0 = 1.2476e-04
Validation rmse logD = 0.593576
Validation R2 logD = 0.728292
Epoch 89
Train function
Loss = 1.4733e-05, PNorm = 63.6531, GNorm = 0.1450, lr_0 = 1.2415e-04
Loss = 1.7004e-05, PNorm = 63.6551, GNorm = 0.1031, lr_0 = 1.2360e-04
Loss = 1.2350e-05, PNorm = 63.6559, GNorm = 0.0601, lr_0 = 1.2306e-04
Loss = 1.3814e-05, PNorm = 63.6571, GNorm = 0.0881, lr_0 = 1.2251e-04
Loss = 9.6128e-06, PNorm = 63.6582, GNorm = 0.1062, lr_0 = 1.2197e-04
Validation rmse logD = 0.591672
Validation R2 logD = 0.730033
Epoch 90
Train function
Loss = 2.2110e-05, PNorm = 63.6601, GNorm = 0.1758, lr_0 = 1.2143e-04
Loss = 1.4517e-05, PNorm = 63.6616, GNorm = 0.1089, lr_0 = 1.2089e-04
Loss = 1.2739e-05, PNorm = 63.6613, GNorm = 0.1587, lr_0 = 1.2036e-04
Loss = 1.1786e-05, PNorm = 63.6619, GNorm = 0.0842, lr_0 = 1.1983e-04
Loss = 9.1379e-06, PNorm = 63.6630, GNorm = 0.0650, lr_0 = 1.1930e-04
Loss = 1.6265e-05, PNorm = 63.6653, GNorm = 0.0862, lr_0 = 1.1877e-04
Validation rmse logD = 0.593430
Validation R2 logD = 0.728426
Epoch 91
Train function
Loss = 1.6789e-05, PNorm = 63.6652, GNorm = 0.1062, lr_0 = 1.1819e-04
Loss = 1.3613e-05, PNorm = 63.6666, GNorm = 0.0845, lr_0 = 1.1767e-04
Loss = 1.7429e-05, PNorm = 63.6677, GNorm = 0.0892, lr_0 = 1.1715e-04
Loss = 1.2427e-05, PNorm = 63.6691, GNorm = 0.2097, lr_0 = 1.1663e-04
Loss = 1.1416e-05, PNorm = 63.6700, GNorm = 0.1363, lr_0 = 1.1611e-04
Validation rmse logD = 0.591749
Validation R2 logD = 0.729962
Epoch 92
Train function
Loss = 1.0135e-05, PNorm = 63.6709, GNorm = 0.0872, lr_0 = 1.1555e-04
Loss = 1.4864e-05, PNorm = 63.6730, GNorm = 0.0897, lr_0 = 1.1504e-04
Loss = 1.9020e-05, PNorm = 63.6737, GNorm = 0.0962, lr_0 = 1.1453e-04
Loss = 1.4079e-05, PNorm = 63.6749, GNorm = 0.0423, lr_0 = 1.1402e-04
Loss = 1.1849e-05, PNorm = 63.6764, GNorm = 0.0570, lr_0 = 1.1352e-04
Validation rmse logD = 0.591155
Validation R2 logD = 0.730504
Epoch 93
Train function
Loss = 8.4085e-06, PNorm = 63.6770, GNorm = 0.0721, lr_0 = 1.1297e-04
Loss = 8.2645e-06, PNorm = 63.6781, GNorm = 0.0588, lr_0 = 1.1247e-04
Loss = 9.5582e-06, PNorm = 63.6785, GNorm = 0.0538, lr_0 = 1.1197e-04
Loss = 1.0229e-05, PNorm = 63.6794, GNorm = 0.1579, lr_0 = 1.1147e-04
Loss = 1.5510e-05, PNorm = 63.6804, GNorm = 0.2459, lr_0 = 1.1098e-04
Loss = 1.7148e-05, PNorm = 63.6822, GNorm = 0.1672, lr_0 = 1.1049e-04
Validation rmse logD = 0.593112
Validation R2 logD = 0.728717
Epoch 94
Train function
Loss = 1.4654e-05, PNorm = 63.6836, GNorm = 0.1247, lr_0 = 1.0995e-04
Loss = 1.5357e-05, PNorm = 63.6839, GNorm = 0.1478, lr_0 = 1.0947e-04
Loss = 1.5995e-05, PNorm = 63.6850, GNorm = 0.2901, lr_0 = 1.0898e-04
Loss = 2.0504e-05, PNorm = 63.6857, GNorm = 0.2707, lr_0 = 1.0850e-04
Loss = 2.1238e-05, PNorm = 63.6878, GNorm = 0.2125, lr_0 = 1.0802e-04
Validation rmse logD = 0.594481
Validation R2 logD = 0.727463
Epoch 95
Train function
Loss = 1.3151e-05, PNorm = 63.6890, GNorm = 0.1528, lr_0 = 1.0749e-04
Loss = 1.1603e-05, PNorm = 63.6899, GNorm = 0.0521, lr_0 = 1.0702e-04
Loss = 1.4753e-05, PNorm = 63.6902, GNorm = 0.1080, lr_0 = 1.0654e-04
Loss = 1.5873e-05, PNorm = 63.6915, GNorm = 0.1926, lr_0 = 1.0607e-04
Loss = 1.8841e-05, PNorm = 63.6931, GNorm = 0.1524, lr_0 = 1.0560e-04
Validation rmse logD = 0.592817
Validation R2 logD = 0.728987
Epoch 96
Train function
Loss = 1.3996e-05, PNorm = 63.6951, GNorm = 0.0825, lr_0 = 1.0509e-04
Loss = 1.3333e-05, PNorm = 63.6958, GNorm = 0.0671, lr_0 = 1.0463e-04
Loss = 1.0201e-05, PNorm = 63.6962, GNorm = 0.1426, lr_0 = 1.0416e-04
Loss = 1.2986e-05, PNorm = 63.6967, GNorm = 0.1271, lr_0 = 1.0370e-04
Loss = 1.2685e-05, PNorm = 63.6976, GNorm = 0.1010, lr_0 = 1.0324e-04
Loss = 1.0437e-05, PNorm = 63.6990, GNorm = 0.0432, lr_0 = 1.0279e-04
Loss = 1.2598e-05, PNorm = 63.6992, GNorm = 0.1012, lr_0 = 1.0274e-04
Validation rmse logD = 0.592311
Validation R2 logD = 0.729450
Epoch 97
Train function
Loss = 7.7532e-06, PNorm = 63.7001, GNorm = 0.0437, lr_0 = 1.0229e-04
Loss = 1.0634e-05, PNorm = 63.7009, GNorm = 0.1062, lr_0 = 1.0183e-04
Loss = 1.1749e-05, PNorm = 63.7021, GNorm = 0.0967, lr_0 = 1.0138e-04
Loss = 1.4650e-05, PNorm = 63.7037, GNorm = 0.1455, lr_0 = 1.0094e-04
Loss = 8.7144e-06, PNorm = 63.7040, GNorm = 0.0506, lr_0 = 1.0049e-04
Validation rmse logD = 0.593491
Validation R2 logD = 0.728370
Epoch 98
Train function
Loss = 6.1994e-06, PNorm = 63.7045, GNorm = 0.0365, lr_0 = 1.0000e-04
Loss = 6.7276e-06, PNorm = 63.7052, GNorm = 0.0894, lr_0 = 1.0000e-04
Loss = 1.1598e-05, PNorm = 63.7067, GNorm = 0.2842, lr_0 = 1.0000e-04
Loss = 1.1546e-05, PNorm = 63.7080, GNorm = 0.0707, lr_0 = 1.0000e-04
Loss = 1.0288e-05, PNorm = 63.7084, GNorm = 0.0372, lr_0 = 1.0000e-04
Validation rmse logD = 0.591495
Validation R2 logD = 0.730194
Epoch 99
Train function
Loss = 8.1961e-06, PNorm = 63.7094, GNorm = 0.1882, lr_0 = 1.0000e-04
Loss = 9.5616e-06, PNorm = 63.7106, GNorm = 0.2072, lr_0 = 1.0000e-04
Loss = 1.4766e-05, PNorm = 63.7115, GNorm = 0.2095, lr_0 = 1.0000e-04
Loss = 9.8155e-06, PNorm = 63.7126, GNorm = 0.0508, lr_0 = 1.0000e-04
Loss = 9.2770e-06, PNorm = 63.7137, GNorm = 0.1068, lr_0 = 1.0000e-04
Loss = 1.3182e-05, PNorm = 63.7146, GNorm = 0.2981, lr_0 = 1.0000e-04
Validation rmse logD = 0.592154
Validation R2 logD = 0.729593
Model 0 best validation rmse = 0.582175 on epoch 36
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logD = 0.559066
Model 0 test R2 logD = 0.783831
Ensemble test rmse  logD= 0.559066
Ensemble test R2  logD= 0.783831
Fold 1
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_312/folds/fold_1',
 'save_smiles_splits': False,
 'seed': 1,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD'],
 'task_names': ['logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 2655,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 1
Total size = 3,541 | train size = 2,656 | val size = 885 | test size = 625
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=895, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,222,401
Moving model to cuda
Epoch 0
Train function
Loss = 2.0395e-02, PNorm = 52.4950, GNorm = 12.1558, lr_0 = 1.9340e-04
Loss = 1.9330e-02, PNorm = 52.5025, GNorm = 5.1380, lr_0 = 2.7830e-04
Loss = 1.8232e-02, PNorm = 52.5151, GNorm = 3.5321, lr_0 = 3.6321e-04
Loss = 1.6600e-02, PNorm = 52.5358, GNorm = 4.4418, lr_0 = 4.4811e-04
Loss = 1.5472e-02, PNorm = 52.5591, GNorm = 2.4529, lr_0 = 5.3302e-04
Validation rmse logD = 1.092263
Validation R2 logD = 0.212752
Epoch 1
Train function
Loss = 1.7850e-02, PNorm = 52.6006, GNorm = 2.0124, lr_0 = 6.2642e-04
Loss = 1.5792e-02, PNorm = 52.6501, GNorm = 1.3563, lr_0 = 7.1132e-04
Loss = 1.2295e-02, PNorm = 52.7130, GNorm = 1.6831, lr_0 = 7.9623e-04
Loss = 1.3838e-02, PNorm = 52.7861, GNorm = 6.0194, lr_0 = 8.8113e-04
Loss = 1.6905e-02, PNorm = 52.8690, GNorm = 5.0282, lr_0 = 9.6604e-04
Validation rmse logD = 1.029653
Validation R2 logD = 0.300417
Epoch 2
Train function
Loss = 1.4122e-02, PNorm = 52.9894, GNorm = 1.1943, lr_0 = 9.9690e-04
Loss = 1.1476e-02, PNorm = 53.1076, GNorm = 0.9392, lr_0 = 9.9249e-04
Loss = 1.1941e-02, PNorm = 53.2346, GNorm = 1.6962, lr_0 = 9.8810e-04
Loss = 1.1062e-02, PNorm = 53.3580, GNorm = 1.4594, lr_0 = 9.8373e-04
Loss = 1.2700e-02, PNorm = 53.5079, GNorm = 1.8479, lr_0 = 9.7938e-04
Validation rmse logD = 0.909314
Validation R2 logD = 0.454387
Epoch 3
Train function
Loss = 9.9146e-03, PNorm = 53.6470, GNorm = 1.7359, lr_0 = 9.7462e-04
Loss = 1.0653e-02, PNorm = 53.7389, GNorm = 3.1085, lr_0 = 9.7030e-04
Loss = 1.0881e-02, PNorm = 53.8395, GNorm = 5.1671, lr_0 = 9.6601e-04
Loss = 9.0520e-03, PNorm = 53.9438, GNorm = 1.6329, lr_0 = 9.6174e-04
Loss = 9.1481e-03, PNorm = 54.0562, GNorm = 2.8637, lr_0 = 9.5749e-04
Loss = 9.8269e-03, PNorm = 54.1582, GNorm = 1.0301, lr_0 = 9.5325e-04
Validation rmse logD = 0.875887
Validation R2 logD = 0.493764
Epoch 4
Train function
Loss = 8.8049e-03, PNorm = 54.2561, GNorm = 1.4810, lr_0 = 9.4861e-04
Loss = 7.5424e-03, PNorm = 54.3425, GNorm = 3.8844, lr_0 = 9.4442e-04
Loss = 7.4386e-03, PNorm = 54.4533, GNorm = 1.3958, lr_0 = 9.4024e-04
Loss = 7.2021e-03, PNorm = 54.5502, GNorm = 1.3491, lr_0 = 9.3608e-04
Loss = 7.4424e-03, PNorm = 54.6370, GNorm = 1.4961, lr_0 = 9.3194e-04
Validation rmse logD = 0.794683
Validation R2 logD = 0.583279
Epoch 5
Train function
Loss = 8.2022e-03, PNorm = 54.7429, GNorm = 1.0227, lr_0 = 9.2741e-04
Loss = 7.9218e-03, PNorm = 54.8567, GNorm = 1.7008, lr_0 = 9.2330e-04
Loss = 7.7050e-03, PNorm = 54.9474, GNorm = 1.4266, lr_0 = 9.1922e-04
Loss = 5.9384e-03, PNorm = 55.0530, GNorm = 1.5689, lr_0 = 9.1515e-04
Loss = 7.0634e-03, PNorm = 55.1294, GNorm = 2.0116, lr_0 = 9.1111e-04
Validation rmse logD = 0.746518
Validation R2 logD = 0.632263
Epoch 6
Train function
Loss = 7.1513e-03, PNorm = 55.2374, GNorm = 2.8675, lr_0 = 9.0667e-04
Loss = 6.6711e-03, PNorm = 55.3427, GNorm = 2.3802, lr_0 = 9.0266e-04
Loss = 6.3889e-03, PNorm = 55.4565, GNorm = 1.5516, lr_0 = 8.9867e-04
Loss = 4.7938e-03, PNorm = 55.5654, GNorm = 0.7902, lr_0 = 8.9469e-04
Loss = 5.5047e-03, PNorm = 55.6576, GNorm = 0.9020, lr_0 = 8.9074e-04
Loss = 6.7368e-03, PNorm = 55.7245, GNorm = 2.2228, lr_0 = 8.8680e-04
Validation rmse logD = 0.691856
Validation R2 logD = 0.684144
Epoch 7
Train function
Loss = 4.7109e-03, PNorm = 55.8201, GNorm = 1.0142, lr_0 = 8.8248e-04
Loss = 4.7555e-03, PNorm = 55.9271, GNorm = 0.8559, lr_0 = 8.7858e-04
Loss = 5.3370e-03, PNorm = 56.0264, GNorm = 0.6102, lr_0 = 8.7469e-04
Loss = 5.1451e-03, PNorm = 56.1259, GNorm = 1.1582, lr_0 = 8.7082e-04
Loss = 5.0047e-03, PNorm = 56.2132, GNorm = 2.7378, lr_0 = 8.6697e-04
Validation rmse logD = 0.702327
Validation R2 logD = 0.674511
Epoch 8
Train function
Loss = 4.8718e-03, PNorm = 56.3104, GNorm = 2.0796, lr_0 = 8.6276e-04
Loss = 4.2145e-03, PNorm = 56.3982, GNorm = 0.8435, lr_0 = 8.5894e-04
Loss = 4.2057e-03, PNorm = 56.4693, GNorm = 1.4038, lr_0 = 8.5514e-04
Loss = 4.5134e-03, PNorm = 56.5385, GNorm = 1.2528, lr_0 = 8.5136e-04
Loss = 5.0827e-03, PNorm = 56.6267, GNorm = 0.7538, lr_0 = 8.4759e-04
Validation rmse logD = 0.670331
Validation R2 logD = 0.703492
Epoch 9
Train function
Loss = 2.6487e-03, PNorm = 56.7063, GNorm = 1.0017, lr_0 = 8.4384e-04
Loss = 3.5659e-03, PNorm = 56.7797, GNorm = 1.1571, lr_0 = 8.4011e-04
Loss = 3.3337e-03, PNorm = 56.8742, GNorm = 0.6266, lr_0 = 8.3639e-04
Loss = 3.7140e-03, PNorm = 56.9541, GNorm = 0.7688, lr_0 = 8.3269e-04
Loss = 4.2327e-03, PNorm = 57.0217, GNorm = 1.3109, lr_0 = 8.2901e-04
Loss = 4.0563e-03, PNorm = 57.0924, GNorm = 1.8868, lr_0 = 8.2534e-04
Validation rmse logD = 0.658231
Validation R2 logD = 0.714100
Epoch 10
Train function
Loss = 3.8832e-03, PNorm = 57.1860, GNorm = 1.0130, lr_0 = 8.2133e-04
Loss = 3.3473e-03, PNorm = 57.2782, GNorm = 0.7051, lr_0 = 8.1770e-04
Loss = 3.6075e-03, PNorm = 57.3581, GNorm = 0.9831, lr_0 = 8.1408e-04
Loss = 2.7727e-03, PNorm = 57.4257, GNorm = 0.5761, lr_0 = 8.1048e-04
Loss = 3.0509e-03, PNorm = 57.4945, GNorm = 1.7711, lr_0 = 8.0689e-04
Validation rmse logD = 0.676900
Validation R2 logD = 0.697652
Epoch 11
Train function
Loss = 4.9328e-03, PNorm = 57.5955, GNorm = 1.7309, lr_0 = 8.0297e-04
Loss = 3.3215e-03, PNorm = 57.7101, GNorm = 1.3007, lr_0 = 7.9942e-04
Loss = 3.2792e-03, PNorm = 57.8114, GNorm = 0.8885, lr_0 = 7.9588e-04
Loss = 3.9603e-03, PNorm = 57.9182, GNorm = 0.9118, lr_0 = 7.9236e-04
Loss = 3.0776e-03, PNorm = 58.0152, GNorm = 1.6484, lr_0 = 7.8885e-04
Validation rmse logD = 0.632572
Validation R2 logD = 0.735955
Epoch 12
Train function
Loss = 2.4996e-03, PNorm = 58.0974, GNorm = 1.2375, lr_0 = 7.8502e-04
Loss = 3.3816e-03, PNorm = 58.1670, GNorm = 1.3430, lr_0 = 7.8154e-04
Loss = 2.7138e-03, PNorm = 58.2257, GNorm = 2.1882, lr_0 = 7.7809e-04
Loss = 2.8894e-03, PNorm = 58.2906, GNorm = 1.1393, lr_0 = 7.7465e-04
Loss = 2.6966e-03, PNorm = 58.3569, GNorm = 0.7898, lr_0 = 7.7122e-04
Loss = 2.7117e-03, PNorm = 58.4224, GNorm = 0.9612, lr_0 = 7.6781e-04
Loss = 2.3694e-02, PNorm = 58.4294, GNorm = 2.8885, lr_0 = 7.6747e-04
Validation rmse logD = 0.634698
Validation R2 logD = 0.734178
Epoch 13
Train function
Loss = 2.5446e-03, PNorm = 58.4903, GNorm = 0.5844, lr_0 = 7.6407e-04
Loss = 2.2933e-03, PNorm = 58.5643, GNorm = 1.1661, lr_0 = 7.6069e-04
Loss = 2.3585e-03, PNorm = 58.6230, GNorm = 1.0205, lr_0 = 7.5733e-04
Loss = 2.4288e-03, PNorm = 58.6884, GNorm = 0.6991, lr_0 = 7.5398e-04
Loss = 2.2721e-03, PNorm = 58.7487, GNorm = 0.6365, lr_0 = 7.5064e-04
Validation rmse logD = 0.643112
Validation R2 logD = 0.727083
Epoch 14
Train function
Loss = 3.1549e-03, PNorm = 58.8232, GNorm = 1.6475, lr_0 = 7.4699e-04
Loss = 2.7260e-03, PNorm = 58.8999, GNorm = 2.1721, lr_0 = 7.4369e-04
Loss = 2.5783e-03, PNorm = 58.9543, GNorm = 2.3153, lr_0 = 7.4040e-04
Loss = 2.3334e-03, PNorm = 59.0198, GNorm = 0.8159, lr_0 = 7.3712e-04
Loss = 2.1873e-03, PNorm = 59.0827, GNorm = 0.8495, lr_0 = 7.3386e-04
Validation rmse logD = 0.625425
Validation R2 logD = 0.741888
Epoch 15
Train function
Loss = 2.1202e-03, PNorm = 59.1466, GNorm = 1.7073, lr_0 = 7.3029e-04
Loss = 1.9223e-03, PNorm = 59.2044, GNorm = 0.7515, lr_0 = 7.2706e-04
Loss = 1.5912e-03, PNorm = 59.2519, GNorm = 1.5331, lr_0 = 7.2385e-04
Loss = 2.0176e-03, PNorm = 59.3110, GNorm = 1.2275, lr_0 = 7.2064e-04
Loss = 1.9044e-03, PNorm = 59.3577, GNorm = 0.9741, lr_0 = 7.1746e-04
Validation rmse logD = 0.592356
Validation R2 logD = 0.768462
Epoch 16
Train function
Loss = 6.5285e-04, PNorm = 59.4200, GNorm = 0.5353, lr_0 = 7.1397e-04
Loss = 1.5428e-03, PNorm = 59.4744, GNorm = 1.4689, lr_0 = 7.1081e-04
Loss = 1.6124e-03, PNorm = 59.5257, GNorm = 1.3002, lr_0 = 7.0766e-04
Loss = 2.0967e-03, PNorm = 59.5773, GNorm = 1.4460, lr_0 = 7.0453e-04
Loss = 1.9570e-03, PNorm = 59.6161, GNorm = 1.4000, lr_0 = 7.0142e-04
Loss = 2.1634e-03, PNorm = 59.6653, GNorm = 0.5576, lr_0 = 6.9831e-04
Validation rmse logD = 0.616213
Validation R2 logD = 0.749435
Epoch 17
Train function
Loss = 1.5697e-03, PNorm = 59.7304, GNorm = 0.7401, lr_0 = 6.9523e-04
Loss = 1.3955e-03, PNorm = 59.7723, GNorm = 0.6204, lr_0 = 6.9215e-04
Loss = 1.4290e-03, PNorm = 59.8095, GNorm = 0.7173, lr_0 = 6.8909e-04
Loss = 1.3757e-03, PNorm = 59.8589, GNorm = 0.7276, lr_0 = 6.8604e-04
Loss = 1.6080e-03, PNorm = 59.8978, GNorm = 1.2077, lr_0 = 6.8301e-04
Validation rmse logD = 0.623884
Validation R2 logD = 0.743158
Epoch 18
Train function
Loss = 1.4303e-03, PNorm = 59.9415, GNorm = 0.7629, lr_0 = 6.7968e-04
Loss = 1.2468e-03, PNorm = 59.9887, GNorm = 0.5031, lr_0 = 6.7668e-04
Loss = 1.0685e-03, PNorm = 60.0264, GNorm = 0.4726, lr_0 = 6.7368e-04
Loss = 1.3095e-03, PNorm = 60.0667, GNorm = 0.6089, lr_0 = 6.7070e-04
Loss = 1.1700e-03, PNorm = 60.1070, GNorm = 0.5607, lr_0 = 6.6774e-04
Validation rmse logD = 0.592609
Validation R2 logD = 0.768264
Epoch 19
Train function
Loss = 6.8897e-04, PNorm = 60.1486, GNorm = 0.4211, lr_0 = 6.6449e-04
Loss = 1.1098e-03, PNorm = 60.1826, GNorm = 0.6720, lr_0 = 6.6155e-04
Loss = 1.1375e-03, PNorm = 60.2255, GNorm = 0.9615, lr_0 = 6.5862e-04
Loss = 1.0941e-03, PNorm = 60.2628, GNorm = 0.9113, lr_0 = 6.5571e-04
Loss = 1.1646e-03, PNorm = 60.2961, GNorm = 0.6532, lr_0 = 6.5281e-04
Loss = 1.1405e-03, PNorm = 60.3287, GNorm = 0.4848, lr_0 = 6.4992e-04
Validation rmse logD = 0.586777
Validation R2 logD = 0.772803
Epoch 20
Train function
Loss = 9.5609e-04, PNorm = 60.3645, GNorm = 1.2946, lr_0 = 6.4676e-04
Loss = 1.0827e-03, PNorm = 60.3967, GNorm = 1.7118, lr_0 = 6.4390e-04
Loss = 9.2833e-04, PNorm = 60.4316, GNorm = 1.1052, lr_0 = 6.4105e-04
Loss = 9.3082e-04, PNorm = 60.4689, GNorm = 0.5566, lr_0 = 6.3822e-04
Loss = 1.1142e-03, PNorm = 60.5071, GNorm = 1.9755, lr_0 = 6.3539e-04
Validation rmse logD = 0.613126
Validation R2 logD = 0.751940
Epoch 21
Train function
Loss = 8.9105e-04, PNorm = 60.5415, GNorm = 1.1327, lr_0 = 6.3230e-04
Loss = 1.0258e-03, PNorm = 60.5773, GNorm = 1.0628, lr_0 = 6.2950e-04
Loss = 8.7840e-04, PNorm = 60.6124, GNorm = 0.5891, lr_0 = 6.2672e-04
Loss = 8.6426e-04, PNorm = 60.6438, GNorm = 0.9238, lr_0 = 6.2395e-04
Loss = 7.2985e-04, PNorm = 60.6730, GNorm = 0.4607, lr_0 = 6.2119e-04
Validation rmse logD = 0.645243
Validation R2 logD = 0.725272
Epoch 22
Train function
Loss = 1.5256e-03, PNorm = 60.7012, GNorm = 2.7988, lr_0 = 6.1817e-04
Loss = 1.0015e-03, PNorm = 60.7383, GNorm = 2.1525, lr_0 = 6.1543e-04
Loss = 7.7184e-04, PNorm = 60.7710, GNorm = 0.4763, lr_0 = 6.1271e-04
Loss = 8.0486e-04, PNorm = 60.8015, GNorm = 0.7606, lr_0 = 6.1000e-04
Loss = 8.2331e-04, PNorm = 60.8239, GNorm = 0.7859, lr_0 = 6.0730e-04
Loss = 8.4310e-04, PNorm = 60.8573, GNorm = 1.3721, lr_0 = 6.0461e-04
Validation rmse logD = 0.615376
Validation R2 logD = 0.750116
Epoch 23
Train function
Loss = 7.6755e-04, PNorm = 60.8834, GNorm = 0.8530, lr_0 = 6.0167e-04
Loss = 7.0116e-04, PNorm = 60.9145, GNorm = 0.5046, lr_0 = 5.9901e-04
Loss = 8.9937e-04, PNorm = 60.9375, GNorm = 0.6343, lr_0 = 5.9636e-04
Loss = 7.5795e-04, PNorm = 60.9676, GNorm = 0.9124, lr_0 = 5.9372e-04
Loss = 7.4881e-04, PNorm = 60.9967, GNorm = 0.5699, lr_0 = 5.9110e-04
Validation rmse logD = 0.608475
Validation R2 logD = 0.755689
Epoch 24
Train function
Loss = 5.5032e-04, PNorm = 61.0185, GNorm = 0.4887, lr_0 = 5.8822e-04
Loss = 5.6841e-04, PNorm = 61.0422, GNorm = 0.6375, lr_0 = 5.8562e-04
Loss = 5.2058e-04, PNorm = 61.0588, GNorm = 0.5010, lr_0 = 5.8303e-04
Loss = 6.8688e-04, PNorm = 61.0755, GNorm = 0.8973, lr_0 = 5.8045e-04
Loss = 6.7325e-04, PNorm = 61.0973, GNorm = 0.3217, lr_0 = 5.7788e-04
Validation rmse logD = 0.612718
Validation R2 logD = 0.752270
Epoch 25
Train function
Loss = 9.4338e-04, PNorm = 61.1311, GNorm = 2.0093, lr_0 = 5.7533e-04
Loss = 5.9385e-04, PNorm = 61.1512, GNorm = 0.7769, lr_0 = 5.7278e-04
Loss = 5.9119e-04, PNorm = 61.1750, GNorm = 1.4968, lr_0 = 5.7025e-04
Loss = 5.7857e-04, PNorm = 61.1984, GNorm = 1.1049, lr_0 = 5.6773e-04
Loss = 5.8292e-04, PNorm = 61.2219, GNorm = 0.5646, lr_0 = 5.6522e-04
Loss = 6.5603e-04, PNorm = 61.2408, GNorm = 0.3531, lr_0 = 5.6272e-04
Validation rmse logD = 0.596038
Validation R2 logD = 0.765574
Epoch 26
Train function
Loss = 5.9207e-04, PNorm = 61.2616, GNorm = 0.7106, lr_0 = 5.5998e-04
Loss = 5.9283e-04, PNorm = 61.2853, GNorm = 0.6712, lr_0 = 5.5750e-04
Loss = 5.2534e-04, PNorm = 61.3095, GNorm = 0.8317, lr_0 = 5.5503e-04
Loss = 5.1552e-04, PNorm = 61.3289, GNorm = 1.3734, lr_0 = 5.5258e-04
Loss = 5.3769e-04, PNorm = 61.3532, GNorm = 0.4468, lr_0 = 5.5014e-04
Validation rmse logD = 0.598867
Validation R2 logD = 0.763344
Epoch 27
Train function
Loss = 5.1915e-04, PNorm = 61.3783, GNorm = 0.6738, lr_0 = 5.4746e-04
Loss = 6.1654e-04, PNorm = 61.4027, GNorm = 0.6658, lr_0 = 5.4504e-04
Loss = 5.0444e-04, PNorm = 61.4226, GNorm = 0.6534, lr_0 = 5.4263e-04
Loss = 4.6875e-04, PNorm = 61.4456, GNorm = 0.3753, lr_0 = 5.4023e-04
Loss = 6.4326e-04, PNorm = 61.4624, GNorm = 1.1123, lr_0 = 5.3784e-04
Validation rmse logD = 0.610495
Validation R2 logD = 0.754065
Epoch 28
Train function
Loss = 6.1954e-04, PNorm = 61.4951, GNorm = 0.7679, lr_0 = 5.3522e-04
Loss = 4.8756e-04, PNorm = 61.5185, GNorm = 0.4199, lr_0 = 5.3285e-04
Loss = 3.9507e-04, PNorm = 61.5386, GNorm = 0.6468, lr_0 = 5.3050e-04
Loss = 4.5638e-04, PNorm = 61.5519, GNorm = 0.4195, lr_0 = 5.2815e-04
Loss = 4.4489e-04, PNorm = 61.5703, GNorm = 0.3707, lr_0 = 5.2581e-04
Loss = 4.4845e-04, PNorm = 61.5795, GNorm = 0.3634, lr_0 = 5.2349e-04
Loss = 3.8770e-03, PNorm = 61.5799, GNorm = 1.7276, lr_0 = 5.2326e-04
Validation rmse logD = 0.592490
Validation R2 logD = 0.768357
Epoch 29
Train function
Loss = 4.0451e-04, PNorm = 61.5967, GNorm = 0.3749, lr_0 = 5.2094e-04
Loss = 3.7747e-04, PNorm = 61.6115, GNorm = 0.6500, lr_0 = 5.1864e-04
Loss = 4.3862e-04, PNorm = 61.6338, GNorm = 0.7830, lr_0 = 5.1634e-04
Loss = 4.4654e-04, PNorm = 61.6577, GNorm = 0.3911, lr_0 = 5.1406e-04
Loss = 4.6439e-04, PNorm = 61.6747, GNorm = 0.3022, lr_0 = 5.1178e-04
Validation rmse logD = 0.605333
Validation R2 logD = 0.758206
Epoch 30
Train function
Loss = 3.2210e-04, PNorm = 61.6956, GNorm = 0.7554, lr_0 = 5.0930e-04
Loss = 4.2329e-04, PNorm = 61.7144, GNorm = 0.4430, lr_0 = 5.0704e-04
Loss = 4.4278e-04, PNorm = 61.7357, GNorm = 0.9765, lr_0 = 5.0480e-04
Loss = 4.2549e-04, PNorm = 61.7537, GNorm = 0.9473, lr_0 = 5.0257e-04
Loss = 4.7101e-04, PNorm = 61.7707, GNorm = 0.8501, lr_0 = 5.0034e-04
Validation rmse logD = 0.615169
Validation R2 logD = 0.750284
Epoch 31
Train function
Loss = 4.5208e-04, PNorm = 61.7914, GNorm = 0.8635, lr_0 = 4.9791e-04
Loss = 3.6662e-04, PNorm = 61.8054, GNorm = 0.7484, lr_0 = 4.9571e-04
Loss = 3.8800e-04, PNorm = 61.8209, GNorm = 0.6151, lr_0 = 4.9351e-04
Loss = 3.2764e-04, PNorm = 61.8390, GNorm = 0.6066, lr_0 = 4.9133e-04
Loss = 2.8567e-04, PNorm = 61.8552, GNorm = 0.2664, lr_0 = 4.8916e-04
Validation rmse logD = 0.591731
Validation R2 logD = 0.768950
Epoch 32
Train function
Loss = 1.9057e-04, PNorm = 61.8724, GNorm = 0.3255, lr_0 = 4.8678e-04
Loss = 3.8175e-04, PNorm = 61.8867, GNorm = 0.3478, lr_0 = 4.8463e-04
Loss = 3.7242e-04, PNorm = 61.8983, GNorm = 0.3394, lr_0 = 4.8248e-04
Loss = 4.2744e-04, PNorm = 61.9110, GNorm = 1.3709, lr_0 = 4.8035e-04
Loss = 3.9251e-04, PNorm = 61.9271, GNorm = 0.9484, lr_0 = 4.7822e-04
Loss = 3.3694e-04, PNorm = 61.9472, GNorm = 0.4582, lr_0 = 4.7611e-04
Validation rmse logD = 0.600454
Validation R2 logD = 0.762088
Epoch 33
Train function
Loss = 3.4198e-04, PNorm = 61.9673, GNorm = 0.3583, lr_0 = 4.7379e-04
Loss = 3.3495e-04, PNorm = 61.9786, GNorm = 0.3102, lr_0 = 4.7170e-04
Loss = 3.3029e-04, PNorm = 61.9922, GNorm = 0.3998, lr_0 = 4.6961e-04
Loss = 3.5955e-04, PNorm = 62.0053, GNorm = 0.8528, lr_0 = 4.6753e-04
Loss = 2.6769e-04, PNorm = 62.0182, GNorm = 0.8265, lr_0 = 4.6546e-04
Validation rmse logD = 0.590373
Validation R2 logD = 0.770010
Epoch 34
Train function
Loss = 3.6913e-04, PNorm = 62.0341, GNorm = 1.3380, lr_0 = 4.6341e-04
Loss = 3.0895e-04, PNorm = 62.0481, GNorm = 0.3940, lr_0 = 4.6136e-04
Loss = 2.7621e-04, PNorm = 62.0602, GNorm = 0.2463, lr_0 = 4.5931e-04
Loss = 2.2967e-04, PNorm = 62.0748, GNorm = 0.1861, lr_0 = 4.5728e-04
Loss = 2.9103e-04, PNorm = 62.0852, GNorm = 0.2775, lr_0 = 4.5526e-04
Validation rmse logD = 0.602056
Validation R2 logD = 0.760816
Epoch 35
Train function
Loss = 1.7522e-04, PNorm = 62.0989, GNorm = 0.3138, lr_0 = 4.5305e-04
Loss = 2.6253e-04, PNorm = 62.1103, GNorm = 0.3667, lr_0 = 4.5104e-04
Loss = 2.4768e-04, PNorm = 62.1246, GNorm = 0.4022, lr_0 = 4.4905e-04
Loss = 2.3031e-04, PNorm = 62.1387, GNorm = 0.5985, lr_0 = 4.4706e-04
Loss = 2.4859e-04, PNorm = 62.1482, GNorm = 0.2548, lr_0 = 4.4508e-04
Loss = 2.3772e-04, PNorm = 62.1575, GNorm = 0.2518, lr_0 = 4.4311e-04
Validation rmse logD = 0.591008
Validation R2 logD = 0.769514
Epoch 36
Train function
Loss = 1.9886e-04, PNorm = 62.1715, GNorm = 0.2062, lr_0 = 4.4096e-04
Loss = 2.1749e-04, PNorm = 62.1781, GNorm = 0.1897, lr_0 = 4.3901e-04
Loss = 2.3473e-04, PNorm = 62.1894, GNorm = 0.8761, lr_0 = 4.3707e-04
Loss = 2.6356e-04, PNorm = 62.2033, GNorm = 0.8171, lr_0 = 4.3513e-04
Loss = 2.1441e-04, PNorm = 62.2150, GNorm = 0.3766, lr_0 = 4.3321e-04
Validation rmse logD = 0.596643
Validation R2 logD = 0.765098
Epoch 37
Train function
Loss = 2.5720e-04, PNorm = 62.2322, GNorm = 0.8719, lr_0 = 4.3110e-04
Loss = 1.7655e-04, PNorm = 62.2416, GNorm = 0.3644, lr_0 = 4.2919e-04
Loss = 2.0895e-04, PNorm = 62.2559, GNorm = 0.2469, lr_0 = 4.2729e-04
Loss = 1.9262e-04, PNorm = 62.2649, GNorm = 0.2236, lr_0 = 4.2540e-04
Loss = 2.1316e-04, PNorm = 62.2727, GNorm = 0.2868, lr_0 = 4.2352e-04
Validation rmse logD = 0.596915
Validation R2 logD = 0.764884
Epoch 38
Train function
Loss = 2.4944e-04, PNorm = 62.2859, GNorm = 0.3513, lr_0 = 4.2146e-04
Loss = 2.5653e-04, PNorm = 62.3024, GNorm = 0.6608, lr_0 = 4.1960e-04
Loss = 3.0753e-04, PNorm = 62.3173, GNorm = 0.4619, lr_0 = 4.1774e-04
Loss = 2.0186e-04, PNorm = 62.3279, GNorm = 0.4033, lr_0 = 4.1589e-04
Loss = 1.9571e-04, PNorm = 62.3377, GNorm = 0.2316, lr_0 = 4.1406e-04
Loss = 2.2571e-04, PNorm = 62.3527, GNorm = 0.2722, lr_0 = 4.1222e-04
Validation rmse logD = 0.596332
Validation R2 logD = 0.765343
Epoch 39
Train function
Loss = 2.0040e-04, PNorm = 62.3622, GNorm = 0.2164, lr_0 = 4.1022e-04
Loss = 2.0188e-04, PNorm = 62.3732, GNorm = 0.3110, lr_0 = 4.0840e-04
Loss = 2.3226e-04, PNorm = 62.3823, GNorm = 0.4116, lr_0 = 4.0660e-04
Loss = 1.9098e-04, PNorm = 62.3922, GNorm = 0.2809, lr_0 = 4.0480e-04
Loss = 1.9862e-04, PNorm = 62.4020, GNorm = 0.2216, lr_0 = 4.0301e-04
Validation rmse logD = 0.606467
Validation R2 logD = 0.757299
Epoch 40
Train function
Loss = 2.7535e-04, PNorm = 62.4157, GNorm = 0.5143, lr_0 = 4.0105e-04
Loss = 4.1324e-04, PNorm = 62.4273, GNorm = 0.3223, lr_0 = 3.9927e-04
Loss = 2.5755e-04, PNorm = 62.4390, GNorm = 0.3983, lr_0 = 3.9751e-04
Loss = 2.7889e-04, PNorm = 62.4491, GNorm = 0.3777, lr_0 = 3.9575e-04
Loss = 2.1179e-04, PNorm = 62.4641, GNorm = 0.4358, lr_0 = 3.9400e-04
Validation rmse logD = 0.594269
Validation R2 logD = 0.766963
Epoch 41
Train function
Loss = 1.6805e-04, PNorm = 62.4778, GNorm = 0.2933, lr_0 = 3.9208e-04
Loss = 1.8058e-04, PNorm = 62.4855, GNorm = 0.4324, lr_0 = 3.9035e-04
Loss = 2.1784e-04, PNorm = 62.4961, GNorm = 0.3908, lr_0 = 3.8862e-04
Loss = 2.0743e-04, PNorm = 62.5037, GNorm = 0.3022, lr_0 = 3.8690e-04
Loss = 1.5577e-04, PNorm = 62.5136, GNorm = 0.4133, lr_0 = 3.8519e-04
Loss = 1.7164e-04, PNorm = 62.5179, GNorm = 0.2741, lr_0 = 3.8349e-04
Validation rmse logD = 0.599128
Validation R2 logD = 0.763138
Epoch 42
Train function
Loss = 1.3880e-04, PNorm = 62.5238, GNorm = 0.3665, lr_0 = 3.8179e-04
Loss = 1.3732e-04, PNorm = 62.5339, GNorm = 0.2183, lr_0 = 3.8010e-04
Loss = 1.5743e-04, PNorm = 62.5433, GNorm = 0.3439, lr_0 = 3.7842e-04
Loss = 1.4603e-04, PNorm = 62.5508, GNorm = 0.2810, lr_0 = 3.7675e-04
Loss = 1.3795e-04, PNorm = 62.5581, GNorm = 0.3623, lr_0 = 3.7508e-04
Validation rmse logD = 0.593981
Validation R2 logD = 0.767189
Epoch 43
Train function
Loss = 1.2811e-04, PNorm = 62.5677, GNorm = 0.2468, lr_0 = 3.7326e-04
Loss = 1.1062e-04, PNorm = 62.5743, GNorm = 0.1796, lr_0 = 3.7160e-04
Loss = 1.1047e-04, PNorm = 62.5793, GNorm = 0.2441, lr_0 = 3.6996e-04
Loss = 1.1412e-04, PNorm = 62.5847, GNorm = 0.1934, lr_0 = 3.6832e-04
Loss = 1.1684e-04, PNorm = 62.5915, GNorm = 0.1917, lr_0 = 3.6669e-04
Validation rmse logD = 0.595722
Validation R2 logD = 0.765823
Epoch 44
Train function
Loss = 1.0466e-04, PNorm = 62.5984, GNorm = 0.2195, lr_0 = 3.6491e-04
Loss = 1.1278e-04, PNorm = 62.6028, GNorm = 0.4089, lr_0 = 3.6330e-04
Loss = 1.0495e-04, PNorm = 62.6071, GNorm = 0.1551, lr_0 = 3.6169e-04
Loss = 1.0263e-04, PNorm = 62.6131, GNorm = 0.3300, lr_0 = 3.6009e-04
Loss = 1.5919e-04, PNorm = 62.6230, GNorm = 0.3503, lr_0 = 3.5850e-04
Loss = 1.0874e-04, PNorm = 62.6331, GNorm = 0.2732, lr_0 = 3.5691e-04
Loss = 1.1839e-03, PNorm = 62.6341, GNorm = 0.9870, lr_0 = 3.5675e-04
Validation rmse logD = 0.594987
Validation R2 logD = 0.766400
Epoch 45
Train function
Loss = 1.3182e-04, PNorm = 62.6400, GNorm = 0.3062, lr_0 = 3.5518e-04
Loss = 1.3034e-04, PNorm = 62.6442, GNorm = 0.3579, lr_0 = 3.5360e-04
Loss = 1.0578e-04, PNorm = 62.6504, GNorm = 0.1805, lr_0 = 3.5204e-04
Loss = 1.1435e-04, PNorm = 62.6578, GNorm = 0.4101, lr_0 = 3.5048e-04
Loss = 1.2622e-04, PNorm = 62.6652, GNorm = 0.4887, lr_0 = 3.4893e-04
Validation rmse logD = 0.599619
Validation R2 logD = 0.762749
Epoch 46
Train function
Loss = 1.3002e-04, PNorm = 62.6737, GNorm = 0.1693, lr_0 = 3.4724e-04
Loss = 1.1060e-04, PNorm = 62.6827, GNorm = 0.2811, lr_0 = 3.4570e-04
Loss = 1.1182e-04, PNorm = 62.6896, GNorm = 0.1452, lr_0 = 3.4417e-04
Loss = 1.0895e-04, PNorm = 62.6959, GNorm = 0.3821, lr_0 = 3.4265e-04
Loss = 1.0071e-04, PNorm = 62.7027, GNorm = 0.2146, lr_0 = 3.4113e-04
Validation rmse logD = 0.598804
Validation R2 logD = 0.763393
Epoch 47
Train function
Loss = 7.1810e-05, PNorm = 62.7080, GNorm = 0.2326, lr_0 = 3.3947e-04
Loss = 8.0253e-05, PNorm = 62.7142, GNorm = 0.2676, lr_0 = 3.3797e-04
Loss = 9.5465e-05, PNorm = 62.7211, GNorm = 0.1850, lr_0 = 3.3648e-04
Loss = 9.2310e-05, PNorm = 62.7244, GNorm = 0.3609, lr_0 = 3.3499e-04
Loss = 8.7402e-05, PNorm = 62.7296, GNorm = 0.2953, lr_0 = 3.3351e-04
Validation rmse logD = 0.596264
Validation R2 logD = 0.765396
Epoch 48
Train function
Loss = 6.5357e-05, PNorm = 62.7339, GNorm = 0.1835, lr_0 = 3.3188e-04
Loss = 8.3368e-05, PNorm = 62.7424, GNorm = 0.4728, lr_0 = 3.3042e-04
Loss = 9.1200e-05, PNorm = 62.7498, GNorm = 0.2140, lr_0 = 3.2895e-04
Loss = 9.1728e-05, PNorm = 62.7539, GNorm = 0.6196, lr_0 = 3.2750e-04
Loss = 8.9124e-05, PNorm = 62.7593, GNorm = 0.3324, lr_0 = 3.2605e-04
Loss = 1.0686e-04, PNorm = 62.7639, GNorm = 0.1813, lr_0 = 3.2461e-04
Validation rmse logD = 0.595194
Validation R2 logD = 0.766238
Epoch 49
Train function
Loss = 1.0395e-04, PNorm = 62.7690, GNorm = 0.1831, lr_0 = 3.2303e-04
Loss = 8.8843e-05, PNorm = 62.7725, GNorm = 0.1290, lr_0 = 3.2160e-04
Loss = 8.8573e-05, PNorm = 62.7800, GNorm = 0.1843, lr_0 = 3.2018e-04
Loss = 1.0077e-04, PNorm = 62.7872, GNorm = 0.1522, lr_0 = 3.1876e-04
Loss = 8.6493e-05, PNorm = 62.7931, GNorm = 0.1998, lr_0 = 3.1735e-04
Validation rmse logD = 0.594093
Validation R2 logD = 0.767102
Epoch 50
Train function
Loss = 8.9902e-05, PNorm = 62.7948, GNorm = 0.2788, lr_0 = 3.1595e-04
Loss = 8.4398e-05, PNorm = 62.7999, GNorm = 0.3047, lr_0 = 3.1455e-04
Loss = 7.6281e-05, PNorm = 62.8046, GNorm = 0.3624, lr_0 = 3.1316e-04
Loss = 7.4249e-05, PNorm = 62.8089, GNorm = 0.1186, lr_0 = 3.1177e-04
Loss = 1.0185e-04, PNorm = 62.8162, GNorm = 0.2511, lr_0 = 3.1039e-04
Validation rmse logD = 0.596543
Validation R2 logD = 0.765177
Epoch 51
Train function
Loss = 6.8292e-05, PNorm = 62.8243, GNorm = 0.1943, lr_0 = 3.0888e-04
Loss = 6.6082e-05, PNorm = 62.8315, GNorm = 0.1978, lr_0 = 3.0752e-04
Loss = 7.6314e-05, PNorm = 62.8383, GNorm = 0.2102, lr_0 = 3.0616e-04
Loss = 7.3051e-05, PNorm = 62.8432, GNorm = 0.2237, lr_0 = 3.0480e-04
Loss = 7.6031e-05, PNorm = 62.8472, GNorm = 0.2520, lr_0 = 3.0346e-04
Loss = 8.6140e-05, PNorm = 62.8536, GNorm = 0.2019, lr_0 = 3.0211e-04
Validation rmse logD = 0.600851
Validation R2 logD = 0.761773
Epoch 52
Train function
Loss = 6.2899e-05, PNorm = 62.8590, GNorm = 0.1950, lr_0 = 3.0064e-04
Loss = 8.2369e-05, PNorm = 62.8604, GNorm = 0.2296, lr_0 = 2.9931e-04
Loss = 9.3581e-05, PNorm = 62.8671, GNorm = 0.2150, lr_0 = 2.9799e-04
Loss = 1.0544e-04, PNorm = 62.8725, GNorm = 0.2450, lr_0 = 2.9667e-04
Loss = 8.7082e-05, PNorm = 62.8801, GNorm = 0.3064, lr_0 = 2.9536e-04
Validation rmse logD = 0.599309
Validation R2 logD = 0.762994
Epoch 53
Train function
Loss = 8.5262e-05, PNorm = 62.8864, GNorm = 0.4212, lr_0 = 2.9392e-04
Loss = 7.9571e-05, PNorm = 62.8911, GNorm = 0.2203, lr_0 = 2.9262e-04
Loss = 7.8262e-05, PNorm = 62.8972, GNorm = 0.1757, lr_0 = 2.9133e-04
Loss = 8.7249e-05, PNorm = 62.9026, GNorm = 0.2542, lr_0 = 2.9004e-04
Loss = 7.6149e-05, PNorm = 62.9081, GNorm = 0.2195, lr_0 = 2.8876e-04
Validation rmse logD = 0.600438
Validation R2 logD = 0.762100
Epoch 54
Train function
Loss = 7.9950e-05, PNorm = 62.9121, GNorm = 0.3173, lr_0 = 2.8735e-04
Loss = 6.7080e-05, PNorm = 62.9155, GNorm = 0.1596, lr_0 = 2.8608e-04
Loss = 6.2683e-05, PNorm = 62.9217, GNorm = 0.2208, lr_0 = 2.8482e-04
Loss = 6.6761e-05, PNorm = 62.9253, GNorm = 0.5168, lr_0 = 2.8356e-04
Loss = 8.0487e-05, PNorm = 62.9313, GNorm = 0.2561, lr_0 = 2.8230e-04
Loss = 6.0124e-05, PNorm = 62.9366, GNorm = 0.1973, lr_0 = 2.8105e-04
Validation rmse logD = 0.600728
Validation R2 logD = 0.761871
Epoch 55
Train function
Loss = 4.8474e-05, PNorm = 62.9404, GNorm = 0.1382, lr_0 = 2.7969e-04
Loss = 5.5357e-05, PNorm = 62.9445, GNorm = 0.1971, lr_0 = 2.7845e-04
Loss = 4.2673e-05, PNorm = 62.9467, GNorm = 0.1494, lr_0 = 2.7722e-04
Loss = 7.0331e-05, PNorm = 62.9507, GNorm = 0.1511, lr_0 = 2.7599e-04
Loss = 6.3234e-05, PNorm = 62.9556, GNorm = 0.1536, lr_0 = 2.7477e-04
Validation rmse logD = 0.598224
Validation R2 logD = 0.763852
Epoch 56
Train function
Loss = 5.5441e-05, PNorm = 62.9624, GNorm = 0.4274, lr_0 = 2.7343e-04
Loss = 7.2996e-05, PNorm = 62.9674, GNorm = 0.2695, lr_0 = 2.7222e-04
Loss = 5.1240e-05, PNorm = 62.9706, GNorm = 0.3684, lr_0 = 2.7102e-04
Loss = 7.4470e-05, PNorm = 62.9729, GNorm = 0.1814, lr_0 = 2.6982e-04
Loss = 5.8825e-05, PNorm = 62.9781, GNorm = 0.1487, lr_0 = 2.6863e-04
Validation rmse logD = 0.600664
Validation R2 logD = 0.761921
Epoch 57
Train function
Loss = 6.8029e-05, PNorm = 62.9843, GNorm = 0.1810, lr_0 = 2.6732e-04
Loss = 5.0488e-05, PNorm = 62.9891, GNorm = 0.1574, lr_0 = 2.6614e-04
Loss = 4.7214e-05, PNorm = 62.9918, GNorm = 0.1082, lr_0 = 2.6496e-04
Loss = 4.5268e-05, PNorm = 62.9940, GNorm = 0.2102, lr_0 = 2.6379e-04
Loss = 5.8143e-05, PNorm = 62.9979, GNorm = 0.1992, lr_0 = 2.6262e-04
Loss = 4.8409e-05, PNorm = 63.0017, GNorm = 0.1083, lr_0 = 2.6146e-04
Loss = 1.8383e-03, PNorm = 63.0023, GNorm = 0.9037, lr_0 = 2.6134e-04
Validation rmse logD = 0.601992
Validation R2 logD = 0.760868
Epoch 58
Train function
Loss = 9.1621e-05, PNorm = 63.0075, GNorm = 0.3544, lr_0 = 2.6019e-04
Loss = 1.0907e-04, PNorm = 63.0124, GNorm = 0.1879, lr_0 = 2.5904e-04
Loss = 9.5750e-05, PNorm = 63.0190, GNorm = 0.1553, lr_0 = 2.5789e-04
Loss = 9.8387e-05, PNorm = 63.0262, GNorm = 0.2271, lr_0 = 2.5675e-04
Loss = 9.3431e-05, PNorm = 63.0315, GNorm = 0.4270, lr_0 = 2.5561e-04
Validation rmse logD = 0.605771
Validation R2 logD = 0.757856
Epoch 59
Train function
Loss = 9.6383e-05, PNorm = 63.0359, GNorm = 0.3468, lr_0 = 2.5448e-04
Loss = 8.6493e-05, PNorm = 63.0390, GNorm = 0.2476, lr_0 = 2.5336e-04
Loss = 9.5653e-05, PNorm = 63.0412, GNorm = 0.6919, lr_0 = 2.5224e-04
Loss = 8.0648e-05, PNorm = 63.0459, GNorm = 0.1414, lr_0 = 2.5112e-04
Loss = 7.4589e-05, PNorm = 63.0515, GNorm = 0.2758, lr_0 = 2.5001e-04
Validation rmse logD = 0.598026
Validation R2 logD = 0.764008
Epoch 60
Train function
Loss = 5.9927e-05, PNorm = 63.0568, GNorm = 0.1798, lr_0 = 2.4879e-04
Loss = 5.2018e-05, PNorm = 63.0614, GNorm = 0.2599, lr_0 = 2.4769e-04
Loss = 8.2896e-05, PNorm = 63.0664, GNorm = 0.3613, lr_0 = 2.4660e-04
Loss = 5.6596e-05, PNorm = 63.0714, GNorm = 0.1361, lr_0 = 2.4551e-04
Loss = 6.0984e-05, PNorm = 63.0754, GNorm = 0.3044, lr_0 = 2.4442e-04
Loss = 6.4144e-05, PNorm = 63.0798, GNorm = 0.1270, lr_0 = 2.4334e-04
Loss = 2.9567e-04, PNorm = 63.0801, GNorm = 0.3612, lr_0 = 2.4323e-04
Validation rmse logD = 0.596978
Validation R2 logD = 0.764834
Epoch 61
Train function
Loss = 4.2392e-05, PNorm = 63.0827, GNorm = 0.2486, lr_0 = 2.4216e-04
Loss = 5.0514e-05, PNorm = 63.0862, GNorm = 0.1695, lr_0 = 2.4109e-04
Loss = 5.0771e-05, PNorm = 63.0895, GNorm = 0.1753, lr_0 = 2.4002e-04
Loss = 4.5775e-05, PNorm = 63.0919, GNorm = 0.2982, lr_0 = 2.3896e-04
Loss = 5.0804e-05, PNorm = 63.0942, GNorm = 0.1266, lr_0 = 2.3790e-04
Validation rmse logD = 0.598811
Validation R2 logD = 0.763388
Epoch 62
Train function
Loss = 4.2863e-05, PNorm = 63.0960, GNorm = 0.1769, lr_0 = 2.3674e-04
Loss = 7.9515e-05, PNorm = 63.1004, GNorm = 0.3296, lr_0 = 2.3570e-04
Loss = 6.3479e-05, PNorm = 63.1043, GNorm = 0.1715, lr_0 = 2.3465e-04
Loss = 4.8182e-05, PNorm = 63.1070, GNorm = 0.1699, lr_0 = 2.3362e-04
Loss = 4.3230e-05, PNorm = 63.1111, GNorm = 0.1348, lr_0 = 2.3258e-04
Validation rmse logD = 0.600595
Validation R2 logD = 0.761976
Epoch 63
Train function
Loss = 4.0899e-05, PNorm = 63.1165, GNorm = 0.2535, lr_0 = 2.3145e-04
Loss = 6.1202e-05, PNorm = 63.1212, GNorm = 0.2898, lr_0 = 2.3043e-04
Loss = 6.3719e-05, PNorm = 63.1238, GNorm = 0.4248, lr_0 = 2.2941e-04
Loss = 3.9129e-05, PNorm = 63.1256, GNorm = 0.2373, lr_0 = 2.2839e-04
Loss = 4.5515e-05, PNorm = 63.1282, GNorm = 0.3294, lr_0 = 2.2738e-04
Validation rmse logD = 0.601345
Validation R2 logD = 0.761381
Epoch 64
Train function
Loss = 3.7479e-05, PNorm = 63.1325, GNorm = 0.1142, lr_0 = 2.2628e-04
Loss = 3.3176e-05, PNorm = 63.1344, GNorm = 0.1393, lr_0 = 2.2528e-04
Loss = 3.2790e-05, PNorm = 63.1358, GNorm = 0.2067, lr_0 = 2.2428e-04
Loss = 3.6560e-05, PNorm = 63.1369, GNorm = 0.3857, lr_0 = 2.2329e-04
Loss = 3.9614e-05, PNorm = 63.1400, GNorm = 0.1229, lr_0 = 2.2230e-04
Loss = 5.1894e-05, PNorm = 63.1437, GNorm = 0.1896, lr_0 = 2.2132e-04
Validation rmse logD = 0.600409
Validation R2 logD = 0.762123
Epoch 65
Train function
Loss = 3.3735e-05, PNorm = 63.1475, GNorm = 0.1131, lr_0 = 2.2024e-04
Loss = 4.2784e-05, PNorm = 63.1497, GNorm = 0.1513, lr_0 = 2.1927e-04
Loss = 4.4176e-05, PNorm = 63.1516, GNorm = 0.1703, lr_0 = 2.1830e-04
Loss = 3.9300e-05, PNorm = 63.1550, GNorm = 0.2558, lr_0 = 2.1733e-04
Loss = 3.9666e-05, PNorm = 63.1583, GNorm = 0.2503, lr_0 = 2.1637e-04
Validation rmse logD = 0.601688
Validation R2 logD = 0.761109
Epoch 66
Train function
Loss = 3.6947e-05, PNorm = 63.1610, GNorm = 0.4070, lr_0 = 2.1532e-04
Loss = 3.0258e-05, PNorm = 63.1641, GNorm = 0.1259, lr_0 = 2.1436e-04
Loss = 3.5028e-05, PNorm = 63.1667, GNorm = 0.1263, lr_0 = 2.1342e-04
Loss = 3.0273e-05, PNorm = 63.1692, GNorm = 0.2033, lr_0 = 2.1247e-04
Loss = 3.9256e-05, PNorm = 63.1716, GNorm = 0.2474, lr_0 = 2.1153e-04
Validation rmse logD = 0.599403
Validation R2 logD = 0.762920
Epoch 67
Train function
Loss = 1.6643e-05, PNorm = 63.1751, GNorm = 0.1019, lr_0 = 2.1060e-04
Loss = 3.5334e-05, PNorm = 63.1772, GNorm = 0.1062, lr_0 = 2.0966e-04
Loss = 3.6649e-05, PNorm = 63.1775, GNorm = 0.1222, lr_0 = 2.0874e-04
Loss = 2.7566e-05, PNorm = 63.1811, GNorm = 0.1275, lr_0 = 2.0781e-04
Loss = 3.0227e-05, PNorm = 63.1826, GNorm = 0.1717, lr_0 = 2.0689e-04
Loss = 2.9486e-05, PNorm = 63.1855, GNorm = 0.1024, lr_0 = 2.0598e-04
Validation rmse logD = 0.600173
Validation R2 logD = 0.762311
Epoch 68
Train function
Loss = 3.3641e-05, PNorm = 63.1864, GNorm = 0.3280, lr_0 = 2.0498e-04
Loss = 3.3128e-05, PNorm = 63.1880, GNorm = 0.1160, lr_0 = 2.0407e-04
Loss = 3.4714e-05, PNorm = 63.1915, GNorm = 0.0872, lr_0 = 2.0317e-04
Loss = 5.0119e-05, PNorm = 63.1945, GNorm = 0.0811, lr_0 = 2.0227e-04
Loss = 2.6344e-05, PNorm = 63.1985, GNorm = 0.1568, lr_0 = 2.0137e-04
Validation rmse logD = 0.599181
Validation R2 logD = 0.763095
Epoch 69
Train function
Loss = 2.3578e-05, PNorm = 63.2011, GNorm = 0.1291, lr_0 = 2.0039e-04
Loss = 4.1013e-05, PNorm = 63.2037, GNorm = 0.2673, lr_0 = 1.9951e-04
Loss = 3.2189e-05, PNorm = 63.2063, GNorm = 0.0721, lr_0 = 1.9863e-04
Loss = 2.6109e-05, PNorm = 63.2086, GNorm = 0.1758, lr_0 = 1.9775e-04
Loss = 3.8466e-05, PNorm = 63.2112, GNorm = 0.0716, lr_0 = 1.9687e-04
Validation rmse logD = 0.599317
Validation R2 logD = 0.762988
Epoch 70
Train function
Loss = 3.4607e-05, PNorm = 63.2139, GNorm = 0.1637, lr_0 = 1.9592e-04
Loss = 3.8432e-05, PNorm = 63.2171, GNorm = 0.5012, lr_0 = 1.9505e-04
Loss = 3.8166e-05, PNorm = 63.2209, GNorm = 0.3591, lr_0 = 1.9419e-04
Loss = 4.1189e-05, PNorm = 63.2237, GNorm = 0.2965, lr_0 = 1.9333e-04
Loss = 3.8424e-05, PNorm = 63.2260, GNorm = 0.1865, lr_0 = 1.9247e-04
Loss = 2.9790e-05, PNorm = 63.2285, GNorm = 0.1025, lr_0 = 1.9162e-04
Validation rmse logD = 0.602533
Validation R2 logD = 0.760438
Epoch 71
Train function
Loss = 3.3705e-05, PNorm = 63.2274, GNorm = 0.1350, lr_0 = 1.9069e-04
Loss = 3.7308e-05, PNorm = 63.2298, GNorm = 0.3179, lr_0 = 1.8984e-04
Loss = 2.5624e-05, PNorm = 63.2344, GNorm = 0.1127, lr_0 = 1.8900e-04
Loss = 7.9447e-05, PNorm = 63.2371, GNorm = 0.1273, lr_0 = 1.8817e-04
Loss = 3.1987e-05, PNorm = 63.2410, GNorm = 0.0965, lr_0 = 1.8734e-04
Validation rmse logD = 0.601754
Validation R2 logD = 0.761056
Epoch 72
Train function
Loss = 3.1578e-05, PNorm = 63.2431, GNorm = 0.1207, lr_0 = 1.8643e-04
Loss = 2.9524e-05, PNorm = 63.2458, GNorm = 0.1911, lr_0 = 1.8560e-04
Loss = 2.2655e-05, PNorm = 63.2474, GNorm = 0.1261, lr_0 = 1.8478e-04
Loss = 2.6937e-05, PNorm = 63.2488, GNorm = 0.1150, lr_0 = 1.8396e-04
Loss = 2.5757e-05, PNorm = 63.2510, GNorm = 0.2160, lr_0 = 1.8315e-04
Validation rmse logD = 0.603016
Validation R2 logD = 0.760053
Epoch 73
Train function
Loss = 3.0348e-05, PNorm = 63.2537, GNorm = 0.1602, lr_0 = 1.8226e-04
Loss = 3.2851e-05, PNorm = 63.2565, GNorm = 0.1186, lr_0 = 1.8145e-04
Loss = 2.9257e-05, PNorm = 63.2581, GNorm = 0.1760, lr_0 = 1.8065e-04
Loss = 2.9348e-05, PNorm = 63.2600, GNorm = 0.1629, lr_0 = 1.7985e-04
Loss = 2.7835e-05, PNorm = 63.2620, GNorm = 0.1100, lr_0 = 1.7905e-04
Loss = 3.6176e-05, PNorm = 63.2641, GNorm = 0.1581, lr_0 = 1.7826e-04
Loss = 4.9201e-05, PNorm = 63.2644, GNorm = 0.1645, lr_0 = 1.7818e-04
Validation rmse logD = 0.602315
Validation R2 logD = 0.760611
Epoch 74
Train function
Loss = 2.9072e-05, PNorm = 63.2665, GNorm = 0.0930, lr_0 = 1.7739e-04
Loss = 2.9187e-05, PNorm = 63.2684, GNorm = 0.1300, lr_0 = 1.7661e-04
Loss = 1.9736e-05, PNorm = 63.2696, GNorm = 0.0752, lr_0 = 1.7583e-04
Loss = 2.5030e-05, PNorm = 63.2712, GNorm = 0.2443, lr_0 = 1.7505e-04
Loss = 2.9936e-05, PNorm = 63.2740, GNorm = 0.1839, lr_0 = 1.7428e-04
Validation rmse logD = 0.604840
Validation R2 logD = 0.758599
Epoch 75
Train function
Loss = 3.1261e-05, PNorm = 63.2746, GNorm = 0.1660, lr_0 = 1.7351e-04
Loss = 2.3870e-05, PNorm = 63.2753, GNorm = 0.2089, lr_0 = 1.7274e-04
Loss = 2.7303e-05, PNorm = 63.2775, GNorm = 0.0873, lr_0 = 1.7197e-04
Loss = 2.5607e-05, PNorm = 63.2804, GNorm = 0.1753, lr_0 = 1.7121e-04
Loss = 1.7999e-05, PNorm = 63.2827, GNorm = 0.1175, lr_0 = 1.7046e-04
Validation rmse logD = 0.602994
Validation R2 logD = 0.760071
Epoch 76
Train function
Loss = 2.4366e-05, PNorm = 63.2850, GNorm = 0.1418, lr_0 = 1.6963e-04
Loss = 1.6310e-05, PNorm = 63.2866, GNorm = 0.1298, lr_0 = 1.6888e-04
Loss = 1.7098e-05, PNorm = 63.2880, GNorm = 0.0971, lr_0 = 1.6813e-04
Loss = 1.6852e-05, PNorm = 63.2895, GNorm = 0.0870, lr_0 = 1.6739e-04
Loss = 1.7780e-05, PNorm = 63.2907, GNorm = 0.0865, lr_0 = 1.6665e-04
Loss = 2.1947e-05, PNorm = 63.2928, GNorm = 0.2072, lr_0 = 1.6591e-04
Loss = 6.5820e-05, PNorm = 63.2928, GNorm = 0.2102, lr_0 = 1.6584e-04
Validation rmse logD = 0.603170
Validation R2 logD = 0.759930
Epoch 77
Train function
Loss = 1.4259e-05, PNorm = 63.2934, GNorm = 0.0673, lr_0 = 1.6510e-04
Loss = 1.5420e-05, PNorm = 63.2954, GNorm = 0.0679, lr_0 = 1.6437e-04
Loss = 1.9891e-05, PNorm = 63.2969, GNorm = 0.1438, lr_0 = 1.6364e-04
Loss = 1.4458e-05, PNorm = 63.2979, GNorm = 0.0763, lr_0 = 1.6292e-04
Loss = 1.6690e-05, PNorm = 63.2992, GNorm = 0.2132, lr_0 = 1.6220e-04
Validation rmse logD = 0.601630
Validation R2 logD = 0.761155
Epoch 78
Train function
Loss = 1.8966e-05, PNorm = 63.3008, GNorm = 0.0515, lr_0 = 1.6141e-04
Loss = 1.4676e-05, PNorm = 63.3019, GNorm = 0.0613, lr_0 = 1.6070e-04
Loss = 1.5298e-05, PNorm = 63.3038, GNorm = 0.0894, lr_0 = 1.5999e-04
Loss = 1.2992e-05, PNorm = 63.3058, GNorm = 0.0715, lr_0 = 1.5928e-04
Loss = 2.0184e-05, PNorm = 63.3071, GNorm = 0.1276, lr_0 = 1.5857e-04
Validation rmse logD = 0.602162
Validation R2 logD = 0.760732
Epoch 79
Train function
Loss = 1.5956e-05, PNorm = 63.3078, GNorm = 0.0564, lr_0 = 1.5780e-04
Loss = 1.4712e-05, PNorm = 63.3084, GNorm = 0.0793, lr_0 = 1.5710e-04
Loss = 1.6157e-05, PNorm = 63.3096, GNorm = 0.1175, lr_0 = 1.5641e-04
Loss = 1.5278e-05, PNorm = 63.3109, GNorm = 0.0566, lr_0 = 1.5572e-04
Loss = 1.4843e-05, PNorm = 63.3114, GNorm = 0.0810, lr_0 = 1.5503e-04
Validation rmse logD = 0.601580
Validation R2 logD = 0.761195
Epoch 80
Train function
Loss = 1.5790e-05, PNorm = 63.3127, GNorm = 0.1161, lr_0 = 1.5427e-04
Loss = 1.2712e-05, PNorm = 63.3146, GNorm = 0.1455, lr_0 = 1.5359e-04
Loss = 1.4173e-05, PNorm = 63.3171, GNorm = 0.1168, lr_0 = 1.5291e-04
Loss = 1.4406e-05, PNorm = 63.3179, GNorm = 0.1840, lr_0 = 1.5224e-04
Loss = 1.7351e-05, PNorm = 63.3186, GNorm = 0.2034, lr_0 = 1.5156e-04
Loss = 1.6043e-05, PNorm = 63.3198, GNorm = 0.1980, lr_0 = 1.5089e-04
Validation rmse logD = 0.601620
Validation R2 logD = 0.761163
Epoch 81
Train function
Loss = 1.6777e-05, PNorm = 63.3224, GNorm = 0.1394, lr_0 = 1.5016e-04
Loss = 1.7906e-05, PNorm = 63.3234, GNorm = 0.2446, lr_0 = 1.4949e-04
Loss = 1.8780e-05, PNorm = 63.3258, GNorm = 0.1105, lr_0 = 1.4883e-04
Loss = 1.9454e-05, PNorm = 63.3279, GNorm = 0.1146, lr_0 = 1.4817e-04
Loss = 1.1367e-05, PNorm = 63.3288, GNorm = 0.0589, lr_0 = 1.4752e-04
Validation rmse logD = 0.601820
Validation R2 logD = 0.761004
Epoch 82
Train function
Loss = 1.7433e-05, PNorm = 63.3305, GNorm = 0.1440, lr_0 = 1.4680e-04
Loss = 1.8950e-05, PNorm = 63.3318, GNorm = 0.1270, lr_0 = 1.4615e-04
Loss = 1.4034e-05, PNorm = 63.3326, GNorm = 0.0661, lr_0 = 1.4551e-04
Loss = 1.7285e-05, PNorm = 63.3343, GNorm = 0.0926, lr_0 = 1.4486e-04
Loss = 1.6248e-05, PNorm = 63.3352, GNorm = 0.1557, lr_0 = 1.4422e-04
Validation rmse logD = 0.599486
Validation R2 logD = 0.762854
Epoch 83
Train function
Loss = 1.2532e-05, PNorm = 63.3366, GNorm = 0.1394, lr_0 = 1.4352e-04
Loss = 1.2481e-05, PNorm = 63.3380, GNorm = 0.1324, lr_0 = 1.4288e-04
Loss = 1.5168e-05, PNorm = 63.3390, GNorm = 0.2093, lr_0 = 1.4225e-04
Loss = 1.3028e-05, PNorm = 63.3400, GNorm = 0.0664, lr_0 = 1.4162e-04
Loss = 1.0815e-05, PNorm = 63.3423, GNorm = 0.1728, lr_0 = 1.4100e-04
Loss = 1.4788e-05, PNorm = 63.3434, GNorm = 0.2129, lr_0 = 1.4037e-04
Validation rmse logD = 0.602320
Validation R2 logD = 0.760607
Epoch 84
Train function
Loss = 1.7393e-05, PNorm = 63.3440, GNorm = 0.1914, lr_0 = 1.3975e-04
Loss = 1.2927e-05, PNorm = 63.3449, GNorm = 0.0710, lr_0 = 1.3913e-04
Loss = 1.2098e-05, PNorm = 63.3462, GNorm = 0.0687, lr_0 = 1.3852e-04
Loss = 1.6100e-05, PNorm = 63.3474, GNorm = 0.1343, lr_0 = 1.3791e-04
Loss = 1.2265e-05, PNorm = 63.3483, GNorm = 0.0922, lr_0 = 1.3730e-04
Validation rmse logD = 0.602979
Validation R2 logD = 0.760083
Epoch 85
Train function
Loss = 1.3152e-05, PNorm = 63.3500, GNorm = 0.0666, lr_0 = 1.3663e-04
Loss = 1.3688e-05, PNorm = 63.3505, GNorm = 0.1411, lr_0 = 1.3602e-04
Loss = 1.3531e-05, PNorm = 63.3513, GNorm = 0.0612, lr_0 = 1.3542e-04
Loss = 1.4639e-05, PNorm = 63.3527, GNorm = 0.0766, lr_0 = 1.3482e-04
Loss = 1.4068e-05, PNorm = 63.3545, GNorm = 0.0928, lr_0 = 1.3423e-04
Validation rmse logD = 0.602646
Validation R2 logD = 0.760347
Epoch 86
Train function
Loss = 8.1118e-06, PNorm = 63.3546, GNorm = 0.1192, lr_0 = 1.3357e-04
Loss = 1.4608e-05, PNorm = 63.3560, GNorm = 0.0712, lr_0 = 1.3298e-04
Loss = 1.2189e-05, PNorm = 63.3575, GNorm = 0.1203, lr_0 = 1.3239e-04
Loss = 1.2503e-05, PNorm = 63.3590, GNorm = 0.1083, lr_0 = 1.3181e-04
Loss = 1.1989e-05, PNorm = 63.3606, GNorm = 0.1398, lr_0 = 1.3123e-04
Loss = 1.2380e-05, PNorm = 63.3614, GNorm = 0.1197, lr_0 = 1.3065e-04
Validation rmse logD = 0.603386
Validation R2 logD = 0.759758
Epoch 87
Train function
Loss = 1.2381e-05, PNorm = 63.3623, GNorm = 0.0623, lr_0 = 1.3001e-04
Loss = 9.7333e-06, PNorm = 63.3639, GNorm = 0.0769, lr_0 = 1.2944e-04
Loss = 1.2977e-05, PNorm = 63.3643, GNorm = 0.0758, lr_0 = 1.2886e-04
Loss = 1.1386e-05, PNorm = 63.3652, GNorm = 0.0687, lr_0 = 1.2829e-04
Loss = 1.1016e-05, PNorm = 63.3664, GNorm = 0.0610, lr_0 = 1.2773e-04
Validation rmse logD = 0.603657
Validation R2 logD = 0.759542
Epoch 88
Train function
Loss = 1.5318e-05, PNorm = 63.3683, GNorm = 0.1029, lr_0 = 1.2710e-04
Loss = 1.0192e-05, PNorm = 63.3692, GNorm = 0.0711, lr_0 = 1.2654e-04
Loss = 1.1422e-05, PNorm = 63.3705, GNorm = 0.0859, lr_0 = 1.2598e-04
Loss = 9.8351e-06, PNorm = 63.3717, GNorm = 0.0639, lr_0 = 1.2542e-04
Loss = 9.5358e-06, PNorm = 63.3730, GNorm = 0.0962, lr_0 = 1.2487e-04
Validation rmse logD = 0.605533
Validation R2 logD = 0.758046
Epoch 89
Train function
Loss = 2.3790e-05, PNorm = 63.3749, GNorm = 0.1045, lr_0 = 1.2426e-04
Loss = 2.1222e-05, PNorm = 63.3750, GNorm = 0.0723, lr_0 = 1.2371e-04
Loss = 1.7978e-05, PNorm = 63.3753, GNorm = 0.1173, lr_0 = 1.2317e-04
Loss = 1.2586e-05, PNorm = 63.3767, GNorm = 0.0864, lr_0 = 1.2262e-04
Loss = 1.1119e-05, PNorm = 63.3780, GNorm = 0.0557, lr_0 = 1.2208e-04
Loss = 1.1403e-05, PNorm = 63.3789, GNorm = 0.0472, lr_0 = 1.2154e-04
Loss = 5.0248e-05, PNorm = 63.3789, GNorm = 0.1882, lr_0 = 1.2148e-04
Validation rmse logD = 0.603115
Validation R2 logD = 0.759974
Epoch 90
Train function
Loss = 8.7679e-06, PNorm = 63.3800, GNorm = 0.0725, lr_0 = 1.2095e-04
Loss = 8.7278e-06, PNorm = 63.3816, GNorm = 0.1821, lr_0 = 1.2041e-04
Loss = 1.3175e-05, PNorm = 63.3831, GNorm = 0.0793, lr_0 = 1.1988e-04
Loss = 1.2210e-05, PNorm = 63.3841, GNorm = 0.0533, lr_0 = 1.1935e-04
Loss = 1.6360e-05, PNorm = 63.3849, GNorm = 0.2088, lr_0 = 1.1882e-04
Validation rmse logD = 0.602817
Validation R2 logD = 0.760211
Epoch 91
Train function
Loss = 1.1372e-05, PNorm = 63.3863, GNorm = 0.1764, lr_0 = 1.1824e-04
Loss = 9.8283e-06, PNorm = 63.3875, GNorm = 0.0906, lr_0 = 1.1772e-04
Loss = 1.3707e-05, PNorm = 63.3877, GNorm = 0.1055, lr_0 = 1.1720e-04
Loss = 1.4985e-05, PNorm = 63.3887, GNorm = 0.1155, lr_0 = 1.1668e-04
Loss = 1.5193e-05, PNorm = 63.3900, GNorm = 0.2590, lr_0 = 1.1616e-04
Validation rmse logD = 0.602884
Validation R2 logD = 0.760159
Epoch 92
Train function
Loss = 1.2818e-05, PNorm = 63.3912, GNorm = 0.1213, lr_0 = 1.1565e-04
Loss = 1.4869e-05, PNorm = 63.3922, GNorm = 0.0659, lr_0 = 1.1514e-04
Loss = 1.4577e-05, PNorm = 63.3937, GNorm = 0.1942, lr_0 = 1.1463e-04
Loss = 1.2302e-05, PNorm = 63.3946, GNorm = 0.0811, lr_0 = 1.1412e-04
Loss = 1.0608e-05, PNorm = 63.3953, GNorm = 0.0697, lr_0 = 1.1362e-04
Loss = 1.0259e-05, PNorm = 63.3962, GNorm = 0.0672, lr_0 = 1.1312e-04
Loss = 8.4149e-05, PNorm = 63.3963, GNorm = 0.2242, lr_0 = 1.1307e-04
Validation rmse logD = 0.602537
Validation R2 logD = 0.760434
Epoch 93
Train function
Loss = 9.2023e-06, PNorm = 63.3976, GNorm = 0.0817, lr_0 = 1.1257e-04
Loss = 9.1510e-06, PNorm = 63.3979, GNorm = 0.0465, lr_0 = 1.1207e-04
Loss = 1.0442e-05, PNorm = 63.3985, GNorm = 0.1538, lr_0 = 1.1157e-04
Loss = 9.8238e-06, PNorm = 63.3992, GNorm = 0.0936, lr_0 = 1.1108e-04
Loss = 9.1768e-06, PNorm = 63.4004, GNorm = 0.1073, lr_0 = 1.1059e-04
Validation rmse logD = 0.604090
Validation R2 logD = 0.759197
Epoch 94
Train function
Loss = 8.7330e-06, PNorm = 63.4015, GNorm = 0.0469, lr_0 = 1.1005e-04
Loss = 8.1478e-06, PNorm = 63.4018, GNorm = 0.0843, lr_0 = 1.0956e-04
Loss = 7.4522e-06, PNorm = 63.4025, GNorm = 0.0756, lr_0 = 1.0908e-04
Loss = 7.8103e-06, PNorm = 63.4037, GNorm = 0.0900, lr_0 = 1.0860e-04
Loss = 9.0960e-06, PNorm = 63.4045, GNorm = 0.1407, lr_0 = 1.0811e-04
Validation rmse logD = 0.602926
Validation R2 logD = 0.760125
Epoch 95
Train function
Loss = 9.8287e-06, PNorm = 63.4054, GNorm = 0.1483, lr_0 = 1.0759e-04
Loss = 7.2471e-06, PNorm = 63.4061, GNorm = 0.0785, lr_0 = 1.0711e-04
Loss = 6.5756e-06, PNorm = 63.4065, GNorm = 0.0648, lr_0 = 1.0664e-04
Loss = 7.9944e-06, PNorm = 63.4076, GNorm = 0.0696, lr_0 = 1.0617e-04
Loss = 1.1859e-05, PNorm = 63.4084, GNorm = 0.0887, lr_0 = 1.0570e-04
Validation rmse logD = 0.604700
Validation R2 logD = 0.758711
Epoch 96
Train function
Loss = 3.9066e-06, PNorm = 63.4096, GNorm = 0.0836, lr_0 = 1.0518e-04
Loss = 9.6949e-06, PNorm = 63.4111, GNorm = 0.1983, lr_0 = 1.0472e-04
Loss = 9.9606e-06, PNorm = 63.4119, GNorm = 0.2287, lr_0 = 1.0426e-04
Loss = 1.0859e-05, PNorm = 63.4124, GNorm = 0.2151, lr_0 = 1.0379e-04
Loss = 1.0218e-05, PNorm = 63.4133, GNorm = 0.0443, lr_0 = 1.0333e-04
Loss = 1.0642e-05, PNorm = 63.4145, GNorm = 0.0844, lr_0 = 1.0288e-04
Validation rmse logD = 0.602792
Validation R2 logD = 0.760231
Epoch 97
Train function
Loss = 1.4608e-05, PNorm = 63.4145, GNorm = 0.1642, lr_0 = 1.0238e-04
Loss = 1.3428e-05, PNorm = 63.4155, GNorm = 0.1796, lr_0 = 1.0192e-04
Loss = 1.0191e-05, PNorm = 63.4158, GNorm = 0.0649, lr_0 = 1.0147e-04
Loss = 1.2254e-05, PNorm = 63.4172, GNorm = 0.0692, lr_0 = 1.0102e-04
Loss = 1.0550e-05, PNorm = 63.4181, GNorm = 0.0769, lr_0 = 1.0058e-04
Validation rmse logD = 0.603535
Validation R2 logD = 0.759640
Epoch 98
Train function
Loss = 8.6930e-06, PNorm = 63.4190, GNorm = 0.1809, lr_0 = 1.0009e-04
Loss = 1.0181e-05, PNorm = 63.4197, GNorm = 0.0876, lr_0 = 1.0000e-04
Loss = 8.6223e-06, PNorm = 63.4211, GNorm = 0.1038, lr_0 = 1.0000e-04
Loss = 7.0775e-06, PNorm = 63.4220, GNorm = 0.0606, lr_0 = 1.0000e-04
Loss = 7.3244e-06, PNorm = 63.4227, GNorm = 0.0739, lr_0 = 1.0000e-04
Validation rmse logD = 0.602820
Validation R2 logD = 0.760209
Epoch 99
Train function
Loss = 3.7463e-06, PNorm = 63.4228, GNorm = 0.0374, lr_0 = 1.0000e-04
Loss = 6.5450e-06, PNorm = 63.4231, GNorm = 0.2022, lr_0 = 1.0000e-04
Loss = 6.1544e-06, PNorm = 63.4242, GNorm = 0.0486, lr_0 = 1.0000e-04
Loss = 9.1777e-06, PNorm = 63.4244, GNorm = 0.1772, lr_0 = 1.0000e-04
Loss = 1.0506e-05, PNorm = 63.4248, GNorm = 0.1229, lr_0 = 1.0000e-04
Loss = 1.0215e-05, PNorm = 63.4263, GNorm = 0.0648, lr_0 = 1.0000e-04
Validation rmse logD = 0.603269
Validation R2 logD = 0.759852
Model 0 best validation rmse = 0.586777 on epoch 19
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logD = 0.593638
Model 0 test R2 logD = 0.756269
Ensemble test rmse  logD= 0.593638
Ensemble test R2  logD= 0.756269
Fold 2
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_312/folds/fold_2',
 'save_smiles_splits': False,
 'seed': 2,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD'],
 'task_names': ['logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 2656,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 2
Total size = 3,541 | train size = 2,656 | val size = 885 | test size = 625
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=895, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,222,401
Moving model to cuda
Epoch 0
Train function
Loss = 2.1591e-02, PNorm = 52.4974, GNorm = 2.7580, lr_0 = 1.9340e-04
Loss = 1.8867e-02, PNorm = 52.5061, GNorm = 6.8252, lr_0 = 2.7830e-04
Loss = 1.6475e-02, PNorm = 52.5186, GNorm = 2.3313, lr_0 = 3.6321e-04
Loss = 1.5848e-02, PNorm = 52.5337, GNorm = 2.1656, lr_0 = 4.4811e-04
Loss = 1.5387e-02, PNorm = 52.5542, GNorm = 4.2543, lr_0 = 5.3302e-04
Validation rmse logD = 1.065486
Validation R2 logD = 0.237920
Epoch 1
Train function
Loss = 1.6477e-02, PNorm = 52.5916, GNorm = 6.4673, lr_0 = 6.2642e-04
Loss = 1.4299e-02, PNorm = 52.6439, GNorm = 2.5336, lr_0 = 7.1132e-04
Loss = 1.2032e-02, PNorm = 52.7002, GNorm = 4.7558, lr_0 = 7.9623e-04
Loss = 1.3175e-02, PNorm = 52.7692, GNorm = 1.5291, lr_0 = 8.8113e-04
Loss = 1.3551e-02, PNorm = 52.8616, GNorm = 1.7531, lr_0 = 9.6604e-04
Validation rmse logD = 0.927897
Validation R2 logD = 0.422031
Epoch 2
Train function
Loss = 1.2905e-02, PNorm = 52.9743, GNorm = 2.9224, lr_0 = 9.9690e-04
Loss = 1.1799e-02, PNorm = 53.0708, GNorm = 3.5144, lr_0 = 9.9249e-04
Loss = 1.1113e-02, PNorm = 53.1939, GNorm = 4.6735, lr_0 = 9.8810e-04
Loss = 1.1532e-02, PNorm = 53.3070, GNorm = 7.9971, lr_0 = 9.8373e-04
Loss = 1.0662e-02, PNorm = 53.4421, GNorm = 2.5239, lr_0 = 9.7938e-04
Validation rmse logD = 0.891821
Validation R2 logD = 0.466099
Epoch 3
Train function
Loss = 1.1160e-02, PNorm = 53.5220, GNorm = 3.0075, lr_0 = 9.7462e-04
Loss = 8.4381e-03, PNorm = 53.6161, GNorm = 2.3697, lr_0 = 9.7030e-04
Loss = 7.4425e-03, PNorm = 53.7300, GNorm = 1.6807, lr_0 = 9.6601e-04
Loss = 9.2829e-03, PNorm = 53.8372, GNorm = 1.0918, lr_0 = 9.6174e-04
Loss = 9.3407e-03, PNorm = 53.9602, GNorm = 4.7838, lr_0 = 9.5749e-04
Loss = 8.1110e-03, PNorm = 54.0588, GNorm = 2.7311, lr_0 = 9.5325e-04
Validation rmse logD = 0.767442
Validation R2 logD = 0.604637
Epoch 4
Train function
Loss = 6.9569e-03, PNorm = 54.1826, GNorm = 1.1914, lr_0 = 9.4861e-04
Loss = 9.5661e-03, PNorm = 54.2723, GNorm = 1.1255, lr_0 = 9.4442e-04
Loss = 8.5142e-03, PNorm = 54.3542, GNorm = 3.1727, lr_0 = 9.4024e-04
Loss = 8.4967e-03, PNorm = 54.4458, GNorm = 1.0305, lr_0 = 9.3608e-04
Loss = 8.4339e-03, PNorm = 54.5378, GNorm = 1.9053, lr_0 = 9.3194e-04
Validation rmse logD = 0.945770
Validation R2 logD = 0.399552
Epoch 5
Train function
Loss = 6.9015e-03, PNorm = 54.6231, GNorm = 1.9575, lr_0 = 9.2741e-04
Loss = 6.4791e-03, PNorm = 54.7129, GNorm = 1.1544, lr_0 = 9.2330e-04
Loss = 6.1025e-03, PNorm = 54.7907, GNorm = 3.7691, lr_0 = 9.1922e-04
Loss = 6.2858e-03, PNorm = 54.8779, GNorm = 1.3030, lr_0 = 9.1515e-04
Loss = 6.6081e-03, PNorm = 54.9658, GNorm = 2.4461, lr_0 = 9.1111e-04
Validation rmse logD = 0.706740
Validation R2 logD = 0.664708
Epoch 6
Train function
Loss = 4.3246e-03, PNorm = 55.0629, GNorm = 1.0130, lr_0 = 9.0667e-04
Loss = 5.5736e-03, PNorm = 55.1328, GNorm = 1.2186, lr_0 = 9.0266e-04
Loss = 5.0570e-03, PNorm = 55.2104, GNorm = 1.3971, lr_0 = 8.9867e-04
Loss = 5.1613e-03, PNorm = 55.2864, GNorm = 1.4021, lr_0 = 8.9469e-04
Loss = 6.3869e-03, PNorm = 55.3689, GNorm = 3.2956, lr_0 = 8.9074e-04
Loss = 5.4003e-03, PNorm = 55.4427, GNorm = 1.5142, lr_0 = 8.8680e-04
Validation rmse logD = 0.758761
Validation R2 logD = 0.613531
Epoch 7
Train function
Loss = 6.7021e-03, PNorm = 55.5308, GNorm = 5.1365, lr_0 = 8.8248e-04
Loss = 5.0809e-03, PNorm = 55.6313, GNorm = 3.9541, lr_0 = 8.7858e-04
Loss = 4.4580e-03, PNorm = 55.7247, GNorm = 1.1312, lr_0 = 8.7469e-04
Loss = 4.2714e-03, PNorm = 55.7830, GNorm = 1.8213, lr_0 = 8.7082e-04
Loss = 5.0892e-03, PNorm = 55.8493, GNorm = 2.8376, lr_0 = 8.6697e-04
Validation rmse logD = 0.645804
Validation R2 logD = 0.720034
Epoch 8
Train function
Loss = 4.3590e-03, PNorm = 55.9379, GNorm = 0.9181, lr_0 = 8.6276e-04
Loss = 3.6013e-03, PNorm = 56.0165, GNorm = 1.6554, lr_0 = 8.5894e-04
Loss = 4.5016e-03, PNorm = 56.0863, GNorm = 4.7796, lr_0 = 8.5514e-04
Loss = 4.1961e-03, PNorm = 56.1718, GNorm = 1.0189, lr_0 = 8.5136e-04
Loss = 3.9592e-03, PNorm = 56.2360, GNorm = 2.6693, lr_0 = 8.4759e-04
Validation rmse logD = 0.638622
Validation R2 logD = 0.726225
Epoch 9
Train function
Loss = 4.0373e-03, PNorm = 56.2969, GNorm = 3.8010, lr_0 = 8.4384e-04
Loss = 3.5254e-03, PNorm = 56.3476, GNorm = 1.1191, lr_0 = 8.4011e-04
Loss = 3.7194e-03, PNorm = 56.4286, GNorm = 1.8519, lr_0 = 8.3639e-04
Loss = 3.4874e-03, PNorm = 56.4981, GNorm = 1.5484, lr_0 = 8.3269e-04
Loss = 3.5934e-03, PNorm = 56.5735, GNorm = 1.5046, lr_0 = 8.2901e-04
Loss = 4.3398e-03, PNorm = 56.6218, GNorm = 2.1026, lr_0 = 8.2534e-04
Validation rmse logD = 0.670064
Validation R2 logD = 0.698604
Epoch 10
Train function
Loss = 3.8691e-03, PNorm = 56.6912, GNorm = 2.1640, lr_0 = 8.2133e-04
Loss = 3.7124e-03, PNorm = 56.7649, GNorm = 3.5036, lr_0 = 8.1770e-04
Loss = 3.4360e-03, PNorm = 56.8448, GNorm = 2.6128, lr_0 = 8.1408e-04
Loss = 3.1835e-03, PNorm = 56.8959, GNorm = 1.8338, lr_0 = 8.1048e-04
Loss = 2.6980e-03, PNorm = 56.9511, GNorm = 0.9560, lr_0 = 8.0689e-04
Validation rmse logD = 0.694497
Validation R2 logD = 0.676223
Epoch 11
Train function
Loss = 3.6607e-03, PNorm = 57.0392, GNorm = 2.1698, lr_0 = 8.0297e-04
Loss = 2.9777e-03, PNorm = 57.1108, GNorm = 1.5832, lr_0 = 7.9942e-04
Loss = 3.0020e-03, PNorm = 57.1850, GNorm = 1.2279, lr_0 = 7.9588e-04
Loss = 2.7014e-03, PNorm = 57.2465, GNorm = 1.6191, lr_0 = 7.9236e-04
Loss = 2.8280e-03, PNorm = 57.3090, GNorm = 1.3789, lr_0 = 7.8885e-04
Validation rmse logD = 0.629035
Validation R2 logD = 0.734384
Epoch 12
Train function
Loss = 3.0947e-03, PNorm = 57.3637, GNorm = 1.2959, lr_0 = 7.8502e-04
Loss = 2.5791e-03, PNorm = 57.4131, GNorm = 1.1966, lr_0 = 7.8154e-04
Loss = 2.2960e-03, PNorm = 57.4614, GNorm = 1.1772, lr_0 = 7.7809e-04
Loss = 2.3504e-03, PNorm = 57.5125, GNorm = 2.4051, lr_0 = 7.7465e-04
Loss = 2.4035e-03, PNorm = 57.5736, GNorm = 0.6942, lr_0 = 7.7122e-04
Loss = 2.7291e-03, PNorm = 57.6286, GNorm = 1.4767, lr_0 = 7.6781e-04
Loss = 1.7821e-02, PNorm = 57.6350, GNorm = 1.8305, lr_0 = 7.6747e-04
Validation rmse logD = 0.629389
Validation R2 logD = 0.734085
Epoch 13
Train function
Loss = 2.0936e-03, PNorm = 57.6844, GNorm = 0.9366, lr_0 = 7.6407e-04
Loss = 2.6004e-03, PNorm = 57.7316, GNorm = 2.2012, lr_0 = 7.6069e-04
Loss = 2.0755e-03, PNorm = 57.7963, GNorm = 1.0035, lr_0 = 7.5733e-04
Loss = 2.3577e-03, PNorm = 57.8519, GNorm = 1.4513, lr_0 = 7.5398e-04
Loss = 2.1320e-03, PNorm = 57.9225, GNorm = 1.3211, lr_0 = 7.5064e-04
Validation rmse logD = 0.668151
Validation R2 logD = 0.700323
Epoch 14
Train function
Loss = 2.6824e-03, PNorm = 57.9949, GNorm = 1.0423, lr_0 = 7.4699e-04
Loss = 2.1017e-03, PNorm = 58.0367, GNorm = 1.2758, lr_0 = 7.4369e-04
Loss = 2.0831e-03, PNorm = 58.0990, GNorm = 2.3620, lr_0 = 7.4040e-04
Loss = 2.0449e-03, PNorm = 58.1484, GNorm = 0.9829, lr_0 = 7.3712e-04
Loss = 2.0379e-03, PNorm = 58.1919, GNorm = 0.8036, lr_0 = 7.3386e-04
Validation rmse logD = 0.604424
Validation R2 logD = 0.754762
Epoch 15
Train function
Loss = 2.3698e-03, PNorm = 58.2619, GNorm = 1.4801, lr_0 = 7.3029e-04
Loss = 2.0169e-03, PNorm = 58.3171, GNorm = 1.4008, lr_0 = 7.2706e-04
Loss = 1.9360e-03, PNorm = 58.3738, GNorm = 1.0475, lr_0 = 7.2385e-04
Loss = 2.1898e-03, PNorm = 58.4359, GNorm = 1.6259, lr_0 = 7.2064e-04
Loss = 2.0098e-03, PNorm = 58.4952, GNorm = 2.0295, lr_0 = 7.1746e-04
Validation rmse logD = 0.693273
Validation R2 logD = 0.677364
Epoch 16
Train function
Loss = 4.5578e-03, PNorm = 58.5559, GNorm = 5.3392, lr_0 = 7.1397e-04
Loss = 2.9887e-03, PNorm = 58.6045, GNorm = 4.3360, lr_0 = 7.1081e-04
Loss = 2.3397e-03, PNorm = 58.6719, GNorm = 1.9084, lr_0 = 7.0766e-04
Loss = 2.0769e-03, PNorm = 58.7280, GNorm = 1.7550, lr_0 = 7.0453e-04
Loss = 1.8363e-03, PNorm = 58.7848, GNorm = 0.8740, lr_0 = 7.0142e-04
Loss = 1.7457e-03, PNorm = 58.8300, GNorm = 0.7105, lr_0 = 6.9831e-04
Validation rmse logD = 0.618062
Validation R2 logD = 0.743570
Epoch 17
Train function
Loss = 1.4271e-03, PNorm = 58.8753, GNorm = 0.7890, lr_0 = 6.9523e-04
Loss = 1.2457e-03, PNorm = 58.9159, GNorm = 0.4759, lr_0 = 6.9215e-04
Loss = 1.6056e-03, PNorm = 58.9480, GNorm = 1.3858, lr_0 = 6.8909e-04
Loss = 1.5375e-03, PNorm = 58.9777, GNorm = 1.3229, lr_0 = 6.8604e-04
Loss = 1.5211e-03, PNorm = 59.0252, GNorm = 0.7067, lr_0 = 6.8301e-04
Validation rmse logD = 0.597662
Validation R2 logD = 0.760218
Epoch 18
Train function
Loss = 1.1565e-03, PNorm = 59.0617, GNorm = 1.5109, lr_0 = 6.7968e-04
Loss = 1.5837e-03, PNorm = 59.1048, GNorm = 0.7088, lr_0 = 6.7668e-04
Loss = 1.3378e-03, PNorm = 59.1397, GNorm = 1.2986, lr_0 = 6.7368e-04
Loss = 1.1380e-03, PNorm = 59.1718, GNorm = 1.0450, lr_0 = 6.7070e-04
Loss = 1.3179e-03, PNorm = 59.2137, GNorm = 0.5965, lr_0 = 6.6774e-04
Validation rmse logD = 0.624986
Validation R2 logD = 0.737792
Epoch 19
Train function
Loss = 1.4264e-03, PNorm = 59.2517, GNorm = 1.9174, lr_0 = 6.6449e-04
Loss = 1.3886e-03, PNorm = 59.2897, GNorm = 1.5554, lr_0 = 6.6155e-04
Loss = 1.2935e-03, PNorm = 59.3301, GNorm = 0.9044, lr_0 = 6.5862e-04
Loss = 1.1656e-03, PNorm = 59.3633, GNorm = 1.6935, lr_0 = 6.5571e-04
Loss = 1.5372e-03, PNorm = 59.3955, GNorm = 1.6446, lr_0 = 6.5281e-04
Loss = 1.0870e-03, PNorm = 59.4369, GNorm = 0.5595, lr_0 = 6.4992e-04
Validation rmse logD = 0.580224
Validation R2 logD = 0.774006
Epoch 20
Train function
Loss = 1.1217e-03, PNorm = 59.4781, GNorm = 1.3944, lr_0 = 6.4676e-04
Loss = 1.1686e-03, PNorm = 59.5103, GNorm = 1.4429, lr_0 = 6.4390e-04
Loss = 9.4806e-04, PNorm = 59.5435, GNorm = 0.8250, lr_0 = 6.4105e-04
Loss = 9.6635e-04, PNorm = 59.5759, GNorm = 0.9817, lr_0 = 6.3822e-04
Loss = 1.0932e-03, PNorm = 59.6074, GNorm = 0.5426, lr_0 = 6.3539e-04
Validation rmse logD = 0.573655
Validation R2 logD = 0.779094
Epoch 21
Train function
Loss = 7.0495e-04, PNorm = 59.6458, GNorm = 0.4738, lr_0 = 6.3230e-04
Loss = 8.9555e-04, PNorm = 59.6704, GNorm = 0.8604, lr_0 = 6.2950e-04
Loss = 8.9835e-04, PNorm = 59.6850, GNorm = 0.7354, lr_0 = 6.2672e-04
Loss = 9.8725e-04, PNorm = 59.7068, GNorm = 0.6362, lr_0 = 6.2395e-04
Loss = 1.0325e-03, PNorm = 59.7474, GNorm = 0.5769, lr_0 = 6.2119e-04
Validation rmse logD = 0.607735
Validation R2 logD = 0.752068
Epoch 22
Train function
Loss = 1.0587e-03, PNorm = 59.7759, GNorm = 0.5063, lr_0 = 6.1817e-04
Loss = 1.3840e-03, PNorm = 59.8124, GNorm = 1.0563, lr_0 = 6.1543e-04
Loss = 1.3436e-03, PNorm = 59.8550, GNorm = 0.7543, lr_0 = 6.1271e-04
Loss = 1.1088e-03, PNorm = 59.8910, GNorm = 1.0786, lr_0 = 6.1000e-04
Loss = 9.6563e-04, PNorm = 59.9251, GNorm = 2.0868, lr_0 = 6.0730e-04
Loss = 1.0189e-03, PNorm = 59.9598, GNorm = 1.2444, lr_0 = 6.0461e-04
Validation rmse logD = 0.623876
Validation R2 logD = 0.738723
Epoch 23
Train function
Loss = 1.0645e-03, PNorm = 59.9896, GNorm = 1.2288, lr_0 = 6.0167e-04
Loss = 8.5666e-04, PNorm = 60.0276, GNorm = 0.7809, lr_0 = 5.9901e-04
Loss = 6.9938e-04, PNorm = 60.0510, GNorm = 0.7068, lr_0 = 5.9636e-04
Loss = 7.4608e-04, PNorm = 60.0734, GNorm = 1.0349, lr_0 = 5.9372e-04
Loss = 9.3421e-04, PNorm = 60.0978, GNorm = 0.4807, lr_0 = 5.9110e-04
Validation rmse logD = 0.575119
Validation R2 logD = 0.777966
Epoch 24
Train function
Loss = 6.8364e-04, PNorm = 60.1271, GNorm = 0.3547, lr_0 = 5.8822e-04
Loss = 6.7568e-04, PNorm = 60.1520, GNorm = 0.3455, lr_0 = 5.8562e-04
Loss = 5.8032e-04, PNorm = 60.1738, GNorm = 0.9176, lr_0 = 5.8303e-04
Loss = 9.0609e-04, PNorm = 60.1942, GNorm = 0.8638, lr_0 = 5.8045e-04
Loss = 7.6499e-04, PNorm = 60.2076, GNorm = 0.7859, lr_0 = 5.7788e-04
Validation rmse logD = 0.582698
Validation R2 logD = 0.772075
Epoch 25
Train function
Loss = 5.7868e-04, PNorm = 60.2381, GNorm = 0.6205, lr_0 = 5.7533e-04
Loss = 6.5107e-04, PNorm = 60.2626, GNorm = 0.7500, lr_0 = 5.7278e-04
Loss = 5.7839e-04, PNorm = 60.2728, GNorm = 0.5830, lr_0 = 5.7025e-04
Loss = 5.3870e-04, PNorm = 60.2950, GNorm = 0.9504, lr_0 = 5.6773e-04
Loss = 6.5500e-04, PNorm = 60.3109, GNorm = 0.5566, lr_0 = 5.6522e-04
Loss = 6.5116e-04, PNorm = 60.3341, GNorm = 0.7517, lr_0 = 5.6272e-04
Validation rmse logD = 0.578220
Validation R2 logD = 0.775565
Epoch 26
Train function
Loss = 5.5711e-04, PNorm = 60.3652, GNorm = 0.3807, lr_0 = 5.5998e-04
Loss = 6.2545e-04, PNorm = 60.3822, GNorm = 0.4697, lr_0 = 5.5750e-04
Loss = 6.5201e-04, PNorm = 60.3968, GNorm = 0.7229, lr_0 = 5.5503e-04
Loss = 6.8656e-04, PNorm = 60.4170, GNorm = 2.3192, lr_0 = 5.5258e-04
Loss = 7.4722e-04, PNorm = 60.4411, GNorm = 0.4572, lr_0 = 5.5014e-04
Validation rmse logD = 0.580873
Validation R2 logD = 0.773501
Epoch 27
Train function
Loss = 4.9412e-04, PNorm = 60.4727, GNorm = 1.4496, lr_0 = 5.4746e-04
Loss = 6.7768e-04, PNorm = 60.4939, GNorm = 0.4398, lr_0 = 5.4504e-04
Loss = 5.7470e-04, PNorm = 60.5166, GNorm = 0.5433, lr_0 = 5.4263e-04
Loss = 5.4225e-04, PNorm = 60.5415, GNorm = 0.3986, lr_0 = 5.4023e-04
Loss = 5.4109e-04, PNorm = 60.5595, GNorm = 0.6161, lr_0 = 5.3784e-04
Validation rmse logD = 0.571689
Validation R2 logD = 0.780606
Epoch 28
Train function
Loss = 4.6370e-04, PNorm = 60.5804, GNorm = 0.4206, lr_0 = 5.3522e-04
Loss = 4.6594e-04, PNorm = 60.6024, GNorm = 0.6046, lr_0 = 5.3285e-04
Loss = 6.0207e-04, PNorm = 60.6159, GNorm = 0.2946, lr_0 = 5.3050e-04
Loss = 5.1428e-04, PNorm = 60.6349, GNorm = 0.7907, lr_0 = 5.2815e-04
Loss = 5.6454e-04, PNorm = 60.6522, GNorm = 0.6579, lr_0 = 5.2581e-04
Loss = 6.7025e-04, PNorm = 60.6721, GNorm = 1.9649, lr_0 = 5.2349e-04
Loss = 4.0183e-03, PNorm = 60.6731, GNorm = 1.8237, lr_0 = 5.2326e-04
Validation rmse logD = 0.607651
Validation R2 logD = 0.752136
Epoch 29
Train function
Loss = 5.9103e-04, PNorm = 60.6899, GNorm = 0.2945, lr_0 = 5.2094e-04
Loss = 7.1887e-04, PNorm = 60.7193, GNorm = 0.5578, lr_0 = 5.1864e-04
Loss = 5.3061e-04, PNorm = 60.7470, GNorm = 0.7436, lr_0 = 5.1634e-04
Loss = 6.4277e-04, PNorm = 60.7686, GNorm = 1.6807, lr_0 = 5.1406e-04
Loss = 5.9451e-04, PNorm = 60.7935, GNorm = 0.9267, lr_0 = 5.1178e-04
Validation rmse logD = 0.643489
Validation R2 logD = 0.722037
Epoch 30
Train function
Loss = 9.8168e-04, PNorm = 60.8099, GNorm = 0.8133, lr_0 = 5.0930e-04
Loss = 8.4217e-04, PNorm = 60.8439, GNorm = 1.5284, lr_0 = 5.0704e-04
Loss = 6.4802e-04, PNorm = 60.8684, GNorm = 1.0981, lr_0 = 5.0480e-04
Loss = 5.2374e-04, PNorm = 60.8878, GNorm = 1.0103, lr_0 = 5.0257e-04
Loss = 5.0100e-04, PNorm = 60.9075, GNorm = 1.0867, lr_0 = 5.0034e-04
Validation rmse logD = 0.573242
Validation R2 logD = 0.779412
Epoch 31
Train function
Loss = 4.7584e-04, PNorm = 60.9340, GNorm = 0.3183, lr_0 = 4.9791e-04
Loss = 4.5166e-04, PNorm = 60.9494, GNorm = 1.1605, lr_0 = 4.9571e-04
Loss = 4.8495e-04, PNorm = 60.9645, GNorm = 1.1988, lr_0 = 4.9351e-04
Loss = 4.9930e-04, PNorm = 60.9797, GNorm = 1.1422, lr_0 = 4.9133e-04
Loss = 5.2247e-04, PNorm = 60.9927, GNorm = 0.3545, lr_0 = 4.8916e-04
Validation rmse logD = 0.574649
Validation R2 logD = 0.778328
Epoch 32
Train function
Loss = 4.3678e-04, PNorm = 61.0113, GNorm = 0.7045, lr_0 = 4.8678e-04
Loss = 4.9769e-04, PNorm = 61.0343, GNorm = 0.7187, lr_0 = 4.8463e-04
Loss = 3.9848e-04, PNorm = 61.0516, GNorm = 0.4082, lr_0 = 4.8248e-04
Loss = 4.9011e-04, PNorm = 61.0698, GNorm = 0.6360, lr_0 = 4.8035e-04
Loss = 3.4237e-04, PNorm = 61.0851, GNorm = 0.3444, lr_0 = 4.7822e-04
Loss = 3.6265e-04, PNorm = 61.0991, GNorm = 0.3212, lr_0 = 4.7611e-04
Validation rmse logD = 0.583066
Validation R2 logD = 0.771787
Epoch 33
Train function
Loss = 4.1128e-04, PNorm = 61.1118, GNorm = 0.2326, lr_0 = 4.7379e-04
Loss = 3.8469e-04, PNorm = 61.1279, GNorm = 0.3829, lr_0 = 4.7170e-04
Loss = 3.2592e-04, PNorm = 61.1439, GNorm = 0.9388, lr_0 = 4.6961e-04
Loss = 3.4808e-04, PNorm = 61.1576, GNorm = 0.8069, lr_0 = 4.6753e-04
Loss = 3.2451e-04, PNorm = 61.1694, GNorm = 0.5584, lr_0 = 4.6546e-04
Validation rmse logD = 0.575716
Validation R2 logD = 0.777504
Epoch 34
Train function
Loss = 3.9282e-04, PNorm = 61.1777, GNorm = 0.7809, lr_0 = 4.6341e-04
Loss = 3.4967e-04, PNorm = 61.1925, GNorm = 0.8384, lr_0 = 4.6136e-04
Loss = 3.0586e-04, PNorm = 61.2026, GNorm = 0.5142, lr_0 = 4.5931e-04
Loss = 2.8083e-04, PNorm = 61.2114, GNorm = 0.6150, lr_0 = 4.5728e-04
Loss = 3.4273e-04, PNorm = 61.2247, GNorm = 0.8129, lr_0 = 4.5526e-04
Validation rmse logD = 0.576506
Validation R2 logD = 0.776893
Epoch 35
Train function
Loss = 1.6924e-04, PNorm = 61.2412, GNorm = 0.5701, lr_0 = 4.5305e-04
Loss = 3.4423e-04, PNorm = 61.2581, GNorm = 0.4654, lr_0 = 4.5104e-04
Loss = 3.8213e-04, PNorm = 61.2690, GNorm = 0.6332, lr_0 = 4.4905e-04
Loss = 3.6517e-04, PNorm = 61.2835, GNorm = 0.7719, lr_0 = 4.4706e-04
Loss = 3.2184e-04, PNorm = 61.3009, GNorm = 0.3637, lr_0 = 4.4508e-04
Loss = 3.5705e-04, PNorm = 61.3151, GNorm = 0.4261, lr_0 = 4.4311e-04
Validation rmse logD = 0.576490
Validation R2 logD = 0.776906
Epoch 36
Train function
Loss = 2.8684e-04, PNorm = 61.3296, GNorm = 0.7260, lr_0 = 4.4096e-04
Loss = 2.3945e-04, PNorm = 61.3472, GNorm = 0.2898, lr_0 = 4.3901e-04
Loss = 2.5507e-04, PNorm = 61.3628, GNorm = 0.3479, lr_0 = 4.3707e-04
Loss = 2.9585e-04, PNorm = 61.3735, GNorm = 0.2283, lr_0 = 4.3513e-04
Loss = 2.7644e-04, PNorm = 61.3802, GNorm = 0.5106, lr_0 = 4.3321e-04
Validation rmse logD = 0.573545
Validation R2 logD = 0.779179
Epoch 37
Train function
Loss = 2.2760e-04, PNorm = 61.3939, GNorm = 0.5617, lr_0 = 4.3110e-04
Loss = 2.4263e-04, PNorm = 61.4035, GNorm = 0.3245, lr_0 = 4.2919e-04
Loss = 2.6040e-04, PNorm = 61.4118, GNorm = 0.2279, lr_0 = 4.2729e-04
Loss = 2.7129e-04, PNorm = 61.4252, GNorm = 0.3507, lr_0 = 4.2540e-04
Loss = 2.3809e-04, PNorm = 61.4368, GNorm = 0.3315, lr_0 = 4.2352e-04
Validation rmse logD = 0.587254
Validation R2 logD = 0.768497
Epoch 38
Train function
Loss = 2.0383e-04, PNorm = 61.4470, GNorm = 0.3658, lr_0 = 4.2146e-04
Loss = 2.0579e-04, PNorm = 61.4588, GNorm = 0.4146, lr_0 = 4.1960e-04
Loss = 2.7166e-04, PNorm = 61.4712, GNorm = 0.2658, lr_0 = 4.1774e-04
Loss = 2.8537e-04, PNorm = 61.4822, GNorm = 0.6445, lr_0 = 4.1589e-04
Loss = 3.1250e-04, PNorm = 61.4917, GNorm = 1.5277, lr_0 = 4.1406e-04
Loss = 3.9624e-04, PNorm = 61.5057, GNorm = 1.1624, lr_0 = 4.1222e-04
Validation rmse logD = 0.585914
Validation R2 logD = 0.769552
Epoch 39
Train function
Loss = 3.3728e-04, PNorm = 61.5221, GNorm = 0.5899, lr_0 = 4.1022e-04
Loss = 2.4079e-04, PNorm = 61.5308, GNorm = 0.7065, lr_0 = 4.0840e-04
Loss = 2.3629e-04, PNorm = 61.5432, GNorm = 0.3322, lr_0 = 4.0660e-04
Loss = 1.9631e-04, PNorm = 61.5563, GNorm = 0.2657, lr_0 = 4.0480e-04
Loss = 2.5021e-04, PNorm = 61.5643, GNorm = 0.8776, lr_0 = 4.0301e-04
Validation rmse logD = 0.590278
Validation R2 logD = 0.766107
Epoch 40
Train function
Loss = 3.3605e-04, PNorm = 61.5753, GNorm = 0.7052, lr_0 = 4.0105e-04
Loss = 2.2467e-04, PNorm = 61.5893, GNorm = 0.3855, lr_0 = 3.9927e-04
Loss = 2.5159e-04, PNorm = 61.6015, GNorm = 0.8905, lr_0 = 3.9751e-04
Loss = 2.6803e-04, PNorm = 61.6119, GNorm = 0.9206, lr_0 = 3.9575e-04
Loss = 2.3103e-04, PNorm = 61.6220, GNorm = 0.2665, lr_0 = 3.9400e-04
Validation rmse logD = 0.578202
Validation R2 logD = 0.775578
Epoch 41
Train function
Loss = 2.1692e-04, PNorm = 61.6320, GNorm = 0.4210, lr_0 = 3.9208e-04
Loss = 2.2252e-04, PNorm = 61.6395, GNorm = 0.7020, lr_0 = 3.9035e-04
Loss = 1.9563e-04, PNorm = 61.6517, GNorm = 0.6603, lr_0 = 3.8862e-04
Loss = 2.0220e-04, PNorm = 61.6601, GNorm = 0.6576, lr_0 = 3.8690e-04
Loss = 2.4283e-04, PNorm = 61.6660, GNorm = 0.3143, lr_0 = 3.8519e-04
Loss = 2.1155e-04, PNorm = 61.6759, GNorm = 0.6045, lr_0 = 3.8349e-04
Validation rmse logD = 0.582549
Validation R2 logD = 0.772191
Epoch 42
Train function
Loss = 1.8798e-04, PNorm = 61.6858, GNorm = 0.2253, lr_0 = 3.8179e-04
Loss = 2.0199e-04, PNorm = 61.6956, GNorm = 0.4739, lr_0 = 3.8010e-04
Loss = 2.0993e-04, PNorm = 61.7031, GNorm = 0.3380, lr_0 = 3.7842e-04
Loss = 1.7905e-04, PNorm = 61.7134, GNorm = 0.3100, lr_0 = 3.7675e-04
Loss = 1.6877e-04, PNorm = 61.7192, GNorm = 0.2664, lr_0 = 3.7508e-04
Validation rmse logD = 0.575422
Validation R2 logD = 0.777732
Epoch 43
Train function
Loss = 1.7009e-04, PNorm = 61.7289, GNorm = 0.4851, lr_0 = 3.7326e-04
Loss = 1.4938e-04, PNorm = 61.7375, GNorm = 0.3186, lr_0 = 3.7160e-04
Loss = 1.9056e-04, PNorm = 61.7445, GNorm = 0.4510, lr_0 = 3.6996e-04
Loss = 1.6866e-04, PNorm = 61.7549, GNorm = 0.2758, lr_0 = 3.6832e-04
Loss = 1.5810e-04, PNorm = 61.7641, GNorm = 0.2913, lr_0 = 3.6669e-04
Validation rmse logD = 0.579015
Validation R2 logD = 0.774947
Epoch 44
Train function
Loss = 1.5966e-04, PNorm = 61.7758, GNorm = 0.4449, lr_0 = 3.6491e-04
Loss = 1.6148e-04, PNorm = 61.7858, GNorm = 0.7594, lr_0 = 3.6330e-04
Loss = 1.7637e-04, PNorm = 61.7868, GNorm = 0.1815, lr_0 = 3.6169e-04
Loss = 1.5652e-04, PNorm = 61.7947, GNorm = 0.3139, lr_0 = 3.6009e-04
Loss = 1.4344e-04, PNorm = 61.8018, GNorm = 0.3021, lr_0 = 3.5850e-04
Loss = 1.9200e-04, PNorm = 61.8088, GNorm = 0.5195, lr_0 = 3.5691e-04
Loss = 8.1742e-04, PNorm = 61.8098, GNorm = 0.6388, lr_0 = 3.5675e-04
Validation rmse logD = 0.580112
Validation R2 logD = 0.774093
Epoch 45
Train function
Loss = 1.2201e-04, PNorm = 61.8155, GNorm = 0.2206, lr_0 = 3.5518e-04
Loss = 1.4136e-04, PNorm = 61.8203, GNorm = 0.4485, lr_0 = 3.5360e-04
Loss = 1.6996e-04, PNorm = 61.8294, GNorm = 0.3346, lr_0 = 3.5204e-04
Loss = 1.3186e-04, PNorm = 61.8381, GNorm = 0.2516, lr_0 = 3.5048e-04
Loss = 1.3295e-04, PNorm = 61.8451, GNorm = 0.3509, lr_0 = 3.4893e-04
Validation rmse logD = 0.584278
Validation R2 logD = 0.770837
Epoch 46
Train function
Loss = 1.6364e-04, PNorm = 61.8529, GNorm = 0.3869, lr_0 = 3.4724e-04
Loss = 1.6738e-04, PNorm = 61.8607, GNorm = 0.9062, lr_0 = 3.4570e-04
Loss = 1.7337e-04, PNorm = 61.8663, GNorm = 0.8633, lr_0 = 3.4417e-04
Loss = 1.5362e-04, PNorm = 61.8774, GNorm = 0.4070, lr_0 = 3.4265e-04
Loss = 1.7310e-04, PNorm = 61.8865, GNorm = 0.7913, lr_0 = 3.4113e-04
Validation rmse logD = 0.574293
Validation R2 logD = 0.778603
Epoch 47
Train function
Loss = 1.1722e-04, PNorm = 61.8967, GNorm = 0.5420, lr_0 = 3.3947e-04
Loss = 1.2625e-04, PNorm = 61.9050, GNorm = 0.2076, lr_0 = 3.3797e-04
Loss = 1.5268e-04, PNorm = 61.9094, GNorm = 0.6326, lr_0 = 3.3648e-04
Loss = 1.1943e-04, PNorm = 61.9169, GNorm = 0.4285, lr_0 = 3.3499e-04
Loss = 1.8171e-04, PNorm = 61.9248, GNorm = 0.4721, lr_0 = 3.3351e-04
Validation rmse logD = 0.582318
Validation R2 logD = 0.772372
Epoch 48
Train function
Loss = 2.0710e-04, PNorm = 61.9321, GNorm = 1.0280, lr_0 = 3.3188e-04
Loss = 1.6629e-04, PNorm = 61.9405, GNorm = 0.7310, lr_0 = 3.3042e-04
Loss = 1.4535e-04, PNorm = 61.9506, GNorm = 0.2519, lr_0 = 3.2895e-04
Loss = 1.0493e-04, PNorm = 61.9564, GNorm = 0.2288, lr_0 = 3.2750e-04
Loss = 1.2488e-04, PNorm = 61.9610, GNorm = 0.1679, lr_0 = 3.2605e-04
Loss = 1.2796e-04, PNorm = 61.9669, GNorm = 0.3646, lr_0 = 3.2461e-04
Validation rmse logD = 0.576587
Validation R2 logD = 0.776830
Epoch 49
Train function
Loss = 1.0279e-04, PNorm = 61.9752, GNorm = 0.2221, lr_0 = 3.2303e-04
Loss = 1.7383e-04, PNorm = 61.9796, GNorm = 0.6035, lr_0 = 3.2160e-04
Loss = 1.1976e-04, PNorm = 61.9855, GNorm = 0.4045, lr_0 = 3.2018e-04
Loss = 1.1565e-04, PNorm = 61.9928, GNorm = 0.2734, lr_0 = 3.1876e-04
Loss = 1.1726e-04, PNorm = 62.0008, GNorm = 0.1414, lr_0 = 3.1735e-04
Validation rmse logD = 0.578842
Validation R2 logD = 0.775081
Epoch 50
Train function
Loss = 9.4470e-05, PNorm = 62.0061, GNorm = 0.2588, lr_0 = 3.1595e-04
Loss = 1.4279e-04, PNorm = 62.0123, GNorm = 0.4540, lr_0 = 3.1455e-04
Loss = 1.1110e-04, PNorm = 62.0202, GNorm = 0.3069, lr_0 = 3.1316e-04
Loss = 1.1147e-04, PNorm = 62.0263, GNorm = 0.2993, lr_0 = 3.1177e-04
Loss = 9.9580e-05, PNorm = 62.0333, GNorm = 0.1748, lr_0 = 3.1039e-04
Validation rmse logD = 0.577292
Validation R2 logD = 0.776285
Epoch 51
Train function
Loss = 3.6331e-05, PNorm = 62.0406, GNorm = 0.1588, lr_0 = 3.0888e-04
Loss = 9.8538e-05, PNorm = 62.0478, GNorm = 0.2142, lr_0 = 3.0752e-04
Loss = 9.0909e-05, PNorm = 62.0527, GNorm = 0.1753, lr_0 = 3.0616e-04
Loss = 8.6561e-05, PNorm = 62.0566, GNorm = 0.1676, lr_0 = 3.0480e-04
Loss = 7.7923e-05, PNorm = 62.0620, GNorm = 0.3381, lr_0 = 3.0346e-04
Loss = 8.7149e-05, PNorm = 62.0658, GNorm = 0.4020, lr_0 = 3.0211e-04
Validation rmse logD = 0.581594
Validation R2 logD = 0.772938
Epoch 52
Train function
Loss = 9.1078e-05, PNorm = 62.0727, GNorm = 0.3392, lr_0 = 3.0064e-04
Loss = 7.5917e-05, PNorm = 62.0770, GNorm = 0.1498, lr_0 = 2.9931e-04
Loss = 9.1568e-05, PNorm = 62.0811, GNorm = 0.2231, lr_0 = 2.9799e-04
Loss = 9.3525e-05, PNorm = 62.0865, GNorm = 0.2590, lr_0 = 2.9667e-04
Loss = 9.1761e-05, PNorm = 62.0931, GNorm = 0.5528, lr_0 = 2.9536e-04
Validation rmse logD = 0.576304
Validation R2 logD = 0.777049
Epoch 53
Train function
Loss = 1.1478e-04, PNorm = 62.0994, GNorm = 0.3081, lr_0 = 2.9392e-04
Loss = 1.0766e-04, PNorm = 62.1052, GNorm = 0.5223, lr_0 = 2.9262e-04
Loss = 1.0855e-04, PNorm = 62.1106, GNorm = 0.4981, lr_0 = 2.9133e-04
Loss = 9.2929e-05, PNorm = 62.1177, GNorm = 0.4060, lr_0 = 2.9004e-04
Loss = 8.7686e-05, PNorm = 62.1217, GNorm = 0.2103, lr_0 = 2.8876e-04
Validation rmse logD = 0.574717
Validation R2 logD = 0.778276
Epoch 54
Train function
Loss = 4.7798e-05, PNorm = 62.1240, GNorm = 0.1494, lr_0 = 2.8735e-04
Loss = 8.6144e-05, PNorm = 62.1282, GNorm = 0.2019, lr_0 = 2.8608e-04
Loss = 1.2152e-04, PNorm = 62.1331, GNorm = 0.2136, lr_0 = 2.8482e-04
Loss = 8.6265e-05, PNorm = 62.1390, GNorm = 0.1657, lr_0 = 2.8356e-04
Loss = 8.8010e-05, PNorm = 62.1451, GNorm = 0.2037, lr_0 = 2.8230e-04
Loss = 9.0036e-05, PNorm = 62.1531, GNorm = 0.1336, lr_0 = 2.8105e-04
Validation rmse logD = 0.579957
Validation R2 logD = 0.774214
Epoch 55
Train function
Loss = 7.2976e-05, PNorm = 62.1588, GNorm = 0.2001, lr_0 = 2.7969e-04
Loss = 7.6692e-05, PNorm = 62.1619, GNorm = 0.1333, lr_0 = 2.7845e-04
Loss = 6.0885e-05, PNorm = 62.1653, GNorm = 0.1944, lr_0 = 2.7722e-04
Loss = 8.0522e-05, PNorm = 62.1688, GNorm = 0.4377, lr_0 = 2.7599e-04
Loss = 7.8333e-05, PNorm = 62.1730, GNorm = 0.4027, lr_0 = 2.7477e-04
Validation rmse logD = 0.578516
Validation R2 logD = 0.775335
Epoch 56
Train function
Loss = 5.7397e-05, PNorm = 62.1776, GNorm = 0.1557, lr_0 = 2.7343e-04
Loss = 6.6006e-05, PNorm = 62.1823, GNorm = 0.2673, lr_0 = 2.7222e-04
Loss = 7.5556e-05, PNorm = 62.1863, GNorm = 0.2226, lr_0 = 2.7102e-04
Loss = 7.7021e-05, PNorm = 62.1918, GNorm = 0.6500, lr_0 = 2.6982e-04
Loss = 1.0032e-04, PNorm = 62.1964, GNorm = 0.5508, lr_0 = 2.6863e-04
Validation rmse logD = 0.582043
Validation R2 logD = 0.772587
Epoch 57
Train function
Loss = 1.2025e-04, PNorm = 62.2034, GNorm = 0.8226, lr_0 = 2.6732e-04
Loss = 8.2732e-05, PNorm = 62.2085, GNorm = 0.2441, lr_0 = 2.6614e-04
Loss = 8.0058e-05, PNorm = 62.2132, GNorm = 0.4914, lr_0 = 2.6496e-04
Loss = 7.8181e-05, PNorm = 62.2172, GNorm = 0.3306, lr_0 = 2.6379e-04
Loss = 7.5852e-05, PNorm = 62.2218, GNorm = 0.4195, lr_0 = 2.6262e-04
Loss = 6.6566e-05, PNorm = 62.2275, GNorm = 0.1696, lr_0 = 2.6146e-04
Loss = 1.3998e-04, PNorm = 62.2281, GNorm = 0.3506, lr_0 = 2.6134e-04
Validation rmse logD = 0.581329
Validation R2 logD = 0.773145
Epoch 58
Train function
Loss = 6.8795e-05, PNorm = 62.2322, GNorm = 0.2476, lr_0 = 2.6019e-04
Loss = 5.3924e-05, PNorm = 62.2354, GNorm = 0.1548, lr_0 = 2.5904e-04
Loss = 6.5656e-05, PNorm = 62.2393, GNorm = 0.1692, lr_0 = 2.5789e-04
Loss = 6.0910e-05, PNorm = 62.2403, GNorm = 0.1409, lr_0 = 2.5675e-04
Loss = 5.4367e-05, PNorm = 62.2440, GNorm = 0.3392, lr_0 = 2.5561e-04
Validation rmse logD = 0.577116
Validation R2 logD = 0.776421
Epoch 59
Train function
Loss = 6.2178e-05, PNorm = 62.2465, GNorm = 0.2509, lr_0 = 2.5448e-04
Loss = 5.1014e-05, PNorm = 62.2503, GNorm = 0.1881, lr_0 = 2.5336e-04
Loss = 4.4355e-05, PNorm = 62.2554, GNorm = 0.1791, lr_0 = 2.5224e-04
Loss = 5.2977e-05, PNorm = 62.2593, GNorm = 0.3647, lr_0 = 2.5112e-04
Loss = 6.8353e-05, PNorm = 62.2636, GNorm = 0.1664, lr_0 = 2.5001e-04
Validation rmse logD = 0.580560
Validation R2 logD = 0.773744
Epoch 60
Train function
Loss = 3.6064e-05, PNorm = 62.2667, GNorm = 0.2372, lr_0 = 2.4879e-04
Loss = 5.9366e-05, PNorm = 62.2694, GNorm = 0.2389, lr_0 = 2.4769e-04
Loss = 6.3697e-05, PNorm = 62.2725, GNorm = 0.1542, lr_0 = 2.4660e-04
Loss = 7.8901e-05, PNorm = 62.2781, GNorm = 0.5560, lr_0 = 2.4551e-04
Loss = 6.0668e-05, PNorm = 62.2825, GNorm = 0.1957, lr_0 = 2.4442e-04
Loss = 7.4543e-05, PNorm = 62.2870, GNorm = 0.1490, lr_0 = 2.4334e-04
Loss = 7.3322e-05, PNorm = 62.2873, GNorm = 0.1669, lr_0 = 2.4323e-04
Validation rmse logD = 0.582970
Validation R2 logD = 0.771862
Epoch 61
Train function
Loss = 5.5675e-05, PNorm = 62.2894, GNorm = 0.3792, lr_0 = 2.4216e-04
Loss = 4.4127e-05, PNorm = 62.2904, GNorm = 0.1543, lr_0 = 2.4109e-04
Loss = 5.6148e-05, PNorm = 62.2949, GNorm = 0.1606, lr_0 = 2.4002e-04
Loss = 4.7128e-05, PNorm = 62.2979, GNorm = 0.2536, lr_0 = 2.3896e-04
Loss = 5.7924e-05, PNorm = 62.3012, GNorm = 0.1537, lr_0 = 2.3790e-04
Validation rmse logD = 0.583925
Validation R2 logD = 0.771114
Epoch 62
Train function
Loss = 5.1317e-05, PNorm = 62.3032, GNorm = 0.1645, lr_0 = 2.3674e-04
Loss = 5.6262e-05, PNorm = 62.3092, GNorm = 0.2565, lr_0 = 2.3570e-04
Loss = 5.0331e-05, PNorm = 62.3122, GNorm = 0.3380, lr_0 = 2.3465e-04
Loss = 6.0656e-05, PNorm = 62.3186, GNorm = 0.2390, lr_0 = 2.3362e-04
Loss = 4.5838e-05, PNorm = 62.3228, GNorm = 0.1127, lr_0 = 2.3258e-04
Validation rmse logD = 0.583479
Validation R2 logD = 0.771464
Epoch 63
Train function
Loss = 4.2133e-05, PNorm = 62.3250, GNorm = 0.2458, lr_0 = 2.3145e-04
Loss = 5.1019e-05, PNorm = 62.3256, GNorm = 0.1374, lr_0 = 2.3043e-04
Loss = 5.1458e-05, PNorm = 62.3282, GNorm = 0.3075, lr_0 = 2.2941e-04
Loss = 5.9260e-05, PNorm = 62.3338, GNorm = 0.2927, lr_0 = 2.2839e-04
Loss = 6.2931e-05, PNorm = 62.3366, GNorm = 0.5793, lr_0 = 2.2738e-04
Validation rmse logD = 0.582821
Validation R2 logD = 0.771979
Epoch 64
Train function
Loss = 2.7448e-05, PNorm = 62.3409, GNorm = 0.1514, lr_0 = 2.2628e-04
Loss = 5.3164e-05, PNorm = 62.3457, GNorm = 0.5882, lr_0 = 2.2528e-04
Loss = 6.4570e-05, PNorm = 62.3489, GNorm = 0.3837, lr_0 = 2.2428e-04
Loss = 5.6054e-05, PNorm = 62.3520, GNorm = 0.2338, lr_0 = 2.2329e-04
Loss = 6.0080e-05, PNorm = 62.3554, GNorm = 0.1968, lr_0 = 2.2230e-04
Loss = 6.8709e-05, PNorm = 62.3604, GNorm = 0.1124, lr_0 = 2.2132e-04
Validation rmse logD = 0.583506
Validation R2 logD = 0.771443
Epoch 65
Train function
Loss = 5.7816e-05, PNorm = 62.3654, GNorm = 0.1921, lr_0 = 2.2024e-04
Loss = 5.8574e-05, PNorm = 62.3668, GNorm = 0.1415, lr_0 = 2.1927e-04
Loss = 6.1721e-05, PNorm = 62.3714, GNorm = 0.3684, lr_0 = 2.1830e-04
Loss = 5.6882e-05, PNorm = 62.3767, GNorm = 0.4528, lr_0 = 2.1733e-04
Loss = 4.0886e-05, PNorm = 62.3805, GNorm = 0.1127, lr_0 = 2.1637e-04
Validation rmse logD = 0.586773
Validation R2 logD = 0.768876
Epoch 66
Train function
Loss = 3.9844e-05, PNorm = 62.3835, GNorm = 0.2604, lr_0 = 2.1532e-04
Loss = 6.1738e-05, PNorm = 62.3853, GNorm = 0.5661, lr_0 = 2.1436e-04
Loss = 4.8396e-05, PNorm = 62.3896, GNorm = 0.2674, lr_0 = 2.1342e-04
Loss = 5.3781e-05, PNorm = 62.3930, GNorm = 0.5666, lr_0 = 2.1247e-04
Loss = 4.9634e-05, PNorm = 62.3947, GNorm = 0.2721, lr_0 = 2.1153e-04
Validation rmse logD = 0.582766
Validation R2 logD = 0.772022
Epoch 67
Train function
Loss = 4.0348e-05, PNorm = 62.3973, GNorm = 0.3158, lr_0 = 2.1060e-04
Loss = 6.5318e-05, PNorm = 62.3989, GNorm = 0.3255, lr_0 = 2.0966e-04
Loss = 4.5252e-05, PNorm = 62.4018, GNorm = 0.1012, lr_0 = 2.0874e-04
Loss = 5.5018e-05, PNorm = 62.4051, GNorm = 0.1912, lr_0 = 2.0781e-04
Loss = 3.4838e-05, PNorm = 62.4071, GNorm = 0.2781, lr_0 = 2.0689e-04
Loss = 3.5701e-05, PNorm = 62.4102, GNorm = 0.1365, lr_0 = 2.0598e-04
Validation rmse logD = 0.585292
Validation R2 logD = 0.770041
Epoch 68
Train function
Loss = 3.2386e-05, PNorm = 62.4133, GNorm = 0.0921, lr_0 = 2.0498e-04
Loss = 3.4576e-05, PNorm = 62.4155, GNorm = 0.4410, lr_0 = 2.0407e-04
Loss = 2.9776e-05, PNorm = 62.4187, GNorm = 0.1327, lr_0 = 2.0317e-04
Loss = 3.3664e-05, PNorm = 62.4218, GNorm = 0.2127, lr_0 = 2.0227e-04
Loss = 4.5013e-05, PNorm = 62.4237, GNorm = 0.1504, lr_0 = 2.0137e-04
Validation rmse logD = 0.585118
Validation R2 logD = 0.770178
Epoch 69
Train function
Loss = 4.7417e-05, PNorm = 62.4285, GNorm = 0.4711, lr_0 = 2.0039e-04
Loss = 4.3689e-05, PNorm = 62.4308, GNorm = 0.2332, lr_0 = 1.9951e-04
Loss = 4.5766e-05, PNorm = 62.4326, GNorm = 0.4764, lr_0 = 1.9863e-04
Loss = 4.1721e-05, PNorm = 62.4356, GNorm = 0.1928, lr_0 = 1.9775e-04
Loss = 3.5065e-05, PNorm = 62.4384, GNorm = 0.1429, lr_0 = 1.9687e-04
Validation rmse logD = 0.581129
Validation R2 logD = 0.773301
Epoch 70
Train function
Loss = 2.8620e-05, PNorm = 62.4400, GNorm = 0.1628, lr_0 = 1.9592e-04
Loss = 2.4646e-05, PNorm = 62.4420, GNorm = 0.1245, lr_0 = 1.9505e-04
Loss = 3.2197e-05, PNorm = 62.4459, GNorm = 0.1961, lr_0 = 1.9419e-04
Loss = 2.7653e-05, PNorm = 62.4477, GNorm = 0.2147, lr_0 = 1.9333e-04
Loss = 4.0919e-05, PNorm = 62.4495, GNorm = 0.2193, lr_0 = 1.9247e-04
Loss = 3.2095e-05, PNorm = 62.4517, GNorm = 0.0995, lr_0 = 1.9162e-04
Validation rmse logD = 0.582171
Validation R2 logD = 0.772487
Epoch 71
Train function
Loss = 2.9875e-05, PNorm = 62.4535, GNorm = 0.1818, lr_0 = 1.9069e-04
Loss = 2.5828e-05, PNorm = 62.4563, GNorm = 0.1626, lr_0 = 1.8984e-04
Loss = 3.3102e-05, PNorm = 62.4587, GNorm = 0.1224, lr_0 = 1.8900e-04
Loss = 2.6448e-05, PNorm = 62.4607, GNorm = 0.1712, lr_0 = 1.8817e-04
Loss = 2.7589e-05, PNorm = 62.4618, GNorm = 0.0903, lr_0 = 1.8734e-04
Validation rmse logD = 0.583792
Validation R2 logD = 0.771218
Epoch 72
Train function
Loss = 2.3893e-05, PNorm = 62.4630, GNorm = 0.1076, lr_0 = 1.8643e-04
Loss = 2.2683e-05, PNorm = 62.4646, GNorm = 0.3001, lr_0 = 1.8560e-04
Loss = 2.6962e-05, PNorm = 62.4657, GNorm = 0.0654, lr_0 = 1.8478e-04
Loss = 2.1801e-05, PNorm = 62.4673, GNorm = 0.0911, lr_0 = 1.8396e-04
Loss = 3.2750e-05, PNorm = 62.4702, GNorm = 0.1324, lr_0 = 1.8315e-04
Validation rmse logD = 0.582192
Validation R2 logD = 0.772471
Epoch 73
Train function
Loss = 2.0861e-05, PNorm = 62.4722, GNorm = 0.1607, lr_0 = 1.8226e-04
Loss = 2.2217e-05, PNorm = 62.4734, GNorm = 0.0921, lr_0 = 1.8145e-04
Loss = 2.2459e-05, PNorm = 62.4751, GNorm = 0.0672, lr_0 = 1.8065e-04
Loss = 2.8361e-05, PNorm = 62.4768, GNorm = 0.0893, lr_0 = 1.7985e-04
Loss = 2.1319e-05, PNorm = 62.4799, GNorm = 0.1601, lr_0 = 1.7905e-04
Loss = 2.6563e-05, PNorm = 62.4823, GNorm = 0.0969, lr_0 = 1.7826e-04
Loss = 1.4086e-04, PNorm = 62.4827, GNorm = 0.3078, lr_0 = 1.7818e-04
Validation rmse logD = 0.584830
Validation R2 logD = 0.770404
Epoch 74
Train function
Loss = 2.9685e-05, PNorm = 62.4841, GNorm = 0.1322, lr_0 = 1.7739e-04
Loss = 2.4106e-05, PNorm = 62.4852, GNorm = 0.1357, lr_0 = 1.7661e-04
Loss = 3.4870e-05, PNorm = 62.4884, GNorm = 0.2231, lr_0 = 1.7583e-04
Loss = 2.1121e-05, PNorm = 62.4910, GNorm = 0.1006, lr_0 = 1.7505e-04
Loss = 2.2774e-05, PNorm = 62.4933, GNorm = 0.1784, lr_0 = 1.7428e-04
Validation rmse logD = 0.584860
Validation R2 logD = 0.770381
Epoch 75
Train function
Loss = 2.4126e-05, PNorm = 62.4944, GNorm = 0.1129, lr_0 = 1.7351e-04
Loss = 2.8499e-05, PNorm = 62.4958, GNorm = 0.0783, lr_0 = 1.7274e-04
Loss = 2.3660e-05, PNorm = 62.4979, GNorm = 0.0984, lr_0 = 1.7197e-04
Loss = 2.1260e-05, PNorm = 62.4999, GNorm = 0.0765, lr_0 = 1.7121e-04
Loss = 1.8543e-05, PNorm = 62.5011, GNorm = 0.1187, lr_0 = 1.7046e-04
Validation rmse logD = 0.582961
Validation R2 logD = 0.771869
Epoch 76
Train function
Loss = 1.8336e-05, PNorm = 62.5032, GNorm = 0.2330, lr_0 = 1.6963e-04
Loss = 1.9896e-05, PNorm = 62.5061, GNorm = 0.0997, lr_0 = 1.6888e-04
Loss = 2.4354e-05, PNorm = 62.5086, GNorm = 0.1633, lr_0 = 1.6813e-04
Loss = 2.4694e-05, PNorm = 62.5105, GNorm = 0.0923, lr_0 = 1.6739e-04
Loss = 2.2627e-05, PNorm = 62.5121, GNorm = 0.1700, lr_0 = 1.6665e-04
Loss = 1.7532e-05, PNorm = 62.5133, GNorm = 0.2206, lr_0 = 1.6591e-04
Loss = 1.6889e-04, PNorm = 62.5132, GNorm = 0.4027, lr_0 = 1.6584e-04
Validation rmse logD = 0.585862
Validation R2 logD = 0.769593
Epoch 77
Train function
Loss = 2.3506e-05, PNorm = 62.5147, GNorm = 0.0937, lr_0 = 1.6510e-04
Loss = 2.1370e-05, PNorm = 62.5160, GNorm = 0.1307, lr_0 = 1.6437e-04
Loss = 1.8095e-05, PNorm = 62.5171, GNorm = 0.0633, lr_0 = 1.6364e-04
Loss = 3.0739e-05, PNorm = 62.5198, GNorm = 0.2019, lr_0 = 1.6292e-04
Loss = 2.5039e-05, PNorm = 62.5217, GNorm = 0.0751, lr_0 = 1.6220e-04
Validation rmse logD = 0.583376
Validation R2 logD = 0.771544
Epoch 78
Train function
Loss = 4.6386e-05, PNorm = 62.5223, GNorm = 0.1836, lr_0 = 1.6141e-04
Loss = 6.2734e-05, PNorm = 62.5270, GNorm = 0.2904, lr_0 = 1.6070e-04
Loss = 4.3653e-05, PNorm = 62.5308, GNorm = 0.1879, lr_0 = 1.5999e-04
Loss = 3.7276e-05, PNorm = 62.5330, GNorm = 0.1706, lr_0 = 1.5928e-04
Loss = 6.1604e-05, PNorm = 62.5375, GNorm = 0.1337, lr_0 = 1.5857e-04
Validation rmse logD = 0.581016
Validation R2 logD = 0.773389
Epoch 79
Train function
Loss = 3.7404e-05, PNorm = 62.5375, GNorm = 0.1276, lr_0 = 1.5780e-04
Loss = 4.6505e-05, PNorm = 62.5390, GNorm = 0.1556, lr_0 = 1.5710e-04
Loss = 4.4300e-05, PNorm = 62.5416, GNorm = 0.1396, lr_0 = 1.5641e-04
Loss = 4.2989e-05, PNorm = 62.5454, GNorm = 0.1452, lr_0 = 1.5572e-04
Loss = 4.6485e-05, PNorm = 62.5483, GNorm = 0.1975, lr_0 = 1.5503e-04
Validation rmse logD = 0.586763
Validation R2 logD = 0.768884
Epoch 80
Train function
Loss = 3.7611e-05, PNorm = 62.5508, GNorm = 0.2293, lr_0 = 1.5427e-04
Loss = 3.3460e-05, PNorm = 62.5524, GNorm = 0.2781, lr_0 = 1.5359e-04
Loss = 3.3816e-05, PNorm = 62.5536, GNorm = 0.1801, lr_0 = 1.5291e-04
Loss = 3.5502e-05, PNorm = 62.5555, GNorm = 0.5574, lr_0 = 1.5224e-04
Loss = 3.0697e-05, PNorm = 62.5576, GNorm = 0.1830, lr_0 = 1.5156e-04
Loss = 2.9128e-05, PNorm = 62.5604, GNorm = 0.2479, lr_0 = 1.5089e-04
Validation rmse logD = 0.581554
Validation R2 logD = 0.772969
Epoch 81
Train function
Loss = 2.8530e-05, PNorm = 62.5618, GNorm = 0.1187, lr_0 = 1.5016e-04
Loss = 2.5322e-05, PNorm = 62.5639, GNorm = 0.1248, lr_0 = 1.4949e-04
Loss = 2.3361e-05, PNorm = 62.5662, GNorm = 0.1055, lr_0 = 1.4883e-04
Loss = 2.3368e-05, PNorm = 62.5676, GNorm = 0.0994, lr_0 = 1.4817e-04
Loss = 2.9876e-05, PNorm = 62.5693, GNorm = 0.0868, lr_0 = 1.4752e-04
Validation rmse logD = 0.584091
Validation R2 logD = 0.770984
Epoch 82
Train function
Loss = 2.3613e-05, PNorm = 62.5709, GNorm = 0.2455, lr_0 = 1.4680e-04
Loss = 2.2201e-05, PNorm = 62.5731, GNorm = 0.0930, lr_0 = 1.4615e-04
Loss = 2.3761e-05, PNorm = 62.5750, GNorm = 0.1624, lr_0 = 1.4551e-04
Loss = 1.5815e-05, PNorm = 62.5764, GNorm = 0.0950, lr_0 = 1.4486e-04
Loss = 2.3176e-05, PNorm = 62.5781, GNorm = 0.0723, lr_0 = 1.4422e-04
Validation rmse logD = 0.583880
Validation R2 logD = 0.771149
Epoch 83
Train function
Loss = 1.1092e-05, PNorm = 62.5793, GNorm = 0.1624, lr_0 = 1.4352e-04
Loss = 2.0085e-05, PNorm = 62.5801, GNorm = 0.1065, lr_0 = 1.4288e-04
Loss = 1.6914e-05, PNorm = 62.5797, GNorm = 0.1340, lr_0 = 1.4225e-04
Loss = 2.2204e-05, PNorm = 62.5812, GNorm = 0.2306, lr_0 = 1.4162e-04
Loss = 1.8186e-05, PNorm = 62.5829, GNorm = 0.2210, lr_0 = 1.4100e-04
Loss = 2.6657e-05, PNorm = 62.5851, GNorm = 0.1230, lr_0 = 1.4037e-04
Validation rmse logD = 0.583191
Validation R2 logD = 0.771689
Epoch 84
Train function
Loss = 1.1576e-05, PNorm = 62.5865, GNorm = 0.1261, lr_0 = 1.3975e-04
Loss = 1.2793e-05, PNorm = 62.5871, GNorm = 0.1231, lr_0 = 1.3913e-04
Loss = 2.1771e-05, PNorm = 62.5888, GNorm = 0.0816, lr_0 = 1.3852e-04
Loss = 1.5357e-05, PNorm = 62.5907, GNorm = 0.1365, lr_0 = 1.3791e-04
Loss = 1.6689e-05, PNorm = 62.5923, GNorm = 0.2073, lr_0 = 1.3730e-04
Validation rmse logD = 0.584249
Validation R2 logD = 0.770860
Epoch 85
Train function
Loss = 1.7273e-05, PNorm = 62.5938, GNorm = 0.1425, lr_0 = 1.3663e-04
Loss = 1.7843e-05, PNorm = 62.5956, GNorm = 0.0914, lr_0 = 1.3602e-04
Loss = 1.4621e-05, PNorm = 62.5969, GNorm = 0.1301, lr_0 = 1.3542e-04
Loss = 1.6159e-05, PNorm = 62.5982, GNorm = 0.1259, lr_0 = 1.3482e-04
Loss = 1.9482e-05, PNorm = 62.5997, GNorm = 0.2789, lr_0 = 1.3423e-04
Validation rmse logD = 0.585834
Validation R2 logD = 0.769615
Epoch 86
Train function
Loss = 1.7223e-05, PNorm = 62.6009, GNorm = 0.3084, lr_0 = 1.3357e-04
Loss = 2.2847e-05, PNorm = 62.6025, GNorm = 0.1731, lr_0 = 1.3298e-04
Loss = 1.9737e-05, PNorm = 62.6031, GNorm = 0.1134, lr_0 = 1.3239e-04
Loss = 1.9184e-05, PNorm = 62.6045, GNorm = 0.1590, lr_0 = 1.3181e-04
Loss = 1.1436e-05, PNorm = 62.6058, GNorm = 0.1263, lr_0 = 1.3123e-04
Loss = 2.0710e-05, PNorm = 62.6075, GNorm = 0.0911, lr_0 = 1.3065e-04
Validation rmse logD = 0.583590
Validation R2 logD = 0.771376
Epoch 87
Train function
Loss = 1.7634e-05, PNorm = 62.6086, GNorm = 0.1344, lr_0 = 1.3001e-04
Loss = 1.4716e-05, PNorm = 62.6102, GNorm = 0.0822, lr_0 = 1.2944e-04
Loss = 1.5829e-05, PNorm = 62.6111, GNorm = 0.1094, lr_0 = 1.2886e-04
Loss = 1.4451e-05, PNorm = 62.6118, GNorm = 0.1277, lr_0 = 1.2829e-04
Loss = 1.4767e-05, PNorm = 62.6129, GNorm = 0.0945, lr_0 = 1.2773e-04
Validation rmse logD = 0.583468
Validation R2 logD = 0.771472
Epoch 88
Train function
Loss = 1.3817e-05, PNorm = 62.6142, GNorm = 0.2087, lr_0 = 1.2710e-04
Loss = 1.9806e-05, PNorm = 62.6148, GNorm = 0.2581, lr_0 = 1.2654e-04
Loss = 1.9894e-05, PNorm = 62.6168, GNorm = 0.0853, lr_0 = 1.2598e-04
Loss = 2.2548e-05, PNorm = 62.6182, GNorm = 0.2734, lr_0 = 1.2542e-04
Loss = 1.8531e-05, PNorm = 62.6206, GNorm = 0.1196, lr_0 = 1.2487e-04
Validation rmse logD = 0.581800
Validation R2 logD = 0.772777
Epoch 89
Train function
Loss = 1.5740e-05, PNorm = 62.6221, GNorm = 0.2680, lr_0 = 1.2426e-04
Loss = 2.3816e-05, PNorm = 62.6240, GNorm = 0.2566, lr_0 = 1.2371e-04
Loss = 1.8455e-05, PNorm = 62.6253, GNorm = 0.2440, lr_0 = 1.2317e-04
Loss = 2.0301e-05, PNorm = 62.6274, GNorm = 0.1716, lr_0 = 1.2262e-04
Loss = 1.5855e-05, PNorm = 62.6279, GNorm = 0.0764, lr_0 = 1.2208e-04
Loss = 1.4797e-05, PNorm = 62.6283, GNorm = 0.1047, lr_0 = 1.2154e-04
Loss = 7.7531e-05, PNorm = 62.6283, GNorm = 0.1594, lr_0 = 1.2148e-04
Validation rmse logD = 0.584558
Validation R2 logD = 0.770618
Epoch 90
Train function
Loss = 1.1437e-05, PNorm = 62.6285, GNorm = 0.1605, lr_0 = 1.2095e-04
Loss = 1.4772e-05, PNorm = 62.6302, GNorm = 0.1279, lr_0 = 1.2041e-04
Loss = 1.3709e-05, PNorm = 62.6314, GNorm = 0.1174, lr_0 = 1.1988e-04
Loss = 1.0177e-05, PNorm = 62.6327, GNorm = 0.0796, lr_0 = 1.1935e-04
Loss = 1.2251e-05, PNorm = 62.6340, GNorm = 0.0859, lr_0 = 1.1882e-04
Validation rmse logD = 0.584659
Validation R2 logD = 0.770538
Epoch 91
Train function
Loss = 1.1406e-05, PNorm = 62.6351, GNorm = 0.0834, lr_0 = 1.1824e-04
Loss = 1.2493e-05, PNorm = 62.6363, GNorm = 0.1523, lr_0 = 1.1772e-04
Loss = 1.4333e-05, PNorm = 62.6374, GNorm = 0.0865, lr_0 = 1.1720e-04
Loss = 1.2792e-05, PNorm = 62.6381, GNorm = 0.1140, lr_0 = 1.1668e-04
Loss = 1.2235e-05, PNorm = 62.6379, GNorm = 0.0447, lr_0 = 1.1616e-04
Validation rmse logD = 0.584777
Validation R2 logD = 0.770445
Epoch 92
Train function
Loss = 6.4339e-06, PNorm = 62.6387, GNorm = 0.0435, lr_0 = 1.1565e-04
Loss = 1.3655e-05, PNorm = 62.6401, GNorm = 0.1091, lr_0 = 1.1514e-04
Loss = 1.4787e-05, PNorm = 62.6406, GNorm = 0.0842, lr_0 = 1.1463e-04
Loss = 1.3286e-05, PNorm = 62.6426, GNorm = 0.0648, lr_0 = 1.1412e-04
Loss = 1.0570e-05, PNorm = 62.6439, GNorm = 0.0884, lr_0 = 1.1362e-04
Loss = 1.3554e-05, PNorm = 62.6449, GNorm = 0.1664, lr_0 = 1.1312e-04
Loss = 1.8109e-04, PNorm = 62.6450, GNorm = 0.3327, lr_0 = 1.1307e-04
Validation rmse logD = 0.586266
Validation R2 logD = 0.769275
Epoch 93
Train function
Loss = 1.3479e-05, PNorm = 62.6460, GNorm = 0.0848, lr_0 = 1.1257e-04
Loss = 1.1930e-05, PNorm = 62.6471, GNorm = 0.1195, lr_0 = 1.1207e-04
Loss = 1.3052e-05, PNorm = 62.6478, GNorm = 0.1343, lr_0 = 1.1157e-04
Loss = 7.6027e-06, PNorm = 62.6489, GNorm = 0.0422, lr_0 = 1.1108e-04
Loss = 1.0674e-05, PNorm = 62.6498, GNorm = 0.0595, lr_0 = 1.1059e-04
Validation rmse logD = 0.584498
Validation R2 logD = 0.770665
Epoch 94
Train function
Loss = 9.0839e-06, PNorm = 62.6514, GNorm = 0.1585, lr_0 = 1.1005e-04
Loss = 1.1500e-05, PNorm = 62.6521, GNorm = 0.0591, lr_0 = 1.0956e-04
Loss = 9.5630e-06, PNorm = 62.6532, GNorm = 0.1311, lr_0 = 1.0908e-04
Loss = 1.4112e-05, PNorm = 62.6534, GNorm = 0.2157, lr_0 = 1.0860e-04
Loss = 1.3098e-05, PNorm = 62.6538, GNorm = 0.1069, lr_0 = 1.0811e-04
Validation rmse logD = 0.585071
Validation R2 logD = 0.770215
Epoch 95
Train function
Loss = 1.1257e-05, PNorm = 62.6551, GNorm = 0.1421, lr_0 = 1.0759e-04
Loss = 1.0793e-05, PNorm = 62.6563, GNorm = 0.2201, lr_0 = 1.0711e-04
Loss = 1.2239e-05, PNorm = 62.6570, GNorm = 0.0745, lr_0 = 1.0664e-04
Loss = 1.0570e-05, PNorm = 62.6576, GNorm = 0.0913, lr_0 = 1.0617e-04
Loss = 1.1001e-05, PNorm = 62.6583, GNorm = 0.1204, lr_0 = 1.0570e-04
Validation rmse logD = 0.585204
Validation R2 logD = 0.770111
Epoch 96
Train function
Loss = 1.1970e-05, PNorm = 62.6592, GNorm = 0.0491, lr_0 = 1.0518e-04
Loss = 8.8803e-06, PNorm = 62.6597, GNorm = 0.1476, lr_0 = 1.0472e-04
Loss = 9.8517e-06, PNorm = 62.6609, GNorm = 0.0973, lr_0 = 1.0426e-04
Loss = 1.2270e-05, PNorm = 62.6619, GNorm = 0.2193, lr_0 = 1.0379e-04
Loss = 1.3007e-05, PNorm = 62.6628, GNorm = 0.0779, lr_0 = 1.0333e-04
Loss = 8.5733e-06, PNorm = 62.6631, GNorm = 0.0704, lr_0 = 1.0288e-04
Validation rmse logD = 0.587334
Validation R2 logD = 0.768434
Epoch 97
Train function
Loss = 1.1807e-05, PNorm = 62.6639, GNorm = 0.1206, lr_0 = 1.0238e-04
Loss = 9.7265e-06, PNorm = 62.6646, GNorm = 0.0497, lr_0 = 1.0192e-04
Loss = 6.3222e-06, PNorm = 62.6647, GNorm = 0.0517, lr_0 = 1.0147e-04
Loss = 8.0707e-06, PNorm = 62.6656, GNorm = 0.0727, lr_0 = 1.0102e-04
Loss = 8.6535e-06, PNorm = 62.6666, GNorm = 0.1162, lr_0 = 1.0058e-04
Validation rmse logD = 0.585862
Validation R2 logD = 0.769593
Epoch 98
Train function
Loss = 8.5038e-06, PNorm = 62.6676, GNorm = 0.0451, lr_0 = 1.0009e-04
Loss = 1.1873e-05, PNorm = 62.6681, GNorm = 0.1637, lr_0 = 1.0000e-04
Loss = 8.3619e-06, PNorm = 62.6687, GNorm = 0.0750, lr_0 = 1.0000e-04
Loss = 8.8577e-06, PNorm = 62.6698, GNorm = 0.0658, lr_0 = 1.0000e-04
Loss = 9.8842e-06, PNorm = 62.6708, GNorm = 0.0440, lr_0 = 1.0000e-04
Validation rmse logD = 0.587720
Validation R2 logD = 0.768129
Epoch 99
Train function
Loss = 7.6319e-06, PNorm = 62.6731, GNorm = 0.1965, lr_0 = 1.0000e-04
Loss = 9.3407e-06, PNorm = 62.6747, GNorm = 0.1342, lr_0 = 1.0000e-04
Loss = 1.2236e-05, PNorm = 62.6740, GNorm = 0.2374, lr_0 = 1.0000e-04
Loss = 8.9584e-06, PNorm = 62.6742, GNorm = 0.2180, lr_0 = 1.0000e-04
Loss = 1.2764e-05, PNorm = 62.6751, GNorm = 0.1209, lr_0 = 1.0000e-04
Loss = 1.0358e-05, PNorm = 62.6756, GNorm = 0.1101, lr_0 = 1.0000e-04
Validation rmse logD = 0.584157
Validation R2 logD = 0.770932
Model 0 best validation rmse = 0.571689 on epoch 27
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logD = 0.615716
Model 0 test R2 logD = 0.737803
Ensemble test rmse  logD= 0.615716
Ensemble test R2  logD= 0.737803
Fold 3
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_312/folds/fold_3',
 'save_smiles_splits': False,
 'seed': 3,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD'],
 'task_names': ['logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 2656,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 3
Total size = 3,541 | train size = 2,656 | val size = 885 | test size = 625
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=895, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,222,401
Moving model to cuda
Epoch 0
Train function
Loss = 2.0145e-02, PNorm = 52.4954, GNorm = 11.5219, lr_0 = 1.9340e-04
Loss = 1.8374e-02, PNorm = 52.5017, GNorm = 7.8099, lr_0 = 2.7830e-04
Loss = 1.7257e-02, PNorm = 52.5163, GNorm = 3.8274, lr_0 = 3.6321e-04
Loss = 1.6616e-02, PNorm = 52.5364, GNorm = 2.3882, lr_0 = 4.4811e-04
Loss = 1.5470e-02, PNorm = 52.5593, GNorm = 5.0282, lr_0 = 5.3302e-04
Validation rmse logD = 1.083285
Validation R2 logD = 0.182724
Epoch 1
Train function
Loss = 1.4790e-02, PNorm = 52.5932, GNorm = 2.9782, lr_0 = 6.2642e-04
Loss = 1.5713e-02, PNorm = 52.6505, GNorm = 1.6022, lr_0 = 7.1132e-04
Loss = 1.3324e-02, PNorm = 52.7273, GNorm = 1.8980, lr_0 = 7.9623e-04
Loss = 1.5189e-02, PNorm = 52.7987, GNorm = 2.7117, lr_0 = 8.8113e-04
Loss = 1.3515e-02, PNorm = 52.8726, GNorm = 4.9467, lr_0 = 9.6604e-04
Validation rmse logD = 0.940382
Validation R2 logD = 0.384127
Epoch 2
Train function
Loss = 9.8974e-03, PNorm = 52.9781, GNorm = 4.2319, lr_0 = 9.9690e-04
Loss = 1.2356e-02, PNorm = 53.0764, GNorm = 6.7622, lr_0 = 9.9249e-04
Loss = 1.1725e-02, PNorm = 53.1712, GNorm = 1.3941, lr_0 = 9.8810e-04
Loss = 1.0136e-02, PNorm = 53.2716, GNorm = 1.6212, lr_0 = 9.8373e-04
Loss = 1.0793e-02, PNorm = 53.3815, GNorm = 4.6138, lr_0 = 9.7938e-04
Validation rmse logD = 0.894569
Validation R2 logD = 0.442672
Epoch 3
Train function
Loss = 1.0166e-02, PNorm = 53.4969, GNorm = 1.6144, lr_0 = 9.7462e-04
Loss = 9.6994e-03, PNorm = 53.6423, GNorm = 1.4052, lr_0 = 9.7030e-04
Loss = 8.6344e-03, PNorm = 53.7689, GNorm = 1.8854, lr_0 = 9.6601e-04
Loss = 1.0270e-02, PNorm = 53.8408, GNorm = 1.3735, lr_0 = 9.6174e-04
Loss = 8.3002e-03, PNorm = 53.9423, GNorm = 1.5362, lr_0 = 9.5749e-04
Loss = 8.5086e-03, PNorm = 54.0489, GNorm = 2.9941, lr_0 = 9.5325e-04
Validation rmse logD = 0.794234
Validation R2 logD = 0.560680
Epoch 4
Train function
Loss = 7.2548e-03, PNorm = 54.1653, GNorm = 1.8379, lr_0 = 9.4861e-04
Loss = 7.7533e-03, PNorm = 54.2647, GNorm = 5.4691, lr_0 = 9.4442e-04
Loss = 7.6669e-03, PNorm = 54.3596, GNorm = 5.8842, lr_0 = 9.4024e-04
Loss = 7.0010e-03, PNorm = 54.4652, GNorm = 2.5525, lr_0 = 9.3608e-04
Loss = 6.7606e-03, PNorm = 54.5435, GNorm = 0.9371, lr_0 = 9.3194e-04
Validation rmse logD = 0.746618
Validation R2 logD = 0.611778
Epoch 5
Train function
Loss = 6.8718e-03, PNorm = 54.6288, GNorm = 3.0316, lr_0 = 9.2741e-04
Loss = 5.7005e-03, PNorm = 54.7312, GNorm = 1.1980, lr_0 = 9.2330e-04
Loss = 5.7870e-03, PNorm = 54.8254, GNorm = 5.6039, lr_0 = 9.1922e-04
Loss = 7.0622e-03, PNorm = 54.9100, GNorm = 0.7847, lr_0 = 9.1515e-04
Loss = 6.7985e-03, PNorm = 55.0126, GNorm = 1.6045, lr_0 = 9.1111e-04
Validation rmse logD = 0.670093
Validation R2 logD = 0.687281
Epoch 6
Train function
Loss = 7.4027e-03, PNorm = 55.1081, GNorm = 2.8325, lr_0 = 9.0667e-04
Loss = 5.3985e-03, PNorm = 55.2359, GNorm = 1.3529, lr_0 = 9.0266e-04
Loss = 5.1930e-03, PNorm = 55.3194, GNorm = 1.3565, lr_0 = 8.9867e-04
Loss = 4.7900e-03, PNorm = 55.4093, GNorm = 0.9738, lr_0 = 8.9469e-04
Loss = 5.6248e-03, PNorm = 55.4889, GNorm = 3.0405, lr_0 = 8.9074e-04
Loss = 5.2188e-03, PNorm = 55.5670, GNorm = 1.1133, lr_0 = 8.8680e-04
Validation rmse logD = 0.666544
Validation R2 logD = 0.690585
Epoch 7
Train function
Loss = 4.0878e-03, PNorm = 55.6518, GNorm = 2.7807, lr_0 = 8.8248e-04
Loss = 4.8391e-03, PNorm = 55.7285, GNorm = 1.6605, lr_0 = 8.7858e-04
Loss = 4.7598e-03, PNorm = 55.8081, GNorm = 1.5350, lr_0 = 8.7469e-04
Loss = 4.4146e-03, PNorm = 55.8850, GNorm = 0.8209, lr_0 = 8.7082e-04
Loss = 4.6398e-03, PNorm = 55.9565, GNorm = 1.3068, lr_0 = 8.6697e-04
Validation rmse logD = 0.692054
Validation R2 logD = 0.666448
Epoch 8
Train function
Loss = 4.5467e-03, PNorm = 56.0544, GNorm = 1.4215, lr_0 = 8.6276e-04
Loss = 4.7653e-03, PNorm = 56.1459, GNorm = 3.5328, lr_0 = 8.5894e-04
Loss = 4.4622e-03, PNorm = 56.2204, GNorm = 1.3051, lr_0 = 8.5514e-04
Loss = 4.4017e-03, PNorm = 56.3006, GNorm = 2.2755, lr_0 = 8.5136e-04
Loss = 4.1381e-03, PNorm = 56.3753, GNorm = 1.0324, lr_0 = 8.4759e-04
Validation rmse logD = 0.622735
Validation R2 logD = 0.729922
Epoch 9
Train function
Loss = 3.1650e-03, PNorm = 56.4476, GNorm = 1.5312, lr_0 = 8.4384e-04
Loss = 3.5905e-03, PNorm = 56.5147, GNorm = 1.2473, lr_0 = 8.4011e-04
Loss = 4.7610e-03, PNorm = 56.6016, GNorm = 3.7717, lr_0 = 8.3639e-04
Loss = 3.8877e-03, PNorm = 56.6729, GNorm = 2.1685, lr_0 = 8.3269e-04
Loss = 2.9812e-03, PNorm = 56.7526, GNorm = 0.8469, lr_0 = 8.2901e-04
Loss = 3.3901e-03, PNorm = 56.8119, GNorm = 0.7269, lr_0 = 8.2534e-04
Validation rmse logD = 0.577309
Validation R2 logD = 0.767886
Epoch 10
Train function
Loss = 3.2472e-03, PNorm = 56.8746, GNorm = 2.6367, lr_0 = 8.2133e-04
Loss = 3.0946e-03, PNorm = 56.9264, GNorm = 1.4518, lr_0 = 8.1770e-04
Loss = 3.0358e-03, PNorm = 56.9911, GNorm = 0.8701, lr_0 = 8.1408e-04
Loss = 3.8216e-03, PNorm = 57.0715, GNorm = 2.7193, lr_0 = 8.1048e-04
Loss = 3.3837e-03, PNorm = 57.1567, GNorm = 2.3818, lr_0 = 8.0689e-04
Validation rmse logD = 0.613941
Validation R2 logD = 0.737495
Epoch 11
Train function
Loss = 3.1713e-03, PNorm = 57.2241, GNorm = 1.5507, lr_0 = 8.0297e-04
Loss = 3.3915e-03, PNorm = 57.3032, GNorm = 1.1671, lr_0 = 7.9942e-04
Loss = 2.7494e-03, PNorm = 57.3837, GNorm = 1.4289, lr_0 = 7.9588e-04
Loss = 2.7478e-03, PNorm = 57.4514, GNorm = 0.9841, lr_0 = 7.9236e-04
Loss = 2.8292e-03, PNorm = 57.4965, GNorm = 1.0036, lr_0 = 7.8885e-04
Validation rmse logD = 0.622872
Validation R2 logD = 0.729803
Epoch 12
Train function
Loss = 4.0382e-03, PNorm = 57.5649, GNorm = 4.7887, lr_0 = 7.8502e-04
Loss = 3.2562e-03, PNorm = 57.6267, GNorm = 3.5090, lr_0 = 7.8154e-04
Loss = 3.0518e-03, PNorm = 57.7030, GNorm = 2.8626, lr_0 = 7.7809e-04
Loss = 2.2545e-03, PNorm = 57.7648, GNorm = 2.5076, lr_0 = 7.7465e-04
Loss = 2.0997e-03, PNorm = 57.8182, GNorm = 0.8969, lr_0 = 7.7122e-04
Loss = 2.5198e-03, PNorm = 57.8670, GNorm = 2.4852, lr_0 = 7.6781e-04
Loss = 1.0324e-02, PNorm = 57.8718, GNorm = 1.8675, lr_0 = 7.6747e-04
Validation rmse logD = 0.582791
Validation R2 logD = 0.763457
Epoch 13
Train function
Loss = 2.6254e-03, PNorm = 57.9274, GNorm = 4.8325, lr_0 = 7.6407e-04
Loss = 2.8734e-03, PNorm = 57.9890, GNorm = 2.9644, lr_0 = 7.6069e-04
Loss = 2.7547e-03, PNorm = 58.0506, GNorm = 2.8606, lr_0 = 7.5733e-04
Loss = 2.2637e-03, PNorm = 58.1167, GNorm = 1.4265, lr_0 = 7.5398e-04
Loss = 2.6037e-03, PNorm = 58.1679, GNorm = 0.7313, lr_0 = 7.5064e-04
Validation rmse logD = 0.575209
Validation R2 logD = 0.769573
Epoch 14
Train function
Loss = 1.9642e-03, PNorm = 58.2211, GNorm = 1.3147, lr_0 = 7.4699e-04
Loss = 1.9821e-03, PNorm = 58.2695, GNorm = 1.0034, lr_0 = 7.4369e-04
Loss = 1.8058e-03, PNorm = 58.3198, GNorm = 2.7677, lr_0 = 7.4040e-04
Loss = 2.1403e-03, PNorm = 58.3651, GNorm = 1.1436, lr_0 = 7.3712e-04
Loss = 2.0104e-03, PNorm = 58.4146, GNorm = 1.7116, lr_0 = 7.3386e-04
Validation rmse logD = 0.610252
Validation R2 logD = 0.740640
Epoch 15
Train function
Loss = 2.1977e-03, PNorm = 58.4633, GNorm = 2.0405, lr_0 = 7.3029e-04
Loss = 2.2372e-03, PNorm = 58.5163, GNorm = 3.0000, lr_0 = 7.2706e-04
Loss = 2.0334e-03, PNorm = 58.5866, GNorm = 2.2184, lr_0 = 7.2385e-04
Loss = 1.9734e-03, PNorm = 58.6434, GNorm = 1.2676, lr_0 = 7.2064e-04
Loss = 2.2264e-03, PNorm = 58.6972, GNorm = 1.0517, lr_0 = 7.1746e-04
Validation rmse logD = 0.563178
Validation R2 logD = 0.779111
Epoch 16
Train function
Loss = 1.6302e-03, PNorm = 58.7465, GNorm = 0.6597, lr_0 = 7.1397e-04
Loss = 1.4383e-03, PNorm = 58.7975, GNorm = 0.9948, lr_0 = 7.1081e-04
Loss = 1.5595e-03, PNorm = 58.8417, GNorm = 0.8153, lr_0 = 7.0766e-04
Loss = 1.5750e-03, PNorm = 58.8768, GNorm = 1.1579, lr_0 = 7.0453e-04
Loss = 1.3138e-03, PNorm = 58.9224, GNorm = 1.1196, lr_0 = 7.0142e-04
Loss = 1.7671e-03, PNorm = 58.9657, GNorm = 0.9147, lr_0 = 6.9831e-04
Validation rmse logD = 0.562821
Validation R2 logD = 0.779391
Epoch 17
Train function
Loss = 1.2090e-03, PNorm = 59.0054, GNorm = 0.9498, lr_0 = 6.9523e-04
Loss = 1.4898e-03, PNorm = 59.0485, GNorm = 0.6295, lr_0 = 6.9215e-04
Loss = 1.3039e-03, PNorm = 59.0900, GNorm = 1.3189, lr_0 = 6.8909e-04
Loss = 1.4593e-03, PNorm = 59.1250, GNorm = 0.5778, lr_0 = 6.8604e-04
Loss = 1.4723e-03, PNorm = 59.1620, GNorm = 0.5878, lr_0 = 6.8301e-04
Validation rmse logD = 0.574681
Validation R2 logD = 0.769996
Epoch 18
Train function
Loss = 1.3630e-03, PNorm = 59.2060, GNorm = 2.5942, lr_0 = 6.7968e-04
Loss = 1.2861e-03, PNorm = 59.2571, GNorm = 0.9283, lr_0 = 6.7668e-04
Loss = 1.2323e-03, PNorm = 59.2907, GNorm = 1.3298, lr_0 = 6.7368e-04
Loss = 1.3333e-03, PNorm = 59.3365, GNorm = 1.1871, lr_0 = 6.7070e-04
Loss = 1.2847e-03, PNorm = 59.3797, GNorm = 1.5553, lr_0 = 6.6774e-04
Validation rmse logD = 0.605430
Validation R2 logD = 0.744724
Epoch 19
Train function
Loss = 1.4728e-03, PNorm = 59.4154, GNorm = 1.8558, lr_0 = 6.6449e-04
Loss = 1.7891e-03, PNorm = 59.4587, GNorm = 3.2608, lr_0 = 6.6155e-04
Loss = 1.7408e-03, PNorm = 59.5210, GNorm = 3.0120, lr_0 = 6.5862e-04
Loss = 1.5502e-03, PNorm = 59.5910, GNorm = 1.6324, lr_0 = 6.5571e-04
Loss = 1.2909e-03, PNorm = 59.6468, GNorm = 0.9454, lr_0 = 6.5281e-04
Loss = 1.2344e-03, PNorm = 59.6828, GNorm = 0.8743, lr_0 = 6.4992e-04
Validation rmse logD = 0.604047
Validation R2 logD = 0.745888
Epoch 20
Train function
Loss = 1.8213e-03, PNorm = 59.7357, GNorm = 2.2609, lr_0 = 6.4676e-04
Loss = 1.1904e-03, PNorm = 59.7842, GNorm = 0.8469, lr_0 = 6.4390e-04
Loss = 1.4374e-03, PNorm = 59.8254, GNorm = 1.3138, lr_0 = 6.4105e-04
Loss = 1.1797e-03, PNorm = 59.8537, GNorm = 1.4725, lr_0 = 6.3822e-04
Loss = 1.2060e-03, PNorm = 59.8962, GNorm = 1.0423, lr_0 = 6.3539e-04
Validation rmse logD = 0.569283
Validation R2 logD = 0.774296
Epoch 21
Train function
Loss = 1.3388e-03, PNorm = 59.9264, GNorm = 2.7821, lr_0 = 6.3230e-04
Loss = 1.3591e-03, PNorm = 59.9655, GNorm = 1.7466, lr_0 = 6.2950e-04
Loss = 1.3776e-03, PNorm = 60.0001, GNorm = 0.8281, lr_0 = 6.2672e-04
Loss = 1.5970e-03, PNorm = 60.0340, GNorm = 2.0806, lr_0 = 6.2395e-04
Loss = 1.3326e-03, PNorm = 60.0637, GNorm = 1.6020, lr_0 = 6.2119e-04
Validation rmse logD = 0.564937
Validation R2 logD = 0.777728
Epoch 22
Train function
Loss = 1.5721e-03, PNorm = 60.1143, GNorm = 0.8258, lr_0 = 6.1817e-04
Loss = 9.5486e-04, PNorm = 60.1540, GNorm = 1.0868, lr_0 = 6.1543e-04
Loss = 1.0258e-03, PNorm = 60.1831, GNorm = 0.5605, lr_0 = 6.1271e-04
Loss = 9.7687e-04, PNorm = 60.2136, GNorm = 1.4279, lr_0 = 6.1000e-04
Loss = 9.3071e-04, PNorm = 60.2407, GNorm = 0.4873, lr_0 = 6.0730e-04
Loss = 8.7679e-04, PNorm = 60.2676, GNorm = 1.2202, lr_0 = 6.0461e-04
Validation rmse logD = 0.595089
Validation R2 logD = 0.753370
Epoch 23
Train function
Loss = 1.1029e-03, PNorm = 60.2976, GNorm = 0.4842, lr_0 = 6.0167e-04
Loss = 8.1230e-04, PNorm = 60.3276, GNorm = 0.5155, lr_0 = 5.9901e-04
Loss = 9.2352e-04, PNorm = 60.3535, GNorm = 0.6368, lr_0 = 5.9636e-04
Loss = 1.0113e-03, PNorm = 60.3863, GNorm = 0.7436, lr_0 = 5.9372e-04
Loss = 8.2464e-04, PNorm = 60.4112, GNorm = 0.5607, lr_0 = 5.9110e-04
Validation rmse logD = 0.543984
Validation R2 logD = 0.793911
Epoch 24
Train function
Loss = 1.1456e-03, PNorm = 60.4381, GNorm = 2.2440, lr_0 = 5.8822e-04
Loss = 1.0144e-03, PNorm = 60.4693, GNorm = 2.5500, lr_0 = 5.8562e-04
Loss = 9.9136e-04, PNorm = 60.5120, GNorm = 2.0491, lr_0 = 5.8303e-04
Loss = 9.1661e-04, PNorm = 60.5404, GNorm = 2.4168, lr_0 = 5.8045e-04
Loss = 1.1129e-03, PNorm = 60.5640, GNorm = 0.9666, lr_0 = 5.7788e-04
Validation rmse logD = 0.575180
Validation R2 logD = 0.769596
Epoch 25
Train function
Loss = 8.3699e-04, PNorm = 60.5987, GNorm = 1.3181, lr_0 = 5.7533e-04
Loss = 9.1742e-04, PNorm = 60.6289, GNorm = 0.9549, lr_0 = 5.7278e-04
Loss = 7.9164e-04, PNorm = 60.6642, GNorm = 0.7963, lr_0 = 5.7025e-04
Loss = 8.0871e-04, PNorm = 60.6901, GNorm = 0.8248, lr_0 = 5.6773e-04
Loss = 8.6918e-04, PNorm = 60.7064, GNorm = 0.7384, lr_0 = 5.6522e-04
Loss = 8.2563e-04, PNorm = 60.7309, GNorm = 0.7791, lr_0 = 5.6272e-04
Validation rmse logD = 0.567481
Validation R2 logD = 0.775723
Epoch 26
Train function
Loss = 6.1297e-04, PNorm = 60.7516, GNorm = 0.6478, lr_0 = 5.5998e-04
Loss = 5.3318e-04, PNorm = 60.7755, GNorm = 0.5624, lr_0 = 5.5750e-04
Loss = 6.2669e-04, PNorm = 60.7940, GNorm = 0.5373, lr_0 = 5.5503e-04
Loss = 7.8426e-04, PNorm = 60.8191, GNorm = 1.0426, lr_0 = 5.5258e-04
Loss = 8.3399e-04, PNorm = 60.8413, GNorm = 1.2551, lr_0 = 5.5014e-04
Validation rmse logD = 0.573539
Validation R2 logD = 0.770908
Epoch 27
Train function
Loss = 1.1389e-03, PNorm = 60.8764, GNorm = 1.6590, lr_0 = 5.4746e-04
Loss = 9.6841e-04, PNorm = 60.9052, GNorm = 2.4625, lr_0 = 5.4504e-04
Loss = 9.1839e-04, PNorm = 60.9342, GNorm = 1.4019, lr_0 = 5.4263e-04
Loss = 6.8723e-04, PNorm = 60.9621, GNorm = 0.8024, lr_0 = 5.4023e-04
Loss = 5.7082e-04, PNorm = 60.9803, GNorm = 0.6178, lr_0 = 5.3784e-04
Validation rmse logD = 0.550941
Validation R2 logD = 0.788606
Epoch 28
Train function
Loss = 6.7277e-04, PNorm = 60.9940, GNorm = 1.1741, lr_0 = 5.3522e-04
Loss = 5.7030e-04, PNorm = 61.0165, GNorm = 0.7225, lr_0 = 5.3285e-04
Loss = 5.8145e-04, PNorm = 61.0337, GNorm = 0.8418, lr_0 = 5.3050e-04
Loss = 5.6451e-04, PNorm = 61.0552, GNorm = 0.6242, lr_0 = 5.2815e-04
Loss = 6.3590e-04, PNorm = 61.0774, GNorm = 0.7900, lr_0 = 5.2581e-04
Loss = 5.4242e-04, PNorm = 61.0964, GNorm = 0.4009, lr_0 = 5.2349e-04
Loss = 4.1241e-03, PNorm = 61.0991, GNorm = 1.6048, lr_0 = 5.2326e-04
Validation rmse logD = 0.549081
Validation R2 logD = 0.790030
Epoch 29
Train function
Loss = 5.2875e-04, PNorm = 61.1200, GNorm = 0.3080, lr_0 = 5.2094e-04
Loss = 5.2530e-04, PNorm = 61.1411, GNorm = 0.6188, lr_0 = 5.1864e-04
Loss = 5.8481e-04, PNorm = 61.1551, GNorm = 0.9146, lr_0 = 5.1634e-04
Loss = 5.8217e-04, PNorm = 61.1756, GNorm = 1.3422, lr_0 = 5.1406e-04
Loss = 5.7427e-04, PNorm = 61.1944, GNorm = 0.5726, lr_0 = 5.1178e-04
Validation rmse logD = 0.549014
Validation R2 logD = 0.790082
Epoch 30
Train function
Loss = 4.3406e-04, PNorm = 61.2076, GNorm = 0.4706, lr_0 = 5.0930e-04
Loss = 5.3028e-04, PNorm = 61.2263, GNorm = 0.5747, lr_0 = 5.0704e-04
Loss = 4.7810e-04, PNorm = 61.2447, GNorm = 0.4395, lr_0 = 5.0480e-04
Loss = 4.6719e-04, PNorm = 61.2548, GNorm = 0.4243, lr_0 = 5.0257e-04
Loss = 5.7566e-04, PNorm = 61.2730, GNorm = 0.5345, lr_0 = 5.0034e-04
Validation rmse logD = 0.555171
Validation R2 logD = 0.785348
Epoch 31
Train function
Loss = 3.6722e-04, PNorm = 61.2935, GNorm = 0.9214, lr_0 = 4.9791e-04
Loss = 4.9823e-04, PNorm = 61.3075, GNorm = 0.4429, lr_0 = 4.9571e-04
Loss = 4.8309e-04, PNorm = 61.3228, GNorm = 1.0537, lr_0 = 4.9351e-04
Loss = 3.7789e-04, PNorm = 61.3418, GNorm = 0.7353, lr_0 = 4.9133e-04
Loss = 4.6082e-04, PNorm = 61.3580, GNorm = 0.5105, lr_0 = 4.8916e-04
Validation rmse logD = 0.544571
Validation R2 logD = 0.793466
Epoch 32
Train function
Loss = 2.1300e-04, PNorm = 61.3702, GNorm = 0.3517, lr_0 = 4.8678e-04
Loss = 3.6295e-04, PNorm = 61.3898, GNorm = 0.3051, lr_0 = 4.8463e-04
Loss = 3.4700e-04, PNorm = 61.4038, GNorm = 0.2542, lr_0 = 4.8248e-04
Loss = 4.3417e-04, PNorm = 61.4158, GNorm = 0.2883, lr_0 = 4.8035e-04
Loss = 3.1472e-04, PNorm = 61.4262, GNorm = 0.2737, lr_0 = 4.7822e-04
Loss = 5.0458e-04, PNorm = 61.4441, GNorm = 0.2907, lr_0 = 4.7611e-04
Validation rmse logD = 0.553360
Validation R2 logD = 0.786745
Epoch 33
Train function
Loss = 3.5494e-04, PNorm = 61.4593, GNorm = 1.0340, lr_0 = 4.7379e-04
Loss = 3.4194e-04, PNorm = 61.4739, GNorm = 0.7911, lr_0 = 4.7170e-04
Loss = 3.6487e-04, PNorm = 61.4874, GNorm = 0.3433, lr_0 = 4.6961e-04
Loss = 3.7186e-04, PNorm = 61.5003, GNorm = 0.3597, lr_0 = 4.6753e-04
Loss = 5.3962e-04, PNorm = 61.5194, GNorm = 0.3244, lr_0 = 4.6546e-04
Validation rmse logD = 0.554818
Validation R2 logD = 0.785620
Epoch 34
Train function
Loss = 3.5725e-04, PNorm = 61.5349, GNorm = 0.7129, lr_0 = 4.6341e-04
Loss = 3.9812e-04, PNorm = 61.5508, GNorm = 0.6178, lr_0 = 4.6136e-04
Loss = 3.0453e-04, PNorm = 61.5661, GNorm = 0.2587, lr_0 = 4.5931e-04
Loss = 3.8822e-04, PNorm = 61.5802, GNorm = 0.8547, lr_0 = 4.5728e-04
Loss = 3.9283e-04, PNorm = 61.5951, GNorm = 0.6899, lr_0 = 4.5526e-04
Validation rmse logD = 0.551907
Validation R2 logD = 0.787864
Epoch 35
Train function
Loss = 4.4982e-04, PNorm = 61.6119, GNorm = 1.1514, lr_0 = 4.5305e-04
Loss = 2.7828e-04, PNorm = 61.6230, GNorm = 0.3158, lr_0 = 4.5104e-04
Loss = 2.5694e-04, PNorm = 61.6307, GNorm = 0.2943, lr_0 = 4.4905e-04
Loss = 3.3660e-04, PNorm = 61.6395, GNorm = 0.2339, lr_0 = 4.4706e-04
Loss = 3.2061e-04, PNorm = 61.6514, GNorm = 1.0438, lr_0 = 4.4508e-04
Loss = 3.0913e-04, PNorm = 61.6620, GNorm = 0.4330, lr_0 = 4.4311e-04
Validation rmse logD = 0.543120
Validation R2 logD = 0.794565
Epoch 36
Train function
Loss = 3.0027e-04, PNorm = 61.6820, GNorm = 0.3828, lr_0 = 4.4096e-04
Loss = 3.1840e-04, PNorm = 61.6940, GNorm = 0.6803, lr_0 = 4.3901e-04
Loss = 3.7412e-04, PNorm = 61.7108, GNorm = 0.9438, lr_0 = 4.3707e-04
Loss = 3.3296e-04, PNorm = 61.7264, GNorm = 0.6456, lr_0 = 4.3513e-04
Loss = 3.0424e-04, PNorm = 61.7399, GNorm = 0.3631, lr_0 = 4.3321e-04
Validation rmse logD = 0.550298
Validation R2 logD = 0.789099
Epoch 37
Train function
Loss = 2.6807e-04, PNorm = 61.7549, GNorm = 0.9993, lr_0 = 4.3110e-04
Loss = 2.6925e-04, PNorm = 61.7656, GNorm = 0.4254, lr_0 = 4.2919e-04
Loss = 3.3949e-04, PNorm = 61.7782, GNorm = 0.5007, lr_0 = 4.2729e-04
Loss = 2.6316e-04, PNorm = 61.7929, GNorm = 0.2413, lr_0 = 4.2540e-04
Loss = 2.4962e-04, PNorm = 61.8002, GNorm = 0.2794, lr_0 = 4.2352e-04
Validation rmse logD = 0.549557
Validation R2 logD = 0.789666
Epoch 38
Train function
Loss = 1.5706e-04, PNorm = 61.8151, GNorm = 0.2996, lr_0 = 4.2146e-04
Loss = 2.6797e-04, PNorm = 61.8260, GNorm = 0.7510, lr_0 = 4.1960e-04
Loss = 2.5424e-04, PNorm = 61.8387, GNorm = 0.7429, lr_0 = 4.1774e-04
Loss = 2.9554e-04, PNorm = 61.8507, GNorm = 0.2914, lr_0 = 4.1589e-04
Loss = 2.1328e-04, PNorm = 61.8595, GNorm = 0.2380, lr_0 = 4.1406e-04
Loss = 2.6407e-04, PNorm = 61.8696, GNorm = 0.4141, lr_0 = 4.1222e-04
Validation rmse logD = 0.555687
Validation R2 logD = 0.784948
Epoch 39
Train function
Loss = 2.7815e-04, PNorm = 61.8815, GNorm = 0.2410, lr_0 = 4.1022e-04
Loss = 2.3824e-04, PNorm = 61.8902, GNorm = 0.6144, lr_0 = 4.0840e-04
Loss = 2.2401e-04, PNorm = 61.9026, GNorm = 0.6769, lr_0 = 4.0660e-04
Loss = 2.6043e-04, PNorm = 61.9148, GNorm = 0.2336, lr_0 = 4.0480e-04
Loss = 2.8300e-04, PNorm = 61.9305, GNorm = 0.6740, lr_0 = 4.0301e-04
Validation rmse logD = 0.547734
Validation R2 logD = 0.791059
Epoch 40
Train function
Loss = 1.7072e-04, PNorm = 61.9418, GNorm = 0.5368, lr_0 = 4.0105e-04
Loss = 2.7056e-04, PNorm = 61.9564, GNorm = 1.1041, lr_0 = 3.9927e-04
Loss = 2.1932e-04, PNorm = 61.9687, GNorm = 1.1297, lr_0 = 3.9751e-04
Loss = 3.3323e-04, PNorm = 61.9781, GNorm = 0.4223, lr_0 = 3.9575e-04
Loss = 2.4141e-04, PNorm = 61.9922, GNorm = 0.6242, lr_0 = 3.9400e-04
Validation rmse logD = 0.556650
Validation R2 logD = 0.784202
Epoch 41
Train function
Loss = 4.4379e-04, PNorm = 62.0061, GNorm = 1.2946, lr_0 = 3.9208e-04
Loss = 3.1512e-04, PNorm = 62.0193, GNorm = 0.5089, lr_0 = 3.9035e-04
Loss = 3.6562e-04, PNorm = 62.0377, GNorm = 0.4194, lr_0 = 3.8862e-04
Loss = 2.8379e-04, PNorm = 62.0528, GNorm = 0.3416, lr_0 = 3.8690e-04
Loss = 2.8840e-04, PNorm = 62.0688, GNorm = 0.3895, lr_0 = 3.8519e-04
Loss = 3.5398e-04, PNorm = 62.0791, GNorm = 0.8981, lr_0 = 3.8349e-04
Validation rmse logD = 0.549861
Validation R2 logD = 0.789433
Epoch 42
Train function
Loss = 2.1612e-04, PNorm = 62.0876, GNorm = 0.6933, lr_0 = 3.8179e-04
Loss = 2.3927e-04, PNorm = 62.0980, GNorm = 0.3102, lr_0 = 3.8010e-04
Loss = 2.2912e-04, PNorm = 62.1089, GNorm = 0.5689, lr_0 = 3.7842e-04
Loss = 2.1961e-04, PNorm = 62.1209, GNorm = 0.6594, lr_0 = 3.7675e-04
Loss = 2.2842e-04, PNorm = 62.1292, GNorm = 0.3824, lr_0 = 3.7508e-04
Validation rmse logD = 0.547459
Validation R2 logD = 0.791269
Epoch 43
Train function
Loss = 1.5263e-04, PNorm = 62.1392, GNorm = 0.6235, lr_0 = 3.7326e-04
Loss = 1.8791e-04, PNorm = 62.1470, GNorm = 0.2308, lr_0 = 3.7160e-04
Loss = 1.8470e-04, PNorm = 62.1518, GNorm = 0.3014, lr_0 = 3.6996e-04
Loss = 2.1160e-04, PNorm = 62.1593, GNorm = 0.7474, lr_0 = 3.6832e-04
Loss = 1.8380e-04, PNorm = 62.1686, GNorm = 0.6933, lr_0 = 3.6669e-04
Validation rmse logD = 0.550175
Validation R2 logD = 0.789193
Epoch 44
Train function
Loss = 1.7461e-04, PNorm = 62.1772, GNorm = 0.5994, lr_0 = 3.6491e-04
Loss = 1.4763e-04, PNorm = 62.1856, GNorm = 0.5529, lr_0 = 3.6330e-04
Loss = 1.6923e-04, PNorm = 62.1913, GNorm = 0.5881, lr_0 = 3.6169e-04
Loss = 2.1094e-04, PNorm = 62.1990, GNorm = 0.3047, lr_0 = 3.6009e-04
Loss = 1.7958e-04, PNorm = 62.2133, GNorm = 0.2444, lr_0 = 3.5850e-04
Loss = 1.3771e-04, PNorm = 62.2198, GNorm = 0.1861, lr_0 = 3.5691e-04
Loss = 1.4311e-03, PNorm = 62.2200, GNorm = 0.6854, lr_0 = 3.5675e-04
Validation rmse logD = 0.547778
Validation R2 logD = 0.791026
Epoch 45
Train function
Loss = 1.5340e-04, PNorm = 62.2276, GNorm = 0.2974, lr_0 = 3.5518e-04
Loss = 1.5299e-04, PNorm = 62.2386, GNorm = 0.3078, lr_0 = 3.5360e-04
Loss = 1.7195e-04, PNorm = 62.2475, GNorm = 0.4781, lr_0 = 3.5204e-04
Loss = 1.4209e-04, PNorm = 62.2544, GNorm = 0.5141, lr_0 = 3.5048e-04
Loss = 2.0940e-04, PNorm = 62.2593, GNorm = 0.2666, lr_0 = 3.4893e-04
Validation rmse logD = 0.551357
Validation R2 logD = 0.788286
Epoch 46
Train function
Loss = 1.1519e-04, PNorm = 62.2700, GNorm = 0.4460, lr_0 = 3.4724e-04
Loss = 2.0129e-04, PNorm = 62.2793, GNorm = 0.6289, lr_0 = 3.4570e-04
Loss = 1.2381e-04, PNorm = 62.2913, GNorm = 0.2301, lr_0 = 3.4417e-04
Loss = 1.3875e-04, PNorm = 62.2977, GNorm = 0.4065, lr_0 = 3.4265e-04
Loss = 1.2281e-04, PNorm = 62.3052, GNorm = 0.3250, lr_0 = 3.4113e-04
Validation rmse logD = 0.545495
Validation R2 logD = 0.792764
Epoch 47
Train function
Loss = 1.4593e-04, PNorm = 62.3106, GNorm = 0.8959, lr_0 = 3.3947e-04
Loss = 1.3888e-04, PNorm = 62.3194, GNorm = 0.3492, lr_0 = 3.3797e-04
Loss = 1.7469e-04, PNorm = 62.3249, GNorm = 0.2389, lr_0 = 3.3648e-04
Loss = 1.1760e-04, PNorm = 62.3298, GNorm = 0.7832, lr_0 = 3.3499e-04
Loss = 2.1142e-04, PNorm = 62.3358, GNorm = 0.9243, lr_0 = 3.3351e-04
Validation rmse logD = 0.561341
Validation R2 logD = 0.780549
Epoch 48
Train function
Loss = 2.7145e-04, PNorm = 62.3465, GNorm = 1.1550, lr_0 = 3.3188e-04
Loss = 2.1426e-04, PNorm = 62.3539, GNorm = 0.9429, lr_0 = 3.3042e-04
Loss = 1.6636e-04, PNorm = 62.3612, GNorm = 0.5623, lr_0 = 3.2895e-04
Loss = 1.5298e-04, PNorm = 62.3709, GNorm = 0.3880, lr_0 = 3.2750e-04
Loss = 1.8099e-04, PNorm = 62.3829, GNorm = 0.4458, lr_0 = 3.2605e-04
Loss = 1.8047e-04, PNorm = 62.3928, GNorm = 0.7537, lr_0 = 3.2461e-04
Validation rmse logD = 0.550765
Validation R2 logD = 0.788741
Epoch 49
Train function
Loss = 1.4021e-04, PNorm = 62.4058, GNorm = 0.6129, lr_0 = 3.2303e-04
Loss = 1.1569e-04, PNorm = 62.4111, GNorm = 0.1480, lr_0 = 3.2160e-04
Loss = 1.2984e-04, PNorm = 62.4182, GNorm = 0.3745, lr_0 = 3.2018e-04
Loss = 1.8340e-04, PNorm = 62.4235, GNorm = 0.4020, lr_0 = 3.1876e-04
Loss = 1.3628e-04, PNorm = 62.4295, GNorm = 0.4380, lr_0 = 3.1735e-04
Validation rmse logD = 0.552192
Validation R2 logD = 0.787644
Epoch 50
Train function
Loss = 1.1252e-04, PNorm = 62.4370, GNorm = 0.6467, lr_0 = 3.1595e-04
Loss = 1.5679e-04, PNorm = 62.4470, GNorm = 0.5962, lr_0 = 3.1455e-04
Loss = 1.4280e-04, PNorm = 62.4554, GNorm = 0.5296, lr_0 = 3.1316e-04
Loss = 1.6969e-04, PNorm = 62.4600, GNorm = 0.6345, lr_0 = 3.1177e-04
Loss = 1.1739e-04, PNorm = 62.4669, GNorm = 0.2092, lr_0 = 3.1039e-04
Validation rmse logD = 0.547083
Validation R2 logD = 0.791556
Epoch 51
Train function
Loss = 8.3940e-05, PNorm = 62.4762, GNorm = 0.2018, lr_0 = 3.0888e-04
Loss = 1.2281e-04, PNorm = 62.4848, GNorm = 0.2489, lr_0 = 3.0752e-04
Loss = 1.1552e-04, PNorm = 62.4910, GNorm = 0.4461, lr_0 = 3.0616e-04
Loss = 8.3901e-05, PNorm = 62.4954, GNorm = 0.2384, lr_0 = 3.0480e-04
Loss = 1.2984e-04, PNorm = 62.5024, GNorm = 0.8974, lr_0 = 3.0346e-04
Loss = 1.5249e-04, PNorm = 62.5084, GNorm = 1.0529, lr_0 = 3.0211e-04
Validation rmse logD = 0.551237
Validation R2 logD = 0.788379
Epoch 52
Train function
Loss = 1.4758e-04, PNorm = 62.5157, GNorm = 0.3594, lr_0 = 3.0064e-04
Loss = 1.5478e-04, PNorm = 62.5223, GNorm = 0.2949, lr_0 = 2.9931e-04
Loss = 1.3207e-04, PNorm = 62.5295, GNorm = 0.3185, lr_0 = 2.9799e-04
Loss = 9.7267e-05, PNorm = 62.5363, GNorm = 0.2698, lr_0 = 2.9667e-04
Loss = 9.6169e-05, PNorm = 62.5439, GNorm = 0.2826, lr_0 = 2.9536e-04
Validation rmse logD = 0.547805
Validation R2 logD = 0.791006
Epoch 53
Train function
Loss = 1.1614e-04, PNorm = 62.5471, GNorm = 0.2434, lr_0 = 2.9392e-04
Loss = 1.0362e-04, PNorm = 62.5541, GNorm = 0.3171, lr_0 = 2.9262e-04
Loss = 9.6236e-05, PNorm = 62.5588, GNorm = 0.7477, lr_0 = 2.9133e-04
Loss = 1.2403e-04, PNorm = 62.5647, GNorm = 0.2063, lr_0 = 2.9004e-04
Loss = 1.3401e-04, PNorm = 62.5705, GNorm = 0.3040, lr_0 = 2.8876e-04
Validation rmse logD = 0.547607
Validation R2 logD = 0.791157
Epoch 54
Train function
Loss = 9.4196e-05, PNorm = 62.5805, GNorm = 0.2395, lr_0 = 2.8735e-04
Loss = 9.5741e-05, PNorm = 62.5857, GNorm = 0.3584, lr_0 = 2.8608e-04
Loss = 1.0368e-04, PNorm = 62.5909, GNorm = 0.1590, lr_0 = 2.8482e-04
Loss = 1.0912e-04, PNorm = 62.5949, GNorm = 0.3696, lr_0 = 2.8356e-04
Loss = 1.4537e-04, PNorm = 62.6026, GNorm = 0.3698, lr_0 = 2.8230e-04
Loss = 1.1597e-04, PNorm = 62.6074, GNorm = 0.3220, lr_0 = 2.8105e-04
Validation rmse logD = 0.547501
Validation R2 logD = 0.791237
Epoch 55
Train function
Loss = 7.6533e-05, PNorm = 62.6155, GNorm = 0.1903, lr_0 = 2.7969e-04
Loss = 7.7182e-05, PNorm = 62.6214, GNorm = 0.5293, lr_0 = 2.7845e-04
Loss = 1.0047e-04, PNorm = 62.6249, GNorm = 0.3887, lr_0 = 2.7722e-04
Loss = 1.1408e-04, PNorm = 62.6284, GNorm = 0.5295, lr_0 = 2.7599e-04
Loss = 1.2888e-04, PNorm = 62.6343, GNorm = 0.4240, lr_0 = 2.7477e-04
Validation rmse logD = 0.555648
Validation R2 logD = 0.784979
Epoch 56
Train function
Loss = 9.9324e-05, PNorm = 62.6439, GNorm = 0.4389, lr_0 = 2.7343e-04
Loss = 8.1804e-05, PNorm = 62.6485, GNorm = 0.3439, lr_0 = 2.7222e-04
Loss = 1.1253e-04, PNorm = 62.6545, GNorm = 0.2817, lr_0 = 2.7102e-04
Loss = 6.3312e-05, PNorm = 62.6583, GNorm = 0.2495, lr_0 = 2.6982e-04
Loss = 1.0446e-04, PNorm = 62.6631, GNorm = 0.2159, lr_0 = 2.6863e-04
Validation rmse logD = 0.550392
Validation R2 logD = 0.789027
Epoch 57
Train function
Loss = 7.8996e-05, PNorm = 62.6689, GNorm = 0.2557, lr_0 = 2.6732e-04
Loss = 9.0927e-05, PNorm = 62.6739, GNorm = 0.1462, lr_0 = 2.6614e-04
Loss = 7.6622e-05, PNorm = 62.6805, GNorm = 0.1613, lr_0 = 2.6496e-04
Loss = 9.8235e-05, PNorm = 62.6858, GNorm = 0.2184, lr_0 = 2.6379e-04
Loss = 8.7633e-05, PNorm = 62.6909, GNorm = 0.3237, lr_0 = 2.6262e-04
Loss = 5.7138e-05, PNorm = 62.6960, GNorm = 0.1310, lr_0 = 2.6146e-04
Loss = 3.4378e-04, PNorm = 62.6960, GNorm = 0.3536, lr_0 = 2.6134e-04
Validation rmse logD = 0.551026
Validation R2 logD = 0.788541
Epoch 58
Train function
Loss = 6.0517e-05, PNorm = 62.6974, GNorm = 0.2459, lr_0 = 2.6019e-04
Loss = 6.1071e-05, PNorm = 62.7034, GNorm = 0.1299, lr_0 = 2.5904e-04
Loss = 6.9296e-05, PNorm = 62.7078, GNorm = 0.1921, lr_0 = 2.5789e-04
Loss = 4.8450e-05, PNorm = 62.7124, GNorm = 0.2370, lr_0 = 2.5675e-04
Loss = 7.8157e-05, PNorm = 62.7174, GNorm = 0.1781, lr_0 = 2.5561e-04
Validation rmse logD = 0.550924
Validation R2 logD = 0.788619
Epoch 59
Train function
Loss = 6.6258e-05, PNorm = 62.7233, GNorm = 0.4701, lr_0 = 2.5448e-04
Loss = 6.8345e-05, PNorm = 62.7272, GNorm = 0.3522, lr_0 = 2.5336e-04
Loss = 8.1135e-05, PNorm = 62.7312, GNorm = 0.2015, lr_0 = 2.5224e-04
Loss = 9.8979e-05, PNorm = 62.7345, GNorm = 0.5062, lr_0 = 2.5112e-04
Loss = 5.9722e-05, PNorm = 62.7377, GNorm = 0.1231, lr_0 = 2.5001e-04
Validation rmse logD = 0.549726
Validation R2 logD = 0.789537
Epoch 60
Train function
Loss = 6.7645e-05, PNorm = 62.7410, GNorm = 0.2627, lr_0 = 2.4879e-04
Loss = 5.2130e-05, PNorm = 62.7465, GNorm = 0.1584, lr_0 = 2.4769e-04
Loss = 5.5883e-05, PNorm = 62.7516, GNorm = 0.1208, lr_0 = 2.4660e-04
Loss = 7.1793e-05, PNorm = 62.7557, GNorm = 0.1077, lr_0 = 2.4551e-04
Loss = 5.4264e-05, PNorm = 62.7576, GNorm = 0.1930, lr_0 = 2.4442e-04
Loss = 5.9000e-05, PNorm = 62.7601, GNorm = 0.1465, lr_0 = 2.4334e-04
Loss = 1.0912e-04, PNorm = 62.7604, GNorm = 0.1855, lr_0 = 2.4323e-04
Validation rmse logD = 0.549620
Validation R2 logD = 0.789618
Epoch 61
Train function
Loss = 4.2491e-05, PNorm = 62.7639, GNorm = 0.2953, lr_0 = 2.4216e-04
Loss = 3.9003e-05, PNorm = 62.7682, GNorm = 0.2209, lr_0 = 2.4109e-04
Loss = 5.7331e-05, PNorm = 62.7712, GNorm = 0.1042, lr_0 = 2.4002e-04
Loss = 6.2940e-05, PNorm = 62.7749, GNorm = 0.1891, lr_0 = 2.3896e-04
Loss = 5.3571e-05, PNorm = 62.7792, GNorm = 0.1332, lr_0 = 2.3790e-04
Validation rmse logD = 0.557264
Validation R2 logD = 0.783725
Epoch 62
Train function
Loss = 7.8271e-05, PNorm = 62.7820, GNorm = 0.5580, lr_0 = 2.3674e-04
Loss = 4.8421e-05, PNorm = 62.7845, GNorm = 0.1898, lr_0 = 2.3570e-04
Loss = 8.3105e-05, PNorm = 62.7901, GNorm = 0.2414, lr_0 = 2.3465e-04
Loss = 8.7262e-05, PNorm = 62.7952, GNorm = 0.4831, lr_0 = 2.3362e-04
Loss = 5.0245e-05, PNorm = 62.7998, GNorm = 0.1161, lr_0 = 2.3258e-04
Validation rmse logD = 0.549916
Validation R2 logD = 0.789391
Epoch 63
Train function
Loss = 4.3092e-05, PNorm = 62.8038, GNorm = 0.1198, lr_0 = 2.3145e-04
Loss = 4.5259e-05, PNorm = 62.8069, GNorm = 0.1966, lr_0 = 2.3043e-04
Loss = 5.0993e-05, PNorm = 62.8088, GNorm = 0.2078, lr_0 = 2.2941e-04
Loss = 9.6718e-05, PNorm = 62.8138, GNorm = 0.1925, lr_0 = 2.2839e-04
Loss = 6.4210e-05, PNorm = 62.8186, GNorm = 0.2031, lr_0 = 2.2738e-04
Validation rmse logD = 0.550550
Validation R2 logD = 0.788906
Epoch 64
Train function
Loss = 5.1201e-05, PNorm = 62.8215, GNorm = 0.1524, lr_0 = 2.2628e-04
Loss = 7.3718e-05, PNorm = 62.8276, GNorm = 0.3104, lr_0 = 2.2528e-04
Loss = 5.3161e-05, PNorm = 62.8318, GNorm = 0.4388, lr_0 = 2.2428e-04
Loss = 4.5563e-05, PNorm = 62.8346, GNorm = 0.3489, lr_0 = 2.2329e-04
Loss = 6.5847e-05, PNorm = 62.8399, GNorm = 0.3486, lr_0 = 2.2230e-04
Loss = 5.5875e-05, PNorm = 62.8437, GNorm = 0.1958, lr_0 = 2.2132e-04
Validation rmse logD = 0.550860
Validation R2 logD = 0.788668
Epoch 65
Train function
Loss = 3.9544e-05, PNorm = 62.8469, GNorm = 0.1980, lr_0 = 2.2024e-04
Loss = 5.1247e-05, PNorm = 62.8510, GNorm = 0.1797, lr_0 = 2.1927e-04
Loss = 4.6481e-05, PNorm = 62.8539, GNorm = 0.3000, lr_0 = 2.1830e-04
Loss = 6.4843e-05, PNorm = 62.8580, GNorm = 0.1738, lr_0 = 2.1733e-04
Loss = 4.6100e-05, PNorm = 62.8594, GNorm = 0.1571, lr_0 = 2.1637e-04
Validation rmse logD = 0.550183
Validation R2 logD = 0.789187
Epoch 66
Train function
Loss = 6.5210e-05, PNorm = 62.8636, GNorm = 0.2920, lr_0 = 2.1532e-04
Loss = 4.5696e-05, PNorm = 62.8664, GNorm = 0.2698, lr_0 = 2.1436e-04
Loss = 4.2082e-05, PNorm = 62.8689, GNorm = 0.1978, lr_0 = 2.1342e-04
Loss = 4.1761e-05, PNorm = 62.8721, GNorm = 0.0968, lr_0 = 2.1247e-04
Loss = 3.8743e-05, PNorm = 62.8756, GNorm = 0.1846, lr_0 = 2.1153e-04
Validation rmse logD = 0.554304
Validation R2 logD = 0.786017
Epoch 67
Train function
Loss = 5.5955e-05, PNorm = 62.8812, GNorm = 0.5345, lr_0 = 2.1060e-04
Loss = 6.2261e-05, PNorm = 62.8896, GNorm = 0.7070, lr_0 = 2.0966e-04
Loss = 5.7393e-05, PNorm = 62.8947, GNorm = 0.2428, lr_0 = 2.0874e-04
Loss = 6.4574e-05, PNorm = 62.8995, GNorm = 0.3054, lr_0 = 2.0781e-04
Loss = 7.9905e-05, PNorm = 62.9033, GNorm = 0.3679, lr_0 = 2.0689e-04
Loss = 5.7159e-05, PNorm = 62.9061, GNorm = 0.4171, lr_0 = 2.0598e-04
Validation rmse logD = 0.551735
Validation R2 logD = 0.787996
Epoch 68
Train function
Loss = 5.0360e-05, PNorm = 62.9103, GNorm = 0.3212, lr_0 = 2.0498e-04
Loss = 4.3446e-05, PNorm = 62.9140, GNorm = 0.2047, lr_0 = 2.0407e-04
Loss = 4.2561e-05, PNorm = 62.9155, GNorm = 0.3103, lr_0 = 2.0317e-04
Loss = 5.0518e-05, PNorm = 62.9199, GNorm = 0.1589, lr_0 = 2.0227e-04
Loss = 3.9416e-05, PNorm = 62.9221, GNorm = 0.1105, lr_0 = 2.0137e-04
Validation rmse logD = 0.549215
Validation R2 logD = 0.789928
Epoch 69
Train function
Loss = 4.2754e-05, PNorm = 62.9255, GNorm = 0.1481, lr_0 = 2.0039e-04
Loss = 4.5895e-05, PNorm = 62.9285, GNorm = 0.3674, lr_0 = 1.9951e-04
Loss = 4.9857e-05, PNorm = 62.9301, GNorm = 0.2279, lr_0 = 1.9863e-04
Loss = 4.3270e-05, PNorm = 62.9328, GNorm = 0.2083, lr_0 = 1.9775e-04
Loss = 4.4490e-05, PNorm = 62.9336, GNorm = 0.2463, lr_0 = 1.9687e-04
Validation rmse logD = 0.551690
Validation R2 logD = 0.788031
Epoch 70
Train function
Loss = 2.4088e-05, PNorm = 62.9371, GNorm = 0.2507, lr_0 = 1.9592e-04
Loss = 3.5881e-05, PNorm = 62.9380, GNorm = 0.1394, lr_0 = 1.9505e-04
Loss = 4.7571e-05, PNorm = 62.9407, GNorm = 0.1682, lr_0 = 1.9419e-04
Loss = 4.2055e-05, PNorm = 62.9453, GNorm = 0.1252, lr_0 = 1.9333e-04
Loss = 3.9064e-05, PNorm = 62.9472, GNorm = 0.1370, lr_0 = 1.9247e-04
Loss = 3.2653e-05, PNorm = 62.9494, GNorm = 0.1544, lr_0 = 1.9162e-04
Validation rmse logD = 0.553079
Validation R2 logD = 0.786962
Epoch 71
Train function
Loss = 4.4318e-05, PNorm = 62.9509, GNorm = 0.2716, lr_0 = 1.9069e-04
Loss = 4.3044e-05, PNorm = 62.9539, GNorm = 0.4072, lr_0 = 1.8984e-04
Loss = 5.2112e-05, PNorm = 62.9575, GNorm = 0.4277, lr_0 = 1.8900e-04
Loss = 4.4622e-05, PNorm = 62.9618, GNorm = 0.1110, lr_0 = 1.8817e-04
Loss = 4.2990e-05, PNorm = 62.9659, GNorm = 0.2466, lr_0 = 1.8734e-04
Validation rmse logD = 0.554612
Validation R2 logD = 0.785779
Epoch 72
Train function
Loss = 2.9269e-05, PNorm = 62.9682, GNorm = 0.1549, lr_0 = 1.8643e-04
Loss = 2.9296e-05, PNorm = 62.9704, GNorm = 0.1606, lr_0 = 1.8560e-04
Loss = 2.6221e-05, PNorm = 62.9734, GNorm = 0.1514, lr_0 = 1.8478e-04
Loss = 3.5446e-05, PNorm = 62.9751, GNorm = 0.0912, lr_0 = 1.8396e-04
Loss = 3.6869e-05, PNorm = 62.9757, GNorm = 0.3015, lr_0 = 1.8315e-04
Validation rmse logD = 0.551545
Validation R2 logD = 0.788142
Epoch 73
Train function
Loss = 3.4356e-05, PNorm = 62.9781, GNorm = 0.2493, lr_0 = 1.8226e-04
Loss = 3.2129e-05, PNorm = 62.9805, GNorm = 0.1666, lr_0 = 1.8145e-04
Loss = 2.7927e-05, PNorm = 62.9833, GNorm = 0.1621, lr_0 = 1.8065e-04
Loss = 3.2508e-05, PNorm = 62.9846, GNorm = 0.2468, lr_0 = 1.7985e-04
Loss = 2.7485e-05, PNorm = 62.9862, GNorm = 0.1376, lr_0 = 1.7905e-04
Loss = 2.2335e-05, PNorm = 62.9876, GNorm = 0.1086, lr_0 = 1.7826e-04
Loss = 2.2517e-05, PNorm = 62.9877, GNorm = 0.0877, lr_0 = 1.7818e-04
Validation rmse logD = 0.552099
Validation R2 logD = 0.787716
Epoch 74
Train function
Loss = 3.2059e-05, PNorm = 62.9901, GNorm = 0.1057, lr_0 = 1.7739e-04
Loss = 2.1779e-05, PNorm = 62.9918, GNorm = 0.1458, lr_0 = 1.7661e-04
Loss = 2.0970e-05, PNorm = 62.9931, GNorm = 0.0640, lr_0 = 1.7583e-04
Loss = 2.4545e-05, PNorm = 62.9968, GNorm = 0.1839, lr_0 = 1.7505e-04
Loss = 1.9043e-05, PNorm = 62.9984, GNorm = 0.1341, lr_0 = 1.7428e-04
Validation rmse logD = 0.554194
Validation R2 logD = 0.786102
Epoch 75
Train function
Loss = 1.2899e-05, PNorm = 62.9994, GNorm = 0.0851, lr_0 = 1.7351e-04
Loss = 3.0717e-05, PNorm = 63.0007, GNorm = 0.1446, lr_0 = 1.7274e-04
Loss = 2.5637e-05, PNorm = 63.0019, GNorm = 0.1012, lr_0 = 1.7197e-04
Loss = 2.8589e-05, PNorm = 63.0034, GNorm = 0.3140, lr_0 = 1.7121e-04
Loss = 2.7864e-05, PNorm = 63.0061, GNorm = 0.3367, lr_0 = 1.7046e-04
Validation rmse logD = 0.554752
Validation R2 logD = 0.785671
Epoch 76
Train function
Loss = 2.3319e-05, PNorm = 63.0080, GNorm = 0.1428, lr_0 = 1.6963e-04
Loss = 2.4600e-05, PNorm = 63.0104, GNorm = 0.0858, lr_0 = 1.6888e-04
Loss = 3.6507e-05, PNorm = 63.0130, GNorm = 0.2404, lr_0 = 1.6813e-04
Loss = 2.9325e-05, PNorm = 63.0139, GNorm = 0.1027, lr_0 = 1.6739e-04
Loss = 2.7947e-05, PNorm = 63.0153, GNorm = 0.0935, lr_0 = 1.6665e-04
Loss = 2.1910e-05, PNorm = 63.0182, GNorm = 0.1547, lr_0 = 1.6591e-04
Loss = 1.2578e-04, PNorm = 63.0184, GNorm = 0.3428, lr_0 = 1.6584e-04
Validation rmse logD = 0.553394
Validation R2 logD = 0.786719
Epoch 77
Train function
Loss = 2.4408e-05, PNorm = 63.0201, GNorm = 0.0868, lr_0 = 1.6510e-04
Loss = 2.3716e-05, PNorm = 63.0210, GNorm = 0.1358, lr_0 = 1.6437e-04
Loss = 2.8684e-05, PNorm = 63.0224, GNorm = 0.1395, lr_0 = 1.6364e-04
Loss = 2.8993e-05, PNorm = 63.0236, GNorm = 0.1265, lr_0 = 1.6292e-04
Loss = 2.9194e-05, PNorm = 63.0254, GNorm = 0.1124, lr_0 = 1.6220e-04
Validation rmse logD = 0.552763
Validation R2 logD = 0.787206
Epoch 78
Train function
Loss = 2.1850e-05, PNorm = 63.0270, GNorm = 0.0998, lr_0 = 1.6141e-04
Loss = 2.3859e-05, PNorm = 63.0284, GNorm = 0.1401, lr_0 = 1.6070e-04
Loss = 2.1506e-05, PNorm = 63.0311, GNorm = 0.0831, lr_0 = 1.5999e-04
Loss = 2.5356e-05, PNorm = 63.0333, GNorm = 0.1361, lr_0 = 1.5928e-04
Loss = 2.8265e-05, PNorm = 63.0353, GNorm = 0.3101, lr_0 = 1.5857e-04
Validation rmse logD = 0.553022
Validation R2 logD = 0.787006
Epoch 79
Train function
Loss = 1.6993e-05, PNorm = 63.0381, GNorm = 0.0937, lr_0 = 1.5780e-04
Loss = 2.0060e-05, PNorm = 63.0395, GNorm = 0.1038, lr_0 = 1.5710e-04
Loss = 1.9332e-05, PNorm = 63.0418, GNorm = 0.0974, lr_0 = 1.5641e-04
Loss = 2.7061e-05, PNorm = 63.0428, GNorm = 0.0675, lr_0 = 1.5572e-04
Loss = 1.8591e-05, PNorm = 63.0442, GNorm = 0.1363, lr_0 = 1.5503e-04
Validation rmse logD = 0.551850
Validation R2 logD = 0.787907
Epoch 80
Train function
Loss = 2.6910e-05, PNorm = 63.0456, GNorm = 0.1398, lr_0 = 1.5427e-04
Loss = 1.8902e-05, PNorm = 63.0474, GNorm = 0.1972, lr_0 = 1.5359e-04
Loss = 2.2009e-05, PNorm = 63.0491, GNorm = 0.1026, lr_0 = 1.5291e-04
Loss = 2.7293e-05, PNorm = 63.0516, GNorm = 0.1243, lr_0 = 1.5224e-04
Loss = 1.7857e-05, PNorm = 63.0534, GNorm = 0.0718, lr_0 = 1.5156e-04
Loss = 1.9432e-05, PNorm = 63.0545, GNorm = 0.2655, lr_0 = 1.5089e-04
Validation rmse logD = 0.552101
Validation R2 logD = 0.787714
Epoch 81
Train function
Loss = 2.0508e-05, PNorm = 63.0566, GNorm = 0.1153, lr_0 = 1.5016e-04
Loss = 2.0294e-05, PNorm = 63.0585, GNorm = 0.1607, lr_0 = 1.4949e-04
Loss = 1.7751e-05, PNorm = 63.0602, GNorm = 0.0910, lr_0 = 1.4883e-04
Loss = 2.5191e-05, PNorm = 63.0619, GNorm = 0.2658, lr_0 = 1.4817e-04
Loss = 2.6524e-05, PNorm = 63.0638, GNorm = 0.1672, lr_0 = 1.4752e-04
Validation rmse logD = 0.554279
Validation R2 logD = 0.786037
Epoch 82
Train function
Loss = 1.8789e-05, PNorm = 63.0659, GNorm = 0.2624, lr_0 = 1.4680e-04
Loss = 2.4127e-05, PNorm = 63.0673, GNorm = 0.2241, lr_0 = 1.4615e-04
Loss = 1.6996e-05, PNorm = 63.0686, GNorm = 0.0875, lr_0 = 1.4551e-04
Loss = 1.9519e-05, PNorm = 63.0694, GNorm = 0.0919, lr_0 = 1.4486e-04
Loss = 2.2994e-05, PNorm = 63.0709, GNorm = 0.2287, lr_0 = 1.4422e-04
Validation rmse logD = 0.554883
Validation R2 logD = 0.785570
Epoch 83
Train function
Loss = 3.0938e-05, PNorm = 63.0731, GNorm = 0.3876, lr_0 = 1.4352e-04
Loss = 2.7000e-05, PNorm = 63.0745, GNorm = 0.1356, lr_0 = 1.4288e-04
Loss = 2.4996e-05, PNorm = 63.0750, GNorm = 0.1125, lr_0 = 1.4225e-04
Loss = 1.7948e-05, PNorm = 63.0767, GNorm = 0.0718, lr_0 = 1.4162e-04
Loss = 2.4297e-05, PNorm = 63.0792, GNorm = 0.1011, lr_0 = 1.4100e-04
Loss = 1.5763e-05, PNorm = 63.0810, GNorm = 0.0561, lr_0 = 1.4037e-04
Validation rmse logD = 0.553779
Validation R2 logD = 0.786422
Epoch 84
Train function
Loss = 2.0664e-05, PNorm = 63.0827, GNorm = 0.1828, lr_0 = 1.3975e-04
Loss = 1.8297e-05, PNorm = 63.0842, GNorm = 0.1510, lr_0 = 1.3913e-04
Loss = 1.8503e-05, PNorm = 63.0854, GNorm = 0.1886, lr_0 = 1.3852e-04
Loss = 1.4948e-05, PNorm = 63.0861, GNorm = 0.1176, lr_0 = 1.3791e-04
Loss = 1.8783e-05, PNorm = 63.0872, GNorm = 0.4196, lr_0 = 1.3730e-04
Validation rmse logD = 0.552880
Validation R2 logD = 0.787115
Epoch 85
Train function
Loss = 1.6958e-05, PNorm = 63.0893, GNorm = 0.0700, lr_0 = 1.3663e-04
Loss = 1.5465e-05, PNorm = 63.0908, GNorm = 0.1038, lr_0 = 1.3602e-04
Loss = 1.3487e-05, PNorm = 63.0922, GNorm = 0.1446, lr_0 = 1.3542e-04
Loss = 1.5052e-05, PNorm = 63.0927, GNorm = 0.0812, lr_0 = 1.3482e-04
Loss = 1.3795e-05, PNorm = 63.0945, GNorm = 0.0924, lr_0 = 1.3423e-04
Validation rmse logD = 0.552178
Validation R2 logD = 0.787656
Epoch 86
Train function
Loss = 1.2796e-05, PNorm = 63.0964, GNorm = 0.1926, lr_0 = 1.3357e-04
Loss = 1.4639e-05, PNorm = 63.0969, GNorm = 0.0779, lr_0 = 1.3298e-04
Loss = 1.3356e-05, PNorm = 63.0983, GNorm = 0.1555, lr_0 = 1.3239e-04
Loss = 1.2440e-05, PNorm = 63.0990, GNorm = 0.0684, lr_0 = 1.3181e-04
Loss = 2.1354e-05, PNorm = 63.1004, GNorm = 0.1387, lr_0 = 1.3123e-04
Loss = 1.6963e-05, PNorm = 63.1021, GNorm = 0.2106, lr_0 = 1.3065e-04
Validation rmse logD = 0.554688
Validation R2 logD = 0.785720
Epoch 87
Train function
Loss = 2.8578e-05, PNorm = 63.1046, GNorm = 0.1191, lr_0 = 1.3001e-04
Loss = 1.9630e-05, PNorm = 63.1058, GNorm = 0.1308, lr_0 = 1.2944e-04
Loss = 2.3960e-05, PNorm = 63.1066, GNorm = 0.1515, lr_0 = 1.2886e-04
Loss = 2.2543e-05, PNorm = 63.1079, GNorm = 0.1218, lr_0 = 1.2829e-04
Loss = 1.6649e-05, PNorm = 63.1100, GNorm = 0.0878, lr_0 = 1.2773e-04
Validation rmse logD = 0.552666
Validation R2 logD = 0.787279
Epoch 88
Train function
Loss = 1.3806e-05, PNorm = 63.1110, GNorm = 0.0621, lr_0 = 1.2710e-04
Loss = 1.2489e-05, PNorm = 63.1124, GNorm = 0.0733, lr_0 = 1.2654e-04
Loss = 1.4784e-05, PNorm = 63.1139, GNorm = 0.1447, lr_0 = 1.2598e-04
Loss = 3.4622e-05, PNorm = 63.1155, GNorm = 0.3001, lr_0 = 1.2542e-04
Loss = 2.0833e-05, PNorm = 63.1174, GNorm = 0.1881, lr_0 = 1.2487e-04
Validation rmse logD = 0.554121
Validation R2 logD = 0.786158
Epoch 89
Train function
Loss = 2.0421e-05, PNorm = 63.1193, GNorm = 0.1895, lr_0 = 1.2426e-04
Loss = 2.2048e-05, PNorm = 63.1207, GNorm = 0.1061, lr_0 = 1.2371e-04
Loss = 1.3587e-05, PNorm = 63.1218, GNorm = 0.1802, lr_0 = 1.2317e-04
Loss = 1.7826e-05, PNorm = 63.1224, GNorm = 0.2333, lr_0 = 1.2262e-04
Loss = 2.5201e-05, PNorm = 63.1234, GNorm = 0.2353, lr_0 = 1.2208e-04
Loss = 2.0648e-05, PNorm = 63.1260, GNorm = 0.0787, lr_0 = 1.2154e-04
Loss = 2.3381e-05, PNorm = 63.1263, GNorm = 0.0978, lr_0 = 1.2148e-04
Validation rmse logD = 0.554153
Validation R2 logD = 0.786134
Epoch 90
Train function
Loss = 1.5079e-05, PNorm = 63.1274, GNorm = 0.0717, lr_0 = 1.2095e-04
Loss = 1.6640e-05, PNorm = 63.1280, GNorm = 0.2822, lr_0 = 1.2041e-04
Loss = 2.0964e-05, PNorm = 63.1298, GNorm = 0.1108, lr_0 = 1.1988e-04
Loss = 1.4984e-05, PNorm = 63.1310, GNorm = 0.1643, lr_0 = 1.1935e-04
Loss = 1.5143e-05, PNorm = 63.1314, GNorm = 0.1256, lr_0 = 1.1882e-04
Validation rmse logD = 0.551792
Validation R2 logD = 0.787952
Epoch 91
Train function
Loss = 9.8611e-06, PNorm = 63.1328, GNorm = 0.1720, lr_0 = 1.1824e-04
Loss = 1.1004e-05, PNorm = 63.1338, GNorm = 0.0696, lr_0 = 1.1772e-04
Loss = 1.0783e-05, PNorm = 63.1348, GNorm = 0.1067, lr_0 = 1.1720e-04
Loss = 1.4599e-05, PNorm = 63.1357, GNorm = 0.2226, lr_0 = 1.1668e-04
Loss = 8.6060e-06, PNorm = 63.1368, GNorm = 0.0672, lr_0 = 1.1616e-04
Validation rmse logD = 0.553184
Validation R2 logD = 0.786881
Epoch 92
Train function
Loss = 1.1442e-05, PNorm = 63.1378, GNorm = 0.1259, lr_0 = 1.1565e-04
Loss = 1.2534e-05, PNorm = 63.1390, GNorm = 0.1348, lr_0 = 1.1514e-04
Loss = 8.2595e-06, PNorm = 63.1403, GNorm = 0.0924, lr_0 = 1.1463e-04
Loss = 1.3013e-05, PNorm = 63.1416, GNorm = 0.0855, lr_0 = 1.1412e-04
Loss = 1.0082e-05, PNorm = 63.1430, GNorm = 0.0632, lr_0 = 1.1362e-04
Loss = 1.1616e-05, PNorm = 63.1440, GNorm = 0.1410, lr_0 = 1.1312e-04
Loss = 4.2301e-05, PNorm = 63.1440, GNorm = 0.1495, lr_0 = 1.1307e-04
Validation rmse logD = 0.553088
Validation R2 logD = 0.786955
Epoch 93
Train function
Loss = 1.2840e-05, PNorm = 63.1450, GNorm = 0.0951, lr_0 = 1.1257e-04
Loss = 8.9104e-06, PNorm = 63.1455, GNorm = 0.0809, lr_0 = 1.1207e-04
Loss = 1.0673e-05, PNorm = 63.1459, GNorm = 0.1608, lr_0 = 1.1157e-04
Loss = 1.1044e-05, PNorm = 63.1470, GNorm = 0.0645, lr_0 = 1.1108e-04
Loss = 8.3070e-06, PNorm = 63.1481, GNorm = 0.0387, lr_0 = 1.1059e-04
Validation rmse logD = 0.552950
Validation R2 logD = 0.787061
Epoch 94
Train function
Loss = 8.0737e-06, PNorm = 63.1483, GNorm = 0.0528, lr_0 = 1.1005e-04
Loss = 1.0151e-05, PNorm = 63.1494, GNorm = 0.0730, lr_0 = 1.0956e-04
Loss = 8.5395e-06, PNorm = 63.1502, GNorm = 0.1551, lr_0 = 1.0908e-04
Loss = 1.2420e-05, PNorm = 63.1514, GNorm = 0.1353, lr_0 = 1.0860e-04
Loss = 1.0381e-05, PNorm = 63.1525, GNorm = 0.0645, lr_0 = 1.0811e-04
Validation rmse logD = 0.552979
Validation R2 logD = 0.787039
Epoch 95
Train function
Loss = 8.2894e-06, PNorm = 63.1531, GNorm = 0.1283, lr_0 = 1.0759e-04
Loss = 7.8176e-06, PNorm = 63.1537, GNorm = 0.1016, lr_0 = 1.0711e-04
Loss = 1.2755e-05, PNorm = 63.1549, GNorm = 0.0661, lr_0 = 1.0664e-04
Loss = 8.7324e-06, PNorm = 63.1556, GNorm = 0.0728, lr_0 = 1.0617e-04
Loss = 1.0136e-05, PNorm = 63.1560, GNorm = 0.2165, lr_0 = 1.0570e-04
Validation rmse logD = 0.553666
Validation R2 logD = 0.786509
Epoch 96
Train function
Loss = 1.1252e-05, PNorm = 63.1576, GNorm = 0.0648, lr_0 = 1.0518e-04
Loss = 1.1673e-05, PNorm = 63.1586, GNorm = 0.0901, lr_0 = 1.0472e-04
Loss = 6.0082e-06, PNorm = 63.1593, GNorm = 0.0469, lr_0 = 1.0426e-04
Loss = 6.3329e-06, PNorm = 63.1599, GNorm = 0.0771, lr_0 = 1.0379e-04
Loss = 8.9566e-06, PNorm = 63.1607, GNorm = 0.0404, lr_0 = 1.0333e-04
Loss = 1.2692e-05, PNorm = 63.1620, GNorm = 0.1781, lr_0 = 1.0288e-04
Validation rmse logD = 0.552966
Validation R2 logD = 0.787049
Epoch 97
Train function
Loss = 7.7489e-06, PNorm = 63.1627, GNorm = 0.0898, lr_0 = 1.0238e-04
Loss = 9.5247e-06, PNorm = 63.1639, GNorm = 0.0511, lr_0 = 1.0192e-04
Loss = 7.8487e-06, PNorm = 63.1649, GNorm = 0.0710, lr_0 = 1.0147e-04
Loss = 1.3815e-05, PNorm = 63.1653, GNorm = 0.1149, lr_0 = 1.0102e-04
Loss = 1.1894e-05, PNorm = 63.1660, GNorm = 0.1307, lr_0 = 1.0058e-04
Validation rmse logD = 0.554894
Validation R2 logD = 0.785561
Epoch 98
Train function
Loss = 7.9899e-06, PNorm = 63.1662, GNorm = 0.0594, lr_0 = 1.0009e-04
Loss = 8.7166e-06, PNorm = 63.1666, GNorm = 0.0602, lr_0 = 1.0000e-04
Loss = 8.1665e-06, PNorm = 63.1673, GNorm = 0.0609, lr_0 = 1.0000e-04
Loss = 1.3610e-05, PNorm = 63.1685, GNorm = 0.0631, lr_0 = 1.0000e-04
Loss = 8.1823e-06, PNorm = 63.1693, GNorm = 0.0711, lr_0 = 1.0000e-04
Validation rmse logD = 0.553390
Validation R2 logD = 0.786722
Epoch 99
Train function
Loss = 1.1154e-05, PNorm = 63.1713, GNorm = 0.2554, lr_0 = 1.0000e-04
Loss = 1.7062e-05, PNorm = 63.1727, GNorm = 0.0951, lr_0 = 1.0000e-04
Loss = 9.7776e-06, PNorm = 63.1739, GNorm = 0.0898, lr_0 = 1.0000e-04
Loss = 9.9685e-06, PNorm = 63.1754, GNorm = 0.0421, lr_0 = 1.0000e-04
Loss = 6.6135e-06, PNorm = 63.1751, GNorm = 0.1115, lr_0 = 1.0000e-04
Loss = 1.1004e-05, PNorm = 63.1753, GNorm = 0.0442, lr_0 = 1.0000e-04
Validation rmse logD = 0.552568
Validation R2 logD = 0.787356
Model 0 best validation rmse = 0.543120 on epoch 35
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logD = 0.588018
Model 0 test R2 logD = 0.760862
Ensemble test rmse  logD= 0.588018
Ensemble test R2  logD= 0.760862
4-fold cross validation
	Seed 0 ==> test rmse = 0.559066
	Seed 0 ==> test R2 = 0.783831
	Seed 1 ==> test rmse = 0.593638
	Seed 1 ==> test R2 = 0.756269
	Seed 2 ==> test rmse = 0.615716
	Seed 2 ==> test R2 = 0.737803
	Seed 3 ==> test rmse = 0.588018
	Seed 3 ==> test R2 = 0.760862
Overall val rmse logD= 0.570940 +/- 0.016967
Overall val R2 logD = 0.771651 +/- 0.020597
Overall test rmse logD = 0.589109 +/- 0.020200
Overall test R2 logD = 0.759691 +/- 0.016393
Elapsed time = 1:31:40
