Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_train.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 1,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_329/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_validation.csv',
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_train.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 1,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_329/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_validation.csv',
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD'],
 'task_names': ['logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 2,916 | train size = 2,916 | val size = 625 | test size = 625
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=165, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,512,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.9043e-02, PNorm = 55.5563, GNorm = 14.1772, lr_0 = 1.8534e-04
Loss = 1.8923e-02, PNorm = 55.5630, GNorm = 7.7826, lr_0 = 2.6293e-04
Loss = 1.6228e-02, PNorm = 55.5749, GNorm = 3.7866, lr_0 = 3.4052e-04
Loss = 1.7673e-02, PNorm = 55.5938, GNorm = 2.9799, lr_0 = 4.1810e-04
Loss = 1.5613e-02, PNorm = 55.6196, GNorm = 1.4028, lr_0 = 4.9569e-04
Validation rmse logD = 1.008786
Validation R2 logD = 0.282423
Epoch 1
Train function
Loss = 9.7042e-03, PNorm = 55.6593, GNorm = 3.0571, lr_0 = 5.8103e-04
Loss = 1.5040e-02, PNorm = 55.7141, GNorm = 2.9821, lr_0 = 6.5862e-04
Loss = 1.2970e-02, PNorm = 55.7844, GNorm = 6.6768, lr_0 = 7.3621e-04
Loss = 1.4074e-02, PNorm = 55.8717, GNorm = 2.3004, lr_0 = 8.1379e-04
Loss = 1.2908e-02, PNorm = 55.9795, GNorm = 2.4989, lr_0 = 8.9138e-04
Loss = 1.2078e-02, PNorm = 56.0733, GNorm = 1.4100, lr_0 = 9.6897e-04
Validation rmse logD = 0.909033
Validation R2 logD = 0.417319
Epoch 2
Train function
Loss = 1.1955e-02, PNorm = 56.1716, GNorm = 4.5014, lr_0 = 9.9717e-04
Loss = 1.0176e-02, PNorm = 56.2969, GNorm = 2.0299, lr_0 = 9.9314e-04
Loss = 9.4788e-03, PNorm = 56.4111, GNorm = 2.3160, lr_0 = 9.8912e-04
Loss = 9.7042e-03, PNorm = 56.4983, GNorm = 3.5294, lr_0 = 9.8512e-04
Loss = 9.4585e-03, PNorm = 56.5939, GNorm = 2.0163, lr_0 = 9.8114e-04
Loss = 1.1205e-02, PNorm = 56.6801, GNorm = 1.6927, lr_0 = 9.7717e-04
Validation rmse logD = 0.804365
Validation R2 logD = 0.543776
Epoch 3
Train function
Loss = 9.0956e-03, PNorm = 56.7887, GNorm = 1.6612, lr_0 = 9.7283e-04
Loss = 8.2242e-03, PNorm = 56.8556, GNorm = 1.5089, lr_0 = 9.6890e-04
Loss = 1.0149e-02, PNorm = 56.9640, GNorm = 6.5994, lr_0 = 9.6498e-04
Loss = 8.2331e-03, PNorm = 57.0524, GNorm = 0.9500, lr_0 = 9.6108e-04
Loss = 8.2859e-03, PNorm = 57.1403, GNorm = 1.2084, lr_0 = 9.5719e-04
Loss = 9.2267e-03, PNorm = 57.2243, GNorm = 1.9119, lr_0 = 9.5332e-04
Validation rmse logD = 0.912155
Validation R2 logD = 0.413311
Epoch 4
Train function
Loss = 9.7254e-03, PNorm = 57.3050, GNorm = 5.1606, lr_0 = 9.4947e-04
Loss = 7.8961e-03, PNorm = 57.3970, GNorm = 2.4211, lr_0 = 9.4563e-04
Loss = 9.9685e-03, PNorm = 57.4990, GNorm = 5.4010, lr_0 = 9.4181e-04
Loss = 8.3108e-03, PNorm = 57.5789, GNorm = 1.2210, lr_0 = 9.3800e-04
Loss = 8.1405e-03, PNorm = 57.6629, GNorm = 1.1274, lr_0 = 9.3421e-04
Loss = 7.5142e-03, PNorm = 57.7182, GNorm = 2.5724, lr_0 = 9.3043e-04
Validation rmse logD = 0.806220
Validation R2 logD = 0.541671
Epoch 5
Train function
Loss = 7.6788e-03, PNorm = 57.8071, GNorm = 1.5104, lr_0 = 9.2629e-04
Loss = 6.0041e-03, PNorm = 57.8963, GNorm = 1.5875, lr_0 = 9.2255e-04
Loss = 7.2135e-03, PNorm = 57.9614, GNorm = 1.0826, lr_0 = 9.1882e-04
Loss = 7.6087e-03, PNorm = 58.0349, GNorm = 3.4740, lr_0 = 9.1510e-04
Loss = 6.7148e-03, PNorm = 58.1205, GNorm = 4.0964, lr_0 = 9.1141e-04
Validation rmse logD = 0.818589
Validation R2 logD = 0.527499
Epoch 6
Train function
Loss = 1.3106e-02, PNorm = 58.2001, GNorm = 6.4837, lr_0 = 9.0735e-04
Loss = 6.2857e-03, PNorm = 58.3054, GNorm = 0.7625, lr_0 = 9.0368e-04
Loss = 6.9571e-03, PNorm = 58.3974, GNorm = 2.8332, lr_0 = 9.0003e-04
Loss = 6.3005e-03, PNorm = 58.4904, GNorm = 1.1863, lr_0 = 8.9639e-04
Loss = 5.9220e-03, PNorm = 58.5861, GNorm = 2.9536, lr_0 = 8.9277e-04
Loss = 5.8463e-03, PNorm = 58.6631, GNorm = 0.9445, lr_0 = 8.8916e-04
Validation rmse logD = 0.716347
Validation R2 logD = 0.638159
Epoch 7
Train function
Loss = 5.0845e-03, PNorm = 58.7460, GNorm = 1.8857, lr_0 = 8.8556e-04
Loss = 5.9612e-03, PNorm = 58.8276, GNorm = 1.0923, lr_0 = 8.8198e-04
Loss = 5.1954e-03, PNorm = 58.9135, GNorm = 3.9344, lr_0 = 8.7842e-04
Loss = 4.9580e-03, PNorm = 58.9968, GNorm = 1.8395, lr_0 = 8.7487e-04
Loss = 4.9253e-03, PNorm = 59.0633, GNorm = 1.3073, lr_0 = 8.7133e-04
Loss = 4.5897e-03, PNorm = 59.1773, GNorm = 1.0603, lr_0 = 8.6781e-04
Validation rmse logD = 0.688692
Validation R2 logD = 0.665558
Epoch 8
Train function
Loss = 5.0655e-03, PNorm = 59.2660, GNorm = 1.3764, lr_0 = 8.6395e-04
Loss = 4.3574e-03, PNorm = 59.3710, GNorm = 1.5957, lr_0 = 8.6046e-04
Loss = 4.5075e-03, PNorm = 59.4622, GNorm = 1.5229, lr_0 = 8.5698e-04
Loss = 4.8032e-03, PNorm = 59.5534, GNorm = 1.2260, lr_0 = 8.5351e-04
Loss = 4.5346e-03, PNorm = 59.6384, GNorm = 0.9326, lr_0 = 8.5006e-04
Loss = 5.0904e-03, PNorm = 59.7306, GNorm = 2.1844, lr_0 = 8.4663e-04
Validation rmse logD = 0.665189
Validation R2 logD = 0.687996
Epoch 9
Train function
Loss = 4.4672e-03, PNorm = 59.8349, GNorm = 3.1557, lr_0 = 8.4286e-04
Loss = 3.5702e-03, PNorm = 59.9373, GNorm = 0.9098, lr_0 = 8.3945e-04
Loss = 3.9544e-03, PNorm = 60.0133, GNorm = 0.8565, lr_0 = 8.3606e-04
Loss = 4.3717e-03, PNorm = 60.0965, GNorm = 0.7987, lr_0 = 8.3268e-04
Loss = 4.3841e-03, PNorm = 60.1749, GNorm = 2.0883, lr_0 = 8.2931e-04
Loss = 3.8937e-03, PNorm = 60.2651, GNorm = 0.9640, lr_0 = 8.2596e-04
Validation rmse logD = 0.757664
Validation R2 logD = 0.595216
Epoch 10
Train function
Loss = 4.2974e-03, PNorm = 60.3461, GNorm = 1.7590, lr_0 = 8.2262e-04
Loss = 3.2185e-03, PNorm = 60.4327, GNorm = 1.9885, lr_0 = 8.1930e-04
Loss = 3.7996e-03, PNorm = 60.5134, GNorm = 0.6082, lr_0 = 8.1598e-04
Loss = 4.1142e-03, PNorm = 60.5973, GNorm = 1.4049, lr_0 = 8.1269e-04
Loss = 3.1817e-03, PNorm = 60.6697, GNorm = 1.8153, lr_0 = 8.0940e-04
Loss = 3.2983e-03, PNorm = 60.7607, GNorm = 0.6835, lr_0 = 8.0613e-04
Validation rmse logD = 0.611142
Validation R2 logD = 0.736636
Epoch 11
Train function
Loss = 3.0365e-03, PNorm = 60.8676, GNorm = 1.2580, lr_0 = 8.0254e-04
Loss = 2.8529e-03, PNorm = 60.9438, GNorm = 0.7753, lr_0 = 7.9930e-04
Loss = 2.8792e-03, PNorm = 61.0210, GNorm = 1.4267, lr_0 = 7.9607e-04
Loss = 3.3018e-03, PNorm = 61.0860, GNorm = 1.6582, lr_0 = 7.9285e-04
Loss = 4.3211e-03, PNorm = 61.1655, GNorm = 1.0902, lr_0 = 7.8964e-04
Validation rmse logD = 0.614007
Validation R2 logD = 0.734161
Epoch 12
Train function
Loss = 1.9257e-03, PNorm = 61.2349, GNorm = 1.6493, lr_0 = 7.8613e-04
Loss = 2.6812e-03, PNorm = 61.3039, GNorm = 1.4071, lr_0 = 7.8296e-04
Loss = 2.4889e-03, PNorm = 61.3706, GNorm = 1.6634, lr_0 = 7.7979e-04
Loss = 3.0726e-03, PNorm = 61.4298, GNorm = 1.6363, lr_0 = 7.7664e-04
Loss = 3.0785e-03, PNorm = 61.4933, GNorm = 1.0756, lr_0 = 7.7350e-04
Loss = 2.7473e-03, PNorm = 61.5719, GNorm = 2.2762, lr_0 = 7.7037e-04
Validation rmse logD = 0.600558
Validation R2 logD = 0.745680
Epoch 13
Train function
Loss = 2.2537e-03, PNorm = 61.6456, GNorm = 1.3747, lr_0 = 7.6726e-04
Loss = 2.4515e-03, PNorm = 61.7203, GNorm = 1.4784, lr_0 = 7.6415e-04
Loss = 2.5430e-03, PNorm = 61.7880, GNorm = 1.0428, lr_0 = 7.6106e-04
Loss = 2.0180e-03, PNorm = 61.8714, GNorm = 0.8445, lr_0 = 7.5799e-04
Loss = 2.2643e-03, PNorm = 61.9302, GNorm = 2.0429, lr_0 = 7.5492e-04
Loss = 2.5058e-03, PNorm = 61.9877, GNorm = 1.0740, lr_0 = 7.5187e-04
Validation rmse logD = 0.597978
Validation R2 logD = 0.747860
Epoch 14
Train function
Loss = 1.8779e-03, PNorm = 62.0528, GNorm = 0.6510, lr_0 = 7.4853e-04
Loss = 1.7230e-03, PNorm = 62.1207, GNorm = 0.9083, lr_0 = 7.4550e-04
Loss = 2.2475e-03, PNorm = 62.1780, GNorm = 0.8827, lr_0 = 7.4249e-04
Loss = 1.9993e-03, PNorm = 62.2428, GNorm = 2.2753, lr_0 = 7.3949e-04
Loss = 2.6085e-03, PNorm = 62.3088, GNorm = 0.7942, lr_0 = 7.3650e-04
Loss = 2.1583e-03, PNorm = 62.3796, GNorm = 1.7794, lr_0 = 7.3352e-04
Validation rmse logD = 0.623557
Validation R2 logD = 0.725828
Epoch 15
Train function
Loss = 2.0987e-03, PNorm = 62.4485, GNorm = 1.4125, lr_0 = 7.3026e-04
Loss = 2.3078e-03, PNorm = 62.5066, GNorm = 1.8681, lr_0 = 7.2731e-04
Loss = 2.1211e-03, PNorm = 62.5767, GNorm = 0.7166, lr_0 = 7.2437e-04
Loss = 1.9929e-03, PNorm = 62.6438, GNorm = 0.7943, lr_0 = 7.2144e-04
Loss = 1.9437e-03, PNorm = 62.7074, GNorm = 0.7269, lr_0 = 7.1852e-04
Loss = 1.7466e-03, PNorm = 62.7566, GNorm = 0.6590, lr_0 = 7.1562e-04
Validation rmse logD = 0.585849
Validation R2 logD = 0.757985
Epoch 16
Train function
Loss = 1.6616e-03, PNorm = 62.8038, GNorm = 0.7803, lr_0 = 7.1272e-04
Loss = 1.6164e-03, PNorm = 62.8545, GNorm = 0.7332, lr_0 = 7.0984e-04
Loss = 1.8896e-03, PNorm = 62.9063, GNorm = 1.5549, lr_0 = 7.0697e-04
Loss = 1.7206e-03, PNorm = 62.9645, GNorm = 1.6333, lr_0 = 7.0411e-04
Loss = 1.5902e-03, PNorm = 63.0221, GNorm = 2.1331, lr_0 = 7.0127e-04
Loss = 1.9044e-03, PNorm = 63.0519, GNorm = 1.0954, lr_0 = 6.9843e-04
Validation rmse logD = 0.572395
Validation R2 logD = 0.768973
Epoch 17
Train function
Loss = 1.8279e-03, PNorm = 63.1122, GNorm = 3.4079, lr_0 = 6.9533e-04
Loss = 2.1206e-03, PNorm = 63.1754, GNorm = 2.3184, lr_0 = 6.9252e-04
Loss = 1.4535e-03, PNorm = 63.2322, GNorm = 0.7369, lr_0 = 6.8972e-04
Loss = 1.3181e-03, PNorm = 63.2856, GNorm = 0.4804, lr_0 = 6.8693e-04
Loss = 1.3491e-03, PNorm = 63.3217, GNorm = 1.3701, lr_0 = 6.8415e-04
Validation rmse logD = 0.604301
Validation R2 logD = 0.742500
Epoch 18
Train function
Loss = 1.0281e-03, PNorm = 63.3585, GNorm = 1.3716, lr_0 = 6.8111e-04
Loss = 1.2285e-03, PNorm = 63.3907, GNorm = 0.6374, lr_0 = 6.7835e-04
Loss = 1.0908e-03, PNorm = 63.4276, GNorm = 0.6279, lr_0 = 6.7561e-04
Loss = 1.5426e-03, PNorm = 63.4707, GNorm = 0.6005, lr_0 = 6.7288e-04
Loss = 1.1005e-03, PNorm = 63.5114, GNorm = 1.7852, lr_0 = 6.7016e-04
Loss = 1.4630e-03, PNorm = 63.5537, GNorm = 0.7187, lr_0 = 6.6745e-04
Validation rmse logD = 0.582541
Validation R2 logD = 0.760711
Epoch 19
Train function
Loss = 1.3950e-03, PNorm = 63.5902, GNorm = 1.4613, lr_0 = 6.6475e-04
Loss = 1.6302e-03, PNorm = 63.6409, GNorm = 2.2960, lr_0 = 6.6207e-04
Loss = 1.4029e-03, PNorm = 63.6817, GNorm = 2.3834, lr_0 = 6.5939e-04
Loss = 1.1508e-03, PNorm = 63.7242, GNorm = 0.7581, lr_0 = 6.5672e-04
Loss = 1.0751e-03, PNorm = 63.7663, GNorm = 0.6025, lr_0 = 6.5407e-04
Loss = 1.0317e-03, PNorm = 63.8099, GNorm = 1.7084, lr_0 = 6.5142e-04
Validation rmse logD = 0.574214
Validation R2 logD = 0.767503
Epoch 20
Train function
Loss = 1.1766e-03, PNorm = 63.8486, GNorm = 0.5480, lr_0 = 6.4853e-04
Loss = 1.0250e-03, PNorm = 63.8872, GNorm = 1.2973, lr_0 = 6.4591e-04
Loss = 1.2055e-03, PNorm = 63.9248, GNorm = 1.8952, lr_0 = 6.4329e-04
Loss = 1.0247e-03, PNorm = 63.9646, GNorm = 0.6642, lr_0 = 6.4069e-04
Loss = 1.2247e-03, PNorm = 64.0005, GNorm = 0.6885, lr_0 = 6.3810e-04
Loss = 1.2391e-03, PNorm = 64.0441, GNorm = 1.4851, lr_0 = 6.3552e-04
Validation rmse logD = 0.589578
Validation R2 logD = 0.754894
Epoch 21
Train function
Loss = 1.0658e-03, PNorm = 64.0690, GNorm = 1.2069, lr_0 = 6.3270e-04
Loss = 1.1584e-03, PNorm = 64.1119, GNorm = 0.7309, lr_0 = 6.3014e-04
Loss = 9.1310e-04, PNorm = 64.1404, GNorm = 0.9180, lr_0 = 6.2759e-04
Loss = 8.6749e-04, PNorm = 64.1724, GNorm = 0.4505, lr_0 = 6.2506e-04
Loss = 9.4695e-04, PNorm = 64.2129, GNorm = 0.3413, lr_0 = 6.2253e-04
Loss = 9.4178e-04, PNorm = 64.2450, GNorm = 0.7343, lr_0 = 6.2001e-04
Validation rmse logD = 0.582264
Validation R2 logD = 0.760938
Epoch 22
Train function
Loss = 6.5177e-04, PNorm = 64.2767, GNorm = 0.4699, lr_0 = 6.1750e-04
Loss = 8.9876e-04, PNorm = 64.3100, GNorm = 1.1896, lr_0 = 6.1501e-04
Loss = 1.0762e-03, PNorm = 64.3477, GNorm = 0.5886, lr_0 = 6.1252e-04
Loss = 8.0708e-04, PNorm = 64.3805, GNorm = 0.9873, lr_0 = 6.1005e-04
Loss = 8.8057e-04, PNorm = 64.4057, GNorm = 0.3957, lr_0 = 6.0758e-04
Loss = 7.0829e-04, PNorm = 64.4344, GNorm = 1.1228, lr_0 = 6.0512e-04
Validation rmse logD = 0.580375
Validation R2 logD = 0.762487
Epoch 23
Train function
Loss = 7.3539e-04, PNorm = 64.4611, GNorm = 1.3114, lr_0 = 6.0243e-04
Loss = 8.2444e-04, PNorm = 64.4844, GNorm = 0.5491, lr_0 = 6.0000e-04
Loss = 5.2585e-04, PNorm = 64.5080, GNorm = 0.3816, lr_0 = 5.9757e-04
Loss = 6.7773e-04, PNorm = 64.5278, GNorm = 0.7653, lr_0 = 5.9516e-04
Loss = 6.8708e-04, PNorm = 64.5484, GNorm = 0.4274, lr_0 = 5.9275e-04
Validation rmse logD = 0.577120
Validation R2 logD = 0.765143
Epoch 24
Train function
Loss = 3.6525e-04, PNorm = 64.5840, GNorm = 0.4188, lr_0 = 5.9011e-04
Loss = 6.2843e-04, PNorm = 64.6092, GNorm = 0.9277, lr_0 = 5.8773e-04
Loss = 6.1836e-04, PNorm = 64.6372, GNorm = 0.4427, lr_0 = 5.8535e-04
Loss = 8.2010e-04, PNorm = 64.6655, GNorm = 0.3459, lr_0 = 5.8299e-04
Loss = 6.8155e-04, PNorm = 64.6936, GNorm = 0.3882, lr_0 = 5.8063e-04
Loss = 7.6204e-04, PNorm = 64.7211, GNorm = 0.9604, lr_0 = 5.7828e-04
Validation rmse logD = 0.587026
Validation R2 logD = 0.757012
Epoch 25
Train function
Loss = 4.6300e-04, PNorm = 64.7503, GNorm = 0.7884, lr_0 = 5.7594e-04
Loss = 8.8913e-04, PNorm = 64.7835, GNorm = 0.8888, lr_0 = 5.7362e-04
Loss = 5.2490e-04, PNorm = 64.8120, GNorm = 0.7856, lr_0 = 5.7130e-04
Loss = 5.3192e-04, PNorm = 64.8362, GNorm = 0.3759, lr_0 = 5.6899e-04
Loss = 6.8071e-04, PNorm = 64.8595, GNorm = 0.6355, lr_0 = 5.6669e-04
Loss = 5.6525e-04, PNorm = 64.8867, GNorm = 0.5532, lr_0 = 5.6440e-04
Validation rmse logD = 0.567003
Validation R2 logD = 0.773305
Epoch 26
Train function
Loss = 4.8783e-04, PNorm = 64.9097, GNorm = 0.3243, lr_0 = 5.6189e-04
Loss = 4.5957e-04, PNorm = 64.9298, GNorm = 0.3840, lr_0 = 5.5961e-04
Loss = 5.1733e-04, PNorm = 64.9512, GNorm = 0.6604, lr_0 = 5.5735e-04
Loss = 4.3386e-04, PNorm = 64.9677, GNorm = 0.3624, lr_0 = 5.5510e-04
Loss = 6.0020e-04, PNorm = 64.9867, GNorm = 0.9018, lr_0 = 5.5285e-04
Loss = 5.7902e-04, PNorm = 65.0095, GNorm = 0.9158, lr_0 = 5.5062e-04
Validation rmse logD = 0.574028
Validation R2 logD = 0.767653
Epoch 27
Train function
Loss = 5.7256e-04, PNorm = 65.0436, GNorm = 0.6763, lr_0 = 5.4817e-04
Loss = 5.4639e-04, PNorm = 65.0743, GNorm = 0.3375, lr_0 = 5.4596e-04
Loss = 5.5189e-04, PNorm = 65.1036, GNorm = 0.4433, lr_0 = 5.4375e-04
Loss = 4.4603e-04, PNorm = 65.1210, GNorm = 0.4498, lr_0 = 5.4155e-04
Loss = 4.8751e-04, PNorm = 65.1379, GNorm = 0.3902, lr_0 = 5.3936e-04
Loss = 6.1093e-04, PNorm = 65.1631, GNorm = 0.7221, lr_0 = 5.3718e-04
Validation rmse logD = 0.574288
Validation R2 logD = 0.767442
Epoch 28
Train function
Loss = 4.9342e-04, PNorm = 65.1851, GNorm = 0.9102, lr_0 = 5.3479e-04
Loss = 3.7896e-04, PNorm = 65.2010, GNorm = 0.2684, lr_0 = 5.3263e-04
Loss = 4.3428e-04, PNorm = 65.2217, GNorm = 0.3960, lr_0 = 5.3048e-04
Loss = 4.6189e-04, PNorm = 65.2426, GNorm = 0.5435, lr_0 = 5.2833e-04
Loss = 5.0167e-04, PNorm = 65.2583, GNorm = 0.3669, lr_0 = 5.2620e-04
Loss = 4.0805e-04, PNorm = 65.2808, GNorm = 0.4961, lr_0 = 5.2407e-04
Validation rmse logD = 0.583797
Validation R2 logD = 0.759677
Epoch 29
Train function
Loss = 4.2912e-04, PNorm = 65.2986, GNorm = 0.3844, lr_0 = 5.2195e-04
Loss = 4.1592e-04, PNorm = 65.3166, GNorm = 0.5480, lr_0 = 5.1984e-04
Loss = 4.0702e-04, PNorm = 65.3390, GNorm = 0.4205, lr_0 = 5.1774e-04
Loss = 5.1455e-04, PNorm = 65.3593, GNorm = 0.9264, lr_0 = 5.1564e-04
Loss = 3.5806e-04, PNorm = 65.3754, GNorm = 0.3874, lr_0 = 5.1356e-04
Validation rmse logD = 0.565204
Validation R2 logD = 0.774741
Epoch 30
Train function
Loss = 4.6546e-04, PNorm = 65.3930, GNorm = 0.5015, lr_0 = 5.1128e-04
Loss = 2.2750e-04, PNorm = 65.4078, GNorm = 0.2530, lr_0 = 5.0921e-04
Loss = 4.3129e-04, PNorm = 65.4240, GNorm = 0.4921, lr_0 = 5.0715e-04
Loss = 3.9725e-04, PNorm = 65.4415, GNorm = 1.1742, lr_0 = 5.0510e-04
Loss = 3.7269e-04, PNorm = 65.4578, GNorm = 0.3951, lr_0 = 5.0306e-04
Loss = 4.5910e-04, PNorm = 65.4788, GNorm = 0.2159, lr_0 = 5.0102e-04
Validation rmse logD = 0.578813
Validation R2 logD = 0.763763
Epoch 31
Train function
Loss = 3.5497e-04, PNorm = 65.4969, GNorm = 0.4550, lr_0 = 4.9880e-04
Loss = 4.1762e-04, PNorm = 65.5154, GNorm = 0.4697, lr_0 = 4.9678e-04
Loss = 3.9480e-04, PNorm = 65.5286, GNorm = 0.4474, lr_0 = 4.9477e-04
Loss = 3.2117e-04, PNorm = 65.5432, GNorm = 0.5523, lr_0 = 4.9277e-04
Loss = 3.7056e-04, PNorm = 65.5623, GNorm = 0.3799, lr_0 = 4.9078e-04
Loss = 3.3607e-04, PNorm = 65.5809, GNorm = 0.6030, lr_0 = 4.8880e-04
Validation rmse logD = 0.569257
Validation R2 logD = 0.771499
Epoch 32
Train function
Loss = 3.0349e-04, PNorm = 65.5924, GNorm = 0.6020, lr_0 = 4.8682e-04
Loss = 3.0959e-04, PNorm = 65.6113, GNorm = 0.4041, lr_0 = 4.8485e-04
Loss = 2.9250e-04, PNorm = 65.6272, GNorm = 0.5855, lr_0 = 4.8289e-04
Loss = 3.1573e-04, PNorm = 65.6397, GNorm = 0.7318, lr_0 = 4.8094e-04
Loss = 2.6716e-04, PNorm = 65.6506, GNorm = 0.4693, lr_0 = 4.7899e-04
Loss = 3.4245e-04, PNorm = 65.6620, GNorm = 0.2652, lr_0 = 4.7706e-04
Validation rmse logD = 0.578338
Validation R2 logD = 0.764151
Epoch 33
Train function
Loss = 2.8418e-04, PNorm = 65.6842, GNorm = 0.4930, lr_0 = 4.7494e-04
Loss = 2.8717e-04, PNorm = 65.7021, GNorm = 0.4802, lr_0 = 4.7302e-04
Loss = 2.9450e-04, PNorm = 65.7182, GNorm = 0.2139, lr_0 = 4.7110e-04
Loss = 4.0348e-04, PNorm = 65.7343, GNorm = 0.3701, lr_0 = 4.6920e-04
Loss = 3.9720e-04, PNorm = 65.7609, GNorm = 1.0097, lr_0 = 4.6730e-04
Loss = 4.5276e-04, PNorm = 65.7749, GNorm = 0.9172, lr_0 = 4.6541e-04
Validation rmse logD = 0.565242
Validation R2 logD = 0.774711
Epoch 34
Train function
Loss = 2.6176e-04, PNorm = 65.7978, GNorm = 0.4721, lr_0 = 4.6334e-04
Loss = 2.4830e-04, PNorm = 65.8108, GNorm = 0.2897, lr_0 = 4.6147e-04
Loss = 3.0600e-04, PNorm = 65.8203, GNorm = 0.3263, lr_0 = 4.5961e-04
Loss = 2.9188e-04, PNorm = 65.8293, GNorm = 0.6895, lr_0 = 4.5775e-04
Loss = 3.8601e-04, PNorm = 65.8486, GNorm = 0.3811, lr_0 = 4.5590e-04
Loss = 3.1268e-04, PNorm = 65.8610, GNorm = 1.1876, lr_0 = 4.5405e-04
Validation rmse logD = 0.577827
Validation R2 logD = 0.764568
Epoch 35
Train function
Loss = 3.8951e-04, PNorm = 65.8728, GNorm = 0.4534, lr_0 = 4.5222e-04
Loss = 3.1953e-04, PNorm = 65.8914, GNorm = 0.5664, lr_0 = 4.5039e-04
Loss = 2.6850e-04, PNorm = 65.9049, GNorm = 0.4861, lr_0 = 4.4857e-04
Loss = 2.3776e-04, PNorm = 65.9165, GNorm = 0.3587, lr_0 = 4.4676e-04
Loss = 2.4407e-04, PNorm = 65.9283, GNorm = 0.6752, lr_0 = 4.4495e-04
Validation rmse logD = 0.571333
Validation R2 logD = 0.769830
Epoch 36
Train function
Loss = 1.7577e-04, PNorm = 65.9416, GNorm = 0.2892, lr_0 = 4.4297e-04
Loss = 1.9072e-04, PNorm = 65.9534, GNorm = 0.9879, lr_0 = 4.4118e-04
Loss = 2.7312e-04, PNorm = 65.9637, GNorm = 0.2930, lr_0 = 4.3940e-04
Loss = 2.3612e-04, PNorm = 65.9749, GNorm = 0.2564, lr_0 = 4.3762e-04
Loss = 2.6966e-04, PNorm = 65.9865, GNorm = 0.3264, lr_0 = 4.3585e-04
Loss = 1.9552e-04, PNorm = 66.0022, GNorm = 0.4602, lr_0 = 4.3409e-04
Validation rmse logD = 0.573305
Validation R2 logD = 0.768238
Epoch 37
Train function
Loss = 1.2324e-04, PNorm = 66.0199, GNorm = 0.2537, lr_0 = 4.3216e-04
Loss = 1.6478e-04, PNorm = 66.0275, GNorm = 0.7781, lr_0 = 4.3041e-04
Loss = 1.9818e-04, PNorm = 66.0339, GNorm = 0.5046, lr_0 = 4.2867e-04
Loss = 2.3343e-04, PNorm = 66.0469, GNorm = 0.7029, lr_0 = 4.2694e-04
Loss = 2.9258e-04, PNorm = 66.0543, GNorm = 0.9052, lr_0 = 4.2521e-04
Loss = 2.8642e-04, PNorm = 66.0661, GNorm = 0.4179, lr_0 = 4.2349e-04
Validation rmse logD = 0.580674
Validation R2 logD = 0.762242
Epoch 38
Train function
Loss = 1.9510e-04, PNorm = 66.0775, GNorm = 0.9075, lr_0 = 4.2178e-04
Loss = 2.0786e-04, PNorm = 66.0888, GNorm = 0.5740, lr_0 = 4.2008e-04
Loss = 2.0194e-04, PNorm = 66.0987, GNorm = 0.4498, lr_0 = 4.1838e-04
Loss = 2.2519e-04, PNorm = 66.1084, GNorm = 0.2321, lr_0 = 4.1669e-04
Loss = 2.2403e-04, PNorm = 66.1183, GNorm = 0.5344, lr_0 = 4.1500e-04
Loss = 2.3634e-04, PNorm = 66.1311, GNorm = 0.3227, lr_0 = 4.1332e-04
Validation rmse logD = 0.587706
Validation R2 logD = 0.756448
Epoch 39
Train function
Loss = 2.1422e-04, PNorm = 66.1491, GNorm = 0.4718, lr_0 = 4.1149e-04
Loss = 2.0349e-04, PNorm = 66.1594, GNorm = 0.3910, lr_0 = 4.0982e-04
Loss = 2.1514e-04, PNorm = 66.1694, GNorm = 0.2006, lr_0 = 4.0817e-04
Loss = 2.1259e-04, PNorm = 66.1795, GNorm = 0.2718, lr_0 = 4.0652e-04
Loss = 1.5659e-04, PNorm = 66.1894, GNorm = 0.4529, lr_0 = 4.0487e-04
Loss = 2.3860e-04, PNorm = 66.1999, GNorm = 0.4382, lr_0 = 4.0324e-04
Validation rmse logD = 0.581992
Validation R2 logD = 0.761161
Epoch 40
Train function
Loss = 2.0353e-04, PNorm = 66.2144, GNorm = 0.2530, lr_0 = 4.0144e-04
Loss = 1.5904e-04, PNorm = 66.2228, GNorm = 0.2774, lr_0 = 3.9982e-04
Loss = 1.7718e-04, PNorm = 66.2327, GNorm = 0.4965, lr_0 = 3.9820e-04
Loss = 2.0102e-04, PNorm = 66.2401, GNorm = 0.3429, lr_0 = 3.9659e-04
Loss = 1.7761e-04, PNorm = 66.2487, GNorm = 0.4337, lr_0 = 3.9499e-04
Loss = 1.4984e-04, PNorm = 66.2577, GNorm = 0.5655, lr_0 = 3.9339e-04
Validation rmse logD = 0.573437
Validation R2 logD = 0.768131
Epoch 41
Train function
Loss = 1.7157e-04, PNorm = 66.2684, GNorm = 0.9173, lr_0 = 3.9180e-04
Loss = 1.3121e-04, PNorm = 66.2790, GNorm = 0.1735, lr_0 = 3.9022e-04
Loss = 1.5869e-04, PNorm = 66.2868, GNorm = 0.3593, lr_0 = 3.8864e-04
Loss = 2.3008e-04, PNorm = 66.2925, GNorm = 1.1713, lr_0 = 3.8707e-04
Loss = 2.0313e-04, PNorm = 66.3036, GNorm = 0.5774, lr_0 = 3.8551e-04
Validation rmse logD = 0.570067
Validation R2 logD = 0.770848
Epoch 42
Train function
Loss = 5.8683e-05, PNorm = 66.3123, GNorm = 0.2354, lr_0 = 3.8379e-04
Loss = 1.7260e-04, PNorm = 66.3229, GNorm = 0.8091, lr_0 = 3.8224e-04
Loss = 1.5334e-04, PNorm = 66.3347, GNorm = 0.4082, lr_0 = 3.8069e-04
Loss = 1.6681e-04, PNorm = 66.3435, GNorm = 0.3448, lr_0 = 3.7916e-04
Loss = 1.4222e-04, PNorm = 66.3513, GNorm = 0.2180, lr_0 = 3.7762e-04
Loss = 2.0674e-04, PNorm = 66.3575, GNorm = 0.3804, lr_0 = 3.7610e-04
Validation rmse logD = 0.578439
Validation R2 logD = 0.764068
Epoch 43
Train function
Loss = 2.1199e-04, PNorm = 66.3729, GNorm = 0.2490, lr_0 = 3.7442e-04
Loss = 1.4398e-04, PNorm = 66.3816, GNorm = 0.2205, lr_0 = 3.7291e-04
Loss = 1.6608e-04, PNorm = 66.3936, GNorm = 0.6189, lr_0 = 3.7140e-04
Loss = 1.5844e-04, PNorm = 66.4050, GNorm = 0.4238, lr_0 = 3.6990e-04
Loss = 1.3556e-04, PNorm = 66.4130, GNorm = 0.2031, lr_0 = 3.6841e-04
Loss = 1.7061e-04, PNorm = 66.4220, GNorm = 0.4154, lr_0 = 3.6692e-04
Validation rmse logD = 0.570153
Validation R2 logD = 0.770779
Epoch 44
Train function
Loss = 2.0712e-04, PNorm = 66.4289, GNorm = 0.2129, lr_0 = 3.6543e-04
Loss = 1.4447e-04, PNorm = 66.4415, GNorm = 0.2775, lr_0 = 3.6396e-04
Loss = 1.3048e-04, PNorm = 66.4499, GNorm = 0.2692, lr_0 = 3.6248e-04
Loss = 1.0666e-04, PNorm = 66.4562, GNorm = 0.2199, lr_0 = 3.6102e-04
Loss = 1.3986e-04, PNorm = 66.4627, GNorm = 0.4167, lr_0 = 3.5956e-04
Loss = 1.5342e-04, PNorm = 66.4726, GNorm = 0.2645, lr_0 = 3.5811e-04
Validation rmse logD = 0.571868
Validation R2 logD = 0.769399
Epoch 45
Train function
Loss = 1.5239e-04, PNorm = 66.4791, GNorm = 0.2270, lr_0 = 3.5651e-04
Loss = 1.4911e-04, PNorm = 66.4877, GNorm = 0.2169, lr_0 = 3.5507e-04
Loss = 1.0252e-04, PNorm = 66.4935, GNorm = 0.3575, lr_0 = 3.5364e-04
Loss = 1.1426e-04, PNorm = 66.4991, GNorm = 0.8967, lr_0 = 3.5221e-04
Loss = 1.1458e-04, PNorm = 66.5056, GNorm = 0.1433, lr_0 = 3.5078e-04
Loss = 1.0038e-04, PNorm = 66.5110, GNorm = 0.3291, lr_0 = 3.4936e-04
Validation rmse logD = 0.569405
Validation R2 logD = 0.771380
Epoch 46
Train function
Loss = 1.1588e-04, PNorm = 66.5159, GNorm = 0.2539, lr_0 = 3.4781e-04
Loss = 1.1454e-04, PNorm = 66.5210, GNorm = 0.5948, lr_0 = 3.4641e-04
Loss = 1.3540e-04, PNorm = 66.5288, GNorm = 0.5157, lr_0 = 3.4501e-04
Loss = 1.6568e-04, PNorm = 66.5394, GNorm = 0.5344, lr_0 = 3.4361e-04
Loss = 9.7112e-05, PNorm = 66.5466, GNorm = 0.2393, lr_0 = 3.4222e-04
Loss = 1.3799e-04, PNorm = 66.5530, GNorm = 0.6066, lr_0 = 3.4084e-04
Validation rmse logD = 0.575085
Validation R2 logD = 0.766796
Epoch 47
Train function
Loss = 1.3908e-04, PNorm = 66.5576, GNorm = 0.1723, lr_0 = 3.3946e-04
Loss = 1.5032e-04, PNorm = 66.5614, GNorm = 0.5947, lr_0 = 3.3809e-04
Loss = 9.2453e-05, PNorm = 66.5681, GNorm = 0.1775, lr_0 = 3.3672e-04
Loss = 1.4853e-04, PNorm = 66.5781, GNorm = 0.8329, lr_0 = 3.3536e-04
Loss = 1.2661e-04, PNorm = 66.5870, GNorm = 0.4669, lr_0 = 3.3400e-04
Validation rmse logD = 0.578139
Validation R2 logD = 0.764313
Epoch 48
Train function
Loss = 9.6459e-05, PNorm = 66.5925, GNorm = 0.2999, lr_0 = 3.3252e-04
Loss = 1.0642e-04, PNorm = 66.5957, GNorm = 0.3120, lr_0 = 3.3117e-04
Loss = 1.0979e-04, PNorm = 66.6054, GNorm = 0.1470, lr_0 = 3.2984e-04
Loss = 9.9320e-05, PNorm = 66.6103, GNorm = 0.3152, lr_0 = 3.2850e-04
Loss = 9.6966e-05, PNorm = 66.6183, GNorm = 0.2008, lr_0 = 3.2717e-04
Loss = 8.9635e-05, PNorm = 66.6258, GNorm = 0.1643, lr_0 = 3.2585e-04
Validation rmse logD = 0.578834
Validation R2 logD = 0.763746
Epoch 49
Train function
Loss = 8.0743e-05, PNorm = 66.6322, GNorm = 0.4445, lr_0 = 3.2440e-04
Loss = 7.1988e-05, PNorm = 66.6355, GNorm = 0.2248, lr_0 = 3.2309e-04
Loss = 1.2378e-04, PNorm = 66.6400, GNorm = 0.4896, lr_0 = 3.2178e-04
Loss = 8.3820e-05, PNorm = 66.6434, GNorm = 0.2730, lr_0 = 3.2048e-04
Loss = 7.4383e-05, PNorm = 66.6494, GNorm = 0.2768, lr_0 = 3.1919e-04
Loss = 8.7691e-05, PNorm = 66.6566, GNorm = 0.2229, lr_0 = 3.1790e-04
Validation rmse logD = 0.575566
Validation R2 logD = 0.766406
Epoch 50
Train function
Loss = 9.3269e-05, PNorm = 66.6624, GNorm = 0.3141, lr_0 = 3.1661e-04
Loss = 5.6507e-05, PNorm = 66.6674, GNorm = 0.3090, lr_0 = 3.1533e-04
Loss = 7.4276e-05, PNorm = 66.6732, GNorm = 0.3885, lr_0 = 3.1406e-04
Loss = 9.5766e-05, PNorm = 66.6786, GNorm = 0.3162, lr_0 = 3.1279e-04
Loss = 8.7758e-05, PNorm = 66.6873, GNorm = 0.3640, lr_0 = 3.1152e-04
Loss = 8.7478e-05, PNorm = 66.6920, GNorm = 0.1620, lr_0 = 3.1026e-04
Validation rmse logD = 0.572439
Validation R2 logD = 0.768938
Epoch 51
Train function
Loss = 9.0556e-05, PNorm = 66.6964, GNorm = 0.6493, lr_0 = 3.0888e-04
Loss = 7.3538e-05, PNorm = 66.6998, GNorm = 0.1182, lr_0 = 3.0764e-04
Loss = 9.5575e-05, PNorm = 66.7036, GNorm = 0.6809, lr_0 = 3.0639e-04
Loss = 8.5215e-05, PNorm = 66.7087, GNorm = 0.1878, lr_0 = 3.0515e-04
Loss = 1.0102e-04, PNorm = 66.7174, GNorm = 0.2558, lr_0 = 3.0392e-04
Loss = 9.4111e-05, PNorm = 66.7237, GNorm = 0.2342, lr_0 = 3.0269e-04
Validation rmse logD = 0.574025
Validation R2 logD = 0.767655
Epoch 52
Train function
Loss = 7.7163e-05, PNorm = 66.7317, GNorm = 0.2368, lr_0 = 3.0135e-04
Loss = 5.9453e-05, PNorm = 66.7341, GNorm = 0.2369, lr_0 = 3.0013e-04
Loss = 8.0143e-05, PNorm = 66.7379, GNorm = 0.2914, lr_0 = 2.9891e-04
Loss = 8.6345e-05, PNorm = 66.7439, GNorm = 0.1349, lr_0 = 2.9770e-04
Loss = 7.0882e-05, PNorm = 66.7461, GNorm = 0.1526, lr_0 = 2.9650e-04
Loss = 9.6917e-05, PNorm = 66.7556, GNorm = 0.7175, lr_0 = 2.9530e-04
Loss = 2.1312e-04, PNorm = 66.7560, GNorm = 0.2454, lr_0 = 2.9518e-04
Validation rmse logD = 0.576822
Validation R2 logD = 0.765386
Epoch 53
Train function
Loss = 6.2103e-05, PNorm = 66.7598, GNorm = 0.3198, lr_0 = 2.9399e-04
Loss = 8.1580e-05, PNorm = 66.7674, GNorm = 0.3120, lr_0 = 2.9280e-04
Loss = 5.9198e-05, PNorm = 66.7727, GNorm = 0.2206, lr_0 = 2.9162e-04
Loss = 6.9649e-05, PNorm = 66.7772, GNorm = 0.4880, lr_0 = 2.9044e-04
Loss = 6.6405e-05, PNorm = 66.7824, GNorm = 0.1537, lr_0 = 2.8926e-04
Validation rmse logD = 0.577506
Validation R2 logD = 0.764829
Epoch 54
Train function
Loss = 6.7793e-05, PNorm = 66.7871, GNorm = 0.2493, lr_0 = 2.8809e-04
Loss = 8.9188e-05, PNorm = 66.7899, GNorm = 0.3659, lr_0 = 2.8693e-04
Loss = 8.1095e-05, PNorm = 66.7945, GNorm = 0.3800, lr_0 = 2.8577e-04
Loss = 6.3932e-05, PNorm = 66.7997, GNorm = 0.2110, lr_0 = 2.8461e-04
Loss = 5.2312e-05, PNorm = 66.8023, GNorm = 0.1253, lr_0 = 2.8346e-04
Loss = 6.5252e-05, PNorm = 66.8035, GNorm = 0.3375, lr_0 = 2.8232e-04
Validation rmse logD = 0.573712
Validation R2 logD = 0.767909
Epoch 55
Train function
Loss = 3.6743e-05, PNorm = 66.8099, GNorm = 0.1202, lr_0 = 2.8106e-04
Loss = 6.8719e-05, PNorm = 66.8130, GNorm = 0.3358, lr_0 = 2.7993e-04
Loss = 6.2099e-05, PNorm = 66.8174, GNorm = 0.2657, lr_0 = 2.7880e-04
Loss = 4.5519e-05, PNorm = 66.8230, GNorm = 0.1086, lr_0 = 2.7767e-04
Loss = 4.7072e-05, PNorm = 66.8259, GNorm = 0.2013, lr_0 = 2.7655e-04
Loss = 5.8760e-05, PNorm = 66.8316, GNorm = 0.2442, lr_0 = 2.7543e-04
Validation rmse logD = 0.575841
Validation R2 logD = 0.766183
Epoch 56
Train function
Loss = 7.0088e-05, PNorm = 66.8373, GNorm = 0.4558, lr_0 = 2.7420e-04
Loss = 6.4844e-05, PNorm = 66.8416, GNorm = 0.1709, lr_0 = 2.7309e-04
Loss = 4.6019e-05, PNorm = 66.8444, GNorm = 0.1811, lr_0 = 2.7199e-04
Loss = 4.7552e-05, PNorm = 66.8479, GNorm = 0.2727, lr_0 = 2.7089e-04
Loss = 7.1797e-05, PNorm = 66.8533, GNorm = 0.2337, lr_0 = 2.6980e-04
Loss = 8.4060e-05, PNorm = 66.8569, GNorm = 0.1480, lr_0 = 2.6870e-04
Validation rmse logD = 0.577181
Validation R2 logD = 0.765093
Epoch 57
Train function
Loss = 6.7981e-05, PNorm = 66.8615, GNorm = 0.5217, lr_0 = 2.6762e-04
Loss = 6.0474e-05, PNorm = 66.8685, GNorm = 0.2548, lr_0 = 2.6654e-04
Loss = 6.3296e-05, PNorm = 66.8738, GNorm = 0.2390, lr_0 = 2.6546e-04
Loss = 7.1600e-05, PNorm = 66.8775, GNorm = 0.5374, lr_0 = 2.6439e-04
Loss = 7.8636e-05, PNorm = 66.8814, GNorm = 0.1557, lr_0 = 2.6332e-04
Loss = 5.6823e-05, PNorm = 66.8858, GNorm = 0.3439, lr_0 = 2.6225e-04
Validation rmse logD = 0.574437
Validation R2 logD = 0.767322
Epoch 58
Train function
Loss = 7.2442e-05, PNorm = 66.8903, GNorm = 0.1492, lr_0 = 2.6109e-04
Loss = 8.3778e-05, PNorm = 66.8948, GNorm = 0.5248, lr_0 = 2.6003e-04
Loss = 8.3600e-05, PNorm = 66.9041, GNorm = 0.4798, lr_0 = 2.5898e-04
Loss = 7.4412e-05, PNorm = 66.9098, GNorm = 0.5239, lr_0 = 2.5793e-04
Loss = 8.0848e-05, PNorm = 66.9112, GNorm = 0.1751, lr_0 = 2.5689e-04
Loss = 8.3790e-05, PNorm = 66.9177, GNorm = 0.5175, lr_0 = 2.5585e-04
Loss = 3.1025e-04, PNorm = 66.9177, GNorm = 0.6684, lr_0 = 2.5575e-04
Validation rmse logD = 0.576520
Validation R2 logD = 0.765631
Epoch 59
Train function
Loss = 9.9798e-05, PNorm = 66.9204, GNorm = 0.1270, lr_0 = 2.5471e-04
Loss = 8.4990e-05, PNorm = 66.9252, GNorm = 0.3783, lr_0 = 2.5368e-04
Loss = 7.1524e-05, PNorm = 66.9306, GNorm = 0.4789, lr_0 = 2.5266e-04
Loss = 7.3687e-05, PNorm = 66.9329, GNorm = 0.3957, lr_0 = 2.5164e-04
Loss = 7.8391e-05, PNorm = 66.9367, GNorm = 0.3361, lr_0 = 2.5062e-04
Validation rmse logD = 0.578618
Validation R2 logD = 0.763923
Epoch 60
Train function
Loss = 5.9390e-05, PNorm = 66.9400, GNorm = 0.1592, lr_0 = 2.4961e-04
Loss = 5.8333e-05, PNorm = 66.9459, GNorm = 0.3723, lr_0 = 2.4860e-04
Loss = 5.9025e-05, PNorm = 66.9494, GNorm = 0.1324, lr_0 = 2.4759e-04
Loss = 5.6809e-05, PNorm = 66.9541, GNorm = 0.2419, lr_0 = 2.4659e-04
Loss = 5.2761e-05, PNorm = 66.9584, GNorm = 0.1361, lr_0 = 2.4559e-04
Loss = 4.3659e-05, PNorm = 66.9611, GNorm = 0.2080, lr_0 = 2.4460e-04
Validation rmse logD = 0.573720
Validation R2 logD = 0.767903
Epoch 61
Train function
Loss = 2.6541e-05, PNorm = 66.9641, GNorm = 0.1339, lr_0 = 2.4351e-04
Loss = 3.9229e-05, PNorm = 66.9653, GNorm = 0.0931, lr_0 = 2.4253e-04
Loss = 3.9179e-05, PNorm = 66.9675, GNorm = 0.2327, lr_0 = 2.4155e-04
Loss = 4.1117e-05, PNorm = 66.9705, GNorm = 0.1557, lr_0 = 2.4057e-04
Loss = 3.5224e-05, PNorm = 66.9747, GNorm = 0.1705, lr_0 = 2.3960e-04
Loss = 5.1564e-05, PNorm = 66.9775, GNorm = 0.2083, lr_0 = 2.3863e-04
Validation rmse logD = 0.573459
Validation R2 logD = 0.768114
Epoch 62
Train function
Loss = 2.7143e-05, PNorm = 66.9802, GNorm = 0.1131, lr_0 = 2.3757e-04
Loss = 3.4522e-05, PNorm = 66.9823, GNorm = 0.2943, lr_0 = 2.3661e-04
Loss = 3.8865e-05, PNorm = 66.9849, GNorm = 0.2586, lr_0 = 2.3565e-04
Loss = 4.6481e-05, PNorm = 66.9868, GNorm = 0.5432, lr_0 = 2.3470e-04
Loss = 5.7646e-05, PNorm = 66.9897, GNorm = 0.1728, lr_0 = 2.3375e-04
Loss = 6.5603e-05, PNorm = 66.9934, GNorm = 0.7139, lr_0 = 2.3281e-04
Validation rmse logD = 0.574367
Validation R2 logD = 0.767379
Epoch 63
Train function
Loss = 6.3921e-05, PNorm = 66.9971, GNorm = 0.4724, lr_0 = 2.3187e-04
Loss = 5.2586e-05, PNorm = 67.0016, GNorm = 0.2485, lr_0 = 2.3093e-04
Loss = 6.1812e-05, PNorm = 67.0062, GNorm = 0.3582, lr_0 = 2.2999e-04
Loss = 5.0888e-05, PNorm = 67.0101, GNorm = 0.1760, lr_0 = 2.2906e-04
Loss = 6.3924e-05, PNorm = 67.0131, GNorm = 0.5438, lr_0 = 2.2814e-04
Loss = 4.9462e-05, PNorm = 67.0160, GNorm = 0.1860, lr_0 = 2.2722e-04
Validation rmse logD = 0.575499
Validation R2 logD = 0.766460
Epoch 64
Train function
Loss = 5.1457e-05, PNorm = 67.0212, GNorm = 0.1564, lr_0 = 2.2621e-04
Loss = 4.1188e-05, PNorm = 67.0245, GNorm = 0.1443, lr_0 = 2.2529e-04
Loss = 4.2289e-05, PNorm = 67.0256, GNorm = 0.1529, lr_0 = 2.2438e-04
Loss = 4.7709e-05, PNorm = 67.0275, GNorm = 0.2036, lr_0 = 2.2347e-04
Loss = 5.3098e-05, PNorm = 67.0317, GNorm = 0.2624, lr_0 = 2.2257e-04
Loss = 4.6843e-05, PNorm = 67.0341, GNorm = 0.2809, lr_0 = 2.2167e-04
Loss = 2.4476e-04, PNorm = 67.0347, GNorm = 0.3303, lr_0 = 2.2158e-04
Validation rmse logD = 0.572396
Validation R2 logD = 0.768972
Epoch 65
Train function
Loss = 4.4445e-05, PNorm = 67.0383, GNorm = 0.1186, lr_0 = 2.2068e-04
Loss = 4.8043e-05, PNorm = 67.0419, GNorm = 0.2418, lr_0 = 2.1979e-04
Loss = 4.9997e-05, PNorm = 67.0447, GNorm = 0.3447, lr_0 = 2.1890e-04
Loss = 5.5233e-05, PNorm = 67.0462, GNorm = 0.1999, lr_0 = 2.1802e-04
Loss = 4.2645e-05, PNorm = 67.0480, GNorm = 0.1209, lr_0 = 2.1714e-04
Validation rmse logD = 0.576429
Validation R2 logD = 0.765705
Epoch 66
Train function
Loss = 2.1054e-05, PNorm = 67.0516, GNorm = 0.0912, lr_0 = 2.1626e-04
Loss = 3.8971e-05, PNorm = 67.0555, GNorm = 0.2278, lr_0 = 2.1539e-04
Loss = 5.1934e-05, PNorm = 67.0588, GNorm = 0.3472, lr_0 = 2.1451e-04
Loss = 4.6039e-05, PNorm = 67.0606, GNorm = 0.2532, lr_0 = 2.1365e-04
Loss = 7.8757e-05, PNorm = 67.0661, GNorm = 0.2113, lr_0 = 2.1278e-04
Loss = 6.4715e-05, PNorm = 67.0708, GNorm = 0.3213, lr_0 = 2.1192e-04
Validation rmse logD = 0.575519
Validation R2 logD = 0.766444
Epoch 67
Train function
Loss = 6.0130e-05, PNorm = 67.0775, GNorm = 0.4654, lr_0 = 2.1098e-04
Loss = 6.8666e-05, PNorm = 67.0806, GNorm = 0.1544, lr_0 = 2.1013e-04
Loss = 5.6584e-05, PNorm = 67.0831, GNorm = 0.2451, lr_0 = 2.0928e-04
Loss = 5.5411e-05, PNorm = 67.0865, GNorm = 0.2150, lr_0 = 2.0843e-04
Loss = 4.2117e-05, PNorm = 67.0887, GNorm = 0.1860, lr_0 = 2.0759e-04
Loss = 4.7521e-05, PNorm = 67.0915, GNorm = 0.2272, lr_0 = 2.0675e-04
Validation rmse logD = 0.574375
Validation R2 logD = 0.767372
Epoch 68
Train function
Loss = 5.7289e-05, PNorm = 67.0928, GNorm = 0.1232, lr_0 = 2.0583e-04
Loss = 3.6382e-05, PNorm = 67.0944, GNorm = 0.2207, lr_0 = 2.0500e-04
Loss = 4.3584e-05, PNorm = 67.0965, GNorm = 0.1365, lr_0 = 2.0417e-04
Loss = 4.6269e-05, PNorm = 67.1000, GNorm = 0.0764, lr_0 = 2.0335e-04
Loss = 3.2382e-05, PNorm = 67.1029, GNorm = 0.1083, lr_0 = 2.0252e-04
Loss = 4.0125e-05, PNorm = 67.1052, GNorm = 0.2278, lr_0 = 2.0170e-04
Validation rmse logD = 0.575880
Validation R2 logD = 0.766152
Epoch 69
Train function
Loss = 3.2434e-05, PNorm = 67.1063, GNorm = 0.1332, lr_0 = 2.0089e-04
Loss = 3.6951e-05, PNorm = 67.1090, GNorm = 0.1757, lr_0 = 2.0008e-04
Loss = 2.6238e-05, PNorm = 67.1102, GNorm = 0.2574, lr_0 = 1.9927e-04
Loss = 2.8228e-05, PNorm = 67.1110, GNorm = 0.2119, lr_0 = 1.9846e-04
Loss = 3.5217e-05, PNorm = 67.1131, GNorm = 0.2367, lr_0 = 1.9766e-04
Loss = 2.7073e-05, PNorm = 67.1144, GNorm = 0.0713, lr_0 = 1.9686e-04
Validation rmse logD = 0.574069
Validation R2 logD = 0.767620
Epoch 70
Train function
Loss = 3.3641e-05, PNorm = 67.1173, GNorm = 0.1600, lr_0 = 1.9599e-04
Loss = 2.3681e-05, PNorm = 67.1186, GNorm = 0.1298, lr_0 = 1.9519e-04
Loss = 2.2493e-05, PNorm = 67.1207, GNorm = 0.2002, lr_0 = 1.9440e-04
Loss = 2.2518e-05, PNorm = 67.1233, GNorm = 0.0698, lr_0 = 1.9362e-04
Loss = 2.5825e-05, PNorm = 67.1246, GNorm = 0.0936, lr_0 = 1.9284e-04
Loss = 2.7373e-05, PNorm = 67.1264, GNorm = 0.1872, lr_0 = 1.9206e-04
Loss = 6.1957e-05, PNorm = 67.1265, GNorm = 0.2895, lr_0 = 1.9198e-04
Validation rmse logD = 0.574237
Validation R2 logD = 0.767483
Epoch 71
Train function
Loss = 2.6324e-05, PNorm = 67.1298, GNorm = 0.1261, lr_0 = 1.9120e-04
Loss = 2.9544e-05, PNorm = 67.1305, GNorm = 0.2170, lr_0 = 1.9043e-04
Loss = 2.3023e-05, PNorm = 67.1331, GNorm = 0.3798, lr_0 = 1.8966e-04
Loss = 2.3843e-05, PNorm = 67.1337, GNorm = 0.2157, lr_0 = 1.8889e-04
Loss = 2.6665e-05, PNorm = 67.1359, GNorm = 0.0914, lr_0 = 1.8813e-04
Validation rmse logD = 0.574385
Validation R2 logD = 0.767364
Epoch 72
Train function
Loss = 2.1251e-05, PNorm = 67.1377, GNorm = 0.1095, lr_0 = 1.8737e-04
Loss = 2.5936e-05, PNorm = 67.1389, GNorm = 0.0509, lr_0 = 1.8661e-04
Loss = 2.9442e-05, PNorm = 67.1406, GNorm = 0.3416, lr_0 = 1.8586e-04
Loss = 3.1529e-05, PNorm = 67.1432, GNorm = 0.1323, lr_0 = 1.8510e-04
Loss = 2.7695e-05, PNorm = 67.1448, GNorm = 0.1932, lr_0 = 1.8436e-04
Loss = 2.5588e-05, PNorm = 67.1469, GNorm = 0.0822, lr_0 = 1.8361e-04
Validation rmse logD = 0.574080
Validation R2 logD = 0.767611
Epoch 73
Train function
Loss = 2.6894e-05, PNorm = 67.1503, GNorm = 0.2811, lr_0 = 1.8279e-04
Loss = 2.1392e-05, PNorm = 67.1527, GNorm = 0.2098, lr_0 = 1.8206e-04
Loss = 2.8212e-05, PNorm = 67.1539, GNorm = 0.1518, lr_0 = 1.8132e-04
Loss = 3.2811e-05, PNorm = 67.1553, GNorm = 0.1403, lr_0 = 1.8059e-04
Loss = 2.2682e-05, PNorm = 67.1562, GNorm = 0.1281, lr_0 = 1.7986e-04
Loss = 3.3606e-05, PNorm = 67.1583, GNorm = 0.4671, lr_0 = 1.7913e-04
Validation rmse logD = 0.573586
Validation R2 logD = 0.768011
Epoch 74
Train function
Loss = 2.2593e-05, PNorm = 67.1613, GNorm = 0.2274, lr_0 = 1.7833e-04
Loss = 2.2304e-05, PNorm = 67.1637, GNorm = 0.2446, lr_0 = 1.7761e-04
Loss = 2.8881e-05, PNorm = 67.1648, GNorm = 0.1542, lr_0 = 1.7689e-04
Loss = 2.1952e-05, PNorm = 67.1664, GNorm = 0.0904, lr_0 = 1.7618e-04
Loss = 2.1554e-05, PNorm = 67.1684, GNorm = 0.1072, lr_0 = 1.7547e-04
Loss = 2.3915e-05, PNorm = 67.1694, GNorm = 0.0970, lr_0 = 1.7476e-04
Validation rmse logD = 0.576291
Validation R2 logD = 0.765817
Epoch 75
Train function
Loss = 2.8528e-05, PNorm = 67.1696, GNorm = 0.1941, lr_0 = 1.7405e-04
Loss = 2.4667e-05, PNorm = 67.1725, GNorm = 0.1771, lr_0 = 1.7335e-04
Loss = 2.2995e-05, PNorm = 67.1751, GNorm = 0.2261, lr_0 = 1.7265e-04
Loss = 3.0234e-05, PNorm = 67.1757, GNorm = 0.0708, lr_0 = 1.7195e-04
Loss = 2.6936e-05, PNorm = 67.1767, GNorm = 0.2518, lr_0 = 1.7125e-04
Loss = 2.5275e-05, PNorm = 67.1773, GNorm = 0.1761, lr_0 = 1.7056e-04
Validation rmse logD = 0.575061
Validation R2 logD = 0.766816
Epoch 76
Train function
Loss = 2.7521e-05, PNorm = 67.1780, GNorm = 0.3161, lr_0 = 1.6980e-04
Loss = 2.5570e-05, PNorm = 67.1808, GNorm = 0.3323, lr_0 = 1.6912e-04
Loss = 2.1381e-05, PNorm = 67.1827, GNorm = 0.1009, lr_0 = 1.6843e-04
Loss = 2.1234e-05, PNorm = 67.1844, GNorm = 0.0812, lr_0 = 1.6775e-04
Loss = 2.2216e-05, PNorm = 67.1865, GNorm = 0.0936, lr_0 = 1.6707e-04
Loss = 2.3172e-05, PNorm = 67.1885, GNorm = 0.1296, lr_0 = 1.6640e-04
Loss = 9.3804e-05, PNorm = 67.1885, GNorm = 0.3624, lr_0 = 1.6633e-04
Validation rmse logD = 0.574337
Validation R2 logD = 0.767403
Epoch 77
Train function
Loss = 1.8551e-05, PNorm = 67.1891, GNorm = 0.1243, lr_0 = 1.6566e-04
Loss = 2.0884e-05, PNorm = 67.1907, GNorm = 0.3742, lr_0 = 1.6499e-04
Loss = 1.9461e-05, PNorm = 67.1926, GNorm = 0.0603, lr_0 = 1.6432e-04
Loss = 1.7442e-05, PNorm = 67.1942, GNorm = 0.0757, lr_0 = 1.6366e-04
Loss = 1.9943e-05, PNorm = 67.1949, GNorm = 0.0926, lr_0 = 1.6300e-04
Validation rmse logD = 0.575423
Validation R2 logD = 0.766523
Epoch 78
Train function
Loss = 1.9393e-05, PNorm = 67.1971, GNorm = 0.0976, lr_0 = 1.6227e-04
Loss = 1.5708e-05, PNorm = 67.1987, GNorm = 0.0989, lr_0 = 1.6161e-04
Loss = 1.4907e-05, PNorm = 67.2008, GNorm = 0.0768, lr_0 = 1.6096e-04
Loss = 1.6311e-05, PNorm = 67.2031, GNorm = 0.1276, lr_0 = 1.6031e-04
Loss = 1.7687e-05, PNorm = 67.2039, GNorm = 0.0894, lr_0 = 1.5966e-04
Loss = 1.8232e-05, PNorm = 67.2048, GNorm = 0.0777, lr_0 = 1.5902e-04
Validation rmse logD = 0.572653
Validation R2 logD = 0.768765
Epoch 79
Train function
Loss = 1.4753e-05, PNorm = 67.2061, GNorm = 0.2201, lr_0 = 1.5837e-04
Loss = 1.3556e-05, PNorm = 67.2080, GNorm = 0.1248, lr_0 = 1.5773e-04
Loss = 1.5291e-05, PNorm = 67.2084, GNorm = 0.0737, lr_0 = 1.5710e-04
Loss = 1.8340e-05, PNorm = 67.2101, GNorm = 0.1034, lr_0 = 1.5646e-04
Loss = 1.7528e-05, PNorm = 67.2122, GNorm = 0.0911, lr_0 = 1.5583e-04
Loss = 1.5746e-05, PNorm = 67.2129, GNorm = 0.0637, lr_0 = 1.5520e-04
Validation rmse logD = 0.575871
Validation R2 logD = 0.766158
Epoch 80
Train function
Loss = 1.5860e-05, PNorm = 67.2138, GNorm = 0.1722, lr_0 = 1.5451e-04
Loss = 1.6209e-05, PNorm = 67.2149, GNorm = 0.1083, lr_0 = 1.5388e-04
Loss = 1.8143e-05, PNorm = 67.2164, GNorm = 0.0858, lr_0 = 1.5326e-04
Loss = 1.7596e-05, PNorm = 67.2182, GNorm = 0.1124, lr_0 = 1.5264e-04
Loss = 1.6163e-05, PNorm = 67.2192, GNorm = 0.2193, lr_0 = 1.5202e-04
Loss = 1.4438e-05, PNorm = 67.2199, GNorm = 0.0946, lr_0 = 1.5141e-04
Validation rmse logD = 0.573939
Validation R2 logD = 0.767725
Epoch 81
Train function
Loss = 1.3589e-05, PNorm = 67.2216, GNorm = 0.1067, lr_0 = 1.5074e-04
Loss = 1.5154e-05, PNorm = 67.2225, GNorm = 0.2930, lr_0 = 1.5013e-04
Loss = 1.3169e-05, PNorm = 67.2237, GNorm = 0.1299, lr_0 = 1.4952e-04
Loss = 2.1003e-05, PNorm = 67.2242, GNorm = 0.1322, lr_0 = 1.4892e-04
Loss = 1.2702e-05, PNorm = 67.2254, GNorm = 0.1328, lr_0 = 1.4831e-04
Loss = 1.4662e-05, PNorm = 67.2261, GNorm = 0.0891, lr_0 = 1.4771e-04
Validation rmse logD = 0.575402
Validation R2 logD = 0.766539
Epoch 82
Train function
Loss = 1.7572e-05, PNorm = 67.2277, GNorm = 0.0583, lr_0 = 1.4712e-04
Loss = 1.2311e-05, PNorm = 67.2285, GNorm = 0.1144, lr_0 = 1.4652e-04
Loss = 1.4722e-05, PNorm = 67.2296, GNorm = 0.2682, lr_0 = 1.4593e-04
Loss = 1.3135e-05, PNorm = 67.2309, GNorm = 0.1073, lr_0 = 1.4534e-04
Loss = 1.6677e-05, PNorm = 67.2321, GNorm = 0.2202, lr_0 = 1.4475e-04
Loss = 1.5039e-05, PNorm = 67.2343, GNorm = 0.1188, lr_0 = 1.4417e-04
Loss = 2.5451e-05, PNorm = 67.2344, GNorm = 0.1103, lr_0 = 1.4411e-04
Validation rmse logD = 0.573981
Validation R2 logD = 0.767691
Epoch 83
Train function
Loss = 1.2270e-05, PNorm = 67.2349, GNorm = 0.2570, lr_0 = 1.4353e-04
Loss = 1.4541e-05, PNorm = 67.2363, GNorm = 0.0721, lr_0 = 1.4295e-04
Loss = 1.7462e-05, PNorm = 67.2376, GNorm = 0.2644, lr_0 = 1.4237e-04
Loss = 2.0790e-05, PNorm = 67.2384, GNorm = 0.2856, lr_0 = 1.4179e-04
Loss = 1.6328e-05, PNorm = 67.2395, GNorm = 0.2022, lr_0 = 1.4122e-04
Validation rmse logD = 0.574667
Validation R2 logD = 0.767136
Epoch 84
Train function
Loss = 1.9380e-05, PNorm = 67.2402, GNorm = 0.0784, lr_0 = 1.4059e-04
Loss = 2.0026e-05, PNorm = 67.2415, GNorm = 0.0524, lr_0 = 1.4002e-04
Loss = 1.8511e-05, PNorm = 67.2432, GNorm = 0.0830, lr_0 = 1.3946e-04
Loss = 1.6268e-05, PNorm = 67.2444, GNorm = 0.1927, lr_0 = 1.3889e-04
Loss = 1.7739e-05, PNorm = 67.2446, GNorm = 0.0745, lr_0 = 1.3833e-04
Loss = 1.1971e-05, PNorm = 67.2460, GNorm = 0.0611, lr_0 = 1.3777e-04
Validation rmse logD = 0.577991
Validation R2 logD = 0.764434
Epoch 85
Train function
Loss = 1.4351e-05, PNorm = 67.2463, GNorm = 0.2225, lr_0 = 1.3722e-04
Loss = 1.2817e-05, PNorm = 67.2475, GNorm = 0.2310, lr_0 = 1.3666e-04
Loss = 1.5572e-05, PNorm = 67.2493, GNorm = 0.1000, lr_0 = 1.3611e-04
Loss = 1.7443e-05, PNorm = 67.2502, GNorm = 0.1727, lr_0 = 1.3556e-04
Loss = 1.6633e-05, PNorm = 67.2517, GNorm = 0.0900, lr_0 = 1.3501e-04
Loss = 1.3802e-05, PNorm = 67.2532, GNorm = 0.1550, lr_0 = 1.3446e-04
Validation rmse logD = 0.576696
Validation R2 logD = 0.765488
Epoch 86
Train function
Loss = 1.6747e-05, PNorm = 67.2537, GNorm = 0.1098, lr_0 = 1.3387e-04
Loss = 1.2661e-05, PNorm = 67.2542, GNorm = 0.1656, lr_0 = 1.3333e-04
Loss = 1.0292e-05, PNorm = 67.2547, GNorm = 0.0913, lr_0 = 1.3279e-04
Loss = 1.5269e-05, PNorm = 67.2562, GNorm = 0.0624, lr_0 = 1.3225e-04
Loss = 1.3788e-05, PNorm = 67.2567, GNorm = 0.1233, lr_0 = 1.3171e-04
Loss = 1.4573e-05, PNorm = 67.2580, GNorm = 0.0888, lr_0 = 1.3118e-04
Validation rmse logD = 0.575373
Validation R2 logD = 0.766563
Epoch 87
Train function
Loss = 1.0304e-05, PNorm = 67.2595, GNorm = 0.0597, lr_0 = 1.3060e-04
Loss = 1.1250e-05, PNorm = 67.2610, GNorm = 0.1180, lr_0 = 1.3007e-04
Loss = 1.1536e-05, PNorm = 67.2618, GNorm = 0.2133, lr_0 = 1.2955e-04
Loss = 1.2231e-05, PNorm = 67.2626, GNorm = 0.1571, lr_0 = 1.2902e-04
Loss = 1.1622e-05, PNorm = 67.2639, GNorm = 0.0897, lr_0 = 1.2850e-04
Loss = 1.2163e-05, PNorm = 67.2649, GNorm = 0.1894, lr_0 = 1.2798e-04
Validation rmse logD = 0.574293
Validation R2 logD = 0.767438
Epoch 88
Train function
Loss = 1.2150e-05, PNorm = 67.2668, GNorm = 0.2486, lr_0 = 1.2746e-04
Loss = 2.1025e-05, PNorm = 67.2674, GNorm = 0.0701, lr_0 = 1.2695e-04
Loss = 2.0749e-05, PNorm = 67.2697, GNorm = 0.2610, lr_0 = 1.2643e-04
Loss = 1.7275e-05, PNorm = 67.2704, GNorm = 0.1601, lr_0 = 1.2592e-04
Loss = 1.3469e-05, PNorm = 67.2714, GNorm = 0.0674, lr_0 = 1.2541e-04
Loss = 1.4637e-05, PNorm = 67.2729, GNorm = 0.0652, lr_0 = 1.2491e-04
Loss = 5.7128e-06, PNorm = 67.2731, GNorm = 0.0473, lr_0 = 1.2486e-04
Validation rmse logD = 0.575270
Validation R2 logD = 0.766647
Epoch 89
Train function
Loss = 1.1416e-05, PNorm = 67.2744, GNorm = 0.0876, lr_0 = 1.2435e-04
Loss = 9.3599e-06, PNorm = 67.2748, GNorm = 0.0384, lr_0 = 1.2385e-04
Loss = 1.1139e-05, PNorm = 67.2748, GNorm = 0.0670, lr_0 = 1.2335e-04
Loss = 1.5816e-05, PNorm = 67.2755, GNorm = 0.1202, lr_0 = 1.2285e-04
Loss = 1.1366e-05, PNorm = 67.2767, GNorm = 0.0670, lr_0 = 1.2235e-04
Validation rmse logD = 0.575692
Validation R2 logD = 0.766304
Epoch 90
Train function
Loss = 6.2786e-06, PNorm = 67.2774, GNorm = 0.0737, lr_0 = 1.2181e-04
Loss = 9.4187e-06, PNorm = 67.2780, GNorm = 0.0528, lr_0 = 1.2132e-04
Loss = 8.5416e-06, PNorm = 67.2783, GNorm = 0.0867, lr_0 = 1.2083e-04
Loss = 8.5331e-06, PNorm = 67.2789, GNorm = 0.0897, lr_0 = 1.2034e-04
Loss = 9.4225e-06, PNorm = 67.2798, GNorm = 0.1208, lr_0 = 1.1985e-04
Loss = 1.4129e-05, PNorm = 67.2811, GNorm = 0.1291, lr_0 = 1.1937e-04
Validation rmse logD = 0.574591
Validation R2 logD = 0.767197
Epoch 91
Train function
Loss = 7.9285e-06, PNorm = 67.2815, GNorm = 0.0546, lr_0 = 1.1888e-04
Loss = 1.2495e-05, PNorm = 67.2828, GNorm = 0.0549, lr_0 = 1.1840e-04
Loss = 1.0100e-05, PNorm = 67.2836, GNorm = 0.0791, lr_0 = 1.1792e-04
Loss = 8.9341e-06, PNorm = 67.2839, GNorm = 0.1750, lr_0 = 1.1745e-04
Loss = 1.1717e-05, PNorm = 67.2855, GNorm = 0.1647, lr_0 = 1.1697e-04
Loss = 9.9602e-06, PNorm = 67.2863, GNorm = 0.0763, lr_0 = 1.1650e-04
Validation rmse logD = 0.576852
Validation R2 logD = 0.765361
Epoch 92
Train function
Loss = 1.2341e-05, PNorm = 67.2871, GNorm = 0.0529, lr_0 = 1.1598e-04
Loss = 1.1931e-05, PNorm = 67.2881, GNorm = 0.0868, lr_0 = 1.1551e-04
Loss = 1.3627e-05, PNorm = 67.2879, GNorm = 0.2517, lr_0 = 1.1505e-04
Loss = 9.0980e-06, PNorm = 67.2883, GNorm = 0.1492, lr_0 = 1.1458e-04
Loss = 1.1468e-05, PNorm = 67.2887, GNorm = 0.0685, lr_0 = 1.1412e-04
Loss = 9.7784e-06, PNorm = 67.2891, GNorm = 0.0936, lr_0 = 1.1366e-04
Validation rmse logD = 0.575185
Validation R2 logD = 0.766716
Epoch 93
Train function
Loss = 8.6937e-06, PNorm = 67.2909, GNorm = 0.0577, lr_0 = 1.1315e-04
Loss = 7.0843e-06, PNorm = 67.2909, GNorm = 0.0581, lr_0 = 1.1269e-04
Loss = 9.2945e-06, PNorm = 67.2909, GNorm = 0.1701, lr_0 = 1.1224e-04
Loss = 1.0950e-05, PNorm = 67.2920, GNorm = 0.1092, lr_0 = 1.1178e-04
Loss = 7.4110e-06, PNorm = 67.2926, GNorm = 0.0934, lr_0 = 1.1133e-04
Loss = 1.2895e-05, PNorm = 67.2939, GNorm = 0.0982, lr_0 = 1.1088e-04
Validation rmse logD = 0.575698
Validation R2 logD = 0.766299
Epoch 94
Train function
Loss = 6.1916e-06, PNorm = 67.2949, GNorm = 0.0629, lr_0 = 1.1043e-04
Loss = 8.8948e-06, PNorm = 67.2958, GNorm = 0.2026, lr_0 = 1.0999e-04
Loss = 1.2451e-05, PNorm = 67.2970, GNorm = 0.0950, lr_0 = 1.0954e-04
Loss = 9.7024e-06, PNorm = 67.2984, GNorm = 0.1002, lr_0 = 1.0910e-04
Loss = 1.2163e-05, PNorm = 67.3002, GNorm = 0.0451, lr_0 = 1.0866e-04
Loss = 8.3468e-06, PNorm = 67.3010, GNorm = 0.0426, lr_0 = 1.0822e-04
Loss = 5.8355e-05, PNorm = 67.3010, GNorm = 0.1901, lr_0 = 1.0818e-04
Validation rmse logD = 0.574033
Validation R2 logD = 0.767649
Epoch 95
Train function
Loss = 8.5314e-06, PNorm = 67.3020, GNorm = 0.0986, lr_0 = 1.0774e-04
Loss = 1.0777e-05, PNorm = 67.3022, GNorm = 0.1182, lr_0 = 1.0730e-04
Loss = 9.9812e-06, PNorm = 67.3026, GNorm = 0.0822, lr_0 = 1.0687e-04
Loss = 1.3553e-05, PNorm = 67.3027, GNorm = 0.0684, lr_0 = 1.0644e-04
Loss = 9.5340e-06, PNorm = 67.3033, GNorm = 0.0686, lr_0 = 1.0601e-04
Validation rmse logD = 0.576115
Validation R2 logD = 0.765961
Epoch 96
Train function
Loss = 1.0133e-05, PNorm = 67.3036, GNorm = 0.0955, lr_0 = 1.0554e-04
Loss = 9.5205e-06, PNorm = 67.3032, GNorm = 0.0563, lr_0 = 1.0511e-04
Loss = 8.6946e-06, PNorm = 67.3039, GNorm = 0.0980, lr_0 = 1.0468e-04
Loss = 8.9278e-06, PNorm = 67.3050, GNorm = 0.1816, lr_0 = 1.0426e-04
Loss = 1.2061e-05, PNorm = 67.3065, GNorm = 0.1232, lr_0 = 1.0384e-04
Loss = 1.1645e-05, PNorm = 67.3070, GNorm = 0.0825, lr_0 = 1.0342e-04
Validation rmse logD = 0.575697
Validation R2 logD = 0.766300
Epoch 97
Train function
Loss = 1.7127e-05, PNorm = 67.3080, GNorm = 0.0868, lr_0 = 1.0300e-04
Loss = 8.9199e-06, PNorm = 67.3082, GNorm = 0.2207, lr_0 = 1.0258e-04
Loss = 7.8347e-06, PNorm = 67.3091, GNorm = 0.0672, lr_0 = 1.0217e-04
Loss = 9.2733e-06, PNorm = 67.3096, GNorm = 0.1575, lr_0 = 1.0176e-04
Loss = 7.1540e-06, PNorm = 67.3104, GNorm = 0.1322, lr_0 = 1.0135e-04
Loss = 1.2035e-05, PNorm = 67.3114, GNorm = 0.1541, lr_0 = 1.0094e-04
Validation rmse logD = 0.577915
Validation R2 logD = 0.764496
Epoch 98
Train function
Loss = 7.4607e-06, PNorm = 67.3123, GNorm = 0.1118, lr_0 = 1.0049e-04
Loss = 9.8189e-06, PNorm = 67.3128, GNorm = 0.0589, lr_0 = 1.0008e-04
Loss = 1.3215e-05, PNorm = 67.3138, GNorm = 0.0403, lr_0 = 1.0000e-04
Loss = 7.3537e-06, PNorm = 67.3151, GNorm = 0.1441, lr_0 = 1.0000e-04
Loss = 8.7849e-06, PNorm = 67.3156, GNorm = 0.1249, lr_0 = 1.0000e-04
Loss = 8.1771e-06, PNorm = 67.3161, GNorm = 0.0727, lr_0 = 1.0000e-04
Validation rmse logD = 0.575545
Validation R2 logD = 0.766423
Epoch 99
Train function
Loss = 6.0897e-06, PNorm = 67.3172, GNorm = 0.0660, lr_0 = 1.0000e-04
Loss = 8.6274e-06, PNorm = 67.3177, GNorm = 0.2320, lr_0 = 1.0000e-04
Loss = 1.1645e-05, PNorm = 67.3181, GNorm = 0.1160, lr_0 = 1.0000e-04
Loss = 6.4767e-06, PNorm = 67.3196, GNorm = 0.0823, lr_0 = 1.0000e-04
Loss = 6.3886e-06, PNorm = 67.3200, GNorm = 0.0939, lr_0 = 1.0000e-04
Loss = 1.1317e-05, PNorm = 67.3204, GNorm = 0.1717, lr_0 = 1.0000e-04
Validation rmse logD = 0.575536
Validation R2 logD = 0.766430
Model 0 best validation rmse = 0.565204 on epoch 29
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "substructures_encoder.encoder.cached_zero_vector".
Loading pretrained parameter "substructures_encoder.encoder.W_o.weight".
Loading pretrained parameter "substructures_encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logD = 0.585165
Model 0 test R2 logD = 0.763177
Ensemble test rmse  logD= 0.585165
Ensemble test R2  logD= 0.763177
1-fold cross validation
	Seed 0 ==> test rmse = 0.585165
	Seed 0 ==> test R2 = 0.763177
Overall val rmse logD= 0.565204 +/- 0.000000
Overall val R2 logD = 0.774741 +/- 0.000000
Overall test rmse logD = 0.585165 +/- 0.000000
Overall test R2 logD = 0.763177 +/- 0.000000
Elapsed time = 0:50:56
