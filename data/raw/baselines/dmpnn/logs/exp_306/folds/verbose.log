Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_306/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=125, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,500,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.4278e-02, PNorm = 55.2298, GNorm = 11.9961, lr_0 = 4.8077e-04
Validation rmse logS = 1.203626
Validation R2 logS = 0.681586
Epoch 1
Train function
Loss = 6.8882e-03, PNorm = 55.2923, GNorm = 3.4466, lr_0 = 8.6154e-04
Validation rmse logS = 0.974743
Validation R2 logS = 0.791172
Epoch 2
Train function
Loss = 3.1968e-03, PNorm = 55.3855, GNorm = 1.5534, lr_0 = 9.8743e-04
Loss = 4.2274e-03, PNorm = 55.4498, GNorm = 2.2093, lr_0 = 9.6974e-04
Validation rmse logS = 0.970936
Validation R2 logS = 0.792800
Epoch 3
Train function
Loss = 3.3582e-03, PNorm = 55.5051, GNorm = 1.2712, lr_0 = 9.5237e-04
Validation rmse logS = 0.800743
Validation R2 logS = 0.859072
Epoch 4
Train function
Loss = 2.5528e-03, PNorm = 55.5476, GNorm = 3.3705, lr_0 = 9.3363e-04
Validation rmse logS = 0.750277
Validation R2 logS = 0.876277
Epoch 5
Train function
Loss = 2.0859e-03, PNorm = 55.5849, GNorm = 0.9487, lr_0 = 9.1690e-04
Loss = 1.8722e-03, PNorm = 55.6212, GNorm = 1.1340, lr_0 = 9.0048e-04
Loss = 4.4965e-03, PNorm = 55.6247, GNorm = 1.1870, lr_0 = 8.9885e-04
Validation rmse logS = 0.718657
Validation R2 logS = 0.886485
Epoch 6
Train function
Loss = 1.7306e-03, PNorm = 55.6632, GNorm = 0.4779, lr_0 = 8.8275e-04
Validation rmse logS = 0.660741
Validation R2 logS = 0.904044
Epoch 7
Train function
Loss = 1.5500e-03, PNorm = 55.7010, GNorm = 1.9389, lr_0 = 8.6694e-04
Validation rmse logS = 0.672728
Validation R2 logS = 0.900531
Epoch 8
Train function
Loss = 1.7939e-03, PNorm = 55.7363, GNorm = 0.9108, lr_0 = 8.4988e-04
Loss = 1.4763e-03, PNorm = 55.7728, GNorm = 2.3679, lr_0 = 8.3466e-04
Validation rmse logS = 0.699613
Validation R2 logS = 0.892422
Epoch 9
Train function
Loss = 1.3398e-03, PNorm = 55.8119, GNorm = 0.7058, lr_0 = 8.1971e-04
Validation rmse logS = 0.620349
Validation R2 logS = 0.915417
Epoch 10
Train function
Loss = 1.3156e-03, PNorm = 55.8559, GNorm = 0.4488, lr_0 = 8.0357e-04
Validation rmse logS = 0.626606
Validation R2 logS = 0.913702
Epoch 11
Train function
Loss = 1.0474e-03, PNorm = 55.8888, GNorm = 0.6909, lr_0 = 7.8918e-04
Loss = 1.1294e-03, PNorm = 55.9249, GNorm = 1.6918, lr_0 = 7.7504e-04
Validation rmse logS = 0.650803
Validation R2 logS = 0.906909
Epoch 12
Train function
Loss = 9.4750e-04, PNorm = 55.9589, GNorm = 1.5728, lr_0 = 7.5979e-04
Validation rmse logS = 0.634985
Validation R2 logS = 0.911379
Epoch 13
Train function
Loss = 1.0875e-03, PNorm = 55.9945, GNorm = 0.6131, lr_0 = 7.4618e-04
Validation rmse logS = 0.698353
Validation R2 logS = 0.892809
Epoch 14
Train function
Loss = 1.5660e-03, PNorm = 56.0350, GNorm = 2.7729, lr_0 = 7.3149e-04
Loss = 1.3775e-03, PNorm = 56.0694, GNorm = 0.5721, lr_0 = 7.1839e-04
Validation rmse logS = 0.613084
Validation R2 logS = 0.917387
Epoch 15
Train function
Loss = 8.9159e-04, PNorm = 56.1073, GNorm = 0.7162, lr_0 = 7.0552e-04
Validation rmse logS = 0.614547
Validation R2 logS = 0.916992
Epoch 16
Train function
Loss = 8.1318e-04, PNorm = 56.1432, GNorm = 0.4703, lr_0 = 6.9163e-04
Validation rmse logS = 0.640512
Validation R2 logS = 0.909830
Epoch 17
Train function
Loss = 9.9510e-04, PNorm = 56.1749, GNorm = 1.3937, lr_0 = 6.7924e-04
Loss = 8.4551e-04, PNorm = 56.2093, GNorm = 1.2809, lr_0 = 6.6708e-04
Validation rmse logS = 0.712031
Validation R2 logS = 0.888569
Epoch 18
Train function
Loss = 1.0198e-03, PNorm = 56.2458, GNorm = 1.8843, lr_0 = 6.5395e-04
Validation rmse logS = 0.845584
Validation R2 logS = 0.842847
Epoch 19
Train function
Loss = 1.3220e-03, PNorm = 56.2826, GNorm = 2.6951, lr_0 = 6.4223e-04
Validation rmse logS = 0.675041
Validation R2 logS = 0.899846
Epoch 20
Train function
Loss = 1.0580e-03, PNorm = 56.3158, GNorm = 1.6040, lr_0 = 6.2959e-04
Loss = 7.6004e-04, PNorm = 56.3480, GNorm = 0.4004, lr_0 = 6.1831e-04
Validation rmse logS = 0.606225
Validation R2 logS = 0.919225
Epoch 21
Train function
Loss = 6.0690e-04, PNorm = 56.3749, GNorm = 0.4794, lr_0 = 6.0724e-04
Validation rmse logS = 0.592548
Validation R2 logS = 0.922829
Epoch 22
Train function
Loss = 4.3964e-04, PNorm = 56.4029, GNorm = 0.5222, lr_0 = 5.9529e-04
Loss = 6.7559e-04, PNorm = 56.4273, GNorm = 0.9983, lr_0 = 5.8462e-04
Validation rmse logS = 0.616965
Validation R2 logS = 0.916338
Epoch 23
Train function
Loss = 6.6498e-04, PNorm = 56.4556, GNorm = 0.5254, lr_0 = 5.7415e-04
Validation rmse logS = 0.609013
Validation R2 logS = 0.918480
Epoch 24
Train function
Loss = 5.7658e-04, PNorm = 56.4824, GNorm = 0.3314, lr_0 = 5.6285e-04
Validation rmse logS = 0.593143
Validation R2 logS = 0.922673
Epoch 25
Train function
Loss = 5.3142e-04, PNorm = 56.5090, GNorm = 0.7450, lr_0 = 5.5277e-04
Loss = 5.7430e-04, PNorm = 56.5362, GNorm = 1.0807, lr_0 = 5.4287e-04
Loss = 1.3501e-03, PNorm = 56.5385, GNorm = 0.5216, lr_0 = 5.4189e-04
Validation rmse logS = 0.586033
Validation R2 logS = 0.924516
Epoch 26
Train function
Loss = 4.7793e-04, PNorm = 56.5621, GNorm = 0.4077, lr_0 = 5.3218e-04
Validation rmse logS = 0.580758
Validation R2 logS = 0.925869
Epoch 27
Train function
Loss = 4.5846e-04, PNorm = 56.5897, GNorm = 0.8592, lr_0 = 5.2171e-04
Validation rmse logS = 0.595478
Validation R2 logS = 0.922064
Epoch 28
Train function
Loss = 6.4332e-04, PNorm = 56.6144, GNorm = 0.6332, lr_0 = 5.1236e-04
Loss = 3.4653e-04, PNorm = 56.6409, GNorm = 0.6036, lr_0 = 5.0318e-04
Loss = 5.5671e-04, PNorm = 56.6432, GNorm = 0.5320, lr_0 = 5.0228e-04
Validation rmse logS = 0.570325
Validation R2 logS = 0.928509
Epoch 29
Train function
Loss = 3.7669e-04, PNorm = 56.6659, GNorm = 0.3517, lr_0 = 4.9328e-04
Validation rmse logS = 0.581692
Validation R2 logS = 0.925630
Epoch 30
Train function
Loss = 3.6163e-04, PNorm = 56.6896, GNorm = 0.6039, lr_0 = 4.8444e-04
Validation rmse logS = 0.585337
Validation R2 logS = 0.924695
Epoch 31
Train function
Loss = 2.7500e-04, PNorm = 56.7127, GNorm = 0.2563, lr_0 = 4.7491e-04
Loss = 3.2677e-04, PNorm = 56.7360, GNorm = 0.6646, lr_0 = 4.6640e-04
Validation rmse logS = 0.585576
Validation R2 logS = 0.924634
Epoch 32
Train function
Loss = 3.2554e-04, PNorm = 56.7585, GNorm = 1.1258, lr_0 = 4.5805e-04
Validation rmse logS = 0.596206
Validation R2 logS = 0.921873
Epoch 33
Train function
Loss = 3.6217e-04, PNorm = 56.7831, GNorm = 0.4407, lr_0 = 4.4903e-04
Validation rmse logS = 0.597346
Validation R2 logS = 0.921574
Epoch 34
Train function
Loss = 2.6125e-04, PNorm = 56.8031, GNorm = 0.4497, lr_0 = 4.4099e-04
Loss = 3.4463e-04, PNorm = 56.8257, GNorm = 0.7539, lr_0 = 4.3309e-04
Validation rmse logS = 0.590262
Validation R2 logS = 0.923423
Epoch 35
Train function
Loss = 3.5163e-04, PNorm = 56.8471, GNorm = 1.4001, lr_0 = 4.2456e-04
Validation rmse logS = 0.604205
Validation R2 logS = 0.919762
Epoch 36
Train function
Loss = 3.0061e-04, PNorm = 56.8694, GNorm = 0.3351, lr_0 = 4.1696e-04
Validation rmse logS = 0.594054
Validation R2 logS = 0.922436
Epoch 37
Train function
Loss = 2.4313e-04, PNorm = 56.8917, GNorm = 0.4165, lr_0 = 4.0875e-04
Loss = 2.6188e-04, PNorm = 56.9091, GNorm = 0.2894, lr_0 = 4.0143e-04
Validation rmse logS = 0.576244
Validation R2 logS = 0.927017
Epoch 38
Train function
Loss = 2.3711e-04, PNorm = 56.9288, GNorm = 0.5810, lr_0 = 3.9424e-04
Validation rmse logS = 0.599853
Validation R2 logS = 0.920914
Epoch 39
Train function
Loss = 2.6943e-04, PNorm = 56.9468, GNorm = 0.8764, lr_0 = 3.8648e-04
Validation rmse logS = 0.597729
Validation R2 logS = 0.921473
Epoch 40
Train function
Loss = 2.6902e-04, PNorm = 56.9641, GNorm = 0.9591, lr_0 = 3.7956e-04
Loss = 2.8636e-04, PNorm = 56.9810, GNorm = 0.4484, lr_0 = 3.7276e-04
Validation rmse logS = 0.601894
Validation R2 logS = 0.920375
Epoch 41
Train function
Loss = 2.3907e-04, PNorm = 56.9990, GNorm = 0.3548, lr_0 = 3.6542e-04
Validation rmse logS = 0.597102
Validation R2 logS = 0.921638
Epoch 42
Train function
Loss = 1.9910e-04, PNorm = 57.0146, GNorm = 0.3727, lr_0 = 3.5888e-04
Validation rmse logS = 0.585845
Validation R2 logS = 0.924565
Epoch 43
Train function
Loss = 1.2314e-04, PNorm = 57.0284, GNorm = 0.1930, lr_0 = 3.5181e-04
Loss = 1.8789e-04, PNorm = 57.0424, GNorm = 0.2705, lr_0 = 3.4551e-04
Validation rmse logS = 0.580747
Validation R2 logS = 0.925872
Epoch 44
Train function
Loss = 2.0884e-04, PNorm = 57.0576, GNorm = 0.2757, lr_0 = 3.3932e-04
Validation rmse logS = 0.583110
Validation R2 logS = 0.925267
Epoch 45
Train function
Loss = 1.9529e-04, PNorm = 57.0717, GNorm = 0.2925, lr_0 = 3.3264e-04
Loss = 1.9534e-04, PNorm = 57.0846, GNorm = 0.6466, lr_0 = 3.2668e-04
Validation rmse logS = 0.579948
Validation R2 logS = 0.926076
Epoch 46
Train function
Loss = 1.5838e-04, PNorm = 57.0986, GNorm = 0.4973, lr_0 = 3.2083e-04
Validation rmse logS = 0.586740
Validation R2 logS = 0.924334
Epoch 47
Train function
Loss = 1.9816e-04, PNorm = 57.1152, GNorm = 0.5388, lr_0 = 3.1452e-04
Validation rmse logS = 0.604612
Validation R2 logS = 0.919654
Epoch 48
Train function
Loss = 1.9157e-04, PNorm = 57.1279, GNorm = 0.3749, lr_0 = 3.0888e-04
Loss = 2.1671e-04, PNorm = 57.1414, GNorm = 0.7151, lr_0 = 3.0335e-04
Loss = 4.5129e-04, PNorm = 57.1422, GNorm = 0.3889, lr_0 = 3.0280e-04
Validation rmse logS = 0.593629
Validation R2 logS = 0.922547
Epoch 49
Train function
Loss = 1.7652e-04, PNorm = 57.1540, GNorm = 0.2746, lr_0 = 2.9738e-04
Validation rmse logS = 0.598875
Validation R2 logS = 0.921172
Epoch 50
Train function
Loss = 1.8725e-04, PNorm = 57.1648, GNorm = 0.9971, lr_0 = 2.9205e-04
Validation rmse logS = 0.584216
Validation R2 logS = 0.924984
Epoch 51
Train function
Loss = 1.2199e-04, PNorm = 57.1779, GNorm = 0.5814, lr_0 = 2.8630e-04
Loss = 1.7987e-04, PNorm = 57.1875, GNorm = 0.6530, lr_0 = 2.8118e-04
Loss = 1.6431e-04, PNorm = 57.1886, GNorm = 0.2955, lr_0 = 2.8067e-04
Validation rmse logS = 0.588646
Validation R2 logS = 0.923842
Epoch 52
Train function
Loss = 1.6500e-04, PNorm = 57.1997, GNorm = 0.6634, lr_0 = 2.7564e-04
Validation rmse logS = 0.586664
Validation R2 logS = 0.924354
Epoch 53
Train function
Loss = 1.1847e-04, PNorm = 57.2092, GNorm = 0.4242, lr_0 = 2.7070e-04
Validation rmse logS = 0.587126
Validation R2 logS = 0.924234
Epoch 54
Train function
Loss = 8.7715e-05, PNorm = 57.2200, GNorm = 0.1570, lr_0 = 2.6538e-04
Loss = 1.3829e-04, PNorm = 57.2280, GNorm = 0.4179, lr_0 = 2.6062e-04
Validation rmse logS = 0.588533
Validation R2 logS = 0.923871
Epoch 55
Train function
Loss = 1.2633e-04, PNorm = 57.2376, GNorm = 0.2058, lr_0 = 2.5595e-04
Validation rmse logS = 0.592140
Validation R2 logS = 0.922935
Epoch 56
Train function
Loss = 1.2206e-04, PNorm = 57.2464, GNorm = 0.2080, lr_0 = 2.5092e-04
Validation rmse logS = 0.605238
Validation R2 logS = 0.919488
Epoch 57
Train function
Loss = 8.8039e-05, PNorm = 57.2558, GNorm = 0.2791, lr_0 = 2.4642e-04
Loss = 1.2916e-04, PNorm = 57.2639, GNorm = 0.2397, lr_0 = 2.4201e-04
Validation rmse logS = 0.592446
Validation R2 logS = 0.922855
Epoch 58
Train function
Loss = 1.2701e-04, PNorm = 57.2720, GNorm = 0.6360, lr_0 = 2.3724e-04
Validation rmse logS = 0.584666
Validation R2 logS = 0.924868
Epoch 59
Train function
Loss = 1.0418e-04, PNorm = 57.2821, GNorm = 0.2147, lr_0 = 2.3300e-04
Validation rmse logS = 0.587284
Validation R2 logS = 0.924194
Epoch 60
Train function
Loss = 1.2493e-04, PNorm = 57.2907, GNorm = 0.3305, lr_0 = 2.2841e-04
Loss = 9.7343e-05, PNorm = 57.2987, GNorm = 0.3866, lr_0 = 2.2432e-04
Validation rmse logS = 0.590454
Validation R2 logS = 0.923373
Epoch 61
Train function
Loss = 1.0634e-04, PNorm = 57.3066, GNorm = 0.1364, lr_0 = 2.2030e-04
Validation rmse logS = 0.594212
Validation R2 logS = 0.922395
Epoch 62
Train function
Loss = 1.0318e-04, PNorm = 57.3144, GNorm = 0.2411, lr_0 = 2.1596e-04
Validation rmse logS = 0.586113
Validation R2 logS = 0.924496
Epoch 63
Train function
Loss = 1.0895e-04, PNorm = 57.3219, GNorm = 0.3486, lr_0 = 2.1210e-04
Loss = 9.1529e-05, PNorm = 57.3285, GNorm = 0.2176, lr_0 = 2.0830e-04
Validation rmse logS = 0.596074
Validation R2 logS = 0.921907
Epoch 64
Train function
Loss = 7.5414e-05, PNorm = 57.3346, GNorm = 0.4949, lr_0 = 2.0420e-04
Validation rmse logS = 0.595266
Validation R2 logS = 0.922119
Epoch 65
Train function
Loss = 9.2286e-05, PNorm = 57.3417, GNorm = 0.5252, lr_0 = 2.0054e-04
Validation rmse logS = 0.599204
Validation R2 logS = 0.921085
Epoch 66
Train function
Loss = 1.1984e-04, PNorm = 57.3493, GNorm = 0.6325, lr_0 = 1.9659e-04
Loss = 9.5016e-05, PNorm = 57.3548, GNorm = 0.3936, lr_0 = 1.9307e-04
Validation rmse logS = 0.589504
Validation R2 logS = 0.923619
Epoch 67
Train function
Loss = 1.1315e-04, PNorm = 57.3620, GNorm = 0.3588, lr_0 = 1.8961e-04
Validation rmse logS = 0.590146
Validation R2 logS = 0.923453
Epoch 68
Train function
Loss = 7.4330e-05, PNorm = 57.3673, GNorm = 0.1308, lr_0 = 1.8588e-04
Loss = 1.1023e-04, PNorm = 57.3727, GNorm = 1.0716, lr_0 = 1.8255e-04
Validation rmse logS = 0.587195
Validation R2 logS = 0.924217
Epoch 69
Train function
Loss = 9.1662e-05, PNorm = 57.3788, GNorm = 0.3749, lr_0 = 1.7928e-04
Validation rmse logS = 0.598617
Validation R2 logS = 0.921240
Epoch 70
Train function
Loss = 8.3833e-05, PNorm = 57.3848, GNorm = 0.1485, lr_0 = 1.7575e-04
Validation rmse logS = 0.589285
Validation R2 logS = 0.923676
Epoch 71
Train function
Loss = 1.0154e-04, PNorm = 57.3907, GNorm = 0.3424, lr_0 = 1.7260e-04
Loss = 8.6593e-05, PNorm = 57.3956, GNorm = 0.2604, lr_0 = 1.6951e-04
Loss = 8.3211e-05, PNorm = 57.3960, GNorm = 0.1758, lr_0 = 1.6921e-04
Validation rmse logS = 0.590967
Validation R2 logS = 0.923240
Epoch 72
Train function
Loss = 8.1970e-05, PNorm = 57.4020, GNorm = 0.1743, lr_0 = 1.6617e-04
Validation rmse logS = 0.590791
Validation R2 logS = 0.923286
Epoch 73
Train function
Loss = 7.5416e-05, PNorm = 57.4059, GNorm = 0.1816, lr_0 = 1.6320e-04
Validation rmse logS = 0.593009
Validation R2 logS = 0.922709
Epoch 74
Train function
Loss = 8.9074e-05, PNorm = 57.4117, GNorm = 0.1686, lr_0 = 1.5999e-04
Loss = 6.5936e-05, PNorm = 57.4166, GNorm = 0.1696, lr_0 = 1.5712e-04
Validation rmse logS = 0.593525
Validation R2 logS = 0.922574
Epoch 75
Train function
Loss = 5.7676e-05, PNorm = 57.4221, GNorm = 0.3028, lr_0 = 1.5431e-04
Validation rmse logS = 0.595313
Validation R2 logS = 0.922107
Epoch 76
Train function
Loss = 4.5413e-05, PNorm = 57.4267, GNorm = 0.1340, lr_0 = 1.5127e-04
Validation rmse logS = 0.596924
Validation R2 logS = 0.921684
Epoch 77
Train function
Loss = 5.6774e-05, PNorm = 57.4311, GNorm = 0.2512, lr_0 = 1.4829e-04
Loss = 7.2400e-05, PNorm = 57.4356, GNorm = 0.2791, lr_0 = 1.4563e-04
Validation rmse logS = 0.592134
Validation R2 logS = 0.922936
Epoch 78
Train function
Loss = 7.6313e-05, PNorm = 57.4406, GNorm = 0.2144, lr_0 = 1.4303e-04
Validation rmse logS = 0.601646
Validation R2 logS = 0.920441
Epoch 79
Train function
Loss = 7.6503e-05, PNorm = 57.4444, GNorm = 0.4459, lr_0 = 1.4021e-04
Validation rmse logS = 0.597191
Validation R2 logS = 0.921615
Epoch 80
Train function
Loss = 3.6161e-05, PNorm = 57.4486, GNorm = 0.2815, lr_0 = 1.3770e-04
Loss = 7.7406e-05, PNorm = 57.4521, GNorm = 0.2245, lr_0 = 1.3523e-04
Validation rmse logS = 0.596332
Validation R2 logS = 0.921840
Epoch 81
Train function
Loss = 6.4347e-05, PNorm = 57.4564, GNorm = 0.2834, lr_0 = 1.3257e-04
Validation rmse logS = 0.588653
Validation R2 logS = 0.923840
Epoch 82
Train function
Loss = 5.8771e-05, PNorm = 57.4601, GNorm = 0.1454, lr_0 = 1.3020e-04
Validation rmse logS = 0.601106
Validation R2 logS = 0.920583
Epoch 83
Train function
Loss = 7.2972e-05, PNorm = 57.4638, GNorm = 0.5651, lr_0 = 1.2763e-04
Loss = 6.2646e-05, PNorm = 57.4680, GNorm = 0.1378, lr_0 = 1.2535e-04
Validation rmse logS = 0.592863
Validation R2 logS = 0.922747
Epoch 84
Train function
Loss = 4.9487e-05, PNorm = 57.4718, GNorm = 0.1565, lr_0 = 1.2310e-04
Validation rmse logS = 0.598690
Validation R2 logS = 0.921221
Epoch 85
Train function
Loss = 5.5449e-05, PNorm = 57.4762, GNorm = 0.2171, lr_0 = 1.2068e-04
Validation rmse logS = 0.593868
Validation R2 logS = 0.922484
Epoch 86
Train function
Loss = 2.7813e-05, PNorm = 57.4796, GNorm = 0.1691, lr_0 = 1.1852e-04
Loss = 6.9442e-05, PNorm = 57.4825, GNorm = 0.3295, lr_0 = 1.1639e-04
Validation rmse logS = 0.591970
Validation R2 logS = 0.922979
Epoch 87
Train function
Loss = 5.9179e-05, PNorm = 57.4866, GNorm = 0.1564, lr_0 = 1.1410e-04
Validation rmse logS = 0.593929
Validation R2 logS = 0.922468
Epoch 88
Train function
Loss = 4.8526e-05, PNorm = 57.4897, GNorm = 0.1356, lr_0 = 1.1206e-04
Validation rmse logS = 0.597073
Validation R2 logS = 0.921645
Epoch 89
Train function
Loss = 2.1761e-05, PNorm = 57.4928, GNorm = 0.1503, lr_0 = 1.0985e-04
Loss = 5.8502e-05, PNorm = 57.4960, GNorm = 0.1875, lr_0 = 1.0789e-04
Validation rmse logS = 0.593990
Validation R2 logS = 0.922453
Epoch 90
Train function
Loss = 4.2215e-05, PNorm = 57.4991, GNorm = 0.1162, lr_0 = 1.0595e-04
Validation rmse logS = 0.589803
Validation R2 logS = 0.923542
Epoch 91
Train function
Loss = 4.7935e-05, PNorm = 57.5017, GNorm = 0.1751, lr_0 = 1.0387e-04
Loss = 5.2244e-05, PNorm = 57.5045, GNorm = 0.2412, lr_0 = 1.0201e-04
Validation rmse logS = 0.596704
Validation R2 logS = 0.921742
Epoch 92
Train function
Loss = 4.8954e-05, PNorm = 57.5072, GNorm = 0.2016, lr_0 = 1.0018e-04
Validation rmse logS = 0.593342
Validation R2 logS = 0.922622
Epoch 93
Train function
Loss = 6.0738e-05, PNorm = 57.5104, GNorm = 0.1408, lr_0 = 1.0000e-04
Validation rmse logS = 0.591953
Validation R2 logS = 0.922984
Epoch 94
Train function
Loss = 7.1135e-05, PNorm = 57.5137, GNorm = 0.3201, lr_0 = 1.0000e-04
Loss = 4.0369e-05, PNorm = 57.5163, GNorm = 0.1308, lr_0 = 1.0000e-04
Loss = 2.7855e-04, PNorm = 57.5167, GNorm = 0.3330, lr_0 = 1.0000e-04
Validation rmse logS = 0.595222
Validation R2 logS = 0.922130
Epoch 95
Train function
Loss = 4.2127e-05, PNorm = 57.5198, GNorm = 0.1708, lr_0 = 1.0000e-04
Validation rmse logS = 0.595789
Validation R2 logS = 0.921982
Epoch 96
Train function
Loss = 4.6116e-05, PNorm = 57.5227, GNorm = 0.2936, lr_0 = 1.0000e-04
Validation rmse logS = 0.600642
Validation R2 logS = 0.920706
Epoch 97
Train function
Loss = 5.2481e-05, PNorm = 57.5255, GNorm = 0.2947, lr_0 = 1.0000e-04
Loss = 5.3318e-05, PNorm = 57.5285, GNorm = 0.1798, lr_0 = 1.0000e-04
Validation rmse logS = 0.597259
Validation R2 logS = 0.921597
Epoch 98
Train function
Loss = 4.7720e-05, PNorm = 57.5309, GNorm = 0.2245, lr_0 = 1.0000e-04
Validation rmse logS = 0.601032
Validation R2 logS = 0.920603
Epoch 99
Train function
Loss = 4.8115e-05, PNorm = 57.5340, GNorm = 0.0993, lr_0 = 1.0000e-04
Validation rmse logS = 0.593432
Validation R2 logS = 0.922598
Model 0 best validation rmse = 0.570325 on epoch 28
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "substructures_encoder.encoder.cached_zero_vector".
Loading pretrained parameter "substructures_encoder.encoder.W_o.weight".
Loading pretrained parameter "substructures_encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.611049
Model 0 test R2 logS = 0.902389
Ensemble test rmse  logS= 0.611049
Ensemble test R2  logS= 0.902389
Fold 1
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_306/folds/fold_1',
 'save_smiles_splits': False,
 'seed': 1,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 674,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 1
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=125, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,500,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.6005e-02, PNorm = 55.2280, GNorm = 10.5405, lr_0 = 4.8077e-04
Validation rmse logS = 1.335837
Validation R2 logS = 0.577045
Epoch 1
Train function
Loss = 7.7318e-03, PNorm = 55.2939, GNorm = 4.3725, lr_0 = 8.6154e-04
Validation rmse logS = 0.998707
Validation R2 logS = 0.763591
Epoch 2
Train function
Loss = 4.6142e-03, PNorm = 55.3921, GNorm = 3.4265, lr_0 = 9.8743e-04
Loss = 3.1902e-03, PNorm = 55.4579, GNorm = 1.5659, lr_0 = 9.6974e-04
Validation rmse logS = 0.891284
Validation R2 logS = 0.811713
Epoch 3
Train function
Loss = 2.9609e-03, PNorm = 55.5073, GNorm = 1.6651, lr_0 = 9.5237e-04
Validation rmse logS = 0.813054
Validation R2 logS = 0.843315
Epoch 4
Train function
Loss = 2.1802e-03, PNorm = 55.5559, GNorm = 1.5766, lr_0 = 9.3363e-04
Validation rmse logS = 0.765994
Validation R2 logS = 0.860928
Epoch 5
Train function
Loss = 2.5727e-03, PNorm = 55.5926, GNorm = 0.9144, lr_0 = 9.1690e-04
Loss = 2.0979e-03, PNorm = 55.6287, GNorm = 1.5305, lr_0 = 9.0048e-04
Loss = 2.7228e-03, PNorm = 55.6309, GNorm = 1.5563, lr_0 = 8.9885e-04
Validation rmse logS = 0.721673
Validation R2 logS = 0.876556
Epoch 6
Train function
Loss = 1.6977e-03, PNorm = 55.6609, GNorm = 1.0564, lr_0 = 8.8275e-04
Validation rmse logS = 0.777430
Validation R2 logS = 0.856745
Epoch 7
Train function
Loss = 1.9205e-03, PNorm = 55.6898, GNorm = 2.1120, lr_0 = 8.6694e-04
Validation rmse logS = 0.688159
Validation R2 logS = 0.887756
Epoch 8
Train function
Loss = 1.3649e-03, PNorm = 55.7303, GNorm = 0.6165, lr_0 = 8.4988e-04
Loss = 1.4018e-03, PNorm = 55.7696, GNorm = 0.4853, lr_0 = 8.3466e-04
Validation rmse logS = 0.666197
Validation R2 logS = 0.894806
Epoch 9
Train function
Loss = 1.3113e-03, PNorm = 55.8037, GNorm = 0.5304, lr_0 = 8.1971e-04
Validation rmse logS = 0.728540
Validation R2 logS = 0.874196
Epoch 10
Train function
Loss = 1.3228e-03, PNorm = 55.8403, GNorm = 0.4809, lr_0 = 8.0357e-04
Validation rmse logS = 0.733108
Validation R2 logS = 0.872614
Epoch 11
Train function
Loss = 2.5651e-03, PNorm = 55.8753, GNorm = 4.2190, lr_0 = 7.8918e-04
Loss = 1.6740e-03, PNorm = 55.9150, GNorm = 0.9586, lr_0 = 7.7504e-04
Validation rmse logS = 0.690239
Validation R2 logS = 0.887076
Epoch 12
Train function
Loss = 1.2982e-03, PNorm = 55.9553, GNorm = 0.4555, lr_0 = 7.5979e-04
Validation rmse logS = 0.644557
Validation R2 logS = 0.901529
Epoch 13
Train function
Loss = 1.0583e-03, PNorm = 55.9881, GNorm = 0.4323, lr_0 = 7.4618e-04
Validation rmse logS = 0.640025
Validation R2 logS = 0.902909
Epoch 14
Train function
Loss = 8.8884e-04, PNorm = 56.0216, GNorm = 0.4132, lr_0 = 7.3149e-04
Loss = 1.0790e-03, PNorm = 56.0566, GNorm = 1.7023, lr_0 = 7.1839e-04
Validation rmse logS = 0.697175
Validation R2 logS = 0.884795
Epoch 15
Train function
Loss = 1.0684e-03, PNorm = 56.0882, GNorm = 1.0838, lr_0 = 7.0552e-04
Validation rmse logS = 0.623863
Validation R2 logS = 0.907750
Epoch 16
Train function
Loss = 9.8544e-04, PNorm = 56.1287, GNorm = 1.7658, lr_0 = 6.9163e-04
Validation rmse logS = 0.754734
Validation R2 logS = 0.864987
Epoch 17
Train function
Loss = 2.4931e-03, PNorm = 56.1639, GNorm = 3.6157, lr_0 = 6.7924e-04
Loss = 1.0254e-03, PNorm = 56.1991, GNorm = 0.8288, lr_0 = 6.6708e-04
Validation rmse logS = 0.616432
Validation R2 logS = 0.909935
Epoch 18
Train function
Loss = 9.7490e-04, PNorm = 56.2352, GNorm = 1.1953, lr_0 = 6.5395e-04
Validation rmse logS = 0.613683
Validation R2 logS = 0.910736
Epoch 19
Train function
Loss = 6.5316e-04, PNorm = 56.2620, GNorm = 1.0816, lr_0 = 6.4223e-04
Validation rmse logS = 0.613206
Validation R2 logS = 0.910875
Epoch 20
Train function
Loss = 8.7175e-04, PNorm = 56.2912, GNorm = 0.5340, lr_0 = 6.2959e-04
Loss = 6.1068e-04, PNorm = 56.3188, GNorm = 0.4418, lr_0 = 6.1831e-04
Validation rmse logS = 0.605127
Validation R2 logS = 0.913208
Epoch 21
Train function
Loss = 6.1216e-04, PNorm = 56.3535, GNorm = 0.4380, lr_0 = 6.0724e-04
Validation rmse logS = 0.604318
Validation R2 logS = 0.913440
Epoch 22
Train function
Loss = 5.7479e-04, PNorm = 56.3852, GNorm = 0.7078, lr_0 = 5.9529e-04
Loss = 6.7694e-04, PNorm = 56.4103, GNorm = 1.2548, lr_0 = 5.8462e-04
Validation rmse logS = 0.610101
Validation R2 logS = 0.911775
Epoch 23
Train function
Loss = 7.0334e-04, PNorm = 56.4380, GNorm = 0.7291, lr_0 = 5.7415e-04
Validation rmse logS = 0.669306
Validation R2 logS = 0.893822
Epoch 24
Train function
Loss = 7.2276e-04, PNorm = 56.4721, GNorm = 1.7730, lr_0 = 5.6285e-04
Validation rmse logS = 0.612043
Validation R2 logS = 0.911213
Epoch 25
Train function
Loss = 6.1545e-04, PNorm = 56.5038, GNorm = 1.2393, lr_0 = 5.5277e-04
Loss = 5.2807e-04, PNorm = 56.5361, GNorm = 0.7854, lr_0 = 5.4287e-04
Loss = 7.3017e-04, PNorm = 56.5384, GNorm = 0.3242, lr_0 = 5.4189e-04
Validation rmse logS = 0.604399
Validation R2 logS = 0.913417
Epoch 26
Train function
Loss = 5.0793e-04, PNorm = 56.5648, GNorm = 0.8713, lr_0 = 5.3218e-04
Validation rmse logS = 0.601631
Validation R2 logS = 0.914208
Epoch 27
Train function
Loss = 4.3595e-04, PNorm = 56.5897, GNorm = 0.4493, lr_0 = 5.2171e-04
Validation rmse logS = 0.607921
Validation R2 logS = 0.912405
Epoch 28
Train function
Loss = 2.9067e-04, PNorm = 56.6149, GNorm = 0.5376, lr_0 = 5.1236e-04
Loss = 4.3046e-04, PNorm = 56.6381, GNorm = 0.4061, lr_0 = 5.0318e-04
Loss = 1.2750e-03, PNorm = 56.6403, GNorm = 1.1756, lr_0 = 5.0228e-04
Validation rmse logS = 0.609143
Validation R2 logS = 0.912052
Epoch 29
Train function
Loss = 4.3535e-04, PNorm = 56.6638, GNorm = 0.4750, lr_0 = 4.9328e-04
Validation rmse logS = 0.629205
Validation R2 logS = 0.906164
Epoch 30
Train function
Loss = 4.6067e-04, PNorm = 56.6923, GNorm = 0.9232, lr_0 = 4.8444e-04
Validation rmse logS = 0.621780
Validation R2 logS = 0.908365
Epoch 31
Train function
Loss = 3.2632e-04, PNorm = 56.7174, GNorm = 0.3726, lr_0 = 4.7491e-04
Loss = 4.1993e-04, PNorm = 56.7432, GNorm = 0.9523, lr_0 = 4.6640e-04
Validation rmse logS = 0.625196
Validation R2 logS = 0.907356
Epoch 32
Train function
Loss = 4.0568e-04, PNorm = 56.7666, GNorm = 0.8343, lr_0 = 4.5805e-04
Validation rmse logS = 0.600112
Validation R2 logS = 0.914640
Epoch 33
Train function
Loss = 3.2305e-04, PNorm = 56.7913, GNorm = 0.2748, lr_0 = 4.4903e-04
Validation rmse logS = 0.593192
Validation R2 logS = 0.916598
Epoch 34
Train function
Loss = 2.2938e-04, PNorm = 56.8121, GNorm = 0.4395, lr_0 = 4.4099e-04
Loss = 3.5268e-04, PNorm = 56.8303, GNorm = 0.9028, lr_0 = 4.3309e-04
Validation rmse logS = 0.601802
Validation R2 logS = 0.914159
Epoch 35
Train function
Loss = 3.1355e-04, PNorm = 56.8545, GNorm = 0.2269, lr_0 = 4.2456e-04
Validation rmse logS = 0.609735
Validation R2 logS = 0.911881
Epoch 36
Train function
Loss = 3.3495e-04, PNorm = 56.8730, GNorm = 0.5094, lr_0 = 4.1696e-04
Validation rmse logS = 0.609279
Validation R2 logS = 0.912013
Epoch 37
Train function
Loss = 2.6112e-04, PNorm = 56.8901, GNorm = 0.3377, lr_0 = 4.0875e-04
Loss = 3.5729e-04, PNorm = 56.9068, GNorm = 0.6592, lr_0 = 4.0143e-04
Validation rmse logS = 0.601265
Validation R2 logS = 0.914312
Epoch 38
Train function
Loss = 2.6941e-04, PNorm = 56.9267, GNorm = 0.5631, lr_0 = 3.9424e-04
Validation rmse logS = 0.610856
Validation R2 logS = 0.911557
Epoch 39
Train function
Loss = 2.3653e-04, PNorm = 56.9443, GNorm = 0.4329, lr_0 = 3.8648e-04
Validation rmse logS = 0.604501
Validation R2 logS = 0.913387
Epoch 40
Train function
Loss = 3.6416e-04, PNorm = 56.9604, GNorm = 0.9990, lr_0 = 3.7956e-04
Loss = 2.4213e-04, PNorm = 56.9735, GNorm = 0.2942, lr_0 = 3.7276e-04
Validation rmse logS = 0.612618
Validation R2 logS = 0.911046
Epoch 41
Train function
Loss = 1.9886e-04, PNorm = 56.9884, GNorm = 0.2614, lr_0 = 3.6542e-04
Validation rmse logS = 0.600145
Validation R2 logS = 0.914631
Epoch 42
Train function
Loss = 2.8142e-04, PNorm = 57.0009, GNorm = 0.2434, lr_0 = 3.5888e-04
Validation rmse logS = 0.608342
Validation R2 logS = 0.912283
Epoch 43
Train function
Loss = 1.1637e-04, PNorm = 57.0158, GNorm = 0.2421, lr_0 = 3.5181e-04
Loss = 1.8494e-04, PNorm = 57.0298, GNorm = 0.1630, lr_0 = 3.4551e-04
Validation rmse logS = 0.599310
Validation R2 logS = 0.914869
Epoch 44
Train function
Loss = 1.5653e-04, PNorm = 57.0418, GNorm = 0.2069, lr_0 = 3.3932e-04
Validation rmse logS = 0.597443
Validation R2 logS = 0.915398
Epoch 45
Train function
Loss = 1.8824e-04, PNorm = 57.0521, GNorm = 0.5118, lr_0 = 3.3264e-04
Loss = 2.2015e-04, PNorm = 57.0643, GNorm = 0.6819, lr_0 = 3.2668e-04
Validation rmse logS = 0.608396
Validation R2 logS = 0.912268
Epoch 46
Train function
Loss = 1.7553e-04, PNorm = 57.0747, GNorm = 0.3490, lr_0 = 3.2083e-04
Validation rmse logS = 0.596304
Validation R2 logS = 0.915720
Epoch 47
Train function
Loss = 1.3615e-04, PNorm = 57.0869, GNorm = 0.2338, lr_0 = 3.1452e-04
Validation rmse logS = 0.597207
Validation R2 logS = 0.915465
Epoch 48
Train function
Loss = 1.0685e-04, PNorm = 57.0974, GNorm = 0.2875, lr_0 = 3.0888e-04
Loss = 1.5210e-04, PNorm = 57.1078, GNorm = 0.2323, lr_0 = 3.0335e-04
Loss = 4.8640e-04, PNorm = 57.1093, GNorm = 0.5795, lr_0 = 3.0280e-04
Validation rmse logS = 0.604351
Validation R2 logS = 0.913430
Epoch 49
Train function
Loss = 1.5370e-04, PNorm = 57.1207, GNorm = 0.7822, lr_0 = 2.9738e-04
Validation rmse logS = 0.592014
Validation R2 logS = 0.916929
Epoch 50
Train function
Loss = 2.4857e-04, PNorm = 57.1340, GNorm = 1.3367, lr_0 = 2.9205e-04
Validation rmse logS = 0.610629
Validation R2 logS = 0.911623
Epoch 51
Train function
Loss = 1.9871e-04, PNorm = 57.1465, GNorm = 0.2227, lr_0 = 2.8630e-04
Loss = 1.8799e-04, PNorm = 57.1584, GNorm = 0.7378, lr_0 = 2.8118e-04
Loss = 8.8842e-05, PNorm = 57.1596, GNorm = 0.2244, lr_0 = 2.8067e-04
Validation rmse logS = 0.602119
Validation R2 logS = 0.914069
Epoch 52
Train function
Loss = 1.9617e-04, PNorm = 57.1682, GNorm = 0.2267, lr_0 = 2.7564e-04
Validation rmse logS = 0.604144
Validation R2 logS = 0.913490
Epoch 53
Train function
Loss = 1.8301e-04, PNorm = 57.1786, GNorm = 0.2945, lr_0 = 2.7070e-04
Validation rmse logS = 0.602495
Validation R2 logS = 0.913961
Epoch 54
Train function
Loss = 1.5421e-04, PNorm = 57.1913, GNorm = 0.6594, lr_0 = 2.6538e-04
Loss = 1.2949e-04, PNorm = 57.2019, GNorm = 0.2119, lr_0 = 2.6062e-04
Validation rmse logS = 0.602430
Validation R2 logS = 0.913980
Epoch 55
Train function
Loss = 1.7376e-04, PNorm = 57.2096, GNorm = 0.3196, lr_0 = 2.5595e-04
Validation rmse logS = 0.602303
Validation R2 logS = 0.914016
Epoch 56
Train function
Loss = 1.3147e-04, PNorm = 57.2174, GNorm = 0.3385, lr_0 = 2.5092e-04
Validation rmse logS = 0.609087
Validation R2 logS = 0.912068
Epoch 57
Train function
Loss = 1.6864e-04, PNorm = 57.2239, GNorm = 0.5999, lr_0 = 2.4642e-04
Loss = 1.2304e-04, PNorm = 57.2316, GNorm = 0.2141, lr_0 = 2.4201e-04
Validation rmse logS = 0.601626
Validation R2 logS = 0.914209
Epoch 58
Train function
Loss = 1.0184e-04, PNorm = 57.2399, GNorm = 0.2184, lr_0 = 2.3724e-04
Validation rmse logS = 0.597210
Validation R2 logS = 0.915464
Epoch 59
Train function
Loss = 9.5273e-05, PNorm = 57.2485, GNorm = 0.4906, lr_0 = 2.3300e-04
Validation rmse logS = 0.608145
Validation R2 logS = 0.912340
Epoch 60
Train function
Loss = 1.0010e-04, PNorm = 57.2560, GNorm = 0.4572, lr_0 = 2.2841e-04
Loss = 1.1181e-04, PNorm = 57.2631, GNorm = 0.3549, lr_0 = 2.2432e-04
Validation rmse logS = 0.607098
Validation R2 logS = 0.912642
Epoch 61
Train function
Loss = 9.9164e-05, PNorm = 57.2712, GNorm = 0.2571, lr_0 = 2.2030e-04
Validation rmse logS = 0.599599
Validation R2 logS = 0.914786
Epoch 62
Train function
Loss = 9.7306e-05, PNorm = 57.2782, GNorm = 0.2646, lr_0 = 2.1596e-04
Validation rmse logS = 0.604270
Validation R2 logS = 0.913454
Epoch 63
Train function
Loss = 2.2355e-04, PNorm = 57.2844, GNorm = 0.6053, lr_0 = 2.1210e-04
Loss = 9.3423e-05, PNorm = 57.2905, GNorm = 0.2927, lr_0 = 2.0830e-04
Validation rmse logS = 0.598600
Validation R2 logS = 0.915070
Epoch 64
Train function
Loss = 1.0087e-04, PNorm = 57.2976, GNorm = 0.6549, lr_0 = 2.0420e-04
Validation rmse logS = 0.620899
Validation R2 logS = 0.908625
Epoch 65
Train function
Loss = 1.0559e-04, PNorm = 57.3046, GNorm = 0.3709, lr_0 = 2.0054e-04
Validation rmse logS = 0.609082
Validation R2 logS = 0.912070
Epoch 66
Train function
Loss = 1.4693e-04, PNorm = 57.3111, GNorm = 0.1725, lr_0 = 1.9659e-04
Loss = 6.7065e-05, PNorm = 57.3171, GNorm = 0.1992, lr_0 = 1.9307e-04
Validation rmse logS = 0.604743
Validation R2 logS = 0.913318
Epoch 67
Train function
Loss = 6.4668e-05, PNorm = 57.3216, GNorm = 0.1729, lr_0 = 1.8961e-04
Validation rmse logS = 0.605328
Validation R2 logS = 0.913150
Epoch 68
Train function
Loss = 7.7517e-05, PNorm = 57.3285, GNorm = 0.2543, lr_0 = 1.8588e-04
Loss = 7.2384e-05, PNorm = 57.3341, GNorm = 0.3497, lr_0 = 1.8255e-04
Validation rmse logS = 0.604371
Validation R2 logS = 0.913425
Epoch 69
Train function
Loss = 7.2201e-05, PNorm = 57.3391, GNorm = 0.2298, lr_0 = 1.7928e-04
Validation rmse logS = 0.604268
Validation R2 logS = 0.913454
Epoch 70
Train function
Loss = 6.9272e-05, PNorm = 57.3449, GNorm = 0.1864, lr_0 = 1.7575e-04
Validation rmse logS = 0.609924
Validation R2 logS = 0.911826
Epoch 71
Train function
Loss = 5.8425e-05, PNorm = 57.3500, GNorm = 0.1274, lr_0 = 1.7260e-04
Loss = 8.1278e-05, PNorm = 57.3550, GNorm = 0.4369, lr_0 = 1.6951e-04
Loss = 1.3582e-04, PNorm = 57.3554, GNorm = 0.1840, lr_0 = 1.6921e-04
Validation rmse logS = 0.612167
Validation R2 logS = 0.911177
Epoch 72
Train function
Loss = 7.1559e-05, PNorm = 57.3609, GNorm = 0.3945, lr_0 = 1.6617e-04
Validation rmse logS = 0.616153
Validation R2 logS = 0.910016
Epoch 73
Train function
Loss = 6.6548e-05, PNorm = 57.3655, GNorm = 0.1890, lr_0 = 1.6320e-04
Validation rmse logS = 0.602341
Validation R2 logS = 0.914005
Epoch 74
Train function
Loss = 6.3053e-05, PNorm = 57.3707, GNorm = 0.2532, lr_0 = 1.5999e-04
Loss = 6.2951e-05, PNorm = 57.3751, GNorm = 0.3289, lr_0 = 1.5712e-04
Validation rmse logS = 0.608005
Validation R2 logS = 0.912380
Epoch 75
Train function
Loss = 6.5915e-05, PNorm = 57.3800, GNorm = 0.2409, lr_0 = 1.5431e-04
Validation rmse logS = 0.609454
Validation R2 logS = 0.911962
Epoch 76
Train function
Loss = 7.4684e-05, PNorm = 57.3844, GNorm = 0.3516, lr_0 = 1.5127e-04
Validation rmse logS = 0.608496
Validation R2 logS = 0.912239
Epoch 77
Train function
Loss = 5.6703e-05, PNorm = 57.3892, GNorm = 0.1523, lr_0 = 1.4829e-04
Loss = 6.2039e-05, PNorm = 57.3931, GNorm = 0.3615, lr_0 = 1.4563e-04
Validation rmse logS = 0.612768
Validation R2 logS = 0.911002
Epoch 78
Train function
Loss = 6.6300e-05, PNorm = 57.3977, GNorm = 0.3131, lr_0 = 1.4303e-04
Validation rmse logS = 0.611548
Validation R2 logS = 0.911356
Epoch 79
Train function
Loss = 5.6391e-05, PNorm = 57.4020, GNorm = 0.3806, lr_0 = 1.4021e-04
Validation rmse logS = 0.605234
Validation R2 logS = 0.913177
Epoch 80
Train function
Loss = 7.8504e-05, PNorm = 57.4049, GNorm = 0.2373, lr_0 = 1.3770e-04
Loss = 5.2192e-05, PNorm = 57.4090, GNorm = 0.1845, lr_0 = 1.3523e-04
Validation rmse logS = 0.613703
Validation R2 logS = 0.910730
Epoch 81
Train function
Loss = 5.8235e-05, PNorm = 57.4136, GNorm = 0.1449, lr_0 = 1.3257e-04
Validation rmse logS = 0.610898
Validation R2 logS = 0.911545
Epoch 82
Train function
Loss = 4.7819e-05, PNorm = 57.4166, GNorm = 0.1738, lr_0 = 1.3020e-04
Validation rmse logS = 0.609343
Validation R2 logS = 0.911994
Epoch 83
Train function
Loss = 5.0126e-05, PNorm = 57.4203, GNorm = 0.1908, lr_0 = 1.2763e-04
Loss = 5.3598e-05, PNorm = 57.4238, GNorm = 0.1019, lr_0 = 1.2535e-04
Validation rmse logS = 0.611408
Validation R2 logS = 0.911397
Epoch 84
Train function
Loss = 3.5528e-05, PNorm = 57.4265, GNorm = 0.1818, lr_0 = 1.2310e-04
Validation rmse logS = 0.608865
Validation R2 logS = 0.912132
Epoch 85
Train function
Loss = 2.9870e-05, PNorm = 57.4301, GNorm = 0.0961, lr_0 = 1.2068e-04
Validation rmse logS = 0.612925
Validation R2 logS = 0.910957
Epoch 86
Train function
Loss = 4.2944e-05, PNorm = 57.4333, GNorm = 0.1817, lr_0 = 1.1852e-04
Loss = 4.4889e-05, PNorm = 57.4361, GNorm = 0.1849, lr_0 = 1.1639e-04
Validation rmse logS = 0.607485
Validation R2 logS = 0.912530
Epoch 87
Train function
Loss = 5.4380e-05, PNorm = 57.4395, GNorm = 0.1413, lr_0 = 1.1410e-04
Validation rmse logS = 0.608469
Validation R2 logS = 0.912247
Epoch 88
Train function
Loss = 4.6203e-05, PNorm = 57.4430, GNorm = 0.2207, lr_0 = 1.1206e-04
Validation rmse logS = 0.604301
Validation R2 logS = 0.913445
Epoch 89
Train function
Loss = 7.8426e-05, PNorm = 57.4465, GNorm = 0.3054, lr_0 = 1.0985e-04
Loss = 5.3282e-05, PNorm = 57.4492, GNorm = 0.1438, lr_0 = 1.0789e-04
Validation rmse logS = 0.608947
Validation R2 logS = 0.912109
Epoch 90
Train function
Loss = 4.0558e-05, PNorm = 57.4519, GNorm = 0.2241, lr_0 = 1.0595e-04
Validation rmse logS = 0.620428
Validation R2 logS = 0.908763
Epoch 91
Train function
Loss = 5.2244e-05, PNorm = 57.4550, GNorm = 0.2491, lr_0 = 1.0387e-04
Loss = 5.7369e-05, PNorm = 57.4574, GNorm = 0.1086, lr_0 = 1.0201e-04
Validation rmse logS = 0.610325
Validation R2 logS = 0.911710
Epoch 92
Train function
Loss = 5.4242e-05, PNorm = 57.4603, GNorm = 0.3671, lr_0 = 1.0018e-04
Validation rmse logS = 0.619896
Validation R2 logS = 0.908920
Epoch 93
Train function
Loss = 4.3312e-05, PNorm = 57.4634, GNorm = 0.2720, lr_0 = 1.0000e-04
Validation rmse logS = 0.610223
Validation R2 logS = 0.911740
Epoch 94
Train function
Loss = 2.6463e-05, PNorm = 57.4656, GNorm = 0.2894, lr_0 = 1.0000e-04
Loss = 4.9117e-05, PNorm = 57.4683, GNorm = 0.2336, lr_0 = 1.0000e-04
Loss = 8.1877e-05, PNorm = 57.4686, GNorm = 0.3030, lr_0 = 1.0000e-04
Validation rmse logS = 0.612185
Validation R2 logS = 0.911172
Epoch 95
Train function
Loss = 3.8807e-05, PNorm = 57.4711, GNorm = 0.1802, lr_0 = 1.0000e-04
Validation rmse logS = 0.612854
Validation R2 logS = 0.910977
Epoch 96
Train function
Loss = 3.7530e-05, PNorm = 57.4736, GNorm = 0.1263, lr_0 = 1.0000e-04
Validation rmse logS = 0.615710
Validation R2 logS = 0.910146
Epoch 97
Train function
Loss = 3.6310e-05, PNorm = 57.4762, GNorm = 0.2539, lr_0 = 1.0000e-04
Loss = 4.2478e-05, PNorm = 57.4788, GNorm = 0.3799, lr_0 = 1.0000e-04
Validation rmse logS = 0.608304
Validation R2 logS = 0.912294
Epoch 98
Train function
Loss = 4.3733e-05, PNorm = 57.4809, GNorm = 0.2068, lr_0 = 1.0000e-04
Validation rmse logS = 0.608124
Validation R2 logS = 0.912346
Epoch 99
Train function
Loss = 3.3934e-05, PNorm = 57.4837, GNorm = 0.1670, lr_0 = 1.0000e-04
Validation rmse logS = 0.609752
Validation R2 logS = 0.911876
Model 0 best validation rmse = 0.592014 on epoch 49
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "substructures_encoder.encoder.cached_zero_vector".
Loading pretrained parameter "substructures_encoder.encoder.W_o.weight".
Loading pretrained parameter "substructures_encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.666252
Model 0 test R2 logS = 0.883955
Ensemble test rmse  logS= 0.666252
Ensemble test R2  logS= 0.883955
Fold 2
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_306/folds/fold_2',
 'save_smiles_splits': False,
 'seed': 2,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 674,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 2
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=125, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,500,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.6145e-02, PNorm = 55.2300, GNorm = 6.1275, lr_0 = 4.8077e-04
Validation rmse logS = 1.302177
Validation R2 logS = 0.606981
Epoch 1
Train function
Loss = 7.3467e-03, PNorm = 55.3009, GNorm = 4.7513, lr_0 = 8.6154e-04
Validation rmse logS = 0.995151
Validation R2 logS = 0.770464
Epoch 2
Train function
Loss = 4.0316e-03, PNorm = 55.3881, GNorm = 1.8042, lr_0 = 9.8743e-04
Loss = 4.2436e-03, PNorm = 55.4555, GNorm = 2.2175, lr_0 = 9.6974e-04
Validation rmse logS = 0.845204
Validation R2 logS = 0.834424
Epoch 3
Train function
Loss = 3.0862e-03, PNorm = 55.5126, GNorm = 1.6292, lr_0 = 9.5237e-04
Validation rmse logS = 0.882858
Validation R2 logS = 0.819343
Epoch 4
Train function
Loss = 3.0570e-03, PNorm = 55.5701, GNorm = 1.0350, lr_0 = 9.3363e-04
Validation rmse logS = 0.798815
Validation R2 logS = 0.852101
Epoch 5
Train function
Loss = 2.3187e-03, PNorm = 55.6117, GNorm = 1.3501, lr_0 = 9.1690e-04
Loss = 2.2213e-03, PNorm = 55.6456, GNorm = 0.5455, lr_0 = 9.0048e-04
Loss = 5.6532e-03, PNorm = 55.6494, GNorm = 1.1245, lr_0 = 8.9885e-04
Validation rmse logS = 0.718393
Validation R2 logS = 0.880382
Epoch 6
Train function
Loss = 1.8070e-03, PNorm = 55.6812, GNorm = 0.8211, lr_0 = 8.8275e-04
Validation rmse logS = 0.702914
Validation R2 logS = 0.885481
Epoch 7
Train function
Loss = 1.8512e-03, PNorm = 55.7115, GNorm = 1.0012, lr_0 = 8.6694e-04
Validation rmse logS = 0.666057
Validation R2 logS = 0.897176
Epoch 8
Train function
Loss = 1.2488e-03, PNorm = 55.7484, GNorm = 1.4979, lr_0 = 8.4988e-04
Loss = 1.4775e-03, PNorm = 55.7823, GNorm = 2.1366, lr_0 = 8.3466e-04
Validation rmse logS = 0.688579
Validation R2 logS = 0.890104
Epoch 9
Train function
Loss = 1.3646e-03, PNorm = 55.8132, GNorm = 0.5128, lr_0 = 8.1971e-04
Validation rmse logS = 0.635289
Validation R2 logS = 0.906456
Epoch 10
Train function
Loss = 1.0972e-03, PNorm = 55.8547, GNorm = 0.5088, lr_0 = 8.0357e-04
Validation rmse logS = 0.626997
Validation R2 logS = 0.908882
Epoch 11
Train function
Loss = 1.1412e-03, PNorm = 55.8868, GNorm = 0.7458, lr_0 = 7.8918e-04
Loss = 1.3453e-03, PNorm = 55.9226, GNorm = 0.5207, lr_0 = 7.7504e-04
Validation rmse logS = 0.681919
Validation R2 logS = 0.892220
Epoch 12
Train function
Loss = 1.0466e-03, PNorm = 55.9636, GNorm = 0.9470, lr_0 = 7.5979e-04
Validation rmse logS = 0.658798
Validation R2 logS = 0.899404
Epoch 13
Train function
Loss = 1.0481e-03, PNorm = 55.9977, GNorm = 1.1974, lr_0 = 7.4618e-04
Validation rmse logS = 0.617180
Validation R2 logS = 0.911713
Epoch 14
Train function
Loss = 6.6299e-04, PNorm = 56.0348, GNorm = 0.4598, lr_0 = 7.3149e-04
Loss = 9.8640e-04, PNorm = 56.0648, GNorm = 0.9862, lr_0 = 7.1839e-04
Validation rmse logS = 0.626297
Validation R2 logS = 0.909085
Epoch 15
Train function
Loss = 8.1921e-04, PNorm = 56.1015, GNorm = 1.4669, lr_0 = 7.0552e-04
Validation rmse logS = 0.629238
Validation R2 logS = 0.908229
Epoch 16
Train function
Loss = 8.0311e-04, PNorm = 56.1387, GNorm = 0.3480, lr_0 = 6.9163e-04
Validation rmse logS = 0.592055
Validation R2 logS = 0.918755
Epoch 17
Train function
Loss = 7.4693e-04, PNorm = 56.1688, GNorm = 0.3143, lr_0 = 6.7924e-04
Loss = 8.8749e-04, PNorm = 56.1992, GNorm = 0.9026, lr_0 = 6.6708e-04
Validation rmse logS = 0.624488
Validation R2 logS = 0.909610
Epoch 18
Train function
Loss = 8.7742e-04, PNorm = 56.2331, GNorm = 0.6847, lr_0 = 6.5395e-04
Validation rmse logS = 0.615043
Validation R2 logS = 0.912323
Epoch 19
Train function
Loss = 9.7078e-04, PNorm = 56.2627, GNorm = 1.9095, lr_0 = 6.4223e-04
Validation rmse logS = 0.596415
Validation R2 logS = 0.917554
Epoch 20
Train function
Loss = 8.5382e-04, PNorm = 56.2975, GNorm = 1.1995, lr_0 = 6.2959e-04
Loss = 6.6716e-04, PNorm = 56.3223, GNorm = 0.8892, lr_0 = 6.1831e-04
Validation rmse logS = 0.603413
Validation R2 logS = 0.915608
Epoch 21
Train function
Loss = 7.0345e-04, PNorm = 56.3553, GNorm = 2.1460, lr_0 = 6.0724e-04
Validation rmse logS = 0.638939
Validation R2 logS = 0.905378
Epoch 22
Train function
Loss = 7.5673e-04, PNorm = 56.3890, GNorm = 0.3832, lr_0 = 5.9529e-04
Loss = 8.4045e-04, PNorm = 56.4192, GNorm = 0.4758, lr_0 = 5.8462e-04
Validation rmse logS = 0.629395
Validation R2 logS = 0.908184
Epoch 23
Train function
Loss = 7.0098e-04, PNorm = 56.4513, GNorm = 0.4605, lr_0 = 5.7415e-04
Validation rmse logS = 0.571883
Validation R2 logS = 0.924197
Epoch 24
Train function
Loss = 5.2566e-04, PNorm = 56.4801, GNorm = 0.5733, lr_0 = 5.6285e-04
Validation rmse logS = 0.567986
Validation R2 logS = 0.925226
Epoch 25
Train function
Loss = 4.9458e-04, PNorm = 56.5051, GNorm = 1.0907, lr_0 = 5.5277e-04
Loss = 5.7950e-04, PNorm = 56.5321, GNorm = 0.3071, lr_0 = 5.4287e-04
Loss = 1.1279e-03, PNorm = 56.5336, GNorm = 1.2362, lr_0 = 5.4189e-04
Validation rmse logS = 0.579723
Validation R2 logS = 0.922104
Epoch 26
Train function
Loss = 5.4323e-04, PNorm = 56.5595, GNorm = 0.9003, lr_0 = 5.3218e-04
Validation rmse logS = 0.570135
Validation R2 logS = 0.924660
Epoch 27
Train function
Loss = 3.8723e-04, PNorm = 56.5851, GNorm = 0.3287, lr_0 = 5.2171e-04
Validation rmse logS = 0.602295
Validation R2 logS = 0.915920
Epoch 28
Train function
Loss = 5.5099e-04, PNorm = 56.6053, GNorm = 0.3446, lr_0 = 5.1236e-04
Loss = 5.6188e-04, PNorm = 56.6282, GNorm = 0.9222, lr_0 = 5.0318e-04
Loss = 7.0664e-04, PNorm = 56.6302, GNorm = 0.9887, lr_0 = 5.0228e-04
Validation rmse logS = 0.581395
Validation R2 logS = 0.921654
Epoch 29
Train function
Loss = 4.4875e-04, PNorm = 56.6581, GNorm = 0.6434, lr_0 = 4.9328e-04
Validation rmse logS = 0.565412
Validation R2 logS = 0.925902
Epoch 30
Train function
Loss = 4.2284e-04, PNorm = 56.6811, GNorm = 0.3844, lr_0 = 4.8444e-04
Validation rmse logS = 0.574670
Validation R2 logS = 0.923456
Epoch 31
Train function
Loss = 3.3028e-04, PNorm = 56.7043, GNorm = 0.3549, lr_0 = 4.7491e-04
Loss = 3.8872e-04, PNorm = 56.7276, GNorm = 0.7408, lr_0 = 4.6640e-04
Validation rmse logS = 0.557262
Validation R2 logS = 0.928023
Epoch 32
Train function
Loss = 3.2305e-04, PNorm = 56.7486, GNorm = 0.2523, lr_0 = 4.5805e-04
Validation rmse logS = 0.558881
Validation R2 logS = 0.927605
Epoch 33
Train function
Loss = 3.3543e-04, PNorm = 56.7722, GNorm = 0.7270, lr_0 = 4.4903e-04
Validation rmse logS = 0.563067
Validation R2 logS = 0.926516
Epoch 34
Train function
Loss = 4.4612e-04, PNorm = 56.7921, GNorm = 0.5830, lr_0 = 4.4099e-04
Loss = 3.5135e-04, PNorm = 56.8138, GNorm = 0.2962, lr_0 = 4.3309e-04
Validation rmse logS = 0.567045
Validation R2 logS = 0.925474
Epoch 35
Train function
Loss = 3.5696e-04, PNorm = 56.8341, GNorm = 1.3551, lr_0 = 4.2456e-04
Validation rmse logS = 0.581533
Validation R2 logS = 0.921617
Epoch 36
Train function
Loss = 3.3851e-04, PNorm = 56.8525, GNorm = 0.6652, lr_0 = 4.1696e-04
Validation rmse logS = 0.567692
Validation R2 logS = 0.925304
Epoch 37
Train function
Loss = 3.0850e-04, PNorm = 56.8725, GNorm = 0.6677, lr_0 = 4.0875e-04
Loss = 3.1342e-04, PNorm = 56.8938, GNorm = 0.2466, lr_0 = 4.0143e-04
Validation rmse logS = 0.555602
Validation R2 logS = 0.928451
Epoch 38
Train function
Loss = 3.7266e-04, PNorm = 56.9108, GNorm = 0.5319, lr_0 = 3.9424e-04
Validation rmse logS = 0.588165
Validation R2 logS = 0.919819
Epoch 39
Train function
Loss = 2.4530e-04, PNorm = 56.9313, GNorm = 0.9543, lr_0 = 3.8648e-04
Validation rmse logS = 0.576603
Validation R2 logS = 0.922940
Epoch 40
Train function
Loss = 2.1685e-04, PNorm = 56.9470, GNorm = 0.5886, lr_0 = 3.7956e-04
Loss = 2.9659e-04, PNorm = 56.9670, GNorm = 0.5796, lr_0 = 3.7276e-04
Validation rmse logS = 0.556692
Validation R2 logS = 0.928171
Epoch 41
Train function
Loss = 2.0902e-04, PNorm = 56.9872, GNorm = 0.6343, lr_0 = 3.6542e-04
Validation rmse logS = 0.600360
Validation R2 logS = 0.916459
Epoch 42
Train function
Loss = 4.3943e-04, PNorm = 57.0050, GNorm = 1.0323, lr_0 = 3.5888e-04
Validation rmse logS = 0.580818
Validation R2 logS = 0.921809
Epoch 43
Train function
Loss = 3.2862e-04, PNorm = 57.0231, GNorm = 1.0709, lr_0 = 3.5181e-04
Loss = 2.7506e-04, PNorm = 57.0399, GNorm = 0.3002, lr_0 = 3.4551e-04
Validation rmse logS = 0.556147
Validation R2 logS = 0.928311
Epoch 44
Train function
Loss = 1.9389e-04, PNorm = 57.0547, GNorm = 0.6233, lr_0 = 3.3932e-04
Validation rmse logS = 0.556247
Validation R2 logS = 0.928285
Epoch 45
Train function
Loss = 2.0567e-04, PNorm = 57.0690, GNorm = 0.2347, lr_0 = 3.3264e-04
Loss = 2.2422e-04, PNorm = 57.0834, GNorm = 0.4283, lr_0 = 3.2668e-04
Validation rmse logS = 0.563993
Validation R2 logS = 0.926274
Epoch 46
Train function
Loss = 1.8712e-04, PNorm = 57.0956, GNorm = 0.3170, lr_0 = 3.2083e-04
Validation rmse logS = 0.555870
Validation R2 logS = 0.928382
Epoch 47
Train function
Loss = 2.0322e-04, PNorm = 57.1111, GNorm = 0.2687, lr_0 = 3.1452e-04
Validation rmse logS = 0.563083
Validation R2 logS = 0.926512
Epoch 48
Train function
Loss = 1.7281e-04, PNorm = 57.1249, GNorm = 0.5126, lr_0 = 3.0888e-04
Loss = 1.9614e-04, PNorm = 57.1378, GNorm = 0.1780, lr_0 = 3.0335e-04
Loss = 2.0086e-04, PNorm = 57.1389, GNorm = 0.3072, lr_0 = 3.0280e-04
Validation rmse logS = 0.566907
Validation R2 logS = 0.925510
Epoch 49
Train function
Loss = 1.6094e-04, PNorm = 57.1494, GNorm = 0.4989, lr_0 = 2.9738e-04
Validation rmse logS = 0.562976
Validation R2 logS = 0.926540
Epoch 50
Train function
Loss = 1.9085e-04, PNorm = 57.1612, GNorm = 0.6629, lr_0 = 2.9205e-04
Validation rmse logS = 0.565006
Validation R2 logS = 0.926009
Epoch 51
Train function
Loss = 2.3453e-04, PNorm = 57.1748, GNorm = 0.3554, lr_0 = 2.8630e-04
Loss = 1.6133e-04, PNorm = 57.1872, GNorm = 0.5125, lr_0 = 2.8118e-04
Loss = 2.7736e-04, PNorm = 57.1879, GNorm = 0.3207, lr_0 = 2.8067e-04
Validation rmse logS = 0.552293
Validation R2 logS = 0.929301
Epoch 52
Train function
Loss = 1.5992e-04, PNorm = 57.1987, GNorm = 0.5539, lr_0 = 2.7564e-04
Validation rmse logS = 0.551606
Validation R2 logS = 0.929477
Epoch 53
Train function
Loss = 1.3892e-04, PNorm = 57.2101, GNorm = 0.4464, lr_0 = 2.7070e-04
Validation rmse logS = 0.555732
Validation R2 logS = 0.928418
Epoch 54
Train function
Loss = 1.9158e-04, PNorm = 57.2204, GNorm = 1.0360, lr_0 = 2.6538e-04
Loss = 1.5978e-04, PNorm = 57.2313, GNorm = 0.3007, lr_0 = 2.6062e-04
Validation rmse logS = 0.553255
Validation R2 logS = 0.929054
Epoch 55
Train function
Loss = 1.3307e-04, PNorm = 57.2425, GNorm = 0.2122, lr_0 = 2.5595e-04
Validation rmse logS = 0.559790
Validation R2 logS = 0.927369
Epoch 56
Train function
Loss = 1.7204e-04, PNorm = 57.2530, GNorm = 0.1705, lr_0 = 2.5092e-04
Validation rmse logS = 0.554058
Validation R2 logS = 0.928849
Epoch 57
Train function
Loss = 1.1738e-04, PNorm = 57.2634, GNorm = 0.2209, lr_0 = 2.4642e-04
Loss = 1.2648e-04, PNorm = 57.2716, GNorm = 0.4227, lr_0 = 2.4201e-04
Validation rmse logS = 0.571016
Validation R2 logS = 0.924426
Epoch 58
Train function
Loss = 1.2063e-04, PNorm = 57.2812, GNorm = 0.1998, lr_0 = 2.3724e-04
Validation rmse logS = 0.569414
Validation R2 logS = 0.924850
Epoch 59
Train function
Loss = 1.1611e-04, PNorm = 57.2902, GNorm = 0.3251, lr_0 = 2.3300e-04
Validation rmse logS = 0.558855
Validation R2 logS = 0.927611
Epoch 60
Train function
Loss = 1.0903e-04, PNorm = 57.2993, GNorm = 0.2002, lr_0 = 2.2841e-04
Loss = 1.2591e-04, PNorm = 57.3092, GNorm = 0.2498, lr_0 = 2.2432e-04
Validation rmse logS = 0.557364
Validation R2 logS = 0.927997
Epoch 61
Train function
Loss = 1.0518e-04, PNorm = 57.3161, GNorm = 0.2862, lr_0 = 2.2030e-04
Validation rmse logS = 0.558900
Validation R2 logS = 0.927599
Epoch 62
Train function
Loss = 9.6529e-05, PNorm = 57.3258, GNorm = 0.4006, lr_0 = 2.1596e-04
Validation rmse logS = 0.562565
Validation R2 logS = 0.926647
Epoch 63
Train function
Loss = 1.2422e-04, PNorm = 57.3323, GNorm = 0.3289, lr_0 = 2.1210e-04
Loss = 9.9980e-05, PNorm = 57.3398, GNorm = 0.2924, lr_0 = 2.0830e-04
Validation rmse logS = 0.564891
Validation R2 logS = 0.926039
Epoch 64
Train function
Loss = 1.0317e-04, PNorm = 57.3475, GNorm = 0.4677, lr_0 = 2.0420e-04
Validation rmse logS = 0.563854
Validation R2 logS = 0.926310
Epoch 65
Train function
Loss = 1.0606e-04, PNorm = 57.3556, GNorm = 0.3136, lr_0 = 2.0054e-04
Validation rmse logS = 0.566062
Validation R2 logS = 0.925732
Epoch 66
Train function
Loss = 1.0177e-04, PNorm = 57.3657, GNorm = 0.1451, lr_0 = 1.9659e-04
Loss = 1.0659e-04, PNorm = 57.3722, GNorm = 0.4484, lr_0 = 1.9307e-04
Validation rmse logS = 0.560806
Validation R2 logS = 0.927105
Epoch 67
Train function
Loss = 1.0938e-04, PNorm = 57.3792, GNorm = 0.1430, lr_0 = 1.8961e-04
Validation rmse logS = 0.560792
Validation R2 logS = 0.927109
Epoch 68
Train function
Loss = 7.3080e-05, PNorm = 57.3857, GNorm = 0.0964, lr_0 = 1.8588e-04
Loss = 9.2549e-05, PNorm = 57.3926, GNorm = 0.1418, lr_0 = 1.8255e-04
Validation rmse logS = 0.560170
Validation R2 logS = 0.927270
Epoch 69
Train function
Loss = 7.9036e-05, PNorm = 57.3991, GNorm = 0.1102, lr_0 = 1.7928e-04
Validation rmse logS = 0.562458
Validation R2 logS = 0.926675
Epoch 70
Train function
Loss = 6.6284e-05, PNorm = 57.4064, GNorm = 0.0972, lr_0 = 1.7575e-04
Validation rmse logS = 0.567280
Validation R2 logS = 0.925412
Epoch 71
Train function
Loss = 8.0636e-05, PNorm = 57.4123, GNorm = 0.0929, lr_0 = 1.7260e-04
Loss = 6.9859e-05, PNorm = 57.4180, GNorm = 0.2635, lr_0 = 1.6951e-04
Loss = 7.0930e-05, PNorm = 57.4185, GNorm = 0.2145, lr_0 = 1.6921e-04
Validation rmse logS = 0.557813
Validation R2 logS = 0.927881
Epoch 72
Train function
Loss = 7.0939e-05, PNorm = 57.4235, GNorm = 0.2949, lr_0 = 1.6617e-04
Validation rmse logS = 0.561349
Validation R2 logS = 0.926964
Epoch 73
Train function
Loss = 6.1469e-05, PNorm = 57.4300, GNorm = 0.1307, lr_0 = 1.6320e-04
Validation rmse logS = 0.558202
Validation R2 logS = 0.927780
Epoch 74
Train function
Loss = 5.0367e-05, PNorm = 57.4357, GNorm = 0.1410, lr_0 = 1.5999e-04
Loss = 7.3312e-05, PNorm = 57.4401, GNorm = 0.1404, lr_0 = 1.5712e-04
Validation rmse logS = 0.564634
Validation R2 logS = 0.926106
Epoch 75
Train function
Loss = 8.3688e-05, PNorm = 57.4463, GNorm = 0.2757, lr_0 = 1.5431e-04
Validation rmse logS = 0.561668
Validation R2 logS = 0.926881
Epoch 76
Train function
Loss = 5.9640e-05, PNorm = 57.4525, GNorm = 0.3055, lr_0 = 1.5127e-04
Validation rmse logS = 0.559985
Validation R2 logS = 0.927318
Epoch 77
Train function
Loss = 9.1294e-05, PNorm = 57.4576, GNorm = 0.2275, lr_0 = 1.4829e-04
Loss = 6.2502e-05, PNorm = 57.4627, GNorm = 0.2914, lr_0 = 1.4563e-04
Validation rmse logS = 0.557535
Validation R2 logS = 0.927953
Epoch 78
Train function
Loss = 5.6135e-05, PNorm = 57.4673, GNorm = 0.1557, lr_0 = 1.4303e-04
Validation rmse logS = 0.565134
Validation R2 logS = 0.925975
Epoch 79
Train function
Loss = 5.6011e-05, PNorm = 57.4735, GNorm = 0.3334, lr_0 = 1.4021e-04
Validation rmse logS = 0.560238
Validation R2 logS = 0.927253
Epoch 80
Train function
Loss = 5.3902e-05, PNorm = 57.4786, GNorm = 0.1081, lr_0 = 1.3770e-04
Loss = 5.9016e-05, PNorm = 57.4823, GNorm = 0.2024, lr_0 = 1.3523e-04
Validation rmse logS = 0.561655
Validation R2 logS = 0.926884
Epoch 81
Train function
Loss = 5.9549e-05, PNorm = 57.4876, GNorm = 0.1150, lr_0 = 1.3257e-04
Validation rmse logS = 0.563853
Validation R2 logS = 0.926311
Epoch 82
Train function
Loss = 6.6575e-05, PNorm = 57.4923, GNorm = 0.4816, lr_0 = 1.3020e-04
Validation rmse logS = 0.560857
Validation R2 logS = 0.927091
Epoch 83
Train function
Loss = 3.1283e-05, PNorm = 57.4964, GNorm = 0.1472, lr_0 = 1.2763e-04
Loss = 7.0333e-05, PNorm = 57.5001, GNorm = 0.2566, lr_0 = 1.2535e-04
Validation rmse logS = 0.563235
Validation R2 logS = 0.926472
Epoch 84
Train function
Loss = 6.0258e-05, PNorm = 57.5042, GNorm = 0.1892, lr_0 = 1.2310e-04
Validation rmse logS = 0.559779
Validation R2 logS = 0.927372
Epoch 85
Train function
Loss = 5.0392e-05, PNorm = 57.5085, GNorm = 0.1969, lr_0 = 1.2068e-04
Validation rmse logS = 0.559246
Validation R2 logS = 0.927510
Epoch 86
Train function
Loss = 4.4043e-05, PNorm = 57.5118, GNorm = 0.1360, lr_0 = 1.1852e-04
Loss = 6.1352e-05, PNorm = 57.5154, GNorm = 0.2086, lr_0 = 1.1639e-04
Validation rmse logS = 0.564841
Validation R2 logS = 0.926052
Epoch 87
Train function
Loss = 5.4911e-05, PNorm = 57.5194, GNorm = 0.3298, lr_0 = 1.1410e-04
Validation rmse logS = 0.562992
Validation R2 logS = 0.926535
Epoch 88
Train function
Loss = 6.4617e-05, PNorm = 57.5225, GNorm = 0.2176, lr_0 = 1.1206e-04
Validation rmse logS = 0.560627
Validation R2 logS = 0.927151
Epoch 89
Train function
Loss = 6.0321e-05, PNorm = 57.5268, GNorm = 0.1608, lr_0 = 1.0985e-04
Loss = 4.8458e-05, PNorm = 57.5303, GNorm = 0.1130, lr_0 = 1.0789e-04
Validation rmse logS = 0.559847
Validation R2 logS = 0.927354
Epoch 90
Train function
Loss = 4.9649e-05, PNorm = 57.5332, GNorm = 0.4578, lr_0 = 1.0595e-04
Validation rmse logS = 0.562613
Validation R2 logS = 0.926634
Epoch 91
Train function
Loss = 4.2266e-05, PNorm = 57.5364, GNorm = 0.1262, lr_0 = 1.0387e-04
Loss = 5.3518e-05, PNorm = 57.5396, GNorm = 0.4390, lr_0 = 1.0201e-04
Validation rmse logS = 0.560069
Validation R2 logS = 0.927296
Epoch 92
Train function
Loss = 4.4392e-05, PNorm = 57.5425, GNorm = 0.1614, lr_0 = 1.0018e-04
Validation rmse logS = 0.562615
Validation R2 logS = 0.926634
Epoch 93
Train function
Loss = 3.5524e-05, PNorm = 57.5449, GNorm = 0.0854, lr_0 = 1.0000e-04
Validation rmse logS = 0.564263
Validation R2 logS = 0.926203
Epoch 94
Train function
Loss = 2.6368e-05, PNorm = 57.5480, GNorm = 0.1410, lr_0 = 1.0000e-04
Loss = 4.8207e-05, PNorm = 57.5503, GNorm = 0.1976, lr_0 = 1.0000e-04
Loss = 4.0217e-05, PNorm = 57.5506, GNorm = 0.1913, lr_0 = 1.0000e-04
Validation rmse logS = 0.563903
Validation R2 logS = 0.926297
Epoch 95
Train function
Loss = 4.8014e-05, PNorm = 57.5538, GNorm = 0.4142, lr_0 = 1.0000e-04
Validation rmse logS = 0.563876
Validation R2 logS = 0.926305
Epoch 96
Train function
Loss = 5.3859e-05, PNorm = 57.5568, GNorm = 0.5746, lr_0 = 1.0000e-04
Validation rmse logS = 0.565567
Validation R2 logS = 0.925862
Epoch 97
Train function
Loss = 3.3715e-05, PNorm = 57.5603, GNorm = 0.2979, lr_0 = 1.0000e-04
Loss = 4.8720e-05, PNorm = 57.5627, GNorm = 0.2875, lr_0 = 1.0000e-04
Validation rmse logS = 0.560763
Validation R2 logS = 0.927116
Epoch 98
Train function
Loss = 3.6589e-05, PNorm = 57.5657, GNorm = 0.0860, lr_0 = 1.0000e-04
Validation rmse logS = 0.563233
Validation R2 logS = 0.926472
Epoch 99
Train function
Loss = 3.4601e-05, PNorm = 57.5682, GNorm = 0.1275, lr_0 = 1.0000e-04
Validation rmse logS = 0.564115
Validation R2 logS = 0.926242
Model 0 best validation rmse = 0.551606 on epoch 52
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "substructures_encoder.encoder.cached_zero_vector".
Loading pretrained parameter "substructures_encoder.encoder.W_o.weight".
Loading pretrained parameter "substructures_encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.624855
Model 0 test R2 logS = 0.897928
Ensemble test rmse  logS= 0.624855
Ensemble test R2  logS= 0.897928
Fold 3
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_306/folds/fold_3',
 'save_smiles_splits': False,
 'seed': 3,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 674,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 3
Total size = 899 | train size = 675 | val size = 224 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=125, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,500,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.6814e-02, PNorm = 55.2287, GNorm = 11.5008, lr_0 = 4.8077e-04
Validation rmse logS = 1.224888
Validation R2 logS = 0.648124
Epoch 1
Train function
Loss = 6.1836e-03, PNorm = 55.2969, GNorm = 3.1943, lr_0 = 8.6154e-04
Validation rmse logS = 0.975474
Validation R2 logS = 0.776834
Epoch 2
Train function
Loss = 4.8160e-03, PNorm = 55.3790, GNorm = 1.4078, lr_0 = 9.8921e-04
Loss = 3.7330e-03, PNorm = 55.4397, GNorm = 0.9879, lr_0 = 9.7150e-04
Loss = 8.2520e-03, PNorm = 55.4447, GNorm = 1.8350, lr_0 = 9.6974e-04
Validation rmse logS = 0.946273
Validation R2 logS = 0.789995
Epoch 3
Train function
Loss = 3.1951e-03, PNorm = 55.4978, GNorm = 1.4900, lr_0 = 9.5237e-04
Validation rmse logS = 0.725944
Validation R2 logS = 0.876404
Epoch 4
Train function
Loss = 2.5184e-03, PNorm = 55.5450, GNorm = 0.7462, lr_0 = 9.3531e-04
Validation rmse logS = 0.698376
Validation R2 logS = 0.885613
Epoch 5
Train function
Loss = 1.7712e-03, PNorm = 55.5876, GNorm = 1.7977, lr_0 = 9.1690e-04
Loss = 2.1928e-03, PNorm = 55.6258, GNorm = 1.7733, lr_0 = 9.0048e-04
Validation rmse logS = 0.686437
Validation R2 logS = 0.889491
Epoch 6
Train function
Loss = 1.7086e-03, PNorm = 55.6638, GNorm = 0.8408, lr_0 = 8.8435e-04
Validation rmse logS = 0.681433
Validation R2 logS = 0.891096
Epoch 7
Train function
Loss = 1.4163e-03, PNorm = 55.7016, GNorm = 0.5083, lr_0 = 8.6694e-04
Validation rmse logS = 0.717277
Validation R2 logS = 0.879338
Epoch 8
Train function
Loss = 1.2158e-03, PNorm = 55.7314, GNorm = 0.8700, lr_0 = 8.5141e-04
Loss = 1.4137e-03, PNorm = 55.7653, GNorm = 1.8451, lr_0 = 8.3617e-04
Validation rmse logS = 0.640217
Validation R2 logS = 0.903872
Epoch 9
Train function
Loss = 1.4199e-03, PNorm = 55.8052, GNorm = 1.1471, lr_0 = 8.1971e-04
Validation rmse logS = 0.868509
Validation R2 logS = 0.823093
Epoch 10
Train function
Loss = 1.6042e-03, PNorm = 55.8356, GNorm = 1.0234, lr_0 = 8.0502e-04
Validation rmse logS = 0.647093
Validation R2 logS = 0.901796
Epoch 11
Train function
Loss = 1.1644e-03, PNorm = 55.8721, GNorm = 0.7740, lr_0 = 7.8918e-04
Loss = 1.1680e-03, PNorm = 55.9073, GNorm = 1.4330, lr_0 = 7.7504e-04
Validation rmse logS = 0.626511
Validation R2 logS = 0.907943
Epoch 12
Train function
Loss = 1.1096e-03, PNorm = 55.9383, GNorm = 0.9390, lr_0 = 7.6116e-04
Validation rmse logS = 0.637028
Validation R2 logS = 0.904827
Epoch 13
Train function
Loss = 1.0369e-03, PNorm = 55.9715, GNorm = 1.2670, lr_0 = 7.4618e-04
Validation rmse logS = 0.641870
Validation R2 logS = 0.903374
Epoch 14
Train function
Loss = 1.2463e-03, PNorm = 55.9998, GNorm = 0.7681, lr_0 = 7.3281e-04
Loss = 9.1910e-04, PNorm = 56.0341, GNorm = 0.9360, lr_0 = 7.1969e-04
Validation rmse logS = 0.623249
Validation R2 logS = 0.908900
Epoch 15
Train function
Loss = 9.2685e-04, PNorm = 56.0635, GNorm = 0.7959, lr_0 = 7.0552e-04
Validation rmse logS = 0.620852
Validation R2 logS = 0.909599
Epoch 16
Train function
Loss = 6.4948e-04, PNorm = 56.0908, GNorm = 0.6939, lr_0 = 6.9288e-04
Validation rmse logS = 0.630478
Validation R2 logS = 0.906774
Epoch 17
Train function
Loss = 8.7997e-04, PNorm = 56.1227, GNorm = 0.5107, lr_0 = 6.7924e-04
Loss = 8.2955e-04, PNorm = 56.1498, GNorm = 0.7066, lr_0 = 6.6708e-04
Validation rmse logS = 0.612620
Validation R2 logS = 0.911980
Epoch 18
Train function
Loss = 8.2357e-04, PNorm = 56.1814, GNorm = 1.1128, lr_0 = 6.5513e-04
Validation rmse logS = 0.652678
Validation R2 logS = 0.900093
Epoch 19
Train function
Loss = 7.5366e-04, PNorm = 56.2131, GNorm = 0.7419, lr_0 = 6.4223e-04
Loss = 8.9097e-04, PNorm = 56.2377, GNorm = 1.9938, lr_0 = 6.3073e-04
Validation rmse logS = 0.622079
Validation R2 logS = 0.909241
Epoch 20
Train function
Loss = 7.4613e-04, PNorm = 56.2672, GNorm = 0.3328, lr_0 = 6.1943e-04
Validation rmse logS = 0.668572
Validation R2 logS = 0.895168
Epoch 21
Train function
Loss = 6.3899e-04, PNorm = 56.2989, GNorm = 0.6325, lr_0 = 6.0724e-04
Validation rmse logS = 0.622897
Validation R2 logS = 0.909003
Epoch 22
Train function
Loss = 4.0439e-04, PNorm = 56.3282, GNorm = 0.6388, lr_0 = 5.9636e-04
Loss = 6.3596e-04, PNorm = 56.3539, GNorm = 0.6875, lr_0 = 5.8568e-04
Loss = 1.3536e-03, PNorm = 56.3571, GNorm = 0.3649, lr_0 = 5.8462e-04
Validation rmse logS = 0.607855
Validation R2 logS = 0.913344
Epoch 23
Train function
Loss = 5.3127e-04, PNorm = 56.3857, GNorm = 0.5849, lr_0 = 5.7415e-04
Validation rmse logS = 0.600650
Validation R2 logS = 0.915386
Epoch 24
Train function
Loss = 4.6582e-04, PNorm = 56.4131, GNorm = 0.3929, lr_0 = 5.6387e-04
Validation rmse logS = 0.651801
Validation R2 logS = 0.900361
Epoch 25
Train function
Loss = 8.8361e-04, PNorm = 56.4419, GNorm = 2.0314, lr_0 = 5.5277e-04
Loss = 7.9513e-04, PNorm = 56.4677, GNorm = 0.6525, lr_0 = 5.4287e-04
Validation rmse logS = 0.650359
Validation R2 logS = 0.900802
Epoch 26
Train function
Loss = 5.6754e-04, PNorm = 56.5018, GNorm = 0.3067, lr_0 = 5.3314e-04
Validation rmse logS = 0.641442
Validation R2 logS = 0.903504
Epoch 27
Train function
Loss = 5.5902e-04, PNorm = 56.5300, GNorm = 1.6073, lr_0 = 5.2265e-04
Validation rmse logS = 0.611721
Validation R2 logS = 0.912238
Epoch 28
Train function
Loss = 7.0383e-04, PNorm = 56.5560, GNorm = 1.0689, lr_0 = 5.1329e-04
Loss = 4.4231e-04, PNorm = 56.5781, GNorm = 0.8489, lr_0 = 5.0409e-04
Validation rmse logS = 0.648544
Validation R2 logS = 0.901355
Epoch 29
Train function
Loss = 5.8859e-04, PNorm = 56.6052, GNorm = 1.7962, lr_0 = 4.9417e-04
Validation rmse logS = 0.610384
Validation R2 logS = 0.912622
Epoch 30
Train function
Loss = 4.0501e-04, PNorm = 56.6309, GNorm = 0.2620, lr_0 = 4.8532e-04
Validation rmse logS = 0.605018
Validation R2 logS = 0.914151
Epoch 31
Train function
Loss = 4.7564e-04, PNorm = 56.6552, GNorm = 0.9697, lr_0 = 4.7577e-04
Loss = 3.9437e-04, PNorm = 56.6804, GNorm = 0.8916, lr_0 = 4.6725e-04
Validation rmse logS = 0.592032
Validation R2 logS = 0.917797
Epoch 32
Train function
Loss = 4.1637e-04, PNorm = 56.7006, GNorm = 0.5701, lr_0 = 4.5888e-04
Validation rmse logS = 0.598734
Validation R2 logS = 0.915925
Epoch 33
Train function
Loss = 3.1932e-04, PNorm = 56.7241, GNorm = 0.2375, lr_0 = 4.4984e-04
Validation rmse logS = 0.601646
Validation R2 logS = 0.915106
Epoch 34
Train function
Loss = 4.6023e-04, PNorm = 56.7474, GNorm = 0.5818, lr_0 = 4.4179e-04
Loss = 3.8209e-04, PNorm = 56.7657, GNorm = 1.2652, lr_0 = 4.3387e-04
Validation rmse logS = 0.645706
Validation R2 logS = 0.902216
Epoch 35
Train function
Loss = 4.4061e-04, PNorm = 56.7887, GNorm = 0.3992, lr_0 = 4.2533e-04
Validation rmse logS = 0.600570
Validation R2 logS = 0.915409
Epoch 36
Train function
Loss = 3.1444e-04, PNorm = 56.8087, GNorm = 0.9051, lr_0 = 4.1771e-04
Validation rmse logS = 0.597042
Validation R2 logS = 0.916400
Epoch 37
Train function
Loss = 3.5528e-04, PNorm = 56.8293, GNorm = 1.0494, lr_0 = 4.0949e-04
Loss = 3.7034e-04, PNorm = 56.8502, GNorm = 0.3373, lr_0 = 4.0216e-04
Validation rmse logS = 0.609165
Validation R2 logS = 0.912970
Epoch 38
Train function
Loss = 2.7687e-04, PNorm = 56.8718, GNorm = 0.3974, lr_0 = 3.9495e-04
Validation rmse logS = 0.595945
Validation R2 logS = 0.916707
Epoch 39
Train function
Loss = 3.1243e-04, PNorm = 56.8932, GNorm = 0.4744, lr_0 = 3.8718e-04
Loss = 2.7824e-04, PNorm = 56.9135, GNorm = 0.5918, lr_0 = 3.8024e-04
Validation rmse logS = 0.598653
Validation R2 logS = 0.915948
Epoch 40
Train function
Loss = 2.6143e-04, PNorm = 56.9288, GNorm = 0.5653, lr_0 = 3.7343e-04
Validation rmse logS = 0.606932
Validation R2 logS = 0.913607
Epoch 41
Train function
Loss = 3.3090e-04, PNorm = 56.9473, GNorm = 0.7674, lr_0 = 3.6608e-04
Validation rmse logS = 0.616813
Validation R2 logS = 0.910771
Epoch 42
Train function
Loss = 3.0551e-04, PNorm = 56.9653, GNorm = 0.3247, lr_0 = 3.5953e-04
Loss = 2.7986e-04, PNorm = 56.9850, GNorm = 0.3340, lr_0 = 3.5309e-04
Loss = 5.3894e-04, PNorm = 56.9866, GNorm = 0.7529, lr_0 = 3.5245e-04
Validation rmse logS = 0.609999
Validation R2 logS = 0.912732
Epoch 43
Train function
Loss = 2.5942e-04, PNorm = 57.0017, GNorm = 0.3400, lr_0 = 3.4614e-04
Validation rmse logS = 0.615044
Validation R2 logS = 0.911283
Epoch 44
Train function
Loss = 2.3177e-04, PNorm = 57.0209, GNorm = 0.1291, lr_0 = 3.3994e-04
Validation rmse logS = 0.599319
Validation R2 logS = 0.915761
Epoch 45
Train function
Loss = 2.2591e-04, PNorm = 57.0384, GNorm = 0.7474, lr_0 = 3.3324e-04
Loss = 2.0474e-04, PNorm = 57.0523, GNorm = 0.3951, lr_0 = 3.2728e-04
Validation rmse logS = 0.585904
Validation R2 logS = 0.919490
Epoch 46
Train function
Loss = 2.0524e-04, PNorm = 57.0662, GNorm = 0.2148, lr_0 = 3.2141e-04
Validation rmse logS = 0.595612
Validation R2 logS = 0.916800
Epoch 47
Train function
Loss = 2.1637e-04, PNorm = 57.0819, GNorm = 0.5453, lr_0 = 3.1509e-04
Validation rmse logS = 0.601304
Validation R2 logS = 0.915202
Epoch 48
Train function
Loss = 1.8372e-04, PNorm = 57.0970, GNorm = 0.6350, lr_0 = 3.0944e-04
Loss = 2.0672e-04, PNorm = 57.1112, GNorm = 0.3129, lr_0 = 3.0390e-04
Validation rmse logS = 0.594133
Validation R2 logS = 0.917213
Epoch 49
Train function
Loss = 2.0524e-04, PNorm = 57.1247, GNorm = 0.3400, lr_0 = 2.9792e-04
Validation rmse logS = 0.592533
Validation R2 logS = 0.917658
Epoch 50
Train function
Loss = 2.1669e-04, PNorm = 57.1393, GNorm = 0.7571, lr_0 = 2.9258e-04
Validation rmse logS = 0.589079
Validation R2 logS = 0.918615
Epoch 51
Train function
Loss = 2.4847e-04, PNorm = 57.1523, GNorm = 0.5833, lr_0 = 2.8682e-04
Loss = 1.7358e-04, PNorm = 57.1654, GNorm = 0.8373, lr_0 = 2.8169e-04
Validation rmse logS = 0.608121
Validation R2 logS = 0.913268
Epoch 52
Train function
Loss = 2.3979e-04, PNorm = 57.1744, GNorm = 0.8160, lr_0 = 2.7664e-04
Validation rmse logS = 0.590390
Validation R2 logS = 0.918252
Epoch 53
Train function
Loss = 2.3379e-04, PNorm = 57.1865, GNorm = 1.1283, lr_0 = 2.7119e-04
Validation rmse logS = 0.581915
Validation R2 logS = 0.920583
Epoch 54
Train function
Loss = 1.3158e-04, PNorm = 57.1983, GNorm = 0.5567, lr_0 = 2.6634e-04
Loss = 1.6930e-04, PNorm = 57.2098, GNorm = 0.5349, lr_0 = 2.6157e-04
Validation rmse logS = 0.591614
Validation R2 logS = 0.917913
Epoch 55
Train function
Loss = 1.7454e-04, PNorm = 57.2220, GNorm = 0.3428, lr_0 = 2.5642e-04
Validation rmse logS = 0.597762
Validation R2 logS = 0.916198
Epoch 56
Train function
Loss = 1.1518e-04, PNorm = 57.2323, GNorm = 0.3233, lr_0 = 2.5183e-04
Validation rmse logS = 0.586835
Validation R2 logS = 0.919234
Epoch 57
Train function
Loss = 9.6153e-05, PNorm = 57.2422, GNorm = 0.4239, lr_0 = 2.4687e-04
Loss = 1.3531e-04, PNorm = 57.2525, GNorm = 0.3682, lr_0 = 2.4245e-04
Validation rmse logS = 0.585721
Validation R2 logS = 0.919540
Epoch 58
Train function
Loss = 1.4194e-04, PNorm = 57.2608, GNorm = 0.3324, lr_0 = 2.3810e-04
Validation rmse logS = 0.592258
Validation R2 logS = 0.917734
Epoch 59
Train function
Loss = 1.2971e-04, PNorm = 57.2703, GNorm = 0.4612, lr_0 = 2.3342e-04
Loss = 1.3253e-04, PNorm = 57.2790, GNorm = 0.4785, lr_0 = 2.2924e-04
Validation rmse logS = 0.591669
Validation R2 logS = 0.917898
Epoch 60
Train function
Loss = 1.2484e-04, PNorm = 57.2867, GNorm = 0.2296, lr_0 = 2.2513e-04
Validation rmse logS = 0.588644
Validation R2 logS = 0.918735
Epoch 61
Train function
Loss = 1.1343e-04, PNorm = 57.2954, GNorm = 0.2040, lr_0 = 2.2070e-04
Validation rmse logS = 0.594598
Validation R2 logS = 0.917083
Epoch 62
Train function
Loss = 1.2052e-04, PNorm = 57.3035, GNorm = 0.2391, lr_0 = 2.1675e-04
Loss = 1.2226e-04, PNorm = 57.3109, GNorm = 0.2025, lr_0 = 2.1286e-04
Loss = 1.2690e-04, PNorm = 57.3118, GNorm = 0.2622, lr_0 = 2.1248e-04
Validation rmse logS = 0.601644
Validation R2 logS = 0.915106
Epoch 63
Train function
Loss = 1.0296e-04, PNorm = 57.3189, GNorm = 0.5494, lr_0 = 2.0867e-04
Validation rmse logS = 0.584194
Validation R2 logS = 0.919959
Epoch 64
Train function
Loss = 1.6044e-04, PNorm = 57.3262, GNorm = 1.1588, lr_0 = 2.0494e-04
Validation rmse logS = 0.597410
Validation R2 logS = 0.916297
Epoch 65
Train function
Loss = 1.4075e-04, PNorm = 57.3344, GNorm = 0.7837, lr_0 = 2.0090e-04
Loss = 1.3284e-04, PNorm = 57.3423, GNorm = 0.3480, lr_0 = 1.9730e-04
Validation rmse logS = 0.594689
Validation R2 logS = 0.917058
Epoch 66
Train function
Loss = 1.2957e-04, PNorm = 57.3500, GNorm = 0.2991, lr_0 = 1.9377e-04
Validation rmse logS = 0.593733
Validation R2 logS = 0.917324
Epoch 67
Train function
Loss = 1.3188e-04, PNorm = 57.3586, GNorm = 0.2118, lr_0 = 1.8995e-04
Validation rmse logS = 0.588764
Validation R2 logS = 0.918702
Epoch 68
Train function
Loss = 9.6291e-05, PNorm = 57.3653, GNorm = 0.1598, lr_0 = 1.8655e-04
Loss = 1.0104e-04, PNorm = 57.3714, GNorm = 0.3809, lr_0 = 1.8321e-04
Validation rmse logS = 0.592264
Validation R2 logS = 0.917733
Epoch 69
Train function
Loss = 1.1946e-04, PNorm = 57.3769, GNorm = 0.6164, lr_0 = 1.7960e-04
Validation rmse logS = 0.587140
Validation R2 logS = 0.919150
Epoch 70
Train function
Loss = 9.7867e-05, PNorm = 57.3830, GNorm = 0.2540, lr_0 = 1.7639e-04
Validation rmse logS = 0.593157
Validation R2 logS = 0.917484
Epoch 71
Train function
Loss = 5.9660e-05, PNorm = 57.3897, GNorm = 0.1717, lr_0 = 1.7292e-04
Loss = 8.9941e-05, PNorm = 57.3938, GNorm = 0.2203, lr_0 = 1.6982e-04
Validation rmse logS = 0.590747
Validation R2 logS = 0.918154
Epoch 72
Train function
Loss = 7.9996e-05, PNorm = 57.3998, GNorm = 0.2844, lr_0 = 1.6678e-04
Validation rmse logS = 0.595106
Validation R2 logS = 0.916941
Epoch 73
Train function
Loss = 7.2339e-05, PNorm = 57.4058, GNorm = 0.1962, lr_0 = 1.6349e-04
Validation rmse logS = 0.591262
Validation R2 logS = 0.918011
Epoch 74
Train function
Loss = 5.1044e-05, PNorm = 57.4109, GNorm = 0.2218, lr_0 = 1.6057e-04
Loss = 9.1413e-05, PNorm = 57.4170, GNorm = 0.3760, lr_0 = 1.5769e-04
Validation rmse logS = 0.593091
Validation R2 logS = 0.917503
Epoch 75
Train function
Loss = 6.9307e-05, PNorm = 57.4226, GNorm = 0.3312, lr_0 = 1.5459e-04
Validation rmse logS = 0.589873
Validation R2 logS = 0.918395
Epoch 76
Train function
Loss = 6.6734e-05, PNorm = 57.4275, GNorm = 0.2327, lr_0 = 1.5182e-04
Validation rmse logS = 0.591630
Validation R2 logS = 0.917909
Epoch 77
Train function
Loss = 1.1795e-04, PNorm = 57.4327, GNorm = 0.2978, lr_0 = 1.4883e-04
Loss = 6.1430e-05, PNorm = 57.4368, GNorm = 0.1583, lr_0 = 1.4616e-04
Validation rmse logS = 0.592193
Validation R2 logS = 0.917752
Epoch 78
Train function
Loss = 5.6011e-05, PNorm = 57.4417, GNorm = 0.1792, lr_0 = 1.4354e-04
Validation rmse logS = 0.592514
Validation R2 logS = 0.917663
Epoch 79
Train function
Loss = 5.1098e-05, PNorm = 57.4469, GNorm = 0.1237, lr_0 = 1.4072e-04
Loss = 8.3697e-05, PNorm = 57.4513, GNorm = 0.2228, lr_0 = 1.3820e-04
Validation rmse logS = 0.591096
Validation R2 logS = 0.918057
Epoch 80
Train function
Loss = 6.1862e-05, PNorm = 57.4554, GNorm = 0.2850, lr_0 = 1.3572e-04
Validation rmse logS = 0.593040
Validation R2 logS = 0.917517
Epoch 81
Train function
Loss = 6.2554e-05, PNorm = 57.4597, GNorm = 0.1736, lr_0 = 1.3305e-04
Validation rmse logS = 0.592014
Validation R2 logS = 0.917802
Epoch 82
Train function
Loss = 6.9367e-05, PNorm = 57.4638, GNorm = 0.3466, lr_0 = 1.3067e-04
Loss = 6.3936e-05, PNorm = 57.4676, GNorm = 0.4510, lr_0 = 1.2833e-04
Loss = 2.0920e-04, PNorm = 57.4680, GNorm = 0.4754, lr_0 = 1.2810e-04
Validation rmse logS = 0.589909
Validation R2 logS = 0.918386
Epoch 83
Train function
Loss = 6.6081e-05, PNorm = 57.4718, GNorm = 0.2802, lr_0 = 1.2580e-04
Validation rmse logS = 0.597234
Validation R2 logS = 0.916346
Epoch 84
Train function
Loss = 8.1776e-05, PNorm = 57.4760, GNorm = 0.2333, lr_0 = 1.2355e-04
Validation rmse logS = 0.597085
Validation R2 logS = 0.916388
Epoch 85
Train function
Loss = 4.9074e-05, PNorm = 57.4799, GNorm = 0.1817, lr_0 = 1.2112e-04
Loss = 6.4080e-05, PNorm = 57.4834, GNorm = 0.3959, lr_0 = 1.1895e-04
Validation rmse logS = 0.592959
Validation R2 logS = 0.917539
Epoch 86
Train function
Loss = 6.0318e-05, PNorm = 57.4873, GNorm = 0.5016, lr_0 = 1.1682e-04
Validation rmse logS = 0.593199
Validation R2 logS = 0.917473
Epoch 87
Train function
Loss = 6.4127e-05, PNorm = 57.4913, GNorm = 0.6136, lr_0 = 1.1452e-04
Validation rmse logS = 0.597449
Validation R2 logS = 0.916286
Epoch 88
Train function
Loss = 4.7376e-05, PNorm = 57.4945, GNorm = 0.2436, lr_0 = 1.1247e-04
Loss = 6.4547e-05, PNorm = 57.4975, GNorm = 0.1626, lr_0 = 1.1045e-04
Validation rmse logS = 0.595394
Validation R2 logS = 0.916861
Epoch 89
Train function
Loss = 5.2964e-05, PNorm = 57.5011, GNorm = 0.1448, lr_0 = 1.0828e-04
Validation rmse logS = 0.591833
Validation R2 logS = 0.917852
Epoch 90
Train function
Loss = 6.0056e-05, PNorm = 57.5041, GNorm = 0.1009, lr_0 = 1.0634e-04
Validation rmse logS = 0.590280
Validation R2 logS = 0.918283
Epoch 91
Train function
Loss = 3.7849e-05, PNorm = 57.5074, GNorm = 0.0881, lr_0 = 1.0424e-04
Loss = 5.4521e-05, PNorm = 57.5108, GNorm = 0.0841, lr_0 = 1.0238e-04
Validation rmse logS = 0.592751
Validation R2 logS = 0.917597
Epoch 92
Train function
Loss = 3.6213e-05, PNorm = 57.5137, GNorm = 0.3110, lr_0 = 1.0054e-04
Validation rmse logS = 0.594991
Validation R2 logS = 0.916973
Epoch 93
Train function
Loss = 5.5000e-05, PNorm = 57.5169, GNorm = 0.2088, lr_0 = 1.0000e-04
Validation rmse logS = 0.594759
Validation R2 logS = 0.917038
Epoch 94
Train function
Loss = 5.9170e-05, PNorm = 57.5197, GNorm = 0.1858, lr_0 = 1.0000e-04
Loss = 6.1909e-05, PNorm = 57.5225, GNorm = 0.1600, lr_0 = 1.0000e-04
Validation rmse logS = 0.589833
Validation R2 logS = 0.918407
Epoch 95
Train function
Loss = 4.7576e-05, PNorm = 57.5258, GNorm = 0.1925, lr_0 = 1.0000e-04
Validation rmse logS = 0.597749
Validation R2 logS = 0.916202
Epoch 96
Train function
Loss = 5.3624e-05, PNorm = 57.5287, GNorm = 0.2205, lr_0 = 1.0000e-04
Validation rmse logS = 0.596254
Validation R2 logS = 0.916620
Epoch 97
Train function
Loss = 4.9360e-05, PNorm = 57.5317, GNorm = 0.1269, lr_0 = 1.0000e-04
Loss = 5.5463e-05, PNorm = 57.5344, GNorm = 0.2348, lr_0 = 1.0000e-04
Validation rmse logS = 0.592567
Validation R2 logS = 0.917648
Epoch 98
Train function
Loss = 5.2323e-05, PNorm = 57.5370, GNorm = 0.4751, lr_0 = 1.0000e-04
Validation rmse logS = 0.604055
Validation R2 logS = 0.914424
Epoch 99
Train function
Loss = 4.7148e-05, PNorm = 57.5401, GNorm = 0.1836, lr_0 = 1.0000e-04
Loss = 6.1473e-05, PNorm = 57.5431, GNorm = 0.1359, lr_0 = 1.0000e-04
Validation rmse logS = 0.594535
Validation R2 logS = 0.917100
Model 0 best validation rmse = 0.581915 on epoch 53
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "substructures_encoder.encoder.cached_zero_vector".
Loading pretrained parameter "substructures_encoder.encoder.W_o.weight".
Loading pretrained parameter "substructures_encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.646455
Model 0 test R2 logS = 0.890749
Ensemble test rmse  logS= 0.646455
Ensemble test R2  logS= 0.890749
4-fold cross validation
	Seed 0 ==> test rmse = 0.611049
	Seed 0 ==> test R2 = 0.902389
	Seed 1 ==> test rmse = 0.666252
	Seed 1 ==> test R2 = 0.883955
	Seed 2 ==> test rmse = 0.624855
	Seed 2 ==> test R2 = 0.897928
	Seed 3 ==> test rmse = 0.646455
	Seed 3 ==> test R2 = 0.890749
Overall val rmse logS= 0.573965 +/- 0.015018
Overall val R2 logS = 0.923874 +/- 0.005290
Overall test rmse logS = 0.637153 +/- 0.021011
Overall test R2 logS = 0.893755 +/- 0.007018
Elapsed time = 0:24:35
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_306/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=895, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,222,401
Moving model to cuda
Epoch 0
Train function
Loss = 1.6663e-02, PNorm = 52.5080, GNorm = 3.6680, lr_0 = 4.8077e-04
Validation rmse logS = 1.157555
Validation R2 logS = 0.705495
Epoch 1
Train function
Loss = 7.1012e-03, PNorm = 52.5711, GNorm = 10.3615, lr_0 = 8.6154e-04
Validation rmse logS = 1.088776
Validation R2 logS = 0.739453
Epoch 2
Train function
Loss = 4.4841e-03, PNorm = 52.6574, GNorm = 2.9145, lr_0 = 9.8743e-04
Loss = 4.7527e-03, PNorm = 52.7248, GNorm = 2.3064, lr_0 = 9.6974e-04
Validation rmse logS = 1.089961
Validation R2 logS = 0.738885
Epoch 3
Train function
Loss = 3.9872e-03, PNorm = 52.7726, GNorm = 2.7758, lr_0 = 9.5237e-04
Validation rmse logS = 0.821254
Validation R2 logS = 0.851760
Epoch 4
Train function
Loss = 2.4305e-03, PNorm = 52.8158, GNorm = 0.5660, lr_0 = 9.3363e-04
Validation rmse logS = 0.769584
Validation R2 logS = 0.869827
Epoch 5
Train function
Loss = 2.3931e-03, PNorm = 52.8540, GNorm = 1.1955, lr_0 = 9.1690e-04
Loss = 1.9837e-03, PNorm = 52.8877, GNorm = 1.0757, lr_0 = 9.0048e-04
Loss = 4.1940e-03, PNorm = 52.8909, GNorm = 1.2382, lr_0 = 8.9885e-04
Validation rmse logS = 0.720280
Validation R2 logS = 0.885972
Epoch 6
Train function
Loss = 1.8743e-03, PNorm = 52.9224, GNorm = 0.6198, lr_0 = 8.8275e-04
Validation rmse logS = 0.697116
Validation R2 logS = 0.893188
Epoch 7
Train function
Loss = 1.5080e-03, PNorm = 52.9604, GNorm = 1.2535, lr_0 = 8.6694e-04
Validation rmse logS = 0.685664
Validation R2 logS = 0.896669
Epoch 8
Train function
Loss = 1.9984e-03, PNorm = 52.9932, GNorm = 1.1011, lr_0 = 8.4988e-04
Loss = 1.6440e-03, PNorm = 53.0244, GNorm = 0.6856, lr_0 = 8.3466e-04
Validation rmse logS = 0.764081
Validation R2 logS = 0.871682
Epoch 9
Train function
Loss = 1.5434e-03, PNorm = 53.0584, GNorm = 1.8605, lr_0 = 8.1971e-04
Validation rmse logS = 0.665339
Validation R2 logS = 0.902704
Epoch 10
Train function
Loss = 1.4950e-03, PNorm = 53.0967, GNorm = 0.7084, lr_0 = 8.0357e-04
Validation rmse logS = 0.628702
Validation R2 logS = 0.913124
Epoch 11
Train function
Loss = 1.0921e-03, PNorm = 53.1213, GNorm = 0.4012, lr_0 = 7.8918e-04
Loss = 1.1558e-03, PNorm = 53.1475, GNorm = 0.9851, lr_0 = 7.7504e-04
Validation rmse logS = 0.627528
Validation R2 logS = 0.913448
Epoch 12
Train function
Loss = 1.0210e-03, PNorm = 53.1776, GNorm = 0.6516, lr_0 = 7.5979e-04
Validation rmse logS = 0.665323
Validation R2 logS = 0.902709
Epoch 13
Train function
Loss = 1.0835e-03, PNorm = 53.2066, GNorm = 0.4610, lr_0 = 7.4618e-04
Validation rmse logS = 0.619101
Validation R2 logS = 0.915757
Epoch 14
Train function
Loss = 1.1484e-03, PNorm = 53.2378, GNorm = 2.0347, lr_0 = 7.3149e-04
Loss = 1.0869e-03, PNorm = 53.2651, GNorm = 0.7774, lr_0 = 7.1839e-04
Validation rmse logS = 0.631939
Validation R2 logS = 0.912227
Epoch 15
Train function
Loss = 8.5595e-04, PNorm = 53.2927, GNorm = 0.4192, lr_0 = 7.0552e-04
Validation rmse logS = 0.599300
Validation R2 logS = 0.921060
Epoch 16
Train function
Loss = 7.2582e-04, PNorm = 53.3232, GNorm = 0.4113, lr_0 = 6.9163e-04
Validation rmse logS = 0.609991
Validation R2 logS = 0.918218
Epoch 17
Train function
Loss = 7.6383e-04, PNorm = 53.3475, GNorm = 1.0301, lr_0 = 6.7924e-04
Loss = 8.1918e-04, PNorm = 53.3735, GNorm = 0.7314, lr_0 = 6.6708e-04
Validation rmse logS = 0.679311
Validation R2 logS = 0.898575
Epoch 18
Train function
Loss = 9.2579e-04, PNorm = 53.4029, GNorm = 1.5138, lr_0 = 6.5395e-04
Validation rmse logS = 0.796817
Validation R2 logS = 0.860451
Epoch 19
Train function
Loss = 1.0979e-03, PNorm = 53.4338, GNorm = 1.7806, lr_0 = 6.4223e-04
Validation rmse logS = 0.640891
Validation R2 logS = 0.909723
Epoch 20
Train function
Loss = 7.4471e-04, PNorm = 53.4621, GNorm = 0.9900, lr_0 = 6.2959e-04
Loss = 7.9191e-04, PNorm = 53.4937, GNorm = 0.7374, lr_0 = 6.1831e-04
Validation rmse logS = 0.634746
Validation R2 logS = 0.911446
Epoch 21
Train function
Loss = 6.3091e-04, PNorm = 53.5220, GNorm = 0.5031, lr_0 = 6.0724e-04
Validation rmse logS = 0.594473
Validation R2 logS = 0.922326
Epoch 22
Train function
Loss = 4.6946e-04, PNorm = 53.5483, GNorm = 0.7808, lr_0 = 5.9529e-04
Loss = 6.8866e-04, PNorm = 53.5702, GNorm = 1.2584, lr_0 = 5.8462e-04
Validation rmse logS = 0.572867
Validation R2 logS = 0.927870
Epoch 23
Train function
Loss = 5.6279e-04, PNorm = 53.5956, GNorm = 1.0142, lr_0 = 5.7415e-04
Validation rmse logS = 0.593460
Validation R2 logS = 0.922591
Epoch 24
Train function
Loss = 5.8910e-04, PNorm = 53.6162, GNorm = 0.3422, lr_0 = 5.6285e-04
Validation rmse logS = 0.567618
Validation R2 logS = 0.929186
Epoch 25
Train function
Loss = 4.5123e-04, PNorm = 53.6383, GNorm = 0.5227, lr_0 = 5.5277e-04
Loss = 4.8991e-04, PNorm = 53.6602, GNorm = 0.8474, lr_0 = 5.4287e-04
Loss = 1.1837e-03, PNorm = 53.6621, GNorm = 0.5093, lr_0 = 5.4189e-04
Validation rmse logS = 0.574533
Validation R2 logS = 0.927450
Epoch 26
Train function
Loss = 4.4364e-04, PNorm = 53.6831, GNorm = 0.4188, lr_0 = 5.3218e-04
Validation rmse logS = 0.566003
Validation R2 logS = 0.929588
Epoch 27
Train function
Loss = 4.4315e-04, PNorm = 53.7065, GNorm = 0.7536, lr_0 = 5.2171e-04
Validation rmse logS = 0.582884
Validation R2 logS = 0.925325
Epoch 28
Train function
Loss = 5.9342e-04, PNorm = 53.7262, GNorm = 0.6221, lr_0 = 5.1236e-04
Loss = 3.4658e-04, PNorm = 53.7495, GNorm = 0.4012, lr_0 = 5.0318e-04
Loss = 5.1485e-04, PNorm = 53.7515, GNorm = 0.5519, lr_0 = 5.0228e-04
Validation rmse logS = 0.564068
Validation R2 logS = 0.930069
Epoch 29
Train function
Loss = 3.6522e-04, PNorm = 53.7717, GNorm = 0.3457, lr_0 = 4.9328e-04
Validation rmse logS = 0.571080
Validation R2 logS = 0.928319
Epoch 30
Train function
Loss = 3.6981e-04, PNorm = 53.7904, GNorm = 0.5608, lr_0 = 4.8444e-04
Validation rmse logS = 0.570559
Validation R2 logS = 0.928450
Epoch 31
Train function
Loss = 2.6217e-04, PNorm = 53.8107, GNorm = 0.3923, lr_0 = 4.7491e-04
Loss = 3.2134e-04, PNorm = 53.8306, GNorm = 0.7188, lr_0 = 4.6640e-04
Validation rmse logS = 0.565721
Validation R2 logS = 0.929658
Epoch 32
Train function
Loss = 2.9608e-04, PNorm = 53.8492, GNorm = 0.7478, lr_0 = 4.5805e-04
Validation rmse logS = 0.572912
Validation R2 logS = 0.927858
Epoch 33
Train function
Loss = 3.7665e-04, PNorm = 53.8698, GNorm = 0.6483, lr_0 = 4.4903e-04
Validation rmse logS = 0.564796
Validation R2 logS = 0.929888
Epoch 34
Train function
Loss = 2.5853e-04, PNorm = 53.8881, GNorm = 0.2931, lr_0 = 4.4099e-04
Loss = 3.2681e-04, PNorm = 53.9059, GNorm = 0.6966, lr_0 = 4.3309e-04
Validation rmse logS = 0.565062
Validation R2 logS = 0.929822
Epoch 35
Train function
Loss = 3.2644e-04, PNorm = 53.9261, GNorm = 1.2625, lr_0 = 4.2456e-04
Validation rmse logS = 0.585418
Validation R2 logS = 0.924675
Epoch 36
Train function
Loss = 2.6814e-04, PNorm = 53.9433, GNorm = 0.4391, lr_0 = 4.1696e-04
Validation rmse logS = 0.571286
Validation R2 logS = 0.928267
Epoch 37
Train function
Loss = 2.2069e-04, PNorm = 53.9614, GNorm = 0.3750, lr_0 = 4.0875e-04
Loss = 2.5982e-04, PNorm = 53.9770, GNorm = 0.3855, lr_0 = 4.0143e-04
Validation rmse logS = 0.560784
Validation R2 logS = 0.930880
Epoch 38
Train function
Loss = 2.3001e-04, PNorm = 53.9933, GNorm = 0.4971, lr_0 = 3.9424e-04
Validation rmse logS = 0.570091
Validation R2 logS = 0.928567
Epoch 39
Train function
Loss = 2.8096e-04, PNorm = 54.0110, GNorm = 0.9642, lr_0 = 3.8648e-04
Validation rmse logS = 0.572204
Validation R2 logS = 0.928037
Epoch 40
Train function
Loss = 1.6772e-04, PNorm = 54.0261, GNorm = 0.6015, lr_0 = 3.7956e-04
Loss = 3.1341e-04, PNorm = 54.0425, GNorm = 0.3205, lr_0 = 3.7276e-04
Validation rmse logS = 0.582866
Validation R2 logS = 0.925330
Epoch 41
Train function
Loss = 2.4874e-04, PNorm = 54.0585, GNorm = 0.4414, lr_0 = 3.6542e-04
Validation rmse logS = 0.567294
Validation R2 logS = 0.929267
Epoch 42
Train function
Loss = 1.5820e-04, PNorm = 54.0721, GNorm = 0.2449, lr_0 = 3.5888e-04
Validation rmse logS = 0.566282
Validation R2 logS = 0.929519
Epoch 43
Train function
Loss = 1.1313e-04, PNorm = 54.0849, GNorm = 0.2389, lr_0 = 3.5181e-04
Loss = 1.8145e-04, PNorm = 54.0960, GNorm = 0.3036, lr_0 = 3.4551e-04
Validation rmse logS = 0.569837
Validation R2 logS = 0.928631
Epoch 44
Train function
Loss = 2.0503e-04, PNorm = 54.1100, GNorm = 0.5019, lr_0 = 3.3932e-04
Validation rmse logS = 0.564499
Validation R2 logS = 0.929962
Epoch 45
Train function
Loss = 2.0315e-04, PNorm = 54.1224, GNorm = 0.4547, lr_0 = 3.3264e-04
Loss = 1.8194e-04, PNorm = 54.1342, GNorm = 0.4613, lr_0 = 3.2668e-04
Validation rmse logS = 0.560718
Validation R2 logS = 0.930897
Epoch 46
Train function
Loss = 1.4880e-04, PNorm = 54.1469, GNorm = 0.4369, lr_0 = 3.2083e-04
Validation rmse logS = 0.559775
Validation R2 logS = 0.931129
Epoch 47
Train function
Loss = 1.8061e-04, PNorm = 54.1611, GNorm = 0.5923, lr_0 = 3.1452e-04
Validation rmse logS = 0.568134
Validation R2 logS = 0.929057
Epoch 48
Train function
Loss = 1.5221e-04, PNorm = 54.1728, GNorm = 0.4858, lr_0 = 3.0888e-04
Loss = 2.1530e-04, PNorm = 54.1847, GNorm = 0.8787, lr_0 = 3.0335e-04
Loss = 4.5240e-04, PNorm = 54.1855, GNorm = 0.3119, lr_0 = 3.0280e-04
Validation rmse logS = 0.570950
Validation R2 logS = 0.928352
Epoch 49
Train function
Loss = 1.7564e-04, PNorm = 54.1959, GNorm = 0.4000, lr_0 = 2.9738e-04
Validation rmse logS = 0.572857
Validation R2 logS = 0.927872
Epoch 50
Train function
Loss = 1.7907e-04, PNorm = 54.2060, GNorm = 0.8659, lr_0 = 2.9205e-04
Validation rmse logS = 0.560429
Validation R2 logS = 0.930968
Epoch 51
Train function
Loss = 1.2051e-04, PNorm = 54.2178, GNorm = 0.7099, lr_0 = 2.8630e-04
Loss = 1.7577e-04, PNorm = 54.2267, GNorm = 0.7696, lr_0 = 2.8118e-04
Loss = 1.6191e-04, PNorm = 54.2276, GNorm = 0.4191, lr_0 = 2.8067e-04
Validation rmse logS = 0.563010
Validation R2 logS = 0.930331
Epoch 52
Train function
Loss = 1.6239e-04, PNorm = 54.2365, GNorm = 0.5632, lr_0 = 2.7564e-04
Validation rmse logS = 0.564508
Validation R2 logS = 0.929960
Epoch 53
Train function
Loss = 1.1893e-04, PNorm = 54.2447, GNorm = 0.4753, lr_0 = 2.7070e-04
Validation rmse logS = 0.566625
Validation R2 logS = 0.929433
Epoch 54
Train function
Loss = 8.6726e-05, PNorm = 54.2552, GNorm = 0.3143, lr_0 = 2.6538e-04
Loss = 1.2872e-04, PNorm = 54.2635, GNorm = 0.2171, lr_0 = 2.6062e-04
Validation rmse logS = 0.560502
Validation R2 logS = 0.930950
Epoch 55
Train function
Loss = 1.1971e-04, PNorm = 54.2726, GNorm = 0.2968, lr_0 = 2.5595e-04
Validation rmse logS = 0.572313
Validation R2 logS = 0.928009
Epoch 56
Train function
Loss = 1.2797e-04, PNorm = 54.2810, GNorm = 0.2380, lr_0 = 2.5092e-04
Validation rmse logS = 0.579764
Validation R2 logS = 0.926123
Epoch 57
Train function
Loss = 1.0083e-04, PNorm = 54.2894, GNorm = 0.3122, lr_0 = 2.4642e-04
Loss = 1.2507e-04, PNorm = 54.2974, GNorm = 0.2025, lr_0 = 2.4201e-04
Validation rmse logS = 0.565637
Validation R2 logS = 0.929679
Epoch 58
Train function
Loss = 1.2525e-04, PNorm = 54.3056, GNorm = 0.3996, lr_0 = 2.3724e-04
Validation rmse logS = 0.562749
Validation R2 logS = 0.930395
Epoch 59
Train function
Loss = 1.0219e-04, PNorm = 54.3143, GNorm = 0.2781, lr_0 = 2.3300e-04
Validation rmse logS = 0.570530
Validation R2 logS = 0.928457
Epoch 60
Train function
Loss = 1.1800e-04, PNorm = 54.3222, GNorm = 0.4299, lr_0 = 2.2841e-04
Loss = 9.0776e-05, PNorm = 54.3290, GNorm = 0.2141, lr_0 = 2.2432e-04
Validation rmse logS = 0.569614
Validation R2 logS = 0.928687
Epoch 61
Train function
Loss = 1.0399e-04, PNorm = 54.3362, GNorm = 0.2025, lr_0 = 2.2030e-04
Validation rmse logS = 0.569078
Validation R2 logS = 0.928821
Epoch 62
Train function
Loss = 9.3674e-05, PNorm = 54.3432, GNorm = 0.2485, lr_0 = 2.1596e-04
Validation rmse logS = 0.563288
Validation R2 logS = 0.930262
Epoch 63
Train function
Loss = 1.0983e-04, PNorm = 54.3501, GNorm = 0.3490, lr_0 = 2.1210e-04
Loss = 8.6957e-05, PNorm = 54.3568, GNorm = 0.2294, lr_0 = 2.0830e-04
Validation rmse logS = 0.575777
Validation R2 logS = 0.927135
Epoch 64
Train function
Loss = 7.6514e-05, PNorm = 54.3625, GNorm = 0.4764, lr_0 = 2.0420e-04
Validation rmse logS = 0.576026
Validation R2 logS = 0.927072
Epoch 65
Train function
Loss = 9.0209e-05, PNorm = 54.3688, GNorm = 0.3501, lr_0 = 2.0054e-04
Validation rmse logS = 0.571060
Validation R2 logS = 0.928324
Epoch 66
Train function
Loss = 1.1826e-04, PNorm = 54.3754, GNorm = 0.5131, lr_0 = 1.9659e-04
Loss = 8.3192e-05, PNorm = 54.3808, GNorm = 0.6028, lr_0 = 1.9307e-04
Validation rmse logS = 0.564759
Validation R2 logS = 0.929897
Epoch 67
Train function
Loss = 1.1833e-04, PNorm = 54.3874, GNorm = 0.3685, lr_0 = 1.8961e-04
Validation rmse logS = 0.567458
Validation R2 logS = 0.929226
Epoch 68
Train function
Loss = 6.7565e-05, PNorm = 54.3929, GNorm = 0.1234, lr_0 = 1.8588e-04
Loss = 1.1682e-04, PNorm = 54.3980, GNorm = 1.2138, lr_0 = 1.8255e-04
Validation rmse logS = 0.559894
Validation R2 logS = 0.931100
Epoch 69
Train function
Loss = 8.8817e-05, PNorm = 54.4039, GNorm = 0.1996, lr_0 = 1.7928e-04
Validation rmse logS = 0.571791
Validation R2 logS = 0.928141
Epoch 70
Train function
Loss = 8.2803e-05, PNorm = 54.4093, GNorm = 0.2626, lr_0 = 1.7575e-04
Validation rmse logS = 0.565841
Validation R2 logS = 0.929628
Epoch 71
Train function
Loss = 7.8378e-05, PNorm = 54.4149, GNorm = 0.2648, lr_0 = 1.7260e-04
Loss = 7.4796e-05, PNorm = 54.4202, GNorm = 0.0945, lr_0 = 1.6951e-04
Loss = 1.0529e-04, PNorm = 54.4206, GNorm = 0.2235, lr_0 = 1.6921e-04
Validation rmse logS = 0.567630
Validation R2 logS = 0.929183
Epoch 72
Train function
Loss = 7.3510e-05, PNorm = 54.4256, GNorm = 0.1373, lr_0 = 1.6617e-04
Validation rmse logS = 0.566219
Validation R2 logS = 0.929534
Epoch 73
Train function
Loss = 7.2365e-05, PNorm = 54.4299, GNorm = 0.2342, lr_0 = 1.6320e-04
Validation rmse logS = 0.569700
Validation R2 logS = 0.928665
Epoch 74
Train function
Loss = 8.2744e-05, PNorm = 54.4352, GNorm = 0.1229, lr_0 = 1.5999e-04
Loss = 7.0509e-05, PNorm = 54.4398, GNorm = 0.2180, lr_0 = 1.5712e-04
Validation rmse logS = 0.570414
Validation R2 logS = 0.928486
Epoch 75
Train function
Loss = 5.4278e-05, PNorm = 54.4441, GNorm = 0.1644, lr_0 = 1.5431e-04
Validation rmse logS = 0.567653
Validation R2 logS = 0.929177
Epoch 76
Train function
Loss = 4.4611e-05, PNorm = 54.4479, GNorm = 0.1308, lr_0 = 1.5127e-04
Validation rmse logS = 0.569855
Validation R2 logS = 0.928626
Epoch 77
Train function
Loss = 6.1778e-05, PNorm = 54.4518, GNorm = 0.2495, lr_0 = 1.4829e-04
Loss = 7.4109e-05, PNorm = 54.4559, GNorm = 0.3800, lr_0 = 1.4563e-04
Validation rmse logS = 0.566108
Validation R2 logS = 0.929562
Epoch 78
Train function
Loss = 7.8134e-05, PNorm = 54.4598, GNorm = 0.2148, lr_0 = 1.4303e-04
Validation rmse logS = 0.569155
Validation R2 logS = 0.928802
Epoch 79
Train function
Loss = 7.6022e-05, PNorm = 54.4636, GNorm = 0.4271, lr_0 = 1.4021e-04
Validation rmse logS = 0.573737
Validation R2 logS = 0.927650
Epoch 80
Train function
Loss = 3.2783e-05, PNorm = 54.4678, GNorm = 0.3028, lr_0 = 1.3770e-04
Loss = 7.5474e-05, PNorm = 54.4717, GNorm = 0.1986, lr_0 = 1.3523e-04
Validation rmse logS = 0.573221
Validation R2 logS = 0.927781
Epoch 81
Train function
Loss = 6.6450e-05, PNorm = 54.4751, GNorm = 0.3041, lr_0 = 1.3257e-04
Validation rmse logS = 0.568259
Validation R2 logS = 0.929026
Epoch 82
Train function
Loss = 6.4531e-05, PNorm = 54.4786, GNorm = 0.2372, lr_0 = 1.3020e-04
Validation rmse logS = 0.571913
Validation R2 logS = 0.928110
Epoch 83
Train function
Loss = 6.0693e-05, PNorm = 54.4823, GNorm = 0.4844, lr_0 = 1.2763e-04
Loss = 6.7572e-05, PNorm = 54.4861, GNorm = 0.1598, lr_0 = 1.2535e-04
Validation rmse logS = 0.565281
Validation R2 logS = 0.929768
Epoch 84
Train function
Loss = 5.3183e-05, PNorm = 54.4896, GNorm = 0.1770, lr_0 = 1.2310e-04
Validation rmse logS = 0.572996
Validation R2 logS = 0.927837
Epoch 85
Train function
Loss = 6.1114e-05, PNorm = 54.4930, GNorm = 0.1836, lr_0 = 1.2068e-04
Validation rmse logS = 0.568623
Validation R2 logS = 0.928935
Epoch 86
Train function
Loss = 3.3208e-05, PNorm = 54.4959, GNorm = 0.1652, lr_0 = 1.1852e-04
Loss = 6.6011e-05, PNorm = 54.4986, GNorm = 0.1902, lr_0 = 1.1639e-04
Validation rmse logS = 0.567225
Validation R2 logS = 0.929284
Epoch 87
Train function
Loss = 5.7387e-05, PNorm = 54.5022, GNorm = 0.1395, lr_0 = 1.1410e-04
Validation rmse logS = 0.568386
Validation R2 logS = 0.928994
Epoch 88
Train function
Loss = 4.7467e-05, PNorm = 54.5049, GNorm = 0.1447, lr_0 = 1.1206e-04
Validation rmse logS = 0.568776
Validation R2 logS = 0.928896
Epoch 89
Train function
Loss = 2.6582e-05, PNorm = 54.5077, GNorm = 0.1814, lr_0 = 1.0985e-04
Loss = 5.8633e-05, PNorm = 54.5106, GNorm = 0.1702, lr_0 = 1.0789e-04
Validation rmse logS = 0.567434
Validation R2 logS = 0.929232
Epoch 90
Train function
Loss = 4.3093e-05, PNorm = 54.5130, GNorm = 0.1081, lr_0 = 1.0595e-04
Validation rmse logS = 0.564211
Validation R2 logS = 0.930033
Epoch 91
Train function
Loss = 4.7419e-05, PNorm = 54.5155, GNorm = 0.1300, lr_0 = 1.0387e-04
Loss = 5.3898e-05, PNorm = 54.5179, GNorm = 0.2884, lr_0 = 1.0201e-04
Validation rmse logS = 0.569727
Validation R2 logS = 0.928658
Epoch 92
Train function
Loss = 5.0692e-05, PNorm = 54.5201, GNorm = 0.2746, lr_0 = 1.0018e-04
Validation rmse logS = 0.566468
Validation R2 logS = 0.929472
Epoch 93
Train function
Loss = 5.5032e-05, PNorm = 54.5231, GNorm = 0.1433, lr_0 = 1.0000e-04
Validation rmse logS = 0.564227
Validation R2 logS = 0.930029
Epoch 94
Train function
Loss = 6.7822e-05, PNorm = 54.5257, GNorm = 0.2826, lr_0 = 1.0000e-04
Loss = 4.0286e-05, PNorm = 54.5280, GNorm = 0.1040, lr_0 = 1.0000e-04
Loss = 2.7367e-04, PNorm = 54.5282, GNorm = 0.2611, lr_0 = 1.0000e-04
Validation rmse logS = 0.566698
Validation R2 logS = 0.929415
Epoch 95
Train function
Loss = 4.3452e-05, PNorm = 54.5308, GNorm = 0.2507, lr_0 = 1.0000e-04
Validation rmse logS = 0.568770
Validation R2 logS = 0.928898
Epoch 96
Train function
Loss = 4.5577e-05, PNorm = 54.5334, GNorm = 0.1901, lr_0 = 1.0000e-04
Validation rmse logS = 0.569248
Validation R2 logS = 0.928778
Epoch 97
Train function
Loss = 4.7733e-05, PNorm = 54.5359, GNorm = 0.2486, lr_0 = 1.0000e-04
Loss = 5.4670e-05, PNorm = 54.5386, GNorm = 0.1873, lr_0 = 1.0000e-04
Validation rmse logS = 0.574741
Validation R2 logS = 0.927397
Epoch 98
Train function
Loss = 5.1667e-05, PNorm = 54.5407, GNorm = 0.1704, lr_0 = 1.0000e-04
Validation rmse logS = 0.568979
Validation R2 logS = 0.928846
Epoch 99
Train function
Loss = 4.7349e-05, PNorm = 54.5434, GNorm = 0.1628, lr_0 = 1.0000e-04
Validation rmse logS = 0.566490
Validation R2 logS = 0.929467
Model 0 best validation rmse = 0.559775 on epoch 46
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.630592
Model 0 test R2 logS = 0.896045
Ensemble test rmse  logS= 0.630592
Ensemble test R2  logS= 0.896045
Fold 1
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_306/folds/fold_1',
 'save_smiles_splits': False,
 'seed': 1,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 674,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 1
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=895, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,222,401
Moving model to cuda
Epoch 0
Train function
Loss = 2.0286e-02, PNorm = 52.5051, GNorm = 1.3809, lr_0 = 4.8077e-04
Validation rmse logS = 1.422466
Validation R2 logS = 0.520409
Epoch 1
Train function
Loss = 7.2012e-03, PNorm = 52.5729, GNorm = 2.4290, lr_0 = 8.6154e-04
Validation rmse logS = 0.988007
Validation R2 logS = 0.768630
Epoch 2
Train function
Loss = 4.7586e-03, PNorm = 52.6633, GNorm = 4.9715, lr_0 = 9.8743e-04
Loss = 3.6253e-03, PNorm = 52.7149, GNorm = 1.1226, lr_0 = 9.6974e-04
Validation rmse logS = 0.986228
Validation R2 logS = 0.769462
Epoch 3
Train function
Loss = 3.2661e-03, PNorm = 52.7594, GNorm = 1.4206, lr_0 = 9.5237e-04
Validation rmse logS = 0.920814
Validation R2 logS = 0.799030
Epoch 4
Train function
Loss = 2.6811e-03, PNorm = 52.7988, GNorm = 1.8186, lr_0 = 9.3363e-04
Validation rmse logS = 0.901461
Validation R2 logS = 0.807389
Epoch 5
Train function
Loss = 2.5583e-03, PNorm = 52.8317, GNorm = 1.6641, lr_0 = 9.1690e-04
Loss = 2.3984e-03, PNorm = 52.8660, GNorm = 1.3121, lr_0 = 9.0048e-04
Loss = 3.4974e-03, PNorm = 52.8695, GNorm = 2.1651, lr_0 = 8.9885e-04
Validation rmse logS = 0.736142
Validation R2 logS = 0.871557
Epoch 6
Train function
Loss = 1.8989e-03, PNorm = 52.9006, GNorm = 1.9161, lr_0 = 8.8275e-04
Validation rmse logS = 0.693762
Validation R2 logS = 0.885920
Epoch 7
Train function
Loss = 1.7182e-03, PNorm = 52.9343, GNorm = 1.1743, lr_0 = 8.6694e-04
Validation rmse logS = 0.696255
Validation R2 logS = 0.885099
Epoch 8
Train function
Loss = 1.6266e-03, PNorm = 52.9737, GNorm = 1.1272, lr_0 = 8.4988e-04
Loss = 1.5088e-03, PNorm = 53.0098, GNorm = 0.6658, lr_0 = 8.3466e-04
Validation rmse logS = 0.663009
Validation R2 logS = 0.895810
Epoch 9
Train function
Loss = 1.3871e-03, PNorm = 53.0455, GNorm = 0.6174, lr_0 = 8.1971e-04
Validation rmse logS = 0.709435
Validation R2 logS = 0.880708
Epoch 10
Train function
Loss = 1.3766e-03, PNorm = 53.0815, GNorm = 0.8375, lr_0 = 8.0357e-04
Validation rmse logS = 0.686428
Validation R2 logS = 0.888319
Epoch 11
Train function
Loss = 2.2127e-03, PNorm = 53.1171, GNorm = 3.4142, lr_0 = 7.8918e-04
Loss = 1.4885e-03, PNorm = 53.1550, GNorm = 1.6205, lr_0 = 7.7504e-04
Validation rmse logS = 0.666682
Validation R2 logS = 0.894653
Epoch 12
Train function
Loss = 1.2217e-03, PNorm = 53.1896, GNorm = 1.3084, lr_0 = 7.5979e-04
Validation rmse logS = 0.616230
Validation R2 logS = 0.909994
Epoch 13
Train function
Loss = 1.0025e-03, PNorm = 53.2226, GNorm = 0.5004, lr_0 = 7.4618e-04
Validation rmse logS = 0.601430
Validation R2 logS = 0.914265
Epoch 14
Train function
Loss = 8.0048e-04, PNorm = 53.2577, GNorm = 0.9853, lr_0 = 7.3149e-04
Loss = 9.7075e-04, PNorm = 53.2900, GNorm = 1.1731, lr_0 = 7.1839e-04
Validation rmse logS = 0.649571
Validation R2 logS = 0.899991
Epoch 15
Train function
Loss = 8.3615e-04, PNorm = 53.3218, GNorm = 0.6440, lr_0 = 7.0552e-04
Validation rmse logS = 0.598647
Validation R2 logS = 0.915057
Epoch 16
Train function
Loss = 9.4562e-04, PNorm = 53.3594, GNorm = 1.6276, lr_0 = 6.9163e-04
Validation rmse logS = 0.712632
Validation R2 logS = 0.879630
Epoch 17
Train function
Loss = 2.0868e-03, PNorm = 53.3876, GNorm = 3.2044, lr_0 = 6.7924e-04
Loss = 8.9603e-04, PNorm = 53.4249, GNorm = 1.0797, lr_0 = 6.6708e-04
Validation rmse logS = 0.600237
Validation R2 logS = 0.914605
Epoch 18
Train function
Loss = 9.6449e-04, PNorm = 53.4631, GNorm = 1.0208, lr_0 = 6.5395e-04
Validation rmse logS = 0.605205
Validation R2 logS = 0.913185
Epoch 19
Train function
Loss = 5.9405e-04, PNorm = 53.4932, GNorm = 0.7307, lr_0 = 6.4223e-04
Validation rmse logS = 0.593354
Validation R2 logS = 0.916552
Epoch 20
Train function
Loss = 7.2168e-04, PNorm = 53.5204, GNorm = 0.5912, lr_0 = 6.2959e-04
Loss = 6.0616e-04, PNorm = 53.5511, GNorm = 0.4896, lr_0 = 6.1831e-04
Validation rmse logS = 0.582122
Validation R2 logS = 0.919682
Epoch 21
Train function
Loss = 5.7592e-04, PNorm = 53.5825, GNorm = 0.3672, lr_0 = 6.0724e-04
Validation rmse logS = 0.568789
Validation R2 logS = 0.923319
Epoch 22
Train function
Loss = 4.9501e-04, PNorm = 53.6104, GNorm = 0.6282, lr_0 = 5.9529e-04
Loss = 5.5127e-04, PNorm = 53.6336, GNorm = 0.7811, lr_0 = 5.8462e-04
Validation rmse logS = 0.580038
Validation R2 logS = 0.920256
Epoch 23
Train function
Loss = 5.6637e-04, PNorm = 53.6582, GNorm = 0.5082, lr_0 = 5.7415e-04
Validation rmse logS = 0.632409
Validation R2 logS = 0.905206
Epoch 24
Train function
Loss = 5.6398e-04, PNorm = 53.6889, GNorm = 1.0480, lr_0 = 5.6285e-04
Validation rmse logS = 0.612127
Validation R2 logS = 0.911188
Epoch 25
Train function
Loss = 5.0673e-04, PNorm = 53.7155, GNorm = 0.6576, lr_0 = 5.5277e-04
Loss = 5.3431e-04, PNorm = 53.7408, GNorm = 0.4247, lr_0 = 5.4287e-04
Loss = 7.8669e-04, PNorm = 53.7430, GNorm = 0.4518, lr_0 = 5.4189e-04
Validation rmse logS = 0.609221
Validation R2 logS = 0.912030
Epoch 26
Train function
Loss = 5.1016e-04, PNorm = 53.7686, GNorm = 0.3889, lr_0 = 5.3218e-04
Validation rmse logS = 0.584111
Validation R2 logS = 0.919132
Epoch 27
Train function
Loss = 4.1120e-04, PNorm = 53.7941, GNorm = 0.2410, lr_0 = 5.2171e-04
Validation rmse logS = 0.569000
Validation R2 logS = 0.923262
Epoch 28
Train function
Loss = 2.6649e-04, PNorm = 53.8166, GNorm = 0.3216, lr_0 = 5.1236e-04
Loss = 3.5255e-04, PNorm = 53.8395, GNorm = 0.5213, lr_0 = 5.0318e-04
Loss = 7.6596e-04, PNorm = 53.8418, GNorm = 0.6576, lr_0 = 5.0228e-04
Validation rmse logS = 0.600834
Validation R2 logS = 0.914435
Epoch 29
Train function
Loss = 3.5815e-04, PNorm = 53.8630, GNorm = 0.3023, lr_0 = 4.9328e-04
Validation rmse logS = 0.636897
Validation R2 logS = 0.903855
Epoch 30
Train function
Loss = 3.8664e-04, PNorm = 53.8876, GNorm = 0.3016, lr_0 = 4.8444e-04
Validation rmse logS = 0.593369
Validation R2 logS = 0.916548
Epoch 31
Train function
Loss = 2.5777e-04, PNorm = 53.9082, GNorm = 0.3084, lr_0 = 4.7491e-04
Loss = 3.6418e-04, PNorm = 53.9305, GNorm = 0.7783, lr_0 = 4.6640e-04
Validation rmse logS = 0.577306
Validation R2 logS = 0.921005
Epoch 32
Train function
Loss = 3.0246e-04, PNorm = 53.9506, GNorm = 0.6645, lr_0 = 4.5805e-04
Validation rmse logS = 0.579721
Validation R2 logS = 0.920343
Epoch 33
Train function
Loss = 2.6413e-04, PNorm = 53.9709, GNorm = 0.5126, lr_0 = 4.4903e-04
Validation rmse logS = 0.570971
Validation R2 logS = 0.922729
Epoch 34
Train function
Loss = 1.6696e-04, PNorm = 53.9891, GNorm = 0.2966, lr_0 = 4.4099e-04
Loss = 2.9292e-04, PNorm = 54.0055, GNorm = 0.5398, lr_0 = 4.3309e-04
Validation rmse logS = 0.575750
Validation R2 logS = 0.921430
Epoch 35
Train function
Loss = 2.5835e-04, PNorm = 54.0265, GNorm = 0.2520, lr_0 = 4.2456e-04
Validation rmse logS = 0.586908
Validation R2 logS = 0.918355
Epoch 36
Train function
Loss = 2.8478e-04, PNorm = 54.0441, GNorm = 0.4771, lr_0 = 4.1696e-04
Validation rmse logS = 0.582190
Validation R2 logS = 0.919663
Epoch 37
Train function
Loss = 2.1731e-04, PNorm = 54.0598, GNorm = 0.4070, lr_0 = 4.0875e-04
Loss = 2.6917e-04, PNorm = 54.0755, GNorm = 0.3565, lr_0 = 4.0143e-04
Validation rmse logS = 0.583589
Validation R2 logS = 0.919276
Epoch 38
Train function
Loss = 1.7990e-04, PNorm = 54.0919, GNorm = 0.3346, lr_0 = 3.9424e-04
Validation rmse logS = 0.573372
Validation R2 logS = 0.922078
Epoch 39
Train function
Loss = 1.7016e-04, PNorm = 54.1077, GNorm = 0.1177, lr_0 = 3.8648e-04
Validation rmse logS = 0.563897
Validation R2 logS = 0.924632
Epoch 40
Train function
Loss = 2.0553e-04, PNorm = 54.1236, GNorm = 0.5164, lr_0 = 3.7956e-04
Loss = 1.8012e-04, PNorm = 54.1382, GNorm = 0.1926, lr_0 = 3.7276e-04
Validation rmse logS = 0.583869
Validation R2 logS = 0.919199
Epoch 41
Train function
Loss = 1.5308e-04, PNorm = 54.1520, GNorm = 0.2586, lr_0 = 3.6542e-04
Validation rmse logS = 0.573317
Validation R2 logS = 0.922093
Epoch 42
Train function
Loss = 2.0681e-04, PNorm = 54.1648, GNorm = 0.3474, lr_0 = 3.5888e-04
Validation rmse logS = 0.577517
Validation R2 logS = 0.920947
Epoch 43
Train function
Loss = 1.2390e-04, PNorm = 54.1790, GNorm = 0.2936, lr_0 = 3.5181e-04
Loss = 1.3477e-04, PNorm = 54.1905, GNorm = 0.4698, lr_0 = 3.4551e-04
Validation rmse logS = 0.569636
Validation R2 logS = 0.923090
Epoch 44
Train function
Loss = 1.2768e-04, PNorm = 54.2028, GNorm = 0.1563, lr_0 = 3.3932e-04
Validation rmse logS = 0.571714
Validation R2 logS = 0.922528
Epoch 45
Train function
Loss = 1.6347e-04, PNorm = 54.2130, GNorm = 0.3894, lr_0 = 3.3264e-04
Loss = 1.8074e-04, PNorm = 54.2260, GNorm = 0.3745, lr_0 = 3.2668e-04
Validation rmse logS = 0.569867
Validation R2 logS = 0.923028
Epoch 46
Train function
Loss = 1.5339e-04, PNorm = 54.2368, GNorm = 0.5468, lr_0 = 3.2083e-04
Validation rmse logS = 0.567086
Validation R2 logS = 0.923777
Epoch 47
Train function
Loss = 1.1899e-04, PNorm = 54.2497, GNorm = 0.2226, lr_0 = 3.1452e-04
Validation rmse logS = 0.582271
Validation R2 logS = 0.919640
Epoch 48
Train function
Loss = 9.5114e-05, PNorm = 54.2607, GNorm = 0.2386, lr_0 = 3.0888e-04
Loss = 1.2485e-04, PNorm = 54.2708, GNorm = 0.2483, lr_0 = 3.0335e-04
Loss = 3.1722e-04, PNorm = 54.2721, GNorm = 0.5928, lr_0 = 3.0280e-04
Validation rmse logS = 0.571799
Validation R2 logS = 0.922505
Epoch 49
Train function
Loss = 1.3827e-04, PNorm = 54.2821, GNorm = 0.6723, lr_0 = 2.9738e-04
Validation rmse logS = 0.577296
Validation R2 logS = 0.921008
Epoch 50
Train function
Loss = 1.6978e-04, PNorm = 54.2945, GNorm = 0.8801, lr_0 = 2.9205e-04
Validation rmse logS = 0.577605
Validation R2 logS = 0.920923
Epoch 51
Train function
Loss = 1.1651e-04, PNorm = 54.3054, GNorm = 0.2753, lr_0 = 2.8630e-04
Loss = 1.2051e-04, PNorm = 54.3152, GNorm = 0.3558, lr_0 = 2.8118e-04
Loss = 8.8452e-05, PNorm = 54.3160, GNorm = 0.2259, lr_0 = 2.8067e-04
Validation rmse logS = 0.581596
Validation R2 logS = 0.919827
Epoch 52
Train function
Loss = 1.2162e-04, PNorm = 54.3246, GNorm = 0.1500, lr_0 = 2.7564e-04
Validation rmse logS = 0.572399
Validation R2 logS = 0.922342
Epoch 53
Train function
Loss = 8.9268e-05, PNorm = 54.3330, GNorm = 0.1570, lr_0 = 2.7070e-04
Validation rmse logS = 0.580815
Validation R2 logS = 0.920042
Epoch 54
Train function
Loss = 9.8897e-05, PNorm = 54.3427, GNorm = 0.4286, lr_0 = 2.6538e-04
Loss = 1.0122e-04, PNorm = 54.3521, GNorm = 0.1363, lr_0 = 2.6062e-04
Validation rmse logS = 0.572692
Validation R2 logS = 0.922263
Epoch 55
Train function
Loss = 1.2662e-04, PNorm = 54.3603, GNorm = 0.3795, lr_0 = 2.5595e-04
Validation rmse logS = 0.577862
Validation R2 logS = 0.920853
Epoch 56
Train function
Loss = 1.0084e-04, PNorm = 54.3680, GNorm = 0.2849, lr_0 = 2.5092e-04
Validation rmse logS = 0.597109
Validation R2 logS = 0.915493
Epoch 57
Train function
Loss = 1.1248e-04, PNorm = 54.3753, GNorm = 0.1652, lr_0 = 2.4642e-04
Loss = 1.0910e-04, PNorm = 54.3824, GNorm = 0.4275, lr_0 = 2.4201e-04
Validation rmse logS = 0.584580
Validation R2 logS = 0.919002
Epoch 58
Train function
Loss = 9.4986e-05, PNorm = 54.3899, GNorm = 0.1677, lr_0 = 2.3724e-04
Validation rmse logS = 0.573171
Validation R2 logS = 0.922133
Epoch 59
Train function
Loss = 9.7326e-05, PNorm = 54.3979, GNorm = 0.4254, lr_0 = 2.3300e-04
Validation rmse logS = 0.578749
Validation R2 logS = 0.920610
Epoch 60
Train function
Loss = 7.2461e-05, PNorm = 54.4057, GNorm = 0.1740, lr_0 = 2.2841e-04
Loss = 8.6282e-05, PNorm = 54.4123, GNorm = 0.2321, lr_0 = 2.2432e-04
Validation rmse logS = 0.585239
Validation R2 logS = 0.918819
Epoch 61
Train function
Loss = 8.6194e-05, PNorm = 54.4204, GNorm = 0.1748, lr_0 = 2.2030e-04
Validation rmse logS = 0.574229
Validation R2 logS = 0.921845
Epoch 62
Train function
Loss = 7.5722e-05, PNorm = 54.4274, GNorm = 0.1430, lr_0 = 2.1596e-04
Validation rmse logS = 0.577850
Validation R2 logS = 0.920856
Epoch 63
Train function
Loss = 1.2540e-04, PNorm = 54.4335, GNorm = 0.2351, lr_0 = 2.1210e-04
Loss = 7.1381e-05, PNorm = 54.4400, GNorm = 0.4402, lr_0 = 2.0830e-04
Validation rmse logS = 0.584740
Validation R2 logS = 0.918957
Epoch 64
Train function
Loss = 8.0352e-05, PNorm = 54.4471, GNorm = 0.6108, lr_0 = 2.0420e-04
Validation rmse logS = 0.600133
Validation R2 logS = 0.914634
Epoch 65
Train function
Loss = 8.3221e-05, PNorm = 54.4537, GNorm = 0.2074, lr_0 = 2.0054e-04
Validation rmse logS = 0.580970
Validation R2 logS = 0.919999
Epoch 66
Train function
Loss = 1.3855e-04, PNorm = 54.4602, GNorm = 0.2252, lr_0 = 1.9659e-04
Loss = 6.1270e-05, PNorm = 54.4660, GNorm = 0.2240, lr_0 = 1.9307e-04
Validation rmse logS = 0.577803
Validation R2 logS = 0.920869
Epoch 67
Train function
Loss = 5.8406e-05, PNorm = 54.4699, GNorm = 0.2294, lr_0 = 1.8961e-04
Validation rmse logS = 0.578866
Validation R2 logS = 0.920577
Epoch 68
Train function
Loss = 6.1568e-05, PNorm = 54.4763, GNorm = 0.2143, lr_0 = 1.8588e-04
Loss = 5.9190e-05, PNorm = 54.4819, GNorm = 0.2300, lr_0 = 1.8255e-04
Validation rmse logS = 0.575983
Validation R2 logS = 0.921367
Epoch 69
Train function
Loss = 5.6440e-05, PNorm = 54.4869, GNorm = 0.1652, lr_0 = 1.7928e-04
Validation rmse logS = 0.583621
Validation R2 logS = 0.919267
Epoch 70
Train function
Loss = 5.8088e-05, PNorm = 54.4922, GNorm = 0.1099, lr_0 = 1.7575e-04
Validation rmse logS = 0.583261
Validation R2 logS = 0.919367
Epoch 71
Train function
Loss = 4.9742e-05, PNorm = 54.4975, GNorm = 0.2368, lr_0 = 1.7260e-04
Loss = 6.8326e-05, PNorm = 54.5022, GNorm = 0.3141, lr_0 = 1.6951e-04
Loss = 9.1912e-05, PNorm = 54.5026, GNorm = 0.2567, lr_0 = 1.6921e-04
Validation rmse logS = 0.584819
Validation R2 logS = 0.918936
Epoch 72
Train function
Loss = 5.6044e-05, PNorm = 54.5075, GNorm = 0.1970, lr_0 = 1.6617e-04
Validation rmse logS = 0.586146
Validation R2 logS = 0.918567
Epoch 73
Train function
Loss = 5.5255e-05, PNorm = 54.5121, GNorm = 0.1611, lr_0 = 1.6320e-04
Validation rmse logS = 0.576589
Validation R2 logS = 0.921201
Epoch 74
Train function
Loss = 4.5677e-05, PNorm = 54.5174, GNorm = 0.1256, lr_0 = 1.5999e-04
Loss = 5.0402e-05, PNorm = 54.5215, GNorm = 0.1998, lr_0 = 1.5712e-04
Validation rmse logS = 0.583444
Validation R2 logS = 0.919316
Epoch 75
Train function
Loss = 5.4655e-05, PNorm = 54.5259, GNorm = 0.2120, lr_0 = 1.5431e-04
Validation rmse logS = 0.585058
Validation R2 logS = 0.918869
Epoch 76
Train function
Loss = 5.7085e-05, PNorm = 54.5304, GNorm = 0.2424, lr_0 = 1.5127e-04
Validation rmse logS = 0.588838
Validation R2 logS = 0.917818
Epoch 77
Train function
Loss = 4.8885e-05, PNorm = 54.5348, GNorm = 0.1773, lr_0 = 1.4829e-04
Loss = 4.7689e-05, PNorm = 54.5386, GNorm = 0.1956, lr_0 = 1.4563e-04
Validation rmse logS = 0.585014
Validation R2 logS = 0.918882
Epoch 78
Train function
Loss = 5.1647e-05, PNorm = 54.5426, GNorm = 0.1701, lr_0 = 1.4303e-04
Validation rmse logS = 0.580706
Validation R2 logS = 0.920072
Epoch 79
Train function
Loss = 5.3497e-05, PNorm = 54.5470, GNorm = 0.1756, lr_0 = 1.4021e-04
Validation rmse logS = 0.580179
Validation R2 logS = 0.920217
Epoch 80
Train function
Loss = 5.9083e-05, PNorm = 54.5505, GNorm = 0.2974, lr_0 = 1.3770e-04
Loss = 4.0501e-05, PNorm = 54.5543, GNorm = 0.0824, lr_0 = 1.3523e-04
Validation rmse logS = 0.589091
Validation R2 logS = 0.917747
Epoch 81
Train function
Loss = 4.4685e-05, PNorm = 54.5589, GNorm = 0.1725, lr_0 = 1.3257e-04
Validation rmse logS = 0.584481
Validation R2 logS = 0.919029
Epoch 82
Train function
Loss = 3.4149e-05, PNorm = 54.5617, GNorm = 0.2044, lr_0 = 1.3020e-04
Validation rmse logS = 0.588951
Validation R2 logS = 0.917786
Epoch 83
Train function
Loss = 5.3927e-05, PNorm = 54.5656, GNorm = 0.1965, lr_0 = 1.2763e-04
Loss = 4.4690e-05, PNorm = 54.5689, GNorm = 0.0843, lr_0 = 1.2535e-04
Validation rmse logS = 0.584941
Validation R2 logS = 0.918902
Epoch 84
Train function
Loss = 3.4547e-05, PNorm = 54.5717, GNorm = 0.1007, lr_0 = 1.2310e-04
Validation rmse logS = 0.586149
Validation R2 logS = 0.918567
Epoch 85
Train function
Loss = 2.7296e-05, PNorm = 54.5751, GNorm = 0.0719, lr_0 = 1.2068e-04
Validation rmse logS = 0.587084
Validation R2 logS = 0.918307
Epoch 86
Train function
Loss = 2.7566e-05, PNorm = 54.5781, GNorm = 0.1089, lr_0 = 1.1852e-04
Loss = 4.0674e-05, PNorm = 54.5811, GNorm = 0.1648, lr_0 = 1.1639e-04
Validation rmse logS = 0.585971
Validation R2 logS = 0.918616
Epoch 87
Train function
Loss = 4.3896e-05, PNorm = 54.5842, GNorm = 0.2215, lr_0 = 1.1410e-04
Validation rmse logS = 0.585850
Validation R2 logS = 0.918649
Epoch 88
Train function
Loss = 3.0166e-05, PNorm = 54.5874, GNorm = 0.1017, lr_0 = 1.1206e-04
Validation rmse logS = 0.582781
Validation R2 logS = 0.919500
Epoch 89
Train function
Loss = 4.6080e-05, PNorm = 54.5906, GNorm = 0.1767, lr_0 = 1.0985e-04
Loss = 3.9815e-05, PNorm = 54.5933, GNorm = 0.1646, lr_0 = 1.0789e-04
Validation rmse logS = 0.582937
Validation R2 logS = 0.919457
Epoch 90
Train function
Loss = 3.2696e-05, PNorm = 54.5958, GNorm = 0.1449, lr_0 = 1.0595e-04
Validation rmse logS = 0.594210
Validation R2 logS = 0.916311
Epoch 91
Train function
Loss = 4.0754e-05, PNorm = 54.5988, GNorm = 0.2488, lr_0 = 1.0387e-04
Loss = 4.5515e-05, PNorm = 54.6008, GNorm = 0.1463, lr_0 = 1.0201e-04
Validation rmse logS = 0.587013
Validation R2 logS = 0.918326
Epoch 92
Train function
Loss = 4.4045e-05, PNorm = 54.6033, GNorm = 0.1591, lr_0 = 1.0018e-04
Validation rmse logS = 0.593670
Validation R2 logS = 0.916463
Epoch 93
Train function
Loss = 3.7124e-05, PNorm = 54.6063, GNorm = 0.3043, lr_0 = 1.0000e-04
Validation rmse logS = 0.583254
Validation R2 logS = 0.919369
Epoch 94
Train function
Loss = 2.3015e-05, PNorm = 54.6084, GNorm = 0.0748, lr_0 = 1.0000e-04
Loss = 4.2969e-05, PNorm = 54.6112, GNorm = 0.1500, lr_0 = 1.0000e-04
Loss = 3.7537e-05, PNorm = 54.6114, GNorm = 0.2258, lr_0 = 1.0000e-04
Validation rmse logS = 0.589795
Validation R2 logS = 0.917550
Epoch 95
Train function
Loss = 3.3156e-05, PNorm = 54.6137, GNorm = 0.0993, lr_0 = 1.0000e-04
Validation rmse logS = 0.584827
Validation R2 logS = 0.918933
Epoch 96
Train function
Loss = 3.1989e-05, PNorm = 54.6162, GNorm = 0.0880, lr_0 = 1.0000e-04
Validation rmse logS = 0.591287
Validation R2 logS = 0.917133
Epoch 97
Train function
Loss = 2.9920e-05, PNorm = 54.6189, GNorm = 0.2234, lr_0 = 1.0000e-04
Loss = 3.7332e-05, PNorm = 54.6216, GNorm = 0.1905, lr_0 = 1.0000e-04
Validation rmse logS = 0.582398
Validation R2 logS = 0.919605
Epoch 98
Train function
Loss = 3.9508e-05, PNorm = 54.6238, GNorm = 0.2326, lr_0 = 1.0000e-04
Validation rmse logS = 0.586675
Validation R2 logS = 0.918420
Epoch 99
Train function
Loss = 2.8931e-05, PNorm = 54.6265, GNorm = 0.1719, lr_0 = 1.0000e-04
Validation rmse logS = 0.583914
Validation R2 logS = 0.919186
Model 0 best validation rmse = 0.563897 on epoch 39
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.624113
Model 0 test R2 logS = 0.898170
Ensemble test rmse  logS= 0.624113
Ensemble test R2  logS= 0.898170
Fold 2
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_306/folds/fold_2',
 'save_smiles_splits': False,
 'seed': 2,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 674,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 2
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=895, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,222,401
Moving model to cuda
Epoch 0
Train function
Loss = 1.6108e-02, PNorm = 52.5079, GNorm = 3.0987, lr_0 = 4.8077e-04
Validation rmse logS = 1.164023
Validation R2 logS = 0.685951
Epoch 1
Train function
Loss = 7.8703e-03, PNorm = 52.5667, GNorm = 2.0468, lr_0 = 8.6154e-04
Validation rmse logS = 0.964585
Validation R2 logS = 0.784348
Epoch 2
Train function
Loss = 3.3254e-03, PNorm = 52.6424, GNorm = 1.3494, lr_0 = 9.8743e-04
Loss = 4.1407e-03, PNorm = 52.6975, GNorm = 3.2236, lr_0 = 9.6974e-04
Validation rmse logS = 0.907944
Validation R2 logS = 0.808930
Epoch 3
Train function
Loss = 3.1831e-03, PNorm = 52.7435, GNorm = 2.4179, lr_0 = 9.5237e-04
Validation rmse logS = 0.908436
Validation R2 logS = 0.808723
Epoch 4
Train function
Loss = 3.3512e-03, PNorm = 52.7910, GNorm = 0.9954, lr_0 = 9.3363e-04
Validation rmse logS = 0.786650
Validation R2 logS = 0.856571
Epoch 5
Train function
Loss = 2.1580e-03, PNorm = 52.8284, GNorm = 1.6088, lr_0 = 9.1690e-04
Loss = 2.0813e-03, PNorm = 52.8625, GNorm = 0.4988, lr_0 = 9.0048e-04
Loss = 5.5142e-03, PNorm = 52.8661, GNorm = 1.5259, lr_0 = 8.9885e-04
Validation rmse logS = 0.740752
Validation R2 logS = 0.872820
Epoch 6
Train function
Loss = 1.7303e-03, PNorm = 52.8979, GNorm = 0.8521, lr_0 = 8.8275e-04
Validation rmse logS = 0.699312
Validation R2 logS = 0.886652
Epoch 7
Train function
Loss = 1.7463e-03, PNorm = 52.9301, GNorm = 0.5938, lr_0 = 8.6694e-04
Validation rmse logS = 0.664619
Validation R2 logS = 0.897619
Epoch 8
Train function
Loss = 1.1920e-03, PNorm = 52.9633, GNorm = 1.1070, lr_0 = 8.4988e-04
Loss = 1.3811e-03, PNorm = 52.9948, GNorm = 1.6599, lr_0 = 8.3466e-04
Validation rmse logS = 0.683733
Validation R2 logS = 0.891646
Epoch 9
Train function
Loss = 1.3730e-03, PNorm = 53.0282, GNorm = 0.7399, lr_0 = 8.1971e-04
Validation rmse logS = 0.661452
Validation R2 logS = 0.898593
Epoch 10
Train function
Loss = 1.0989e-03, PNorm = 53.0676, GNorm = 0.5770, lr_0 = 8.0357e-04
Validation rmse logS = 0.645736
Validation R2 logS = 0.903354
Epoch 11
Train function
Loss = 1.1276e-03, PNorm = 53.0975, GNorm = 0.6488, lr_0 = 7.8918e-04
Loss = 1.3575e-03, PNorm = 53.1278, GNorm = 0.8395, lr_0 = 7.7504e-04
Validation rmse logS = 0.673775
Validation R2 logS = 0.894779
Epoch 12
Train function
Loss = 9.6403e-04, PNorm = 53.1660, GNorm = 0.4397, lr_0 = 7.5979e-04
Validation rmse logS = 0.616018
Validation R2 logS = 0.912045
Epoch 13
Train function
Loss = 9.1366e-04, PNorm = 53.1965, GNorm = 1.7081, lr_0 = 7.4618e-04
Validation rmse logS = 0.651005
Validation R2 logS = 0.901770
Epoch 14
Train function
Loss = 8.5315e-04, PNorm = 53.2327, GNorm = 0.7245, lr_0 = 7.3149e-04
Loss = 9.8199e-04, PNorm = 53.2629, GNorm = 0.7079, lr_0 = 7.1839e-04
Validation rmse logS = 0.644942
Validation R2 logS = 0.903592
Epoch 15
Train function
Loss = 8.8311e-04, PNorm = 53.2955, GNorm = 1.7245, lr_0 = 7.0552e-04
Validation rmse logS = 0.639011
Validation R2 logS = 0.905357
Epoch 16
Train function
Loss = 8.2174e-04, PNorm = 53.3388, GNorm = 1.4243, lr_0 = 6.9163e-04
Validation rmse logS = 0.626002
Validation R2 logS = 0.909171
Epoch 17
Train function
Loss = 7.7711e-04, PNorm = 53.3704, GNorm = 0.9277, lr_0 = 6.7924e-04
Loss = 8.2501e-04, PNorm = 53.4077, GNorm = 0.5626, lr_0 = 6.6708e-04
Validation rmse logS = 0.618362
Validation R2 logS = 0.911374
Epoch 18
Train function
Loss = 7.3850e-04, PNorm = 53.4415, GNorm = 0.6659, lr_0 = 6.5395e-04
Validation rmse logS = 0.585007
Validation R2 logS = 0.920678
Epoch 19
Train function
Loss = 6.4140e-04, PNorm = 53.4694, GNorm = 0.7791, lr_0 = 6.4223e-04
Validation rmse logS = 0.592664
Validation R2 logS = 0.918588
Epoch 20
Train function
Loss = 5.4523e-04, PNorm = 53.5041, GNorm = 0.5666, lr_0 = 6.2959e-04
Loss = 6.1359e-04, PNorm = 53.5294, GNorm = 0.5917, lr_0 = 6.1831e-04
Validation rmse logS = 0.618764
Validation R2 logS = 0.911259
Epoch 21
Train function
Loss = 6.4169e-04, PNorm = 53.5614, GNorm = 1.9545, lr_0 = 6.0724e-04
Validation rmse logS = 0.585565
Validation R2 logS = 0.920526
Epoch 22
Train function
Loss = 6.9644e-04, PNorm = 53.5974, GNorm = 0.9059, lr_0 = 5.9529e-04
Loss = 7.2016e-04, PNorm = 53.6286, GNorm = 1.3716, lr_0 = 5.8462e-04
Validation rmse logS = 0.581437
Validation R2 logS = 0.921643
Epoch 23
Train function
Loss = 7.0154e-04, PNorm = 53.6608, GNorm = 1.2080, lr_0 = 5.7415e-04
Validation rmse logS = 0.574490
Validation R2 logS = 0.923504
Epoch 24
Train function
Loss = 5.3612e-04, PNorm = 53.6940, GNorm = 0.6145, lr_0 = 5.6285e-04
Validation rmse logS = 0.569780
Validation R2 logS = 0.924753
Epoch 25
Train function
Loss = 3.9153e-04, PNorm = 53.7182, GNorm = 0.4953, lr_0 = 5.5277e-04
Loss = 4.9727e-04, PNorm = 53.7456, GNorm = 0.5268, lr_0 = 5.4287e-04
Loss = 1.1363e-03, PNorm = 53.7474, GNorm = 1.1957, lr_0 = 5.4189e-04
Validation rmse logS = 0.590912
Validation R2 logS = 0.919068
Epoch 26
Train function
Loss = 4.5124e-04, PNorm = 53.7716, GNorm = 0.3245, lr_0 = 5.3218e-04
Validation rmse logS = 0.579543
Validation R2 logS = 0.922153
Epoch 27
Train function
Loss = 3.4623e-04, PNorm = 53.7973, GNorm = 0.2544, lr_0 = 5.2171e-04
Validation rmse logS = 0.586268
Validation R2 logS = 0.920335
Epoch 28
Train function
Loss = 4.7061e-04, PNorm = 53.8177, GNorm = 0.6717, lr_0 = 5.1236e-04
Loss = 4.0389e-04, PNorm = 53.8406, GNorm = 0.3731, lr_0 = 5.0318e-04
Loss = 3.7796e-04, PNorm = 53.8428, GNorm = 0.6037, lr_0 = 5.0228e-04
Validation rmse logS = 0.568733
Validation R2 logS = 0.925029
Epoch 29
Train function
Loss = 3.2556e-04, PNorm = 53.8677, GNorm = 0.5360, lr_0 = 4.9328e-04
Validation rmse logS = 0.558078
Validation R2 logS = 0.927812
Epoch 30
Train function
Loss = 3.3078e-04, PNorm = 53.8909, GNorm = 0.2936, lr_0 = 4.8444e-04
Validation rmse logS = 0.574970
Validation R2 logS = 0.923376
Epoch 31
Train function
Loss = 2.6147e-04, PNorm = 53.9146, GNorm = 0.3476, lr_0 = 4.7491e-04
Loss = 3.0263e-04, PNorm = 53.9360, GNorm = 0.4822, lr_0 = 4.6640e-04
Validation rmse logS = 0.551537
Validation R2 logS = 0.929494
Epoch 32
Train function
Loss = 2.5254e-04, PNorm = 53.9585, GNorm = 0.1832, lr_0 = 4.5805e-04
Validation rmse logS = 0.562961
Validation R2 logS = 0.926543
Epoch 33
Train function
Loss = 2.8258e-04, PNorm = 53.9804, GNorm = 0.4910, lr_0 = 4.4903e-04
Validation rmse logS = 0.556279
Validation R2 logS = 0.928277
Epoch 34
Train function
Loss = 3.6287e-04, PNorm = 54.0020, GNorm = 0.4944, lr_0 = 4.4099e-04
Loss = 3.0138e-04, PNorm = 54.0228, GNorm = 0.4264, lr_0 = 4.3309e-04
Validation rmse logS = 0.559910
Validation R2 logS = 0.927338
Epoch 35
Train function
Loss = 2.5878e-04, PNorm = 54.0432, GNorm = 0.5207, lr_0 = 4.2456e-04
Validation rmse logS = 0.568516
Validation R2 logS = 0.925087
Epoch 36
Train function
Loss = 2.3803e-04, PNorm = 54.0594, GNorm = 0.2543, lr_0 = 4.1696e-04
Validation rmse logS = 0.568516
Validation R2 logS = 0.925087
Epoch 37
Train function
Loss = 2.3816e-04, PNorm = 54.0789, GNorm = 0.4369, lr_0 = 4.0875e-04
Loss = 2.3449e-04, PNorm = 54.0962, GNorm = 0.2050, lr_0 = 4.0143e-04
Validation rmse logS = 0.551508
Validation R2 logS = 0.929502
Epoch 38
Train function
Loss = 2.7507e-04, PNorm = 54.1135, GNorm = 0.4803, lr_0 = 3.9424e-04
Validation rmse logS = 0.573694
Validation R2 logS = 0.923716
Epoch 39
Train function
Loss = 1.7798e-04, PNorm = 54.1328, GNorm = 0.4198, lr_0 = 3.8648e-04
Validation rmse logS = 0.561748
Validation R2 logS = 0.926860
Epoch 40
Train function
Loss = 1.5615e-04, PNorm = 54.1477, GNorm = 0.6134, lr_0 = 3.7956e-04
Loss = 2.2145e-04, PNorm = 54.1643, GNorm = 0.4525, lr_0 = 3.7276e-04
Validation rmse logS = 0.559239
Validation R2 logS = 0.927512
Epoch 41
Train function
Loss = 1.4737e-04, PNorm = 54.1821, GNorm = 0.5755, lr_0 = 3.6542e-04
Validation rmse logS = 0.590060
Validation R2 logS = 0.919301
Epoch 42
Train function
Loss = 3.6846e-04, PNorm = 54.1993, GNorm = 0.7378, lr_0 = 3.5888e-04
Validation rmse logS = 0.589982
Validation R2 logS = 0.919323
Epoch 43
Train function
Loss = 2.4998e-04, PNorm = 54.2200, GNorm = 0.8853, lr_0 = 3.5181e-04
Loss = 2.5441e-04, PNorm = 54.2391, GNorm = 0.3973, lr_0 = 3.4551e-04
Validation rmse logS = 0.561760
Validation R2 logS = 0.926857
Epoch 44
Train function
Loss = 1.7238e-04, PNorm = 54.2537, GNorm = 0.7335, lr_0 = 3.3932e-04
Validation rmse logS = 0.554669
Validation R2 logS = 0.928692
Epoch 45
Train function
Loss = 1.4710e-04, PNorm = 54.2676, GNorm = 0.2521, lr_0 = 3.3264e-04
Loss = 1.6398e-04, PNorm = 54.2787, GNorm = 0.4926, lr_0 = 3.2668e-04
Validation rmse logS = 0.565835
Validation R2 logS = 0.925792
Epoch 46
Train function
Loss = 1.3439e-04, PNorm = 54.2910, GNorm = 0.2452, lr_0 = 3.2083e-04
Validation rmse logS = 0.565714
Validation R2 logS = 0.925823
Epoch 47
Train function
Loss = 1.5274e-04, PNorm = 54.3045, GNorm = 0.2757, lr_0 = 3.1452e-04
Validation rmse logS = 0.566307
Validation R2 logS = 0.925668
Epoch 48
Train function
Loss = 1.7011e-04, PNorm = 54.3164, GNorm = 0.7931, lr_0 = 3.0888e-04
Loss = 1.6745e-04, PNorm = 54.3268, GNorm = 0.2413, lr_0 = 3.0335e-04
Loss = 1.7901e-04, PNorm = 54.3278, GNorm = 0.4163, lr_0 = 3.0280e-04
Validation rmse logS = 0.566619
Validation R2 logS = 0.925586
Epoch 49
Train function
Loss = 1.2667e-04, PNorm = 54.3386, GNorm = 0.3887, lr_0 = 2.9738e-04
Validation rmse logS = 0.562190
Validation R2 logS = 0.926745
Epoch 50
Train function
Loss = 1.3560e-04, PNorm = 54.3498, GNorm = 0.4070, lr_0 = 2.9205e-04
Validation rmse logS = 0.559967
Validation R2 logS = 0.927323
Epoch 51
Train function
Loss = 1.3923e-04, PNorm = 54.3608, GNorm = 0.2528, lr_0 = 2.8630e-04
Loss = 1.1246e-04, PNorm = 54.3712, GNorm = 0.4250, lr_0 = 2.8118e-04
Loss = 2.1528e-04, PNorm = 54.3720, GNorm = 0.2367, lr_0 = 2.8067e-04
Validation rmse logS = 0.555716
Validation R2 logS = 0.928422
Epoch 52
Train function
Loss = 1.1708e-04, PNorm = 54.3819, GNorm = 0.2336, lr_0 = 2.7564e-04
Validation rmse logS = 0.555757
Validation R2 logS = 0.928411
Epoch 53
Train function
Loss = 9.2631e-05, PNorm = 54.3913, GNorm = 0.1208, lr_0 = 2.7070e-04
Validation rmse logS = 0.555697
Validation R2 logS = 0.928427
Epoch 54
Train function
Loss = 1.0910e-04, PNorm = 54.4003, GNorm = 0.5875, lr_0 = 2.6538e-04
Loss = 1.0796e-04, PNorm = 54.4094, GNorm = 0.1704, lr_0 = 2.6062e-04
Validation rmse logS = 0.558219
Validation R2 logS = 0.927776
Epoch 55
Train function
Loss = 9.0751e-05, PNorm = 54.4188, GNorm = 0.1840, lr_0 = 2.5595e-04
Validation rmse logS = 0.563934
Validation R2 logS = 0.926289
Epoch 56
Train function
Loss = 1.3411e-04, PNorm = 54.4292, GNorm = 0.1631, lr_0 = 2.5092e-04
Validation rmse logS = 0.556980
Validation R2 logS = 0.928096
Epoch 57
Train function
Loss = 7.9566e-05, PNorm = 54.4376, GNorm = 0.1358, lr_0 = 2.4642e-04
Loss = 1.0542e-04, PNorm = 54.4449, GNorm = 0.2126, lr_0 = 2.4201e-04
Validation rmse logS = 0.574481
Validation R2 logS = 0.923507
Epoch 58
Train function
Loss = 9.6848e-05, PNorm = 54.4541, GNorm = 0.1957, lr_0 = 2.3724e-04
Validation rmse logS = 0.571839
Validation R2 logS = 0.924208
Epoch 59
Train function
Loss = 9.7775e-05, PNorm = 54.4622, GNorm = 0.1941, lr_0 = 2.3300e-04
Validation rmse logS = 0.561370
Validation R2 logS = 0.926958
Epoch 60
Train function
Loss = 7.4691e-05, PNorm = 54.4709, GNorm = 0.1117, lr_0 = 2.2841e-04
Loss = 1.0531e-04, PNorm = 54.4794, GNorm = 0.2691, lr_0 = 2.2432e-04
Validation rmse logS = 0.559407
Validation R2 logS = 0.927468
Epoch 61
Train function
Loss = 8.7319e-05, PNorm = 54.4862, GNorm = 0.2350, lr_0 = 2.2030e-04
Validation rmse logS = 0.561943
Validation R2 logS = 0.926809
Epoch 62
Train function
Loss = 7.4765e-05, PNorm = 54.4948, GNorm = 0.3263, lr_0 = 2.1596e-04
Validation rmse logS = 0.565225
Validation R2 logS = 0.925952
Epoch 63
Train function
Loss = 1.0800e-04, PNorm = 54.5003, GNorm = 0.2185, lr_0 = 2.1210e-04
Loss = 7.7753e-05, PNorm = 54.5074, GNorm = 0.3816, lr_0 = 2.0830e-04
Validation rmse logS = 0.567678
Validation R2 logS = 0.925308
Epoch 64
Train function
Loss = 8.4377e-05, PNorm = 54.5145, GNorm = 0.3882, lr_0 = 2.0420e-04
Validation rmse logS = 0.564730
Validation R2 logS = 0.926081
Epoch 65
Train function
Loss = 8.5459e-05, PNorm = 54.5234, GNorm = 0.2504, lr_0 = 2.0054e-04
Validation rmse logS = 0.566569
Validation R2 logS = 0.925599
Epoch 66
Train function
Loss = 7.9659e-05, PNorm = 54.5319, GNorm = 0.2024, lr_0 = 1.9659e-04
Loss = 9.0192e-05, PNorm = 54.5393, GNorm = 0.4585, lr_0 = 1.9307e-04
Validation rmse logS = 0.560723
Validation R2 logS = 0.927126
Epoch 67
Train function
Loss = 9.3285e-05, PNorm = 54.5455, GNorm = 0.1204, lr_0 = 1.8961e-04
Validation rmse logS = 0.568703
Validation R2 logS = 0.925037
Epoch 68
Train function
Loss = 6.7659e-05, PNorm = 54.5514, GNorm = 0.2133, lr_0 = 1.8588e-04
Loss = 7.5597e-05, PNorm = 54.5570, GNorm = 0.2932, lr_0 = 1.8255e-04
Validation rmse logS = 0.566518
Validation R2 logS = 0.925612
Epoch 69
Train function
Loss = 6.6244e-05, PNorm = 54.5635, GNorm = 0.1803, lr_0 = 1.7928e-04
Validation rmse logS = 0.563324
Validation R2 logS = 0.926449
Epoch 70
Train function
Loss = 4.9262e-05, PNorm = 54.5693, GNorm = 0.0871, lr_0 = 1.7575e-04
Validation rmse logS = 0.565418
Validation R2 logS = 0.925901
Epoch 71
Train function
Loss = 6.0450e-05, PNorm = 54.5744, GNorm = 0.0921, lr_0 = 1.7260e-04
Loss = 5.3502e-05, PNorm = 54.5797, GNorm = 0.2894, lr_0 = 1.6951e-04
Loss = 3.6719e-05, PNorm = 54.5801, GNorm = 0.1088, lr_0 = 1.6921e-04
Validation rmse logS = 0.562111
Validation R2 logS = 0.926765
Epoch 72
Train function
Loss = 5.2161e-05, PNorm = 54.5845, GNorm = 0.3808, lr_0 = 1.6617e-04
Validation rmse logS = 0.565521
Validation R2 logS = 0.925874
Epoch 73
Train function
Loss = 4.3932e-05, PNorm = 54.5896, GNorm = 0.0805, lr_0 = 1.6320e-04
Validation rmse logS = 0.563053
Validation R2 logS = 0.926520
Epoch 74
Train function
Loss = 3.9815e-05, PNorm = 54.5947, GNorm = 0.1737, lr_0 = 1.5999e-04
Loss = 6.3869e-05, PNorm = 54.5979, GNorm = 0.1179, lr_0 = 1.5712e-04
Validation rmse logS = 0.566279
Validation R2 logS = 0.925675
Epoch 75
Train function
Loss = 6.7567e-05, PNorm = 54.6037, GNorm = 0.1824, lr_0 = 1.5431e-04
Validation rmse logS = 0.560109
Validation R2 logS = 0.927286
Epoch 76
Train function
Loss = 4.7923e-05, PNorm = 54.6093, GNorm = 0.1894, lr_0 = 1.5127e-04
Validation rmse logS = 0.561988
Validation R2 logS = 0.926797
Epoch 77
Train function
Loss = 6.5129e-05, PNorm = 54.6140, GNorm = 0.2007, lr_0 = 1.4829e-04
Loss = 4.5220e-05, PNorm = 54.6185, GNorm = 0.1924, lr_0 = 1.4563e-04
Validation rmse logS = 0.562284
Validation R2 logS = 0.926720
Epoch 78
Train function
Loss = 4.2300e-05, PNorm = 54.6227, GNorm = 0.1625, lr_0 = 1.4303e-04
Validation rmse logS = 0.564253
Validation R2 logS = 0.926206
Epoch 79
Train function
Loss = 4.5080e-05, PNorm = 54.6281, GNorm = 0.3012, lr_0 = 1.4021e-04
Validation rmse logS = 0.563943
Validation R2 logS = 0.926287
Epoch 80
Train function
Loss = 4.8100e-05, PNorm = 54.6325, GNorm = 0.1295, lr_0 = 1.3770e-04
Loss = 4.5543e-05, PNorm = 54.6363, GNorm = 0.1339, lr_0 = 1.3523e-04
Validation rmse logS = 0.562542
Validation R2 logS = 0.926653
Epoch 81
Train function
Loss = 4.5676e-05, PNorm = 54.6407, GNorm = 0.3007, lr_0 = 1.3257e-04
Validation rmse logS = 0.565767
Validation R2 logS = 0.925809
Epoch 82
Train function
Loss = 5.1808e-05, PNorm = 54.6443, GNorm = 0.2891, lr_0 = 1.3020e-04
Validation rmse logS = 0.565645
Validation R2 logS = 0.925842
Epoch 83
Train function
Loss = 2.1569e-05, PNorm = 54.6484, GNorm = 0.1364, lr_0 = 1.2763e-04
Loss = 5.2961e-05, PNorm = 54.6518, GNorm = 0.3536, lr_0 = 1.2535e-04
Validation rmse logS = 0.566258
Validation R2 logS = 0.925681
Epoch 84
Train function
Loss = 4.5998e-05, PNorm = 54.6555, GNorm = 0.1161, lr_0 = 1.2310e-04
Validation rmse logS = 0.566025
Validation R2 logS = 0.925742
Epoch 85
Train function
Loss = 3.9685e-05, PNorm = 54.6596, GNorm = 0.2548, lr_0 = 1.2068e-04
Validation rmse logS = 0.565285
Validation R2 logS = 0.925936
Epoch 86
Train function
Loss = 3.3381e-05, PNorm = 54.6627, GNorm = 0.0808, lr_0 = 1.1852e-04
Loss = 4.6516e-05, PNorm = 54.6664, GNorm = 0.1845, lr_0 = 1.1639e-04
Validation rmse logS = 0.566518
Validation R2 logS = 0.925612
Epoch 87
Train function
Loss = 4.2659e-05, PNorm = 54.6702, GNorm = 0.2642, lr_0 = 1.1410e-04
Validation rmse logS = 0.566826
Validation R2 logS = 0.925531
Epoch 88
Train function
Loss = 3.9583e-05, PNorm = 54.6733, GNorm = 0.1734, lr_0 = 1.1206e-04
Validation rmse logS = 0.564822
Validation R2 logS = 0.926057
Epoch 89
Train function
Loss = 6.0807e-05, PNorm = 54.6768, GNorm = 0.1462, lr_0 = 1.0985e-04
Loss = 3.5243e-05, PNorm = 54.6800, GNorm = 0.1341, lr_0 = 1.0789e-04
Validation rmse logS = 0.567245
Validation R2 logS = 0.925421
Epoch 90
Train function
Loss = 3.9350e-05, PNorm = 54.6826, GNorm = 0.2498, lr_0 = 1.0595e-04
Validation rmse logS = 0.566919
Validation R2 logS = 0.925507
Epoch 91
Train function
Loss = 2.7034e-05, PNorm = 54.6858, GNorm = 0.1083, lr_0 = 1.0387e-04
Loss = 4.4544e-05, PNorm = 54.6886, GNorm = 0.3156, lr_0 = 1.0201e-04
Validation rmse logS = 0.565371
Validation R2 logS = 0.925913
Epoch 92
Train function
Loss = 3.1677e-05, PNorm = 54.6911, GNorm = 0.0900, lr_0 = 1.0018e-04
Validation rmse logS = 0.567093
Validation R2 logS = 0.925461
Epoch 93
Train function
Loss = 2.6865e-05, PNorm = 54.6936, GNorm = 0.0821, lr_0 = 1.0000e-04
Validation rmse logS = 0.566652
Validation R2 logS = 0.925577
Epoch 94
Train function
Loss = 2.3437e-05, PNorm = 54.6964, GNorm = 0.0912, lr_0 = 1.0000e-04
Loss = 3.4870e-05, PNorm = 54.6988, GNorm = 0.1855, lr_0 = 1.0000e-04
Loss = 2.7476e-05, PNorm = 54.6991, GNorm = 0.0936, lr_0 = 1.0000e-04
Validation rmse logS = 0.566219
Validation R2 logS = 0.925691
Epoch 95
Train function
Loss = 3.3789e-05, PNorm = 54.7019, GNorm = 0.3175, lr_0 = 1.0000e-04
Validation rmse logS = 0.568191
Validation R2 logS = 0.925172
Epoch 96
Train function
Loss = 4.1144e-05, PNorm = 54.7048, GNorm = 0.4944, lr_0 = 1.0000e-04
Validation rmse logS = 0.570435
Validation R2 logS = 0.924580
Epoch 97
Train function
Loss = 2.2456e-05, PNorm = 54.7081, GNorm = 0.1683, lr_0 = 1.0000e-04
Loss = 3.5331e-05, PNorm = 54.7104, GNorm = 0.2482, lr_0 = 1.0000e-04
Validation rmse logS = 0.565595
Validation R2 logS = 0.925854
Epoch 98
Train function
Loss = 2.8255e-05, PNorm = 54.7128, GNorm = 0.0538, lr_0 = 1.0000e-04
Validation rmse logS = 0.568593
Validation R2 logS = 0.925066
Epoch 99
Train function
Loss = 2.4300e-05, PNorm = 54.7154, GNorm = 0.0842, lr_0 = 1.0000e-04
Validation rmse logS = 0.565935
Validation R2 logS = 0.925765
Model 0 best validation rmse = 0.551508 on epoch 37
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.621727
Model 0 test R2 logS = 0.898948
Ensemble test rmse  logS= 0.621727
Ensemble test R2  logS= 0.898948
Fold 3
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_306/folds/fold_3',
 'save_smiles_splits': False,
 'seed': 3,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 674,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 3
Total size = 899 | train size = 675 | val size = 224 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=895, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,222,401
Moving model to cuda
Epoch 0
Train function
Loss = 1.8768e-02, PNorm = 52.5058, GNorm = 7.4981, lr_0 = 4.8077e-04
Validation rmse logS = 1.338717
Validation R2 logS = 0.579685
Epoch 1
Train function
Loss = 6.3896e-03, PNorm = 52.5742, GNorm = 1.7541, lr_0 = 8.6154e-04
Validation rmse logS = 1.441065
Validation R2 logS = 0.512960
Epoch 2
Train function
Loss = 6.9371e-03, PNorm = 52.6620, GNorm = 8.5047, lr_0 = 9.8921e-04
Loss = 5.2419e-03, PNorm = 52.7290, GNorm = 0.6349, lr_0 = 9.7150e-04
Loss = 1.1048e-02, PNorm = 52.7347, GNorm = 1.1419, lr_0 = 9.6974e-04
Validation rmse logS = 0.963418
Validation R2 logS = 0.782316
Epoch 3
Train function
Loss = 4.1657e-03, PNorm = 52.7946, GNorm = 1.8950, lr_0 = 9.5237e-04
Validation rmse logS = 0.773843
Validation R2 logS = 0.859556
Epoch 4
Train function
Loss = 3.0074e-03, PNorm = 52.8345, GNorm = 0.8549, lr_0 = 9.3531e-04
Validation rmse logS = 0.756969
Validation R2 logS = 0.865614
Epoch 5
Train function
Loss = 2.3233e-03, PNorm = 52.8709, GNorm = 1.9350, lr_0 = 9.1690e-04
Loss = 2.3585e-03, PNorm = 52.9056, GNorm = 2.5083, lr_0 = 9.0048e-04
Validation rmse logS = 0.806949
Validation R2 logS = 0.847282
Epoch 6
Train function
Loss = 2.3716e-03, PNorm = 52.9336, GNorm = 2.5207, lr_0 = 8.8435e-04
Validation rmse logS = 0.681185
Validation R2 logS = 0.891175
Epoch 7
Train function
Loss = 1.6862e-03, PNorm = 52.9753, GNorm = 1.6186, lr_0 = 8.6694e-04
Validation rmse logS = 0.763966
Validation R2 logS = 0.863118
Epoch 8
Train function
Loss = 1.8495e-03, PNorm = 53.0062, GNorm = 1.8651, lr_0 = 8.5141e-04
Loss = 1.8259e-03, PNorm = 53.0424, GNorm = 0.7746, lr_0 = 8.3617e-04
Validation rmse logS = 0.693133
Validation R2 logS = 0.887324
Epoch 9
Train function
Loss = 1.8279e-03, PNorm = 53.0835, GNorm = 2.6449, lr_0 = 8.1971e-04
Validation rmse logS = 0.670879
Validation R2 logS = 0.894443
Epoch 10
Train function
Loss = 1.5251e-03, PNorm = 53.1173, GNorm = 1.0988, lr_0 = 8.0502e-04
Validation rmse logS = 0.637174
Validation R2 logS = 0.904783
Epoch 11
Train function
Loss = 1.2554e-03, PNorm = 53.1490, GNorm = 0.4230, lr_0 = 7.8918e-04
Loss = 1.2389e-03, PNorm = 53.1834, GNorm = 1.4330, lr_0 = 7.7504e-04
Validation rmse logS = 0.672625
Validation R2 logS = 0.893893
Epoch 12
Train function
Loss = 1.3412e-03, PNorm = 53.2119, GNorm = 1.0773, lr_0 = 7.6116e-04
Validation rmse logS = 0.633148
Validation R2 logS = 0.905983
Epoch 13
Train function
Loss = 1.0764e-03, PNorm = 53.2485, GNorm = 1.2473, lr_0 = 7.4618e-04
Validation rmse logS = 0.698348
Validation R2 logS = 0.885622
Epoch 14
Train function
Loss = 1.8717e-03, PNorm = 53.2785, GNorm = 2.3170, lr_0 = 7.3281e-04
Loss = 1.0683e-03, PNorm = 53.3147, GNorm = 0.5492, lr_0 = 7.1969e-04
Validation rmse logS = 0.623303
Validation R2 logS = 0.908884
Epoch 15
Train function
Loss = 8.2892e-04, PNorm = 53.3492, GNorm = 0.5080, lr_0 = 7.0552e-04
Validation rmse logS = 0.614379
Validation R2 logS = 0.911474
Epoch 16
Train function
Loss = 7.2211e-04, PNorm = 53.3809, GNorm = 1.3748, lr_0 = 6.9288e-04
Validation rmse logS = 0.665477
Validation R2 logS = 0.896136
Epoch 17
Train function
Loss = 1.1380e-03, PNorm = 53.4209, GNorm = 1.3185, lr_0 = 6.7924e-04
Loss = 7.7182e-04, PNorm = 53.4526, GNorm = 0.7101, lr_0 = 6.6708e-04
Validation rmse logS = 0.610848
Validation R2 logS = 0.912489
Epoch 18
Train function
Loss = 8.1311e-04, PNorm = 53.4915, GNorm = 0.3049, lr_0 = 6.5513e-04
Validation rmse logS = 0.610998
Validation R2 logS = 0.912446
Epoch 19
Train function
Loss = 6.4087e-04, PNorm = 53.5300, GNorm = 0.5002, lr_0 = 6.4223e-04
Loss = 6.8897e-04, PNorm = 53.5620, GNorm = 1.0346, lr_0 = 6.3073e-04
Validation rmse logS = 0.597588
Validation R2 logS = 0.916247
Epoch 20
Train function
Loss = 6.2418e-04, PNorm = 53.5981, GNorm = 0.4504, lr_0 = 6.1943e-04
Validation rmse logS = 0.608510
Validation R2 logS = 0.913157
Epoch 21
Train function
Loss = 6.0981e-04, PNorm = 53.6342, GNorm = 0.5916, lr_0 = 6.0724e-04
Validation rmse logS = 0.605027
Validation R2 logS = 0.914149
Epoch 22
Train function
Loss = 4.4559e-04, PNorm = 53.6694, GNorm = 1.0144, lr_0 = 5.9636e-04
Loss = 6.0358e-04, PNorm = 53.6969, GNorm = 0.4919, lr_0 = 5.8568e-04
Loss = 1.2790e-03, PNorm = 53.7006, GNorm = 0.2960, lr_0 = 5.8462e-04
Validation rmse logS = 0.620389
Validation R2 logS = 0.909734
Epoch 23
Train function
Loss = 5.3601e-04, PNorm = 53.7364, GNorm = 0.5433, lr_0 = 5.7415e-04
Validation rmse logS = 0.619546
Validation R2 logS = 0.909979
Epoch 24
Train function
Loss = 5.1690e-04, PNorm = 53.7721, GNorm = 0.4127, lr_0 = 5.6387e-04
Validation rmse logS = 0.600951
Validation R2 logS = 0.915302
Epoch 25
Train function
Loss = 5.2389e-04, PNorm = 53.8091, GNorm = 0.6359, lr_0 = 5.5277e-04
Loss = 4.6640e-04, PNorm = 53.8394, GNorm = 0.7454, lr_0 = 5.4287e-04
Validation rmse logS = 0.583872
Validation R2 logS = 0.920047
Epoch 26
Train function
Loss = 4.1981e-04, PNorm = 53.8737, GNorm = 0.3518, lr_0 = 5.3314e-04
Validation rmse logS = 0.598306
Validation R2 logS = 0.916046
Epoch 27
Train function
Loss = 3.6411e-04, PNorm = 53.9080, GNorm = 0.5083, lr_0 = 5.2265e-04
Validation rmse logS = 0.602055
Validation R2 logS = 0.914990
Epoch 28
Train function
Loss = 5.0773e-04, PNorm = 53.9363, GNorm = 0.4506, lr_0 = 5.1329e-04
Loss = 3.3966e-04, PNorm = 53.9646, GNorm = 0.6203, lr_0 = 5.0409e-04
Validation rmse logS = 0.624630
Validation R2 logS = 0.908496
Epoch 29
Train function
Loss = 4.2141e-04, PNorm = 53.9949, GNorm = 0.3483, lr_0 = 4.9417e-04
Validation rmse logS = 0.596028
Validation R2 logS = 0.916683
Epoch 30
Train function
Loss = 3.0761e-04, PNorm = 54.0245, GNorm = 0.5725, lr_0 = 4.8532e-04
Validation rmse logS = 0.597743
Validation R2 logS = 0.916203
Epoch 31
Train function
Loss = 4.1330e-04, PNorm = 54.0537, GNorm = 0.7775, lr_0 = 4.7577e-04
Loss = 3.7592e-04, PNorm = 54.0810, GNorm = 0.4356, lr_0 = 4.6725e-04
Validation rmse logS = 0.585085
Validation R2 logS = 0.919715
Epoch 32
Train function
Loss = 4.0905e-04, PNorm = 54.1040, GNorm = 0.3563, lr_0 = 4.5888e-04
Validation rmse logS = 0.596137
Validation R2 logS = 0.916653
Epoch 33
Train function
Loss = 2.9784e-04, PNorm = 54.1321, GNorm = 0.4578, lr_0 = 4.4984e-04
Validation rmse logS = 0.583631
Validation R2 logS = 0.920113
Epoch 34
Train function
Loss = 4.3116e-04, PNorm = 54.1585, GNorm = 0.4980, lr_0 = 4.4179e-04
Loss = 2.7209e-04, PNorm = 54.1801, GNorm = 0.2792, lr_0 = 4.3387e-04
Validation rmse logS = 0.610616
Validation R2 logS = 0.912555
Epoch 35
Train function
Loss = 2.6336e-04, PNorm = 54.2066, GNorm = 0.5129, lr_0 = 4.2533e-04
Validation rmse logS = 0.586067
Validation R2 logS = 0.919445
Epoch 36
Train function
Loss = 2.1834e-04, PNorm = 54.2282, GNorm = 0.5339, lr_0 = 4.1771e-04
Validation rmse logS = 0.581764
Validation R2 logS = 0.920624
Epoch 37
Train function
Loss = 2.1670e-04, PNorm = 54.2501, GNorm = 0.4766, lr_0 = 4.0949e-04
Loss = 2.4927e-04, PNorm = 54.2707, GNorm = 0.3007, lr_0 = 4.0216e-04
Validation rmse logS = 0.599871
Validation R2 logS = 0.915606
Epoch 38
Train function
Loss = 2.0581e-04, PNorm = 54.2917, GNorm = 0.2268, lr_0 = 3.9495e-04
Validation rmse logS = 0.587846
Validation R2 logS = 0.918955
Epoch 39
Train function
Loss = 2.3362e-04, PNorm = 54.3116, GNorm = 0.1927, lr_0 = 3.8718e-04
Loss = 1.8821e-04, PNorm = 54.3320, GNorm = 0.3744, lr_0 = 3.8024e-04
Validation rmse logS = 0.590300
Validation R2 logS = 0.918277
Epoch 40
Train function
Loss = 1.6042e-04, PNorm = 54.3472, GNorm = 0.1978, lr_0 = 3.7343e-04
Validation rmse logS = 0.604262
Validation R2 logS = 0.914366
Epoch 41
Train function
Loss = 1.9991e-04, PNorm = 54.3649, GNorm = 1.0222, lr_0 = 3.6608e-04
Validation rmse logS = 0.589648
Validation R2 logS = 0.918458
Epoch 42
Train function
Loss = 2.2837e-04, PNorm = 54.3831, GNorm = 0.6575, lr_0 = 3.5953e-04
Loss = 2.0753e-04, PNorm = 54.3996, GNorm = 0.4294, lr_0 = 3.5309e-04
Loss = 2.3329e-04, PNorm = 54.4015, GNorm = 0.4212, lr_0 = 3.5245e-04
Validation rmse logS = 0.594412
Validation R2 logS = 0.917135
Epoch 43
Train function
Loss = 1.7728e-04, PNorm = 54.4159, GNorm = 0.5267, lr_0 = 3.4614e-04
Validation rmse logS = 0.594656
Validation R2 logS = 0.917067
Epoch 44
Train function
Loss = 1.7474e-04, PNorm = 54.4317, GNorm = 0.3945, lr_0 = 3.3994e-04
Validation rmse logS = 0.596301
Validation R2 logS = 0.916607
Epoch 45
Train function
Loss = 1.5817e-04, PNorm = 54.4474, GNorm = 0.2007, lr_0 = 3.3324e-04
Loss = 1.4171e-04, PNorm = 54.4606, GNorm = 0.3358, lr_0 = 3.2728e-04
Validation rmse logS = 0.586973
Validation R2 logS = 0.919196
Epoch 46
Train function
Loss = 1.3743e-04, PNorm = 54.4734, GNorm = 0.3061, lr_0 = 3.2141e-04
Validation rmse logS = 0.591118
Validation R2 logS = 0.918051
Epoch 47
Train function
Loss = 1.4586e-04, PNorm = 54.4863, GNorm = 0.1253, lr_0 = 3.1509e-04
Validation rmse logS = 0.597844
Validation R2 logS = 0.916175
Epoch 48
Train function
Loss = 1.2764e-04, PNorm = 54.4999, GNorm = 0.5644, lr_0 = 3.0944e-04
Loss = 1.4541e-04, PNorm = 54.5115, GNorm = 0.4195, lr_0 = 3.0390e-04
Validation rmse logS = 0.597024
Validation R2 logS = 0.916405
Epoch 49
Train function
Loss = 1.2850e-04, PNorm = 54.5237, GNorm = 0.2493, lr_0 = 2.9792e-04
Validation rmse logS = 0.588204
Validation R2 logS = 0.918857
Epoch 50
Train function
Loss = 1.2873e-04, PNorm = 54.5360, GNorm = 0.4360, lr_0 = 2.9258e-04
Validation rmse logS = 0.581667
Validation R2 logS = 0.920650
Epoch 51
Train function
Loss = 1.6420e-04, PNorm = 54.5476, GNorm = 0.3637, lr_0 = 2.8682e-04
Loss = 1.2220e-04, PNorm = 54.5602, GNorm = 0.2734, lr_0 = 2.8169e-04
Validation rmse logS = 0.596611
Validation R2 logS = 0.916521
Epoch 52
Train function
Loss = 1.5387e-04, PNorm = 54.5700, GNorm = 0.4110, lr_0 = 2.7664e-04
Validation rmse logS = 0.588943
Validation R2 logS = 0.918653
Epoch 53
Train function
Loss = 1.2938e-04, PNorm = 54.5821, GNorm = 0.1971, lr_0 = 2.7119e-04
Validation rmse logS = 0.597633
Validation R2 logS = 0.916234
Epoch 54
Train function
Loss = 7.6552e-05, PNorm = 54.5920, GNorm = 0.3507, lr_0 = 2.6634e-04
Loss = 1.0960e-04, PNorm = 54.6018, GNorm = 0.2887, lr_0 = 2.6157e-04
Validation rmse logS = 0.588863
Validation R2 logS = 0.918675
Epoch 55
Train function
Loss = 1.1473e-04, PNorm = 54.6119, GNorm = 0.2181, lr_0 = 2.5642e-04
Validation rmse logS = 0.595144
Validation R2 logS = 0.916931
Epoch 56
Train function
Loss = 8.9808e-05, PNorm = 54.6190, GNorm = 0.1816, lr_0 = 2.5183e-04
Validation rmse logS = 0.593440
Validation R2 logS = 0.917406
Epoch 57
Train function
Loss = 6.2927e-05, PNorm = 54.6273, GNorm = 0.4336, lr_0 = 2.4687e-04
Loss = 9.5492e-05, PNorm = 54.6350, GNorm = 0.2854, lr_0 = 2.4245e-04
Validation rmse logS = 0.585910
Validation R2 logS = 0.919488
Epoch 58
Train function
Loss = 1.0995e-04, PNorm = 54.6430, GNorm = 0.5341, lr_0 = 2.3810e-04
Validation rmse logS = 0.597666
Validation R2 logS = 0.916225
Epoch 59
Train function
Loss = 7.0717e-05, PNorm = 54.6522, GNorm = 0.0951, lr_0 = 2.3342e-04
Loss = 1.0182e-04, PNorm = 54.6599, GNorm = 0.5376, lr_0 = 2.2924e-04
Validation rmse logS = 0.588640
Validation R2 logS = 0.918736
Epoch 60
Train function
Loss = 9.2523e-05, PNorm = 54.6671, GNorm = 0.2623, lr_0 = 2.2513e-04
Validation rmse logS = 0.589141
Validation R2 logS = 0.918598
Epoch 61
Train function
Loss = 7.2333e-05, PNorm = 54.6747, GNorm = 0.1517, lr_0 = 2.2070e-04
Validation rmse logS = 0.594987
Validation R2 logS = 0.916974
Epoch 62
Train function
Loss = 6.8701e-05, PNorm = 54.6818, GNorm = 0.1748, lr_0 = 2.1675e-04
Loss = 7.8042e-05, PNorm = 54.6885, GNorm = 0.1445, lr_0 = 2.1286e-04
Loss = 9.1276e-05, PNorm = 54.6891, GNorm = 0.2969, lr_0 = 2.1248e-04
Validation rmse logS = 0.594336
Validation R2 logS = 0.917156
Epoch 63
Train function
Loss = 6.6560e-05, PNorm = 54.6946, GNorm = 0.3446, lr_0 = 2.0867e-04
Validation rmse logS = 0.589579
Validation R2 logS = 0.918477
Epoch 64
Train function
Loss = 9.5949e-05, PNorm = 54.7005, GNorm = 0.4416, lr_0 = 2.0494e-04
Validation rmse logS = 0.597015
Validation R2 logS = 0.916407
Epoch 65
Train function
Loss = 9.9956e-05, PNorm = 54.7073, GNorm = 0.1953, lr_0 = 2.0090e-04
Loss = 8.4905e-05, PNorm = 54.7137, GNorm = 0.3777, lr_0 = 1.9730e-04
Validation rmse logS = 0.600349
Validation R2 logS = 0.915471
Epoch 66
Train function
Loss = 8.4124e-05, PNorm = 54.7194, GNorm = 0.4208, lr_0 = 1.9377e-04
Validation rmse logS = 0.593761
Validation R2 logS = 0.917316
Epoch 67
Train function
Loss = 8.3060e-05, PNorm = 54.7257, GNorm = 0.1635, lr_0 = 1.8995e-04
Validation rmse logS = 0.590793
Validation R2 logS = 0.918141
Epoch 68
Train function
Loss = 8.0320e-05, PNorm = 54.7319, GNorm = 0.2311, lr_0 = 1.8655e-04
Loss = 6.3983e-05, PNorm = 54.7376, GNorm = 0.3172, lr_0 = 1.8321e-04
Validation rmse logS = 0.592328
Validation R2 logS = 0.917715
Epoch 69
Train function
Loss = 7.0536e-05, PNorm = 54.7428, GNorm = 0.2705, lr_0 = 1.7960e-04
Validation rmse logS = 0.594663
Validation R2 logS = 0.917065
Epoch 70
Train function
Loss = 6.3669e-05, PNorm = 54.7479, GNorm = 0.2758, lr_0 = 1.7639e-04
Validation rmse logS = 0.593783
Validation R2 logS = 0.917310
Epoch 71
Train function
Loss = 2.7022e-05, PNorm = 54.7527, GNorm = 0.0860, lr_0 = 1.7292e-04
Loss = 6.8427e-05, PNorm = 54.7567, GNorm = 0.1986, lr_0 = 1.6982e-04
Validation rmse logS = 0.593829
Validation R2 logS = 0.917297
Epoch 72
Train function
Loss = 6.8067e-05, PNorm = 54.7622, GNorm = 0.3800, lr_0 = 1.6678e-04
Validation rmse logS = 0.599598
Validation R2 logS = 0.915682
Epoch 73
Train function
Loss = 5.7123e-05, PNorm = 54.7659, GNorm = 0.2661, lr_0 = 1.6349e-04
Validation rmse logS = 0.595573
Validation R2 logS = 0.916811
Epoch 74
Train function
Loss = 2.6870e-05, PNorm = 54.7705, GNorm = 0.1051, lr_0 = 1.6057e-04
Loss = 5.2231e-05, PNorm = 54.7752, GNorm = 0.1382, lr_0 = 1.5769e-04
Validation rmse logS = 0.592396
Validation R2 logS = 0.917696
Epoch 75
Train function
Loss = 4.3464e-05, PNorm = 54.7795, GNorm = 0.2354, lr_0 = 1.5459e-04
Validation rmse logS = 0.592703
Validation R2 logS = 0.917611
Epoch 76
Train function
Loss = 3.7777e-05, PNorm = 54.7835, GNorm = 0.0995, lr_0 = 1.5182e-04
Validation rmse logS = 0.595269
Validation R2 logS = 0.916896
Epoch 77
Train function
Loss = 5.2054e-05, PNorm = 54.7877, GNorm = 0.1598, lr_0 = 1.4883e-04
Loss = 4.3819e-05, PNorm = 54.7908, GNorm = 0.1269, lr_0 = 1.4616e-04
Validation rmse logS = 0.597017
Validation R2 logS = 0.916407
Epoch 78
Train function
Loss = 5.0955e-05, PNorm = 54.7950, GNorm = 0.1433, lr_0 = 1.4354e-04
Validation rmse logS = 0.595244
Validation R2 logS = 0.916903
Epoch 79
Train function
Loss = 2.8566e-05, PNorm = 54.7995, GNorm = 0.0973, lr_0 = 1.4072e-04
Loss = 5.4912e-05, PNorm = 54.8036, GNorm = 0.2620, lr_0 = 1.3820e-04
Validation rmse logS = 0.595283
Validation R2 logS = 0.916892
Epoch 80
Train function
Loss = 4.0813e-05, PNorm = 54.8064, GNorm = 0.1354, lr_0 = 1.3572e-04
Validation rmse logS = 0.595227
Validation R2 logS = 0.916907
Epoch 81
Train function
Loss = 3.9927e-05, PNorm = 54.8100, GNorm = 0.1069, lr_0 = 1.3305e-04
Validation rmse logS = 0.599489
Validation R2 logS = 0.915713
Epoch 82
Train function
Loss = 4.9948e-05, PNorm = 54.8133, GNorm = 0.2242, lr_0 = 1.3067e-04
Loss = 3.5365e-05, PNorm = 54.8162, GNorm = 0.1179, lr_0 = 1.2833e-04
Loss = 1.6316e-04, PNorm = 54.8165, GNorm = 0.2765, lr_0 = 1.2810e-04
Validation rmse logS = 0.596620
Validation R2 logS = 0.916518
Epoch 83
Train function
Loss = 3.9495e-05, PNorm = 54.8197, GNorm = 0.2595, lr_0 = 1.2580e-04
Validation rmse logS = 0.597282
Validation R2 logS = 0.916332
Epoch 84
Train function
Loss = 4.7413e-05, PNorm = 54.8229, GNorm = 0.1606, lr_0 = 1.2355e-04
Validation rmse logS = 0.598408
Validation R2 logS = 0.916017
Epoch 85
Train function
Loss = 2.7314e-05, PNorm = 54.8260, GNorm = 0.1126, lr_0 = 1.2112e-04
Loss = 4.3509e-05, PNorm = 54.8289, GNorm = 0.3292, lr_0 = 1.1895e-04
Validation rmse logS = 0.597122
Validation R2 logS = 0.916377
Epoch 86
Train function
Loss = 4.0056e-05, PNorm = 54.8321, GNorm = 0.1531, lr_0 = 1.1682e-04
Validation rmse logS = 0.597498
Validation R2 logS = 0.916272
Epoch 87
Train function
Loss = 3.1954e-05, PNorm = 54.8351, GNorm = 0.3173, lr_0 = 1.1452e-04
Validation rmse logS = 0.598295
Validation R2 logS = 0.916049
Epoch 88
Train function
Loss = 4.7535e-05, PNorm = 54.8376, GNorm = 0.1006, lr_0 = 1.1247e-04
Loss = 4.2969e-05, PNorm = 54.8401, GNorm = 0.1374, lr_0 = 1.1045e-04
Validation rmse logS = 0.599478
Validation R2 logS = 0.915716
Epoch 89
Train function
Loss = 3.4513e-05, PNorm = 54.8426, GNorm = 0.1065, lr_0 = 1.0828e-04
Validation rmse logS = 0.596660
Validation R2 logS = 0.916507
Epoch 90
Train function
Loss = 3.9554e-05, PNorm = 54.8454, GNorm = 0.1625, lr_0 = 1.0634e-04
Validation rmse logS = 0.596191
Validation R2 logS = 0.916638
Epoch 91
Train function
Loss = 2.9692e-05, PNorm = 54.8480, GNorm = 0.1201, lr_0 = 1.0424e-04
Loss = 3.6833e-05, PNorm = 54.8504, GNorm = 0.0785, lr_0 = 1.0238e-04
Validation rmse logS = 0.597445
Validation R2 logS = 0.916287
Epoch 92
Train function
Loss = 2.4976e-05, PNorm = 54.8524, GNorm = 0.1729, lr_0 = 1.0054e-04
Validation rmse logS = 0.598632
Validation R2 logS = 0.915954
Epoch 93
Train function
Loss = 2.9856e-05, PNorm = 54.8554, GNorm = 0.0856, lr_0 = 1.0000e-04
Validation rmse logS = 0.597973
Validation R2 logS = 0.916139
Epoch 94
Train function
Loss = 4.3471e-05, PNorm = 54.8576, GNorm = 0.1807, lr_0 = 1.0000e-04
Loss = 3.8424e-05, PNorm = 54.8600, GNorm = 0.1407, lr_0 = 1.0000e-04
Validation rmse logS = 0.598883
Validation R2 logS = 0.915883
Epoch 95
Train function
Loss = 3.0133e-05, PNorm = 54.8627, GNorm = 0.3097, lr_0 = 1.0000e-04
Validation rmse logS = 0.600108
Validation R2 logS = 0.915539
Epoch 96
Train function
Loss = 3.4666e-05, PNorm = 54.8652, GNorm = 0.1154, lr_0 = 1.0000e-04
Validation rmse logS = 0.598009
Validation R2 logS = 0.916129
Epoch 97
Train function
Loss = 3.0403e-05, PNorm = 54.8674, GNorm = 0.1152, lr_0 = 1.0000e-04
Loss = 3.1770e-05, PNorm = 54.8697, GNorm = 0.1900, lr_0 = 1.0000e-04
Validation rmse logS = 0.599840
Validation R2 logS = 0.915615
Epoch 98
Train function
Loss = 3.8508e-05, PNorm = 54.8720, GNorm = 0.1634, lr_0 = 1.0000e-04
Validation rmse logS = 0.600607
Validation R2 logS = 0.915398
Epoch 99
Train function
Loss = 2.7662e-05, PNorm = 54.8745, GNorm = 0.2769, lr_0 = 1.0000e-04
Loss = 4.1488e-05, PNorm = 54.8770, GNorm = 0.1683, lr_0 = 1.0000e-04
Validation rmse logS = 0.597770
Validation R2 logS = 0.916196
Model 0 best validation rmse = 0.581667 on epoch 50
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.669955
Model 0 test R2 logS = 0.882662
Ensemble test rmse  logS= 0.669955
Ensemble test R2  logS= 0.882662
4-fold cross validation
	Seed 0 ==> test rmse = 0.630592
	Seed 0 ==> test R2 = 0.896045
	Seed 1 ==> test rmse = 0.624113
	Seed 1 ==> test R2 = 0.898170
	Seed 2 ==> test rmse = 0.621727
	Seed 2 ==> test R2 = 0.898948
	Seed 3 ==> test rmse = 0.669955
	Seed 3 ==> test R2 = 0.882662
Overall val rmse logS= 0.564212 +/- 0.011021
Overall val R2 logS = 0.926478 +/- 0.004128
Overall test rmse logS = 0.636597 +/- 0.019531
Overall test R2 logS = 0.893956 +/- 0.006607
Elapsed time = 0:13:59
