Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_316/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/./data/raw/baselines/dmpnn_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_316/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_316/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 11,710 | train size = 8,782 | val size = 2,928 | test size = 2,067
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=165, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,512,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.7491e-02, PNorm = 55.5559, GNorm = 2.4764, lr_0 = 1.2829e-04
Loss = 1.6326e-02, PNorm = 55.5614, GNorm = 7.1719, lr_0 = 1.5400e-04
Loss = 1.1754e-02, PNorm = 55.5702, GNorm = 3.9471, lr_0 = 1.7971e-04
Loss = 7.8849e-03, PNorm = 55.5827, GNorm = 5.4332, lr_0 = 2.0543e-04
Loss = 8.1736e-03, PNorm = 55.5962, GNorm = 5.5384, lr_0 = 2.3114e-04
Loss = 8.2410e-03, PNorm = 55.6099, GNorm = 2.4050, lr_0 = 2.5686e-04
Loss = 6.1990e-03, PNorm = 55.6285, GNorm = 3.9170, lr_0 = 2.8257e-04
Loss = 5.7455e-03, PNorm = 55.6455, GNorm = 3.4454, lr_0 = 3.0829e-04
Loss = 6.3113e-03, PNorm = 55.6626, GNorm = 2.0466, lr_0 = 3.3400e-04
Loss = 4.8091e-03, PNorm = 55.6850, GNorm = 0.8709, lr_0 = 3.5971e-04
Loss = 4.3743e-03, PNorm = 55.7057, GNorm = 4.8761, lr_0 = 3.8543e-04
Loss = 5.1034e-03, PNorm = 55.7288, GNorm = 2.9512, lr_0 = 4.1114e-04
Loss = 4.2460e-03, PNorm = 55.7547, GNorm = 5.4385, lr_0 = 4.3686e-04
Loss = 4.8942e-03, PNorm = 55.7854, GNorm = 5.5397, lr_0 = 4.6257e-04
Loss = 5.4368e-03, PNorm = 55.8154, GNorm = 1.1137, lr_0 = 4.8829e-04
Loss = 4.1473e-03, PNorm = 55.8559, GNorm = 4.8636, lr_0 = 5.1400e-04
Loss = 3.7497e-03, PNorm = 55.8995, GNorm = 0.9910, lr_0 = 5.3971e-04
Validation rmse logP = 0.770504
Validation R2 logP = 0.825764
Epoch 1
Train function
Loss = 3.2565e-03, PNorm = 55.9424, GNorm = 4.2843, lr_0 = 5.6800e-04
Loss = 3.1402e-03, PNorm = 55.9850, GNorm = 2.0326, lr_0 = 5.9371e-04
Loss = 3.4586e-03, PNorm = 56.0276, GNorm = 1.9191, lr_0 = 6.1943e-04
Loss = 3.4138e-03, PNorm = 56.0736, GNorm = 1.6212, lr_0 = 6.4514e-04
Loss = 3.6558e-03, PNorm = 56.1115, GNorm = 3.9146, lr_0 = 6.7086e-04
Loss = 4.2530e-03, PNorm = 56.1664, GNorm = 0.7506, lr_0 = 6.9657e-04
Loss = 3.1912e-03, PNorm = 56.2238, GNorm = 1.5641, lr_0 = 7.2229e-04
Loss = 3.3245e-03, PNorm = 56.2825, GNorm = 2.2361, lr_0 = 7.4800e-04
Loss = 2.8018e-03, PNorm = 56.3251, GNorm = 1.5032, lr_0 = 7.7371e-04
Loss = 2.7792e-03, PNorm = 56.3728, GNorm = 1.8132, lr_0 = 7.9943e-04
Loss = 2.8041e-03, PNorm = 56.4293, GNorm = 0.8169, lr_0 = 8.2514e-04
Loss = 4.1351e-03, PNorm = 56.4790, GNorm = 2.4186, lr_0 = 8.5086e-04
Loss = 3.1796e-03, PNorm = 56.5510, GNorm = 4.3683, lr_0 = 8.7657e-04
Loss = 3.2674e-03, PNorm = 56.6061, GNorm = 0.5767, lr_0 = 9.0229e-04
Loss = 3.4661e-03, PNorm = 56.6715, GNorm = 2.9863, lr_0 = 9.2800e-04
Loss = 3.1649e-03, PNorm = 56.7180, GNorm = 2.1936, lr_0 = 9.5371e-04
Loss = 2.9141e-03, PNorm = 56.7797, GNorm = 1.2704, lr_0 = 9.7943e-04
Loss = 2.9201e-03, PNorm = 56.8540, GNorm = 1.6636, lr_0 = 9.9973e-04
Validation rmse logP = 0.741330
Validation R2 logP = 0.838709
Epoch 2
Train function
Loss = 3.2830e-03, PNorm = 56.9222, GNorm = 3.3426, lr_0 = 9.9839e-04
Loss = 2.7313e-03, PNorm = 57.0193, GNorm = 2.9656, lr_0 = 9.9705e-04
Loss = 2.3155e-03, PNorm = 57.0833, GNorm = 1.6780, lr_0 = 9.9571e-04
Loss = 2.2578e-03, PNorm = 57.1416, GNorm = 0.7051, lr_0 = 9.9438e-04
Loss = 2.4273e-03, PNorm = 57.2081, GNorm = 1.6603, lr_0 = 9.9304e-04
Loss = 2.3197e-03, PNorm = 57.2682, GNorm = 1.1777, lr_0 = 9.9171e-04
Loss = 2.1915e-03, PNorm = 57.3124, GNorm = 0.4805, lr_0 = 9.9038e-04
Loss = 2.4551e-03, PNorm = 57.3626, GNorm = 3.6821, lr_0 = 9.8905e-04
Loss = 2.2755e-03, PNorm = 57.4169, GNorm = 1.3999, lr_0 = 9.8772e-04
Loss = 2.4887e-03, PNorm = 57.4725, GNorm = 1.7477, lr_0 = 9.8640e-04
Loss = 2.6287e-03, PNorm = 57.5172, GNorm = 1.1858, lr_0 = 9.8508e-04
Loss = 2.1383e-03, PNorm = 57.5832, GNorm = 0.7934, lr_0 = 9.8375e-04
Loss = 2.1913e-03, PNorm = 57.6379, GNorm = 1.3162, lr_0 = 9.8243e-04
Loss = 2.0838e-03, PNorm = 57.6960, GNorm = 0.8066, lr_0 = 9.8112e-04
Loss = 2.4050e-03, PNorm = 57.7369, GNorm = 2.1943, lr_0 = 9.7980e-04
Loss = 2.3198e-03, PNorm = 57.7995, GNorm = 1.4889, lr_0 = 9.7848e-04
Loss = 2.2701e-03, PNorm = 57.8499, GNorm = 1.9499, lr_0 = 9.7717e-04
Validation rmse logP = 0.642543
Validation R2 logP = 0.878831
Epoch 3
Train function
Loss = 2.5820e-03, PNorm = 57.9120, GNorm = 0.5266, lr_0 = 9.7573e-04
Loss = 2.8468e-03, PNorm = 57.9630, GNorm = 1.2459, lr_0 = 9.7442e-04
Loss = 2.1640e-03, PNorm = 58.0377, GNorm = 0.6572, lr_0 = 9.7311e-04
Loss = 1.8255e-03, PNorm = 58.0981, GNorm = 0.6288, lr_0 = 9.7181e-04
Loss = 1.8833e-03, PNorm = 58.1324, GNorm = 1.0804, lr_0 = 9.7050e-04
Loss = 1.7898e-03, PNorm = 58.1690, GNorm = 0.5815, lr_0 = 9.6920e-04
Loss = 2.9755e-03, PNorm = 58.2262, GNorm = 2.2559, lr_0 = 9.6790e-04
Loss = 2.6408e-03, PNorm = 58.3001, GNorm = 0.6725, lr_0 = 9.6660e-04
Loss = 2.3459e-03, PNorm = 58.3532, GNorm = 0.7261, lr_0 = 9.6531e-04
Loss = 1.8735e-03, PNorm = 58.4268, GNorm = 1.4619, lr_0 = 9.6401e-04
Loss = 1.5595e-03, PNorm = 58.4824, GNorm = 0.4411, lr_0 = 9.6272e-04
Loss = 1.7514e-03, PNorm = 58.5505, GNorm = 0.5643, lr_0 = 9.6143e-04
Loss = 1.8049e-03, PNorm = 58.5959, GNorm = 0.4939, lr_0 = 9.6014e-04
Loss = 1.6082e-03, PNorm = 58.6395, GNorm = 0.9760, lr_0 = 9.5885e-04
Loss = 1.9101e-03, PNorm = 58.6822, GNorm = 0.7927, lr_0 = 9.5756e-04
Loss = 1.5708e-03, PNorm = 58.7435, GNorm = 0.3730, lr_0 = 9.5628e-04
Loss = 2.1648e-03, PNorm = 58.7792, GNorm = 0.5615, lr_0 = 9.5499e-04
Loss = 1.8282e-03, PNorm = 58.8332, GNorm = 0.4792, lr_0 = 9.5371e-04
Validation rmse logP = 0.622584
Validation R2 logP = 0.886242
Epoch 4
Train function
Loss = 1.8758e-03, PNorm = 58.8911, GNorm = 1.3162, lr_0 = 9.5243e-04
Loss = 1.5895e-03, PNorm = 58.9434, GNorm = 1.2318, lr_0 = 9.5115e-04
Loss = 1.5017e-03, PNorm = 58.9964, GNorm = 0.4375, lr_0 = 9.4988e-04
Loss = 1.7385e-03, PNorm = 59.0390, GNorm = 0.4378, lr_0 = 9.4860e-04
Loss = 1.6333e-03, PNorm = 59.0863, GNorm = 0.3419, lr_0 = 9.4733e-04
Loss = 1.2978e-03, PNorm = 59.1239, GNorm = 0.3244, lr_0 = 9.4606e-04
Loss = 1.4354e-03, PNorm = 59.1686, GNorm = 0.5122, lr_0 = 9.4479e-04
Loss = 1.5967e-03, PNorm = 59.2224, GNorm = 0.6599, lr_0 = 9.4352e-04
Loss = 1.5362e-03, PNorm = 59.2808, GNorm = 1.0830, lr_0 = 9.4226e-04
Loss = 1.5486e-03, PNorm = 59.3211, GNorm = 0.7088, lr_0 = 9.4099e-04
Loss = 1.3426e-03, PNorm = 59.3608, GNorm = 0.4477, lr_0 = 9.3973e-04
Loss = 1.0886e-03, PNorm = 59.3955, GNorm = 0.6084, lr_0 = 9.3847e-04
Loss = 1.3830e-03, PNorm = 59.4294, GNorm = 0.5677, lr_0 = 9.3721e-04
Loss = 1.7722e-03, PNorm = 59.4710, GNorm = 0.3768, lr_0 = 9.3595e-04
Loss = 1.6234e-03, PNorm = 59.5175, GNorm = 0.9154, lr_0 = 9.3470e-04
Loss = 1.4869e-03, PNorm = 59.5617, GNorm = 0.5332, lr_0 = 9.3344e-04
Loss = 1.2191e-03, PNorm = 59.6119, GNorm = 0.7769, lr_0 = 9.3219e-04
Validation rmse logP = 0.573254
Validation R2 logP = 0.903554
Epoch 5
Train function
Loss = 1.9540e-03, PNorm = 59.6526, GNorm = 0.8687, lr_0 = 9.3094e-04
Loss = 1.4445e-03, PNorm = 59.7313, GNorm = 1.5576, lr_0 = 9.2969e-04
Loss = 1.2412e-03, PNorm = 59.7723, GNorm = 0.7782, lr_0 = 9.2844e-04
Loss = 1.1114e-03, PNorm = 59.8231, GNorm = 1.0846, lr_0 = 9.2720e-04
Loss = 1.4483e-03, PNorm = 59.8812, GNorm = 0.8715, lr_0 = 9.2595e-04
Loss = 1.1575e-03, PNorm = 59.9377, GNorm = 0.3310, lr_0 = 9.2471e-04
Loss = 1.3784e-03, PNorm = 59.9917, GNorm = 0.7670, lr_0 = 9.2347e-04
Loss = 1.0239e-03, PNorm = 60.0282, GNorm = 0.4425, lr_0 = 9.2223e-04
Loss = 1.4255e-03, PNorm = 60.0727, GNorm = 0.8129, lr_0 = 9.2099e-04
Loss = 1.4694e-03, PNorm = 60.1043, GNorm = 0.4321, lr_0 = 9.1976e-04
Loss = 1.5429e-03, PNorm = 60.1617, GNorm = 2.0165, lr_0 = 9.1852e-04
Loss = 1.6203e-03, PNorm = 60.2139, GNorm = 0.8597, lr_0 = 9.1729e-04
Loss = 1.4360e-03, PNorm = 60.2537, GNorm = 0.5665, lr_0 = 9.1606e-04
Loss = 1.5396e-03, PNorm = 60.3066, GNorm = 1.6858, lr_0 = 9.1483e-04
Loss = 1.4965e-03, PNorm = 60.3644, GNorm = 1.2619, lr_0 = 9.1360e-04
Loss = 1.6700e-03, PNorm = 60.4112, GNorm = 1.0740, lr_0 = 9.1238e-04
Loss = 1.4982e-03, PNorm = 60.4770, GNorm = 0.8128, lr_0 = 9.1115e-04
Loss = 1.5030e-03, PNorm = 60.5265, GNorm = 0.7919, lr_0 = 9.0993e-04
Validation rmse logP = 0.528359
Validation R2 logP = 0.918069
Epoch 6
Train function
Loss = 1.2595e-03, PNorm = 60.5835, GNorm = 0.2958, lr_0 = 9.0859e-04
Loss = 1.0699e-03, PNorm = 60.6313, GNorm = 0.3746, lr_0 = 9.0737e-04
Loss = 9.6932e-04, PNorm = 60.6751, GNorm = 1.1322, lr_0 = 9.0615e-04
Loss = 1.3396e-03, PNorm = 60.7245, GNorm = 1.4883, lr_0 = 9.0494e-04
Loss = 1.1201e-03, PNorm = 60.7850, GNorm = 0.9886, lr_0 = 9.0372e-04
Loss = 1.1973e-03, PNorm = 60.8383, GNorm = 0.7517, lr_0 = 9.0251e-04
Loss = 1.5269e-03, PNorm = 60.8915, GNorm = 1.2422, lr_0 = 9.0130e-04
Loss = 1.2901e-03, PNorm = 60.9345, GNorm = 0.6868, lr_0 = 9.0009e-04
Loss = 1.0841e-03, PNorm = 60.9796, GNorm = 1.1246, lr_0 = 8.9888e-04
Loss = 1.0503e-03, PNorm = 61.0171, GNorm = 0.5708, lr_0 = 8.9768e-04
Loss = 1.2222e-03, PNorm = 61.0675, GNorm = 0.9435, lr_0 = 8.9647e-04
Loss = 1.1874e-03, PNorm = 61.1060, GNorm = 0.5942, lr_0 = 8.9527e-04
Loss = 1.0568e-03, PNorm = 61.1545, GNorm = 1.4295, lr_0 = 8.9407e-04
Loss = 1.3036e-03, PNorm = 61.1884, GNorm = 0.6219, lr_0 = 8.9287e-04
Loss = 8.8345e-04, PNorm = 61.2403, GNorm = 0.5407, lr_0 = 8.9167e-04
Loss = 1.5197e-03, PNorm = 61.2752, GNorm = 0.5107, lr_0 = 8.9047e-04
Loss = 1.2482e-03, PNorm = 61.3256, GNorm = 0.6323, lr_0 = 8.8928e-04
Validation rmse logP = 0.538913
Validation R2 logP = 0.914764
Epoch 7
Train function
Loss = 9.4907e-04, PNorm = 61.3720, GNorm = 0.9060, lr_0 = 8.8809e-04
Loss = 8.7088e-04, PNorm = 61.4123, GNorm = 0.8078, lr_0 = 8.8689e-04
Loss = 1.0255e-03, PNorm = 61.4637, GNorm = 0.3597, lr_0 = 8.8570e-04
Loss = 8.9139e-04, PNorm = 61.5099, GNorm = 0.5047, lr_0 = 8.8452e-04
Loss = 9.3670e-04, PNorm = 61.5465, GNorm = 0.4902, lr_0 = 8.8333e-04
Loss = 1.1643e-03, PNorm = 61.5901, GNorm = 0.7319, lr_0 = 8.8214e-04
Loss = 1.0802e-03, PNorm = 61.6293, GNorm = 0.2993, lr_0 = 8.8096e-04
Loss = 1.0408e-03, PNorm = 61.6800, GNorm = 1.1945, lr_0 = 8.7978e-04
Loss = 1.1797e-03, PNorm = 61.7295, GNorm = 1.1231, lr_0 = 8.7860e-04
Loss = 9.2713e-04, PNorm = 61.7707, GNorm = 0.4637, lr_0 = 8.7742e-04
Loss = 1.4389e-03, PNorm = 61.8281, GNorm = 0.4753, lr_0 = 8.7624e-04
Loss = 1.1896e-03, PNorm = 61.8593, GNorm = 0.4186, lr_0 = 8.7507e-04
Loss = 1.3630e-03, PNorm = 61.8991, GNorm = 1.2800, lr_0 = 8.7389e-04
Loss = 9.9529e-04, PNorm = 61.9518, GNorm = 0.5821, lr_0 = 8.7272e-04
Loss = 1.0260e-03, PNorm = 62.0014, GNorm = 0.8036, lr_0 = 8.7155e-04
Loss = 9.2929e-04, PNorm = 62.0425, GNorm = 0.2322, lr_0 = 8.7038e-04
Loss = 1.0780e-03, PNorm = 62.0807, GNorm = 1.1618, lr_0 = 8.6921e-04
Loss = 1.0712e-03, PNorm = 62.1212, GNorm = 0.3022, lr_0 = 8.6805e-04
Validation rmse logP = 0.514311
Validation R2 logP = 0.922368
Epoch 8
Train function
Loss = 9.3123e-04, PNorm = 62.1604, GNorm = 0.5047, lr_0 = 8.6688e-04
Loss = 9.1580e-04, PNorm = 62.1963, GNorm = 0.4649, lr_0 = 8.6572e-04
Loss = 8.3190e-04, PNorm = 62.2457, GNorm = 1.0664, lr_0 = 8.6456e-04
Loss = 9.2079e-04, PNorm = 62.2874, GNorm = 0.7256, lr_0 = 8.6340e-04
Loss = 9.8112e-04, PNorm = 62.3317, GNorm = 0.7553, lr_0 = 8.6224e-04
Loss = 9.6523e-04, PNorm = 62.3701, GNorm = 0.5821, lr_0 = 8.6108e-04
Loss = 9.8739e-04, PNorm = 62.4081, GNorm = 1.3174, lr_0 = 8.5993e-04
Loss = 1.0270e-03, PNorm = 62.4495, GNorm = 0.2406, lr_0 = 8.5877e-04
Loss = 8.9363e-04, PNorm = 62.4857, GNorm = 0.6320, lr_0 = 8.5762e-04
Loss = 9.5613e-04, PNorm = 62.5354, GNorm = 0.4649, lr_0 = 8.5647e-04
Loss = 7.4065e-04, PNorm = 62.5730, GNorm = 0.3710, lr_0 = 8.5532e-04
Loss = 1.0105e-03, PNorm = 62.6085, GNorm = 0.3955, lr_0 = 8.5417e-04
Loss = 7.9983e-04, PNorm = 62.6414, GNorm = 0.4084, lr_0 = 8.5303e-04
Loss = 6.4142e-04, PNorm = 62.6757, GNorm = 0.4643, lr_0 = 8.5188e-04
Loss = 8.0708e-04, PNorm = 62.7076, GNorm = 0.7645, lr_0 = 8.5074e-04
Loss = 8.3923e-04, PNorm = 62.7469, GNorm = 0.6840, lr_0 = 8.4960e-04
Loss = 8.4837e-04, PNorm = 62.7739, GNorm = 0.5933, lr_0 = 8.4846e-04
Loss = 1.0471e-03, PNorm = 62.8085, GNorm = 1.1313, lr_0 = 8.4732e-04
Loss = 1.6859e-03, PNorm = 62.8112, GNorm = 0.5516, lr_0 = 8.4720e-04
Validation rmse logP = 0.529031
Validation R2 logP = 0.917861
Epoch 9
Train function
Loss = 7.5085e-04, PNorm = 62.8503, GNorm = 0.4922, lr_0 = 8.4607e-04
Loss = 7.2718e-04, PNorm = 62.8881, GNorm = 0.3486, lr_0 = 8.4493e-04
Loss = 6.6177e-04, PNorm = 62.9255, GNorm = 0.3436, lr_0 = 8.4380e-04
Loss = 6.6075e-04, PNorm = 62.9696, GNorm = 0.2635, lr_0 = 8.4267e-04
Loss = 5.9731e-04, PNorm = 63.0133, GNorm = 0.4845, lr_0 = 8.4154e-04
Loss = 7.5689e-04, PNorm = 63.0498, GNorm = 0.4378, lr_0 = 8.4041e-04
Loss = 7.6209e-04, PNorm = 63.0852, GNorm = 0.4734, lr_0 = 8.3928e-04
Loss = 8.5333e-04, PNorm = 63.1318, GNorm = 0.8056, lr_0 = 8.3815e-04
Loss = 7.5368e-04, PNorm = 63.1748, GNorm = 0.5682, lr_0 = 8.3703e-04
Loss = 6.9744e-04, PNorm = 63.2163, GNorm = 0.6493, lr_0 = 8.3591e-04
Loss = 1.0081e-03, PNorm = 63.2447, GNorm = 0.4077, lr_0 = 8.3478e-04
Loss = 7.9539e-04, PNorm = 63.2829, GNorm = 0.6128, lr_0 = 8.3366e-04
Loss = 8.9343e-04, PNorm = 63.3278, GNorm = 0.8013, lr_0 = 8.3255e-04
Loss = 7.6108e-04, PNorm = 63.3631, GNorm = 0.3297, lr_0 = 8.3143e-04
Loss = 9.5000e-04, PNorm = 63.4084, GNorm = 0.2093, lr_0 = 8.3031e-04
Loss = 8.5750e-04, PNorm = 63.4527, GNorm = 0.3168, lr_0 = 8.2920e-04
Loss = 1.0534e-03, PNorm = 63.4961, GNorm = 0.9006, lr_0 = 8.2809e-04
Validation rmse logP = 0.484475
Validation R2 logP = 0.931114
Epoch 10
Train function
Loss = 8.7610e-04, PNorm = 63.5424, GNorm = 0.5600, lr_0 = 8.2698e-04
Loss = 8.1244e-04, PNorm = 63.5782, GNorm = 0.4255, lr_0 = 8.2587e-04
Loss = 6.5255e-04, PNorm = 63.6186, GNorm = 0.4187, lr_0 = 8.2476e-04
Loss = 6.5917e-04, PNorm = 63.6614, GNorm = 0.9551, lr_0 = 8.2365e-04
Loss = 6.8647e-04, PNorm = 63.7012, GNorm = 0.8445, lr_0 = 8.2255e-04
Loss = 7.3107e-04, PNorm = 63.7393, GNorm = 0.3488, lr_0 = 8.2144e-04
Loss = 8.8092e-04, PNorm = 63.7871, GNorm = 0.3994, lr_0 = 8.2034e-04
Loss = 6.9568e-04, PNorm = 63.8390, GNorm = 0.6433, lr_0 = 8.1924e-04
Loss = 6.1878e-04, PNorm = 63.8912, GNorm = 0.4448, lr_0 = 8.1814e-04
Loss = 6.0705e-04, PNorm = 63.9291, GNorm = 0.5421, lr_0 = 8.1704e-04
Loss = 6.4061e-04, PNorm = 63.9591, GNorm = 0.5825, lr_0 = 8.1595e-04
Loss = 4.7631e-04, PNorm = 63.9737, GNorm = 0.3503, lr_0 = 8.1485e-04
Loss = 8.9796e-04, PNorm = 64.0080, GNorm = 0.2511, lr_0 = 8.1376e-04
Loss = 8.0115e-04, PNorm = 64.0474, GNorm = 0.6707, lr_0 = 8.1267e-04
Loss = 6.4290e-04, PNorm = 64.0894, GNorm = 0.2312, lr_0 = 8.1158e-04
Loss = 6.6221e-04, PNorm = 64.1331, GNorm = 0.6743, lr_0 = 8.1049e-04
Loss = 7.8893e-04, PNorm = 64.1680, GNorm = 0.4957, lr_0 = 8.0940e-04
Loss = 7.1764e-04, PNorm = 64.2132, GNorm = 0.4661, lr_0 = 8.0831e-04
Validation rmse logP = 0.513631
Validation R2 logP = 0.922573
Epoch 11
Train function
Loss = 7.6764e-04, PNorm = 64.2482, GNorm = 0.8897, lr_0 = 8.0723e-04
Loss = 7.6563e-04, PNorm = 64.2899, GNorm = 0.9996, lr_0 = 8.0615e-04
Loss = 6.6770e-04, PNorm = 64.3323, GNorm = 0.8746, lr_0 = 8.0506e-04
Loss = 6.4566e-04, PNorm = 64.3700, GNorm = 0.3187, lr_0 = 8.0398e-04
Loss = 5.8673e-04, PNorm = 64.3991, GNorm = 0.2734, lr_0 = 8.0291e-04
Loss = 6.1695e-04, PNorm = 64.4359, GNorm = 0.3690, lr_0 = 8.0183e-04
Loss = 6.9277e-04, PNorm = 64.4784, GNorm = 0.2166, lr_0 = 8.0075e-04
Loss = 5.7329e-04, PNorm = 64.5069, GNorm = 0.4585, lr_0 = 7.9968e-04
Loss = 5.0961e-04, PNorm = 64.5392, GNorm = 0.4301, lr_0 = 7.9861e-04
Loss = 6.7896e-04, PNorm = 64.5730, GNorm = 0.4032, lr_0 = 7.9753e-04
Loss = 6.6222e-04, PNorm = 64.6175, GNorm = 0.4848, lr_0 = 7.9646e-04
Loss = 6.2422e-04, PNorm = 64.6514, GNorm = 0.4195, lr_0 = 7.9540e-04
Loss = 5.9558e-04, PNorm = 64.6788, GNorm = 0.5858, lr_0 = 7.9433e-04
Loss = 7.5806e-04, PNorm = 64.7086, GNorm = 1.1236, lr_0 = 7.9326e-04
Loss = 7.7826e-04, PNorm = 64.7531, GNorm = 0.6625, lr_0 = 7.9220e-04
Loss = 5.8235e-04, PNorm = 64.7924, GNorm = 0.2593, lr_0 = 7.9114e-04
Loss = 6.8012e-04, PNorm = 64.8246, GNorm = 0.6716, lr_0 = 7.9007e-04
Validation rmse logP = 0.509903
Validation R2 logP = 0.923693
Epoch 12
Train function
Loss = 9.0683e-04, PNorm = 64.8726, GNorm = 1.2565, lr_0 = 7.8891e-04
Loss = 7.2936e-04, PNorm = 64.9266, GNorm = 0.4685, lr_0 = 7.8785e-04
Loss = 6.5175e-04, PNorm = 64.9788, GNorm = 1.1434, lr_0 = 7.8679e-04
Loss = 6.8175e-04, PNorm = 65.0353, GNorm = 0.7017, lr_0 = 7.8574e-04
Loss = 6.6650e-04, PNorm = 65.0672, GNorm = 0.3901, lr_0 = 7.8468e-04
Loss = 5.2472e-04, PNorm = 65.1018, GNorm = 0.4085, lr_0 = 7.8363e-04
Loss = 4.8761e-04, PNorm = 65.1351, GNorm = 0.2487, lr_0 = 7.8258e-04
Loss = 7.1937e-04, PNorm = 65.1582, GNorm = 0.7021, lr_0 = 7.8153e-04
Loss = 7.1151e-04, PNorm = 65.2055, GNorm = 0.4273, lr_0 = 7.8048e-04
Loss = 5.5886e-04, PNorm = 65.2458, GNorm = 0.5296, lr_0 = 7.7943e-04
Loss = 4.7080e-04, PNorm = 65.2909, GNorm = 0.2007, lr_0 = 7.7839e-04
Loss = 4.8799e-04, PNorm = 65.3205, GNorm = 0.2576, lr_0 = 7.7734e-04
Loss = 6.8874e-04, PNorm = 65.3516, GNorm = 0.6611, lr_0 = 7.7630e-04
Loss = 6.3766e-04, PNorm = 65.3976, GNorm = 0.5683, lr_0 = 7.7526e-04
Loss = 6.4490e-04, PNorm = 65.4349, GNorm = 0.2193, lr_0 = 7.7422e-04
Loss = 5.0826e-04, PNorm = 65.4648, GNorm = 0.8754, lr_0 = 7.7318e-04
Loss = 5.4805e-04, PNorm = 65.4963, GNorm = 0.2422, lr_0 = 7.7214e-04
Loss = 5.5735e-04, PNorm = 65.5226, GNorm = 0.3687, lr_0 = 7.7111e-04
Validation rmse logP = 0.472721
Validation R2 logP = 0.934416
Epoch 13
Train function
Loss = 4.9976e-04, PNorm = 65.5490, GNorm = 0.2724, lr_0 = 7.7007e-04
Loss = 4.6454e-04, PNorm = 65.5778, GNorm = 0.8877, lr_0 = 7.6904e-04
Loss = 4.5332e-04, PNorm = 65.6075, GNorm = 0.4729, lr_0 = 7.6801e-04
Loss = 4.2735e-04, PNorm = 65.6358, GNorm = 0.3394, lr_0 = 7.6698e-04
Loss = 5.2416e-04, PNorm = 65.6694, GNorm = 0.2972, lr_0 = 7.6595e-04
Loss = 4.2370e-04, PNorm = 65.7045, GNorm = 0.4903, lr_0 = 7.6492e-04
Loss = 4.8326e-04, PNorm = 65.7355, GNorm = 0.2914, lr_0 = 7.6389e-04
Loss = 5.0233e-04, PNorm = 65.7701, GNorm = 0.3718, lr_0 = 7.6287e-04
Loss = 4.9908e-04, PNorm = 65.7986, GNorm = 0.7270, lr_0 = 7.6184e-04
Loss = 4.8212e-04, PNorm = 65.8356, GNorm = 0.2069, lr_0 = 7.6082e-04
Loss = 4.8537e-04, PNorm = 65.8739, GNorm = 0.3571, lr_0 = 7.5980e-04
Loss = 4.2686e-04, PNorm = 65.9020, GNorm = 0.4568, lr_0 = 7.5878e-04
Loss = 4.8812e-04, PNorm = 65.9291, GNorm = 0.5032, lr_0 = 7.5776e-04
Loss = 4.4298e-04, PNorm = 65.9497, GNorm = 0.2781, lr_0 = 7.5675e-04
Loss = 4.5200e-04, PNorm = 65.9821, GNorm = 0.3291, lr_0 = 7.5573e-04
Loss = 4.6329e-04, PNorm = 66.0092, GNorm = 0.3001, lr_0 = 7.5472e-04
Loss = 5.3113e-04, PNorm = 66.0401, GNorm = 0.3559, lr_0 = 7.5370e-04
Validation rmse logP = 0.482013
Validation R2 logP = 0.931813
Epoch 14
Train function
Loss = 3.0283e-04, PNorm = 66.0801, GNorm = 0.5165, lr_0 = 7.5259e-04
Loss = 4.3853e-04, PNorm = 66.1158, GNorm = 0.5000, lr_0 = 7.5158e-04
Loss = 4.5634e-04, PNorm = 66.1540, GNorm = 0.2882, lr_0 = 7.5057e-04
Loss = 4.2028e-04, PNorm = 66.1767, GNorm = 0.3292, lr_0 = 7.4957e-04
Loss = 4.8701e-04, PNorm = 66.2085, GNorm = 0.4216, lr_0 = 7.4856e-04
Loss = 4.8904e-04, PNorm = 66.2479, GNorm = 0.4784, lr_0 = 7.4756e-04
Loss = 6.3209e-04, PNorm = 66.2859, GNorm = 0.8364, lr_0 = 7.4655e-04
Loss = 5.0297e-04, PNorm = 66.3159, GNorm = 0.4251, lr_0 = 7.4555e-04
Loss = 4.8957e-04, PNorm = 66.3479, GNorm = 0.5364, lr_0 = 7.4455e-04
Loss = 4.3128e-04, PNorm = 66.3807, GNorm = 0.2979, lr_0 = 7.4355e-04
Loss = 4.8506e-04, PNorm = 66.4078, GNorm = 0.3269, lr_0 = 7.4256e-04
Loss = 4.3164e-04, PNorm = 66.4333, GNorm = 0.3928, lr_0 = 7.4156e-04
Loss = 3.8110e-04, PNorm = 66.4690, GNorm = 0.6127, lr_0 = 7.4056e-04
Loss = 3.4741e-04, PNorm = 66.4939, GNorm = 0.3893, lr_0 = 7.3957e-04
Loss = 3.6846e-04, PNorm = 66.5243, GNorm = 0.5964, lr_0 = 7.3858e-04
Loss = 3.9051e-04, PNorm = 66.5593, GNorm = 0.4303, lr_0 = 7.3759e-04
Loss = 3.7590e-04, PNorm = 66.5874, GNorm = 0.2988, lr_0 = 7.3660e-04
Loss = 6.0776e-04, PNorm = 66.6116, GNorm = 0.7381, lr_0 = 7.3561e-04
Validation rmse logP = 0.469406
Validation R2 logP = 0.935333
Epoch 15
Train function
Loss = 3.5979e-04, PNorm = 66.6403, GNorm = 0.2707, lr_0 = 7.3462e-04
Loss = 3.3672e-04, PNorm = 66.6781, GNorm = 0.3095, lr_0 = 7.3364e-04
Loss = 3.3134e-04, PNorm = 66.7048, GNorm = 0.6317, lr_0 = 7.3265e-04
Loss = 3.7895e-04, PNorm = 66.7376, GNorm = 0.4816, lr_0 = 7.3167e-04
Loss = 3.3309e-04, PNorm = 66.7596, GNorm = 0.4973, lr_0 = 7.3069e-04
Loss = 3.6241e-04, PNorm = 66.7862, GNorm = 0.1996, lr_0 = 7.2971e-04
Loss = 3.3354e-04, PNorm = 66.8126, GNorm = 0.4896, lr_0 = 7.2873e-04
Loss = 4.6455e-04, PNorm = 66.8398, GNorm = 0.2135, lr_0 = 7.2775e-04
Loss = 4.3100e-04, PNorm = 66.8568, GNorm = 0.4563, lr_0 = 7.2677e-04
Loss = 3.1881e-04, PNorm = 66.8741, GNorm = 0.3527, lr_0 = 7.2580e-04
Loss = 4.0778e-04, PNorm = 66.9067, GNorm = 0.5015, lr_0 = 7.2483e-04
Loss = 4.8576e-04, PNorm = 66.9460, GNorm = 0.4523, lr_0 = 7.2385e-04
Loss = 4.9607e-04, PNorm = 66.9814, GNorm = 0.5259, lr_0 = 7.2288e-04
Loss = 5.8257e-04, PNorm = 67.0088, GNorm = 0.4334, lr_0 = 7.2191e-04
Loss = 4.1099e-04, PNorm = 67.0409, GNorm = 0.2777, lr_0 = 7.2094e-04
Loss = 3.6632e-04, PNorm = 67.0722, GNorm = 0.2869, lr_0 = 7.1998e-04
Loss = 4.3782e-04, PNorm = 67.0971, GNorm = 0.3427, lr_0 = 7.1901e-04
Loss = 4.0348e-04, PNorm = 67.1210, GNorm = 0.3229, lr_0 = 7.1804e-04
Validation rmse logP = 0.480574
Validation R2 logP = 0.932219
Epoch 16
Train function
Loss = 3.4013e-04, PNorm = 67.1513, GNorm = 0.5318, lr_0 = 7.1708e-04
Loss = 2.9742e-04, PNorm = 67.1771, GNorm = 0.2554, lr_0 = 7.1612e-04
Loss = 2.7680e-04, PNorm = 67.2091, GNorm = 0.3955, lr_0 = 7.1516e-04
Loss = 3.4343e-04, PNorm = 67.2285, GNorm = 0.5092, lr_0 = 7.1420e-04
Loss = 4.1062e-04, PNorm = 67.2504, GNorm = 0.3636, lr_0 = 7.1324e-04
Loss = 3.2532e-04, PNorm = 67.2753, GNorm = 0.2158, lr_0 = 7.1228e-04
Loss = 3.0751e-04, PNorm = 67.3004, GNorm = 0.5459, lr_0 = 7.1133e-04
Loss = 3.5892e-04, PNorm = 67.3254, GNorm = 0.4693, lr_0 = 7.1037e-04
Loss = 4.7852e-04, PNorm = 67.3519, GNorm = 0.4237, lr_0 = 7.0942e-04
Loss = 4.3450e-04, PNorm = 67.3836, GNorm = 0.4606, lr_0 = 7.0847e-04
Loss = 3.7602e-04, PNorm = 67.4192, GNorm = 0.4360, lr_0 = 7.0752e-04
Loss = 3.1974e-04, PNorm = 67.4458, GNorm = 0.3139, lr_0 = 7.0657e-04
Loss = 3.6994e-04, PNorm = 67.4656, GNorm = 0.2574, lr_0 = 7.0562e-04
Loss = 4.1386e-04, PNorm = 67.4907, GNorm = 0.4831, lr_0 = 7.0467e-04
Loss = 4.4731e-04, PNorm = 67.5172, GNorm = 0.6980, lr_0 = 7.0373e-04
Loss = 3.7284e-04, PNorm = 67.5521, GNorm = 0.2012, lr_0 = 7.0278e-04
Loss = 4.5246e-04, PNorm = 67.5822, GNorm = 0.3591, lr_0 = 7.0184e-04
Validation rmse logP = 0.465406
Validation R2 logP = 0.936430
Epoch 17
Train function
Loss = 2.9529e-04, PNorm = 67.6236, GNorm = 0.2098, lr_0 = 7.0081e-04
Loss = 3.7322e-04, PNorm = 67.6571, GNorm = 0.2889, lr_0 = 6.9987e-04
Loss = 3.7191e-04, PNorm = 67.6757, GNorm = 0.6857, lr_0 = 6.9893e-04
Loss = 3.7814e-04, PNorm = 67.7052, GNorm = 0.3904, lr_0 = 6.9799e-04
Loss = 3.7406e-04, PNorm = 67.7440, GNorm = 0.2283, lr_0 = 6.9705e-04
Loss = 4.7100e-04, PNorm = 67.7762, GNorm = 0.4928, lr_0 = 6.9612e-04
Loss = 3.9785e-04, PNorm = 67.7999, GNorm = 0.4425, lr_0 = 6.9518e-04
Loss = 4.5226e-04, PNorm = 67.8309, GNorm = 0.2225, lr_0 = 6.9425e-04
Loss = 3.4647e-04, PNorm = 67.8688, GNorm = 0.3653, lr_0 = 6.9332e-04
Loss = 3.5488e-04, PNorm = 67.8993, GNorm = 0.3380, lr_0 = 6.9239e-04
Loss = 3.0691e-04, PNorm = 67.9361, GNorm = 0.1831, lr_0 = 6.9146e-04
Loss = 3.2868e-04, PNorm = 67.9572, GNorm = 0.3124, lr_0 = 6.9053e-04
Loss = 3.0686e-04, PNorm = 67.9797, GNorm = 0.3495, lr_0 = 6.8961e-04
Loss = 3.7633e-04, PNorm = 68.0058, GNorm = 0.2562, lr_0 = 6.8868e-04
Loss = 3.0065e-04, PNorm = 68.0216, GNorm = 0.1670, lr_0 = 6.8776e-04
Loss = 3.0105e-04, PNorm = 68.0379, GNorm = 0.1554, lr_0 = 6.8683e-04
Loss = 3.4435e-04, PNorm = 68.0625, GNorm = 0.6192, lr_0 = 6.8591e-04
Loss = 3.5484e-04, PNorm = 68.0931, GNorm = 0.3578, lr_0 = 6.8499e-04
Validation rmse logP = 0.464587
Validation R2 logP = 0.936654
Epoch 18
Train function
Loss = 2.6903e-04, PNorm = 68.1059, GNorm = 0.1691, lr_0 = 6.8407e-04
Loss = 2.9090e-04, PNorm = 68.1302, GNorm = 0.2231, lr_0 = 6.8315e-04
Loss = 3.4121e-04, PNorm = 68.1518, GNorm = 0.2617, lr_0 = 6.8224e-04
Loss = 2.7584e-04, PNorm = 68.1752, GNorm = 0.2059, lr_0 = 6.8132e-04
Loss = 2.8579e-04, PNorm = 68.2034, GNorm = 0.2091, lr_0 = 6.8041e-04
Loss = 2.5696e-04, PNorm = 68.2256, GNorm = 0.2474, lr_0 = 6.7950e-04
Loss = 2.8054e-04, PNorm = 68.2446, GNorm = 0.2287, lr_0 = 6.7858e-04
Loss = 2.7555e-04, PNorm = 68.2604, GNorm = 0.2805, lr_0 = 6.7767e-04
Loss = 2.9244e-04, PNorm = 68.2854, GNorm = 0.2131, lr_0 = 6.7676e-04
Loss = 3.7354e-04, PNorm = 68.3101, GNorm = 0.4720, lr_0 = 6.7586e-04
Loss = 3.1780e-04, PNorm = 68.3370, GNorm = 0.8753, lr_0 = 6.7495e-04
Loss = 4.2930e-04, PNorm = 68.3615, GNorm = 0.7658, lr_0 = 6.7404e-04
Loss = 3.7837e-04, PNorm = 68.4017, GNorm = 0.2479, lr_0 = 6.7314e-04
Loss = 3.5045e-04, PNorm = 68.4373, GNorm = 0.2514, lr_0 = 6.7224e-04
Loss = 4.3439e-04, PNorm = 68.4777, GNorm = 0.7610, lr_0 = 6.7133e-04
Loss = 3.5621e-04, PNorm = 68.5105, GNorm = 0.6073, lr_0 = 6.7043e-04
Loss = 4.0098e-04, PNorm = 68.5406, GNorm = 0.2188, lr_0 = 6.6953e-04
Validation rmse logP = 0.483923
Validation R2 logP = 0.931271
Epoch 19
Train function
Loss = 5.3290e-04, PNorm = 68.5663, GNorm = 0.8747, lr_0 = 6.6864e-04
Loss = 4.6561e-04, PNorm = 68.6153, GNorm = 0.7727, lr_0 = 6.6774e-04
Loss = 4.7870e-04, PNorm = 68.6451, GNorm = 0.6063, lr_0 = 6.6684e-04
Loss = 4.5155e-04, PNorm = 68.7007, GNorm = 0.1958, lr_0 = 6.6595e-04
Loss = 2.8881e-04, PNorm = 68.7381, GNorm = 0.4458, lr_0 = 6.6505e-04
Loss = 3.9102e-04, PNorm = 68.7645, GNorm = 0.5137, lr_0 = 6.6416e-04
Loss = 2.9194e-04, PNorm = 68.7963, GNorm = 0.3379, lr_0 = 6.6327e-04
Loss = 2.4992e-04, PNorm = 68.8146, GNorm = 0.4316, lr_0 = 6.6238e-04
Loss = 2.7333e-04, PNorm = 68.8415, GNorm = 0.2864, lr_0 = 6.6149e-04
Loss = 3.5811e-04, PNorm = 68.8675, GNorm = 0.3064, lr_0 = 6.6060e-04
Loss = 2.8498e-04, PNorm = 68.8880, GNorm = 0.4410, lr_0 = 6.5972e-04
Loss = 2.6962e-04, PNorm = 68.9082, GNorm = 0.1770, lr_0 = 6.5883e-04
Loss = 2.4892e-04, PNorm = 68.9176, GNorm = 0.1453, lr_0 = 6.5795e-04
Loss = 2.6995e-04, PNorm = 68.9321, GNorm = 0.2443, lr_0 = 6.5707e-04
Loss = 3.0875e-04, PNorm = 68.9608, GNorm = 0.5450, lr_0 = 6.5618e-04
Loss = 2.9017e-04, PNorm = 68.9842, GNorm = 0.5427, lr_0 = 6.5530e-04
Loss = 3.6344e-04, PNorm = 69.0177, GNorm = 0.3487, lr_0 = 6.5443e-04
Loss = 3.8123e-04, PNorm = 69.0444, GNorm = 0.6359, lr_0 = 6.5355e-04
Validation rmse logP = 0.461491
Validation R2 logP = 0.937495
Epoch 20
Train function
Loss = 2.7971e-04, PNorm = 69.0649, GNorm = 0.3070, lr_0 = 6.5258e-04
Loss = 3.0264e-04, PNorm = 69.0865, GNorm = 0.4823, lr_0 = 6.5171e-04
Loss = 2.7363e-04, PNorm = 69.1099, GNorm = 0.4326, lr_0 = 6.5083e-04
Loss = 2.5285e-04, PNorm = 69.1304, GNorm = 0.5538, lr_0 = 6.4996e-04
Loss = 3.3203e-04, PNorm = 69.1494, GNorm = 0.4218, lr_0 = 6.4909e-04
Loss = 3.7479e-04, PNorm = 69.1716, GNorm = 0.5592, lr_0 = 6.4822e-04
Loss = 2.4646e-04, PNorm = 69.2046, GNorm = 0.2579, lr_0 = 6.4735e-04
Loss = 2.9495e-04, PNorm = 69.2290, GNorm = 0.3337, lr_0 = 6.4648e-04
Loss = 3.3522e-04, PNorm = 69.2508, GNorm = 0.2045, lr_0 = 6.4561e-04
Loss = 3.1463e-04, PNorm = 69.2784, GNorm = 0.2582, lr_0 = 6.4474e-04
Loss = 2.6690e-04, PNorm = 69.3093, GNorm = 0.1914, lr_0 = 6.4388e-04
Loss = 3.3560e-04, PNorm = 69.3307, GNorm = 0.2650, lr_0 = 6.4302e-04
Loss = 2.7971e-04, PNorm = 69.3530, GNorm = 0.4260, lr_0 = 6.4215e-04
Loss = 3.4121e-04, PNorm = 69.3818, GNorm = 0.2890, lr_0 = 6.4129e-04
Loss = 2.5898e-04, PNorm = 69.4016, GNorm = 0.3068, lr_0 = 6.4043e-04
Loss = 2.9690e-04, PNorm = 69.4271, GNorm = 1.0014, lr_0 = 6.3957e-04
Loss = 3.7999e-04, PNorm = 69.4561, GNorm = 1.0554, lr_0 = 6.3871e-04
Validation rmse logP = 0.456384
Validation R2 logP = 0.938871
Epoch 21
Train function
Loss = 1.7778e-04, PNorm = 69.4897, GNorm = 0.1933, lr_0 = 6.3786e-04
Loss = 2.1490e-04, PNorm = 69.5082, GNorm = 0.2334, lr_0 = 6.3700e-04
Loss = 2.1692e-04, PNorm = 69.5225, GNorm = 0.2713, lr_0 = 6.3615e-04
Loss = 2.2365e-04, PNorm = 69.5419, GNorm = 0.3949, lr_0 = 6.3529e-04
Loss = 2.3785e-04, PNorm = 69.5605, GNorm = 0.4950, lr_0 = 6.3444e-04
Loss = 2.2049e-04, PNorm = 69.5864, GNorm = 0.3229, lr_0 = 6.3359e-04
Loss = 2.8909e-04, PNorm = 69.6114, GNorm = 0.4496, lr_0 = 6.3274e-04
Loss = 2.3609e-04, PNorm = 69.6383, GNorm = 0.2177, lr_0 = 6.3189e-04
Loss = 2.5243e-04, PNorm = 69.6614, GNorm = 0.2192, lr_0 = 6.3104e-04
Loss = 2.7941e-04, PNorm = 69.6791, GNorm = 0.1423, lr_0 = 6.3020e-04
Loss = 3.3745e-04, PNorm = 69.7000, GNorm = 0.2475, lr_0 = 6.2935e-04
Loss = 3.0497e-04, PNorm = 69.7277, GNorm = 0.2648, lr_0 = 6.2851e-04
Loss = 2.4670e-04, PNorm = 69.7473, GNorm = 0.3613, lr_0 = 6.2766e-04
Loss = 2.3768e-04, PNorm = 69.7639, GNorm = 0.4454, lr_0 = 6.2682e-04
Loss = 2.9666e-04, PNorm = 69.7697, GNorm = 0.2175, lr_0 = 6.2598e-04
Loss = 3.3351e-04, PNorm = 69.8004, GNorm = 0.3284, lr_0 = 6.2514e-04
Loss = 2.5006e-04, PNorm = 69.8254, GNorm = 0.2188, lr_0 = 6.2430e-04
Loss = 2.8892e-04, PNorm = 69.8498, GNorm = 0.3235, lr_0 = 6.2346e-04
Validation rmse logP = 0.474619
Validation R2 logP = 0.933888
Epoch 22
Train function
Loss = 2.4769e-04, PNorm = 69.8714, GNorm = 0.4259, lr_0 = 6.2263e-04
Loss = 2.6932e-04, PNorm = 69.8944, GNorm = 0.3972, lr_0 = 6.2179e-04
Loss = 2.1465e-04, PNorm = 69.9142, GNorm = 0.1937, lr_0 = 6.2096e-04
Loss = 2.6107e-04, PNorm = 69.9377, GNorm = 0.4838, lr_0 = 6.2012e-04
Loss = 2.5253e-04, PNorm = 69.9472, GNorm = 0.3260, lr_0 = 6.1929e-04
Loss = 2.1684e-04, PNorm = 69.9651, GNorm = 0.2472, lr_0 = 6.1846e-04
Loss = 2.0197e-04, PNorm = 69.9857, GNorm = 0.2573, lr_0 = 6.1763e-04
Loss = 2.5288e-04, PNorm = 70.0030, GNorm = 0.2779, lr_0 = 6.1680e-04
Loss = 2.0761e-04, PNorm = 70.0220, GNorm = 0.5791, lr_0 = 6.1597e-04
Loss = 2.4538e-04, PNorm = 70.0409, GNorm = 0.2360, lr_0 = 6.1515e-04
Loss = 2.4333e-04, PNorm = 70.0616, GNorm = 0.1965, lr_0 = 6.1432e-04
Loss = 1.8812e-04, PNorm = 70.0764, GNorm = 0.2496, lr_0 = 6.1350e-04
Loss = 2.0125e-04, PNorm = 70.0878, GNorm = 0.2281, lr_0 = 6.1268e-04
Loss = 2.1009e-04, PNorm = 70.1036, GNorm = 0.4588, lr_0 = 6.1185e-04
Loss = 2.3893e-04, PNorm = 70.1253, GNorm = 0.1988, lr_0 = 6.1103e-04
Loss = 2.2883e-04, PNorm = 70.1442, GNorm = 0.3556, lr_0 = 6.1021e-04
Loss = 2.0406e-04, PNorm = 70.1585, GNorm = 0.2122, lr_0 = 6.0939e-04
Validation rmse logP = 0.457005
Validation R2 logP = 0.938704
Epoch 23
Train function
Loss = 1.7506e-04, PNorm = 70.1864, GNorm = 0.2056, lr_0 = 6.0849e-04
Loss = 1.9291e-04, PNorm = 70.2014, GNorm = 0.2753, lr_0 = 6.0768e-04
Loss = 2.0900e-04, PNorm = 70.2125, GNorm = 0.6241, lr_0 = 6.0686e-04
Loss = 2.3722e-04, PNorm = 70.2214, GNorm = 0.7796, lr_0 = 6.0605e-04
Loss = 1.7737e-04, PNorm = 70.2405, GNorm = 0.1331, lr_0 = 6.0524e-04
Loss = 1.9123e-04, PNorm = 70.2666, GNorm = 0.2892, lr_0 = 6.0442e-04
Loss = 1.9742e-04, PNorm = 70.2816, GNorm = 0.1728, lr_0 = 6.0361e-04
Loss = 2.1248e-04, PNorm = 70.3020, GNorm = 0.1787, lr_0 = 6.0280e-04
Loss = 1.8522e-04, PNorm = 70.3205, GNorm = 0.2291, lr_0 = 6.0199e-04
Loss = 1.9203e-04, PNorm = 70.3513, GNorm = 0.2799, lr_0 = 6.0119e-04
Loss = 1.9880e-04, PNorm = 70.3684, GNorm = 0.3933, lr_0 = 6.0038e-04
Loss = 2.1279e-04, PNorm = 70.3832, GNorm = 0.3350, lr_0 = 5.9957e-04
Loss = 2.0755e-04, PNorm = 70.3996, GNorm = 0.3052, lr_0 = 5.9877e-04
Loss = 1.8521e-04, PNorm = 70.4133, GNorm = 0.2925, lr_0 = 5.9797e-04
Loss = 1.9222e-04, PNorm = 70.4307, GNorm = 0.5300, lr_0 = 5.9716e-04
Loss = 3.1026e-04, PNorm = 70.4534, GNorm = 0.1902, lr_0 = 5.9636e-04
Loss = 2.3260e-04, PNorm = 70.4735, GNorm = 0.1506, lr_0 = 5.9556e-04
Loss = 2.9145e-04, PNorm = 70.5097, GNorm = 0.7177, lr_0 = 5.9476e-04
Validation rmse logP = 0.463885
Validation R2 logP = 0.936845
Epoch 24
Train function
Loss = 2.0123e-04, PNorm = 70.5339, GNorm = 0.2189, lr_0 = 5.9397e-04
Loss = 1.9791e-04, PNorm = 70.5579, GNorm = 0.3980, lr_0 = 5.9317e-04
Loss = 1.9342e-04, PNorm = 70.5677, GNorm = 0.2492, lr_0 = 5.9237e-04
Loss = 1.8865e-04, PNorm = 70.5894, GNorm = 0.1310, lr_0 = 5.9158e-04
Loss = 1.6481e-04, PNorm = 70.6131, GNorm = 0.1581, lr_0 = 5.9078e-04
Loss = 1.7544e-04, PNorm = 70.6361, GNorm = 0.1485, lr_0 = 5.8999e-04
Loss = 1.6473e-04, PNorm = 70.6490, GNorm = 0.2112, lr_0 = 5.8920e-04
Loss = 1.6720e-04, PNorm = 70.6643, GNorm = 0.2307, lr_0 = 5.8841e-04
Loss = 2.2323e-04, PNorm = 70.6857, GNorm = 0.3810, lr_0 = 5.8762e-04
Loss = 2.0988e-04, PNorm = 70.7033, GNorm = 0.2119, lr_0 = 5.8683e-04
Loss = 1.9886e-04, PNorm = 70.7197, GNorm = 0.5413, lr_0 = 5.8604e-04
Loss = 2.4147e-04, PNorm = 70.7449, GNorm = 0.3476, lr_0 = 5.8526e-04
Loss = 2.0526e-04, PNorm = 70.7611, GNorm = 0.2546, lr_0 = 5.8447e-04
Loss = 2.4420e-04, PNorm = 70.7830, GNorm = 0.3024, lr_0 = 5.8369e-04
Loss = 1.8652e-04, PNorm = 70.8065, GNorm = 0.3176, lr_0 = 5.8290e-04
Loss = 2.0675e-04, PNorm = 70.8255, GNorm = 0.3575, lr_0 = 5.8212e-04
Loss = 1.7867e-04, PNorm = 70.8493, GNorm = 0.2691, lr_0 = 5.8134e-04
Loss = 2.0005e-04, PNorm = 70.8553, GNorm = 0.2653, lr_0 = 5.8056e-04
