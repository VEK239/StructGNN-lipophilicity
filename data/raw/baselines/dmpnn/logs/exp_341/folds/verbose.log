Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_best'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD', 'logP'],
 'task_names': ['logD', 'logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=499, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 414,902
Moving model to cuda
Epoch 0
Train function
Loss = 1.9976e-02, PNorm = 35.0897, GNorm = 4.1144, lr_0 = 1.2200e-04
Loss = 1.9948e-02, PNorm = 35.0916, GNorm = 3.7086, lr_0 = 1.4200e-04
Loss = 1.5198e-02, PNorm = 35.0950, GNorm = 5.5799, lr_0 = 1.6200e-04
Loss = 1.3925e-02, PNorm = 35.0991, GNorm = 3.5163, lr_0 = 1.8200e-04
Loss = 1.3778e-02, PNorm = 35.1040, GNorm = 6.1062, lr_0 = 2.0200e-04
Loss = 1.1762e-02, PNorm = 35.1086, GNorm = 2.7936, lr_0 = 2.2200e-04
Loss = 9.6353e-03, PNorm = 35.1158, GNorm = 2.2301, lr_0 = 2.4200e-04
Loss = 1.0510e-02, PNorm = 35.1228, GNorm = 2.6535, lr_0 = 2.6200e-04
Loss = 8.7578e-03, PNorm = 35.1312, GNorm = 3.0003, lr_0 = 2.8200e-04
Loss = 7.6346e-03, PNorm = 35.1405, GNorm = 5.2295, lr_0 = 3.0200e-04
Loss = 8.0946e-03, PNorm = 35.1488, GNorm = 1.9133, lr_0 = 3.2200e-04
Loss = 9.0692e-03, PNorm = 35.1590, GNorm = 7.3742, lr_0 = 3.4200e-04
Loss = 9.2227e-03, PNorm = 35.1702, GNorm = 4.7346, lr_0 = 3.6200e-04
Loss = 7.4747e-03, PNorm = 35.1816, GNorm = 3.0333, lr_0 = 3.8200e-04
Loss = 7.2203e-03, PNorm = 35.1940, GNorm = 2.5792, lr_0 = 4.0200e-04
Loss = 6.7861e-03, PNorm = 35.2114, GNorm = 2.8959, lr_0 = 4.2200e-04
Loss = 7.2321e-03, PNorm = 35.2264, GNorm = 1.9655, lr_0 = 4.4200e-04
Loss = 7.6254e-03, PNorm = 35.2454, GNorm = 3.1361, lr_0 = 4.6200e-04
Loss = 5.8946e-03, PNorm = 35.2567, GNorm = 2.3058, lr_0 = 4.8200e-04
Loss = 6.3124e-03, PNorm = 35.2699, GNorm = 4.0191, lr_0 = 5.0200e-04
Loss = 5.9955e-03, PNorm = 35.2884, GNorm = 0.9291, lr_0 = 5.2200e-04
Loss = 5.8557e-03, PNorm = 35.3074, GNorm = 2.2882, lr_0 = 5.4200e-04
Validation rmse logD = 0.911702
Validation R2 logD = 0.418676
Validation rmse logP = 0.875806
Validation R2 logP = 0.788851
Epoch 1
Train function
Loss = 5.7604e-03, PNorm = 35.3267, GNorm = 2.9603, lr_0 = 5.6400e-04
Loss = 6.1488e-03, PNorm = 35.3431, GNorm = 5.8294, lr_0 = 5.8400e-04
Loss = 5.5698e-03, PNorm = 35.3613, GNorm = 1.2178, lr_0 = 6.0400e-04
Loss = 5.7392e-03, PNorm = 35.3844, GNorm = 7.3213, lr_0 = 6.2400e-04
Loss = 6.7854e-03, PNorm = 35.4044, GNorm = 3.4981, lr_0 = 6.4400e-04
Loss = 5.5204e-03, PNorm = 35.4295, GNorm = 2.9583, lr_0 = 6.6400e-04
Loss = 4.9738e-03, PNorm = 35.4482, GNorm = 2.4650, lr_0 = 6.8400e-04
Loss = 5.0726e-03, PNorm = 35.4678, GNorm = 2.0897, lr_0 = 7.0400e-04
Loss = 5.3033e-03, PNorm = 35.4934, GNorm = 2.2148, lr_0 = 7.2400e-04
Loss = 6.2883e-03, PNorm = 35.5197, GNorm = 4.4177, lr_0 = 7.4400e-04
Loss = 5.1991e-03, PNorm = 35.5509, GNorm = 3.2274, lr_0 = 7.6400e-04
Loss = 4.8484e-03, PNorm = 35.5792, GNorm = 2.0745, lr_0 = 7.8400e-04
Loss = 5.4484e-03, PNorm = 35.6043, GNorm = 2.4488, lr_0 = 8.0400e-04
Loss = 5.0224e-03, PNorm = 35.6331, GNorm = 1.9733, lr_0 = 8.2400e-04
Loss = 5.0902e-03, PNorm = 35.6613, GNorm = 1.4672, lr_0 = 8.4400e-04
Loss = 5.0732e-03, PNorm = 35.6886, GNorm = 2.5791, lr_0 = 8.6400e-04
Loss = 4.2656e-03, PNorm = 35.7193, GNorm = 2.4681, lr_0 = 8.8400e-04
Loss = 4.8722e-03, PNorm = 35.7548, GNorm = 1.5593, lr_0 = 9.0400e-04
Loss = 4.5416e-03, PNorm = 35.7902, GNorm = 1.7941, lr_0 = 9.2400e-04
Loss = 5.3429e-03, PNorm = 35.8248, GNorm = 2.5273, lr_0 = 9.4400e-04
Loss = 4.4257e-03, PNorm = 35.8594, GNorm = 3.2945, lr_0 = 9.6400e-04
Loss = 4.9025e-03, PNorm = 35.8981, GNorm = 1.2320, lr_0 = 9.8400e-04
Loss = 5.3621e-03, PNorm = 35.9286, GNorm = 5.0058, lr_0 = 9.9979e-04
Loss = 5.9538e-03, PNorm = 35.9332, GNorm = 1.5970, lr_0 = 9.9969e-04
Validation rmse logD = 0.826052
Validation R2 logD = 0.522771
Validation rmse logP = 0.690806
Validation R2 logP = 0.868633
Epoch 2
Train function
Loss = 4.3315e-03, PNorm = 35.9558, GNorm = 1.4483, lr_0 = 9.9864e-04
Loss = 5.2577e-03, PNorm = 35.9943, GNorm = 1.4090, lr_0 = 9.9760e-04
Loss = 3.8897e-03, PNorm = 36.0343, GNorm = 2.7217, lr_0 = 9.9656e-04
Loss = 3.8411e-03, PNorm = 36.0691, GNorm = 1.5081, lr_0 = 9.9552e-04
Loss = 4.3725e-03, PNorm = 36.0968, GNorm = 2.4745, lr_0 = 9.9448e-04
Loss = 4.3164e-03, PNorm = 36.1340, GNorm = 3.8727, lr_0 = 9.9344e-04
Loss = 5.5445e-03, PNorm = 36.1572, GNorm = 3.0272, lr_0 = 9.9241e-04
Loss = 3.5235e-03, PNorm = 36.1965, GNorm = 1.6457, lr_0 = 9.9137e-04
Loss = 4.1966e-03, PNorm = 36.2343, GNorm = 1.7425, lr_0 = 9.9034e-04
Loss = 4.3382e-03, PNorm = 36.2704, GNorm = 2.3061, lr_0 = 9.8930e-04
Loss = 4.3971e-03, PNorm = 36.3115, GNorm = 3.8305, lr_0 = 9.8827e-04
Loss = 3.8491e-03, PNorm = 36.3527, GNorm = 2.4135, lr_0 = 9.8724e-04
Loss = 4.0185e-03, PNorm = 36.3875, GNorm = 1.3550, lr_0 = 9.8621e-04
Loss = 3.7084e-03, PNorm = 36.4144, GNorm = 3.5740, lr_0 = 9.8518e-04
Loss = 4.7324e-03, PNorm = 36.4539, GNorm = 2.7882, lr_0 = 9.8415e-04
Loss = 4.9644e-03, PNorm = 36.4923, GNorm = 3.2834, lr_0 = 9.8312e-04
Loss = 4.6318e-03, PNorm = 36.5426, GNorm = 0.7184, lr_0 = 9.8210e-04
Loss = 3.3577e-03, PNorm = 36.5760, GNorm = 1.8317, lr_0 = 9.8107e-04
Loss = 3.4895e-03, PNorm = 36.6050, GNorm = 1.1729, lr_0 = 9.8005e-04
Loss = 3.3745e-03, PNorm = 36.6291, GNorm = 2.5263, lr_0 = 9.7902e-04
Loss = 3.4826e-03, PNorm = 36.6555, GNorm = 1.1155, lr_0 = 9.7800e-04
Loss = 3.3216e-03, PNorm = 36.6830, GNorm = 1.0919, lr_0 = 9.7698e-04
Validation rmse logD = 0.810774
Validation R2 logD = 0.540260
Validation rmse logP = 0.632847
Validation R2 logP = 0.889752
Epoch 3
Train function
Loss = 2.7631e-03, PNorm = 36.7030, GNorm = 1.3556, lr_0 = 9.7596e-04
Loss = 3.0802e-03, PNorm = 36.7290, GNorm = 1.1745, lr_0 = 9.7494e-04
Loss = 3.4683e-03, PNorm = 36.7585, GNorm = 1.9150, lr_0 = 9.7393e-04
Loss = 3.4368e-03, PNorm = 36.7875, GNorm = 1.5622, lr_0 = 9.7291e-04
Loss = 3.1938e-03, PNorm = 36.8136, GNorm = 1.1741, lr_0 = 9.7189e-04
Loss = 3.1052e-03, PNorm = 36.8576, GNorm = 1.2288, lr_0 = 9.7088e-04
Loss = 4.0607e-03, PNorm = 36.8825, GNorm = 1.6254, lr_0 = 9.6987e-04
Loss = 2.7088e-03, PNorm = 36.9119, GNorm = 0.7164, lr_0 = 9.6885e-04
Loss = 3.3425e-03, PNorm = 36.9466, GNorm = 0.7653, lr_0 = 9.6784e-04
Loss = 3.3599e-03, PNorm = 36.9784, GNorm = 1.3950, lr_0 = 9.6683e-04
Loss = 2.9879e-03, PNorm = 37.0158, GNorm = 1.3803, lr_0 = 9.6582e-04
Loss = 3.0273e-03, PNorm = 37.0295, GNorm = 3.4277, lr_0 = 9.6482e-04
Loss = 4.0576e-03, PNorm = 37.0469, GNorm = 1.4266, lr_0 = 9.6381e-04
Loss = 3.5080e-03, PNorm = 37.0763, GNorm = 1.3508, lr_0 = 9.6280e-04
Loss = 3.4138e-03, PNorm = 37.1045, GNorm = 1.7842, lr_0 = 9.6180e-04
Loss = 3.6472e-03, PNorm = 37.1496, GNorm = 1.0940, lr_0 = 9.6079e-04
Loss = 3.7265e-03, PNorm = 37.1733, GNorm = 1.4869, lr_0 = 9.5979e-04
Loss = 2.4423e-03, PNorm = 37.1938, GNorm = 0.5775, lr_0 = 9.5879e-04
Loss = 2.4095e-03, PNorm = 37.2203, GNorm = 0.9051, lr_0 = 9.5779e-04
Loss = 2.8995e-03, PNorm = 37.2465, GNorm = 1.0787, lr_0 = 9.5679e-04
Loss = 2.3865e-03, PNorm = 37.2688, GNorm = 0.7422, lr_0 = 9.5579e-04
Loss = 2.4985e-03, PNorm = 37.2971, GNorm = 0.8221, lr_0 = 9.5479e-04
Loss = 3.1115e-03, PNorm = 37.3143, GNorm = 1.8813, lr_0 = 9.5380e-04
Validation rmse logD = 0.724253
Validation R2 logD = 0.633147
Validation rmse logP = 0.591757
Validation R2 logP = 0.903603
Epoch 4
Train function
Loss = 2.0705e-03, PNorm = 37.3479, GNorm = 0.8599, lr_0 = 9.5270e-04
Loss = 2.3107e-03, PNorm = 37.3709, GNorm = 0.8527, lr_0 = 9.5171e-04
Loss = 2.5842e-03, PNorm = 37.3951, GNorm = 2.6153, lr_0 = 9.5071e-04
Loss = 2.5753e-03, PNorm = 37.4265, GNorm = 1.2196, lr_0 = 9.4972e-04
Loss = 2.2702e-03, PNorm = 37.4536, GNorm = 0.8524, lr_0 = 9.4873e-04
Loss = 2.6867e-03, PNorm = 37.4776, GNorm = 1.2419, lr_0 = 9.4774e-04
Loss = 2.9355e-03, PNorm = 37.4995, GNorm = 1.2074, lr_0 = 9.4675e-04
Loss = 2.7933e-03, PNorm = 37.5316, GNorm = 2.8635, lr_0 = 9.4576e-04
Loss = 2.7762e-03, PNorm = 37.5680, GNorm = 1.7573, lr_0 = 9.4478e-04
Loss = 3.3659e-03, PNorm = 37.6004, GNorm = 1.6474, lr_0 = 9.4379e-04
Loss = 2.5243e-03, PNorm = 37.6368, GNorm = 1.3246, lr_0 = 9.4280e-04
Loss = 2.7198e-03, PNorm = 37.6777, GNorm = 1.5593, lr_0 = 9.4182e-04
Loss = 2.9325e-03, PNorm = 37.7102, GNorm = 1.6313, lr_0 = 9.4084e-04
Loss = 2.4453e-03, PNorm = 37.7294, GNorm = 1.2015, lr_0 = 9.3986e-04
Loss = 2.3716e-03, PNorm = 37.7455, GNorm = 0.7348, lr_0 = 9.3887e-04
Loss = 3.1044e-03, PNorm = 37.7777, GNorm = 1.0506, lr_0 = 9.3789e-04
Loss = 2.8759e-03, PNorm = 37.8048, GNorm = 2.0216, lr_0 = 9.3692e-04
Loss = 2.8055e-03, PNorm = 37.8317, GNorm = 1.6178, lr_0 = 9.3594e-04
Loss = 3.4447e-03, PNorm = 37.8579, GNorm = 1.1647, lr_0 = 9.3496e-04
Loss = 2.5967e-03, PNorm = 37.9007, GNorm = 0.4892, lr_0 = 9.3399e-04
Loss = 3.2199e-03, PNorm = 37.9248, GNorm = 0.8208, lr_0 = 9.3301e-04
Loss = 2.4377e-03, PNorm = 37.9459, GNorm = 0.9798, lr_0 = 9.3204e-04
Validation rmse logD = 0.750871
Validation R2 logD = 0.605685
Validation rmse logP = 0.660493
Validation R2 logP = 0.879909
Epoch 5
Train function
Loss = 3.4935e-03, PNorm = 37.9788, GNorm = 1.5248, lr_0 = 9.3106e-04
Loss = 2.5566e-03, PNorm = 38.0091, GNorm = 2.4225, lr_0 = 9.3009e-04
Loss = 2.4171e-03, PNorm = 38.0339, GNorm = 1.1823, lr_0 = 9.2912e-04
Loss = 2.7307e-03, PNorm = 38.0622, GNorm = 2.4976, lr_0 = 9.2815e-04
Loss = 2.7589e-03, PNorm = 38.0944, GNorm = 0.9575, lr_0 = 9.2718e-04
Loss = 2.9620e-03, PNorm = 38.1345, GNorm = 0.9856, lr_0 = 9.2622e-04
Loss = 2.3637e-03, PNorm = 38.1724, GNorm = 0.7640, lr_0 = 9.2525e-04
Loss = 2.2658e-03, PNorm = 38.2042, GNorm = 1.1233, lr_0 = 9.2428e-04
Loss = 2.3378e-03, PNorm = 38.2305, GNorm = 0.7452, lr_0 = 9.2332e-04
Loss = 2.4747e-03, PNorm = 38.2480, GNorm = 1.4221, lr_0 = 9.2235e-04
Loss = 2.5573e-03, PNorm = 38.2764, GNorm = 2.0941, lr_0 = 9.2139e-04
Loss = 2.9111e-03, PNorm = 38.3042, GNorm = 1.0631, lr_0 = 9.2043e-04
Loss = 2.7511e-03, PNorm = 38.3407, GNorm = 1.3758, lr_0 = 9.1947e-04
Loss = 2.2644e-03, PNorm = 38.3693, GNorm = 0.5925, lr_0 = 9.1851e-04
Loss = 2.2130e-03, PNorm = 38.3941, GNorm = 1.4364, lr_0 = 9.1755e-04
Loss = 3.0309e-03, PNorm = 38.4055, GNorm = 1.4506, lr_0 = 9.1659e-04
Loss = 2.5500e-03, PNorm = 38.4296, GNorm = 1.0440, lr_0 = 9.1564e-04
Loss = 2.3933e-03, PNorm = 38.4495, GNorm = 1.3062, lr_0 = 9.1468e-04
Loss = 2.2083e-03, PNorm = 38.4756, GNorm = 1.3693, lr_0 = 9.1373e-04
Loss = 1.8771e-03, PNorm = 38.5045, GNorm = 0.9581, lr_0 = 9.1277e-04
Loss = 2.0494e-03, PNorm = 38.5305, GNorm = 1.0243, lr_0 = 9.1182e-04
Loss = 2.0717e-03, PNorm = 38.5618, GNorm = 1.3444, lr_0 = 9.1087e-04
Loss = 2.5271e-03, PNorm = 38.5915, GNorm = 1.2314, lr_0 = 9.0992e-04
Validation rmse logD = 0.714466
Validation R2 logD = 0.642994
Validation rmse logP = 0.549506
Validation R2 logP = 0.916877
Epoch 6
Train function
Loss = 2.8042e-03, PNorm = 38.6211, GNorm = 2.2463, lr_0 = 9.0887e-04
Loss = 2.2023e-03, PNorm = 38.6391, GNorm = 0.9680, lr_0 = 9.0792e-04
Loss = 2.1188e-03, PNorm = 38.6680, GNorm = 1.6145, lr_0 = 9.0698e-04
Loss = 2.0070e-03, PNorm = 38.6863, GNorm = 1.2918, lr_0 = 9.0603e-04
Loss = 1.9091e-03, PNorm = 38.7190, GNorm = 1.4519, lr_0 = 9.0508e-04
Loss = 2.8962e-03, PNorm = 38.7397, GNorm = 2.6468, lr_0 = 9.0414e-04
Loss = 2.0513e-03, PNorm = 38.7682, GNorm = 1.3973, lr_0 = 9.0320e-04
Loss = 1.9960e-03, PNorm = 38.8010, GNorm = 0.8151, lr_0 = 9.0225e-04
Loss = 2.5483e-03, PNorm = 38.8319, GNorm = 1.2384, lr_0 = 9.0131e-04
Loss = 2.1484e-03, PNorm = 38.8671, GNorm = 1.0508, lr_0 = 9.0037e-04
Loss = 1.7506e-03, PNorm = 38.9033, GNorm = 0.6756, lr_0 = 8.9943e-04
Loss = 1.8758e-03, PNorm = 38.9222, GNorm = 0.8490, lr_0 = 8.9849e-04
Loss = 2.7173e-03, PNorm = 38.9597, GNorm = 0.8946, lr_0 = 8.9756e-04
Loss = 2.0794e-03, PNorm = 38.9811, GNorm = 1.5586, lr_0 = 8.9662e-04
Loss = 2.0432e-03, PNorm = 39.0094, GNorm = 0.6782, lr_0 = 8.9568e-04
Loss = 1.6395e-03, PNorm = 39.0345, GNorm = 1.1230, lr_0 = 8.9475e-04
Loss = 2.1477e-03, PNorm = 39.0626, GNorm = 1.1751, lr_0 = 8.9381e-04
Loss = 2.0656e-03, PNorm = 39.0843, GNorm = 1.8185, lr_0 = 8.9288e-04
Loss = 2.3211e-03, PNorm = 39.1046, GNorm = 1.4362, lr_0 = 8.9195e-04
Loss = 2.2543e-03, PNorm = 39.1267, GNorm = 0.5083, lr_0 = 8.9102e-04
Loss = 2.3050e-03, PNorm = 39.1533, GNorm = 0.7521, lr_0 = 8.9009e-04
Loss = 1.7230e-03, PNorm = 39.1719, GNorm = 0.7125, lr_0 = 8.8916e-04
Validation rmse logD = 0.694142
Validation R2 logD = 0.663016
Validation rmse logP = 0.557383
Validation R2 logP = 0.914477
Epoch 7
Train function
Loss = 2.1703e-03, PNorm = 39.2013, GNorm = 1.4654, lr_0 = 8.8823e-04
Loss = 1.7278e-03, PNorm = 39.2273, GNorm = 0.9457, lr_0 = 8.8730e-04
Loss = 1.4814e-03, PNorm = 39.2531, GNorm = 0.7003, lr_0 = 8.8638e-04
Loss = 1.5647e-03, PNorm = 39.2728, GNorm = 0.7731, lr_0 = 8.8545e-04
Loss = 2.1106e-03, PNorm = 39.2945, GNorm = 1.4330, lr_0 = 8.8453e-04
Loss = 1.3726e-03, PNorm = 39.3161, GNorm = 0.5047, lr_0 = 8.8361e-04
Loss = 1.6515e-03, PNorm = 39.3413, GNorm = 0.5682, lr_0 = 8.8268e-04
Loss = 2.2264e-03, PNorm = 39.3596, GNorm = 0.9900, lr_0 = 8.8176e-04
Loss = 2.1358e-03, PNorm = 39.3805, GNorm = 0.8245, lr_0 = 8.8084e-04
Loss = 1.8388e-03, PNorm = 39.3992, GNorm = 0.9823, lr_0 = 8.7992e-04
Loss = 1.8428e-03, PNorm = 39.4318, GNorm = 1.0608, lr_0 = 8.7900e-04
Loss = 2.8377e-03, PNorm = 39.4547, GNorm = 1.6794, lr_0 = 8.7809e-04
Loss = 1.9930e-03, PNorm = 39.4860, GNorm = 1.2421, lr_0 = 8.7717e-04
Loss = 1.9046e-03, PNorm = 39.5221, GNorm = 1.8465, lr_0 = 8.7625e-04
Loss = 2.3597e-03, PNorm = 39.5541, GNorm = 1.3232, lr_0 = 8.7534e-04
Loss = 2.2544e-03, PNorm = 39.5853, GNorm = 1.4043, lr_0 = 8.7443e-04
Loss = 1.8434e-03, PNorm = 39.6201, GNorm = 1.9302, lr_0 = 8.7351e-04
Loss = 1.9941e-03, PNorm = 39.6492, GNorm = 0.5000, lr_0 = 8.7260e-04
Loss = 1.8365e-03, PNorm = 39.6793, GNorm = 0.9536, lr_0 = 8.7169e-04
Loss = 1.8100e-03, PNorm = 39.7031, GNorm = 0.7562, lr_0 = 8.7078e-04
Loss = 2.2027e-03, PNorm = 39.7252, GNorm = 0.8522, lr_0 = 8.6987e-04
Loss = 2.0183e-03, PNorm = 39.7523, GNorm = 1.0112, lr_0 = 8.6896e-04
Loss = 1.9280e-03, PNorm = 39.7802, GNorm = 0.9518, lr_0 = 8.6806e-04
Validation rmse logD = 0.765450
Validation R2 logD = 0.590224
Validation rmse logP = 0.573843
Validation R2 logP = 0.909352
Epoch 8
Train function
Loss = 2.0204e-03, PNorm = 39.8002, GNorm = 0.8338, lr_0 = 8.6706e-04
Loss = 1.8089e-03, PNorm = 39.8314, GNorm = 1.4821, lr_0 = 8.6616e-04
Loss = 2.1955e-03, PNorm = 39.8504, GNorm = 0.5007, lr_0 = 8.6525e-04
Loss = 1.5899e-03, PNorm = 39.8791, GNorm = 1.1620, lr_0 = 8.6435e-04
Loss = 1.7880e-03, PNorm = 39.9062, GNorm = 1.4630, lr_0 = 8.6345e-04
Loss = 1.6261e-03, PNorm = 39.9251, GNorm = 1.1327, lr_0 = 8.6255e-04
Loss = 1.6256e-03, PNorm = 39.9517, GNorm = 1.0080, lr_0 = 8.6165e-04
Loss = 1.6783e-03, PNorm = 39.9718, GNorm = 0.9253, lr_0 = 8.6075e-04
Loss = 1.8395e-03, PNorm = 39.9824, GNorm = 1.4438, lr_0 = 8.5985e-04
Loss = 1.5850e-03, PNorm = 40.0017, GNorm = 0.3699, lr_0 = 8.5895e-04
Loss = 1.7470e-03, PNorm = 40.0264, GNorm = 0.7576, lr_0 = 8.5805e-04
Loss = 1.7210e-03, PNorm = 40.0440, GNorm = 1.0126, lr_0 = 8.5716e-04
Loss = 1.8450e-03, PNorm = 40.0774, GNorm = 1.5217, lr_0 = 8.5626e-04
Loss = 1.6290e-03, PNorm = 40.1038, GNorm = 1.2336, lr_0 = 8.5537e-04
Loss = 1.8200e-03, PNorm = 40.1249, GNorm = 1.0798, lr_0 = 8.5448e-04
Loss = 1.7788e-03, PNorm = 40.1523, GNorm = 1.9296, lr_0 = 8.5359e-04
Loss = 1.8573e-03, PNorm = 40.1831, GNorm = 1.0015, lr_0 = 8.5269e-04
Loss = 1.6414e-03, PNorm = 40.2168, GNorm = 0.8884, lr_0 = 8.5180e-04
Loss = 1.7159e-03, PNorm = 40.2368, GNorm = 1.3457, lr_0 = 8.5092e-04
Loss = 1.8769e-03, PNorm = 40.2556, GNorm = 1.0909, lr_0 = 8.5003e-04
Loss = 1.5969e-03, PNorm = 40.2848, GNorm = 0.7446, lr_0 = 8.4914e-04
Loss = 1.5320e-03, PNorm = 40.3041, GNorm = 0.8242, lr_0 = 8.4825e-04
Validation rmse logD = 0.663055
Validation R2 logD = 0.692524
Validation rmse logP = 0.519441
Validation R2 logP = 0.925725
Epoch 9
Train function
Loss = 7.9141e-04, PNorm = 40.3357, GNorm = 0.4673, lr_0 = 8.4737e-04
Loss = 1.3950e-03, PNorm = 40.3620, GNorm = 1.5063, lr_0 = 8.4648e-04
Loss = 1.4257e-03, PNorm = 40.3867, GNorm = 0.8656, lr_0 = 8.4560e-04
Loss = 1.0844e-03, PNorm = 40.4072, GNorm = 0.7841, lr_0 = 8.4472e-04
Loss = 1.2958e-03, PNorm = 40.4250, GNorm = 1.2089, lr_0 = 8.4384e-04
Loss = 1.3842e-03, PNorm = 40.4460, GNorm = 0.6705, lr_0 = 8.4296e-04
Loss = 1.5725e-03, PNorm = 40.4697, GNorm = 1.2918, lr_0 = 8.4208e-04
Loss = 1.5348e-03, PNorm = 40.4901, GNorm = 1.1498, lr_0 = 8.4120e-04
Loss = 1.5106e-03, PNorm = 40.5126, GNorm = 0.6244, lr_0 = 8.4032e-04
Loss = 1.7415e-03, PNorm = 40.5358, GNorm = 1.4120, lr_0 = 8.3944e-04
Loss = 1.5589e-03, PNorm = 40.5650, GNorm = 0.6583, lr_0 = 8.3857e-04
Loss = 1.7201e-03, PNorm = 40.5946, GNorm = 0.7958, lr_0 = 8.3769e-04
Loss = 1.4608e-03, PNorm = 40.6142, GNorm = 0.6664, lr_0 = 8.3682e-04
Loss = 1.3269e-03, PNorm = 40.6310, GNorm = 0.7539, lr_0 = 8.3594e-04
Loss = 1.7345e-03, PNorm = 40.6533, GNorm = 1.3251, lr_0 = 8.3507e-04
Loss = 1.4997e-03, PNorm = 40.6859, GNorm = 1.1627, lr_0 = 8.3420e-04
Loss = 1.9569e-03, PNorm = 40.7043, GNorm = 1.0967, lr_0 = 8.3333e-04
Loss = 1.5133e-03, PNorm = 40.7304, GNorm = 1.6241, lr_0 = 8.3246e-04
Loss = 1.3198e-03, PNorm = 40.7630, GNorm = 0.5829, lr_0 = 8.3159e-04
Loss = 1.5745e-03, PNorm = 40.7801, GNorm = 0.7273, lr_0 = 8.3072e-04
Loss = 1.4949e-03, PNorm = 40.8038, GNorm = 1.2576, lr_0 = 8.2986e-04
Loss = 1.6289e-03, PNorm = 40.8162, GNorm = 0.8191, lr_0 = 8.2899e-04
Loss = 1.5520e-03, PNorm = 40.8375, GNorm = 0.6177, lr_0 = 8.2812e-04
Validation rmse logD = 0.719077
Validation R2 logD = 0.638371
Validation rmse logP = 0.511442
Validation R2 logP = 0.927994
Epoch 10
Train function
Loss = 1.5123e-03, PNorm = 40.8584, GNorm = 1.7028, lr_0 = 8.2717e-04
Loss = 1.4497e-03, PNorm = 40.8724, GNorm = 1.4741, lr_0 = 8.2631e-04
Loss = 1.3188e-03, PNorm = 40.8962, GNorm = 0.8155, lr_0 = 8.2545e-04
Loss = 1.4393e-03, PNorm = 40.9279, GNorm = 0.7508, lr_0 = 8.2459e-04
Loss = 1.6674e-03, PNorm = 40.9548, GNorm = 0.6739, lr_0 = 8.2373e-04
Loss = 1.1952e-03, PNorm = 40.9766, GNorm = 0.7035, lr_0 = 8.2287e-04
Loss = 1.4609e-03, PNorm = 41.0034, GNorm = 0.9022, lr_0 = 8.2201e-04
Loss = 1.5111e-03, PNorm = 41.0297, GNorm = 1.3652, lr_0 = 8.2115e-04
Loss = 1.6068e-03, PNorm = 41.0526, GNorm = 0.8141, lr_0 = 8.2029e-04
Loss = 1.3943e-03, PNorm = 41.0732, GNorm = 1.3531, lr_0 = 8.1944e-04
Loss = 1.4310e-03, PNorm = 41.1063, GNorm = 0.4838, lr_0 = 8.1858e-04
Loss = 1.1070e-03, PNorm = 41.1324, GNorm = 1.1204, lr_0 = 8.1773e-04
Loss = 1.3845e-03, PNorm = 41.1599, GNorm = 0.7002, lr_0 = 8.1687e-04
Loss = 1.7358e-03, PNorm = 41.1940, GNorm = 0.5885, lr_0 = 8.1602e-04
Loss = 1.5914e-03, PNorm = 41.2190, GNorm = 0.6711, lr_0 = 8.1517e-04
Loss = 1.6033e-03, PNorm = 41.2377, GNorm = 0.9348, lr_0 = 8.1432e-04
Loss = 1.3997e-03, PNorm = 41.2580, GNorm = 0.5675, lr_0 = 8.1347e-04
Loss = 1.3577e-03, PNorm = 41.2831, GNorm = 0.8200, lr_0 = 8.1262e-04
Loss = 1.1424e-03, PNorm = 41.3010, GNorm = 0.4920, lr_0 = 8.1177e-04
Loss = 1.3693e-03, PNorm = 41.3174, GNorm = 0.6688, lr_0 = 8.1092e-04
Loss = 1.3373e-03, PNorm = 41.3315, GNorm = 0.6560, lr_0 = 8.1008e-04
Loss = 1.7498e-03, PNorm = 41.3548, GNorm = 0.4593, lr_0 = 8.0923e-04
Loss = 1.4316e-03, PNorm = 41.3799, GNorm = 0.6072, lr_0 = 8.0839e-04
Validation rmse logD = 0.672780
Validation R2 logD = 0.683439
Validation rmse logP = 0.499437
Validation R2 logP = 0.931335
Epoch 11
Train function
Loss = 1.1962e-03, PNorm = 41.3985, GNorm = 1.1785, lr_0 = 8.0754e-04
Loss = 1.0669e-03, PNorm = 41.4141, GNorm = 0.5019, lr_0 = 8.0670e-04
Loss = 1.3766e-03, PNorm = 41.4421, GNorm = 0.3642, lr_0 = 8.0586e-04
Loss = 1.3436e-03, PNorm = 41.4613, GNorm = 0.5124, lr_0 = 8.0502e-04
Loss = 1.3518e-03, PNorm = 41.4863, GNorm = 0.7896, lr_0 = 8.0418e-04
Loss = 1.5096e-03, PNorm = 41.5148, GNorm = 0.9988, lr_0 = 8.0334e-04
Loss = 1.3866e-03, PNorm = 41.5411, GNorm = 1.0166, lr_0 = 8.0250e-04
Loss = 1.1837e-03, PNorm = 41.5544, GNorm = 0.7855, lr_0 = 8.0166e-04
Loss = 1.1660e-03, PNorm = 41.5722, GNorm = 0.7470, lr_0 = 8.0082e-04
Loss = 1.1653e-03, PNorm = 41.5922, GNorm = 0.9974, lr_0 = 7.9999e-04
Loss = 1.0279e-03, PNorm = 41.6129, GNorm = 0.6145, lr_0 = 7.9915e-04
Loss = 1.2354e-03, PNorm = 41.6295, GNorm = 0.8629, lr_0 = 7.9832e-04
Loss = 1.3967e-03, PNorm = 41.6438, GNorm = 1.4015, lr_0 = 7.9749e-04
Loss = 1.5024e-03, PNorm = 41.6687, GNorm = 1.6413, lr_0 = 7.9665e-04
Loss = 1.3092e-03, PNorm = 41.7010, GNorm = 0.6214, lr_0 = 7.9582e-04
Loss = 1.5275e-03, PNorm = 41.7265, GNorm = 1.5335, lr_0 = 7.9499e-04
Loss = 1.1113e-03, PNorm = 41.7588, GNorm = 1.5540, lr_0 = 7.9416e-04
Loss = 1.4919e-03, PNorm = 41.7829, GNorm = 0.5457, lr_0 = 7.9333e-04
Loss = 1.6123e-03, PNorm = 41.8082, GNorm = 0.9642, lr_0 = 7.9251e-04
Loss = 1.1870e-03, PNorm = 41.8237, GNorm = 0.6089, lr_0 = 7.9168e-04
Loss = 1.7556e-03, PNorm = 41.8452, GNorm = 1.4757, lr_0 = 7.9085e-04
Loss = 1.6652e-03, PNorm = 41.8694, GNorm = 1.5301, lr_0 = 7.9003e-04
Validation rmse logD = 0.717849
Validation R2 logD = 0.639605
Validation rmse logP = 0.610379
Validation R2 logP = 0.897441
Epoch 12
Train function
Loss = 1.2378e-03, PNorm = 41.8915, GNorm = 1.8148, lr_0 = 7.8912e-04
Loss = 1.4159e-03, PNorm = 41.9190, GNorm = 0.9253, lr_0 = 7.8830e-04
Loss = 1.1736e-03, PNorm = 41.9466, GNorm = 0.8079, lr_0 = 7.8747e-04
Loss = 1.2429e-03, PNorm = 41.9699, GNorm = 1.2943, lr_0 = 7.8665e-04
Loss = 1.1462e-03, PNorm = 41.9989, GNorm = 0.4395, lr_0 = 7.8583e-04
Loss = 1.2592e-03, PNorm = 42.0181, GNorm = 0.9023, lr_0 = 7.8501e-04
Loss = 1.1841e-03, PNorm = 42.0355, GNorm = 0.8178, lr_0 = 7.8419e-04
Loss = 9.1932e-04, PNorm = 42.0517, GNorm = 0.9985, lr_0 = 7.8337e-04
Loss = 1.1366e-03, PNorm = 42.0605, GNorm = 0.8817, lr_0 = 7.8255e-04
Loss = 1.7977e-03, PNorm = 42.0782, GNorm = 1.7878, lr_0 = 7.8174e-04
Loss = 1.5463e-03, PNorm = 42.1124, GNorm = 0.8063, lr_0 = 7.8092e-04
Loss = 1.3120e-03, PNorm = 42.1349, GNorm = 1.1122, lr_0 = 7.8011e-04
Loss = 1.0248e-03, PNorm = 42.1523, GNorm = 0.4231, lr_0 = 7.7929e-04
Loss = 1.2203e-03, PNorm = 42.1730, GNorm = 1.5189, lr_0 = 7.7848e-04
Loss = 1.0981e-03, PNorm = 42.1928, GNorm = 0.9081, lr_0 = 7.7767e-04
Loss = 1.1593e-03, PNorm = 42.2166, GNorm = 0.4281, lr_0 = 7.7686e-04
Loss = 1.1607e-03, PNorm = 42.2384, GNorm = 0.6640, lr_0 = 7.7604e-04
Loss = 1.0677e-03, PNorm = 42.2565, GNorm = 0.9647, lr_0 = 7.7523e-04
Loss = 1.3928e-03, PNorm = 42.2681, GNorm = 0.5055, lr_0 = 7.7443e-04
Loss = 1.1092e-03, PNorm = 42.2886, GNorm = 0.6940, lr_0 = 7.7362e-04
Loss = 1.2764e-03, PNorm = 42.3093, GNorm = 0.6600, lr_0 = 7.7281e-04
Loss = 9.6936e-04, PNorm = 42.3249, GNorm = 1.3981, lr_0 = 7.7200e-04
Loss = 1.0547e-03, PNorm = 42.3463, GNorm = 0.7416, lr_0 = 7.7120e-04
Validation rmse logD = 0.674060
Validation R2 logD = 0.682233
Validation rmse logP = 0.490265
Validation R2 logP = 0.933834
Epoch 13
Train function
Loss = 1.2351e-03, PNorm = 42.3646, GNorm = 0.8981, lr_0 = 7.7039e-04
Loss = 1.0045e-03, PNorm = 42.3846, GNorm = 0.5955, lr_0 = 7.6959e-04
Loss = 1.0028e-03, PNorm = 42.3939, GNorm = 0.6924, lr_0 = 7.6879e-04
Loss = 9.8918e-04, PNorm = 42.4112, GNorm = 1.0526, lr_0 = 7.6798e-04
Loss = 1.1537e-03, PNorm = 42.4347, GNorm = 1.4137, lr_0 = 7.6718e-04
Loss = 9.7469e-04, PNorm = 42.4522, GNorm = 0.7463, lr_0 = 7.6638e-04
Loss = 1.0026e-03, PNorm = 42.4646, GNorm = 0.8745, lr_0 = 7.6558e-04
Loss = 1.1769e-03, PNorm = 42.4861, GNorm = 0.8689, lr_0 = 7.6478e-04
Loss = 1.1703e-03, PNorm = 42.4992, GNorm = 0.8645, lr_0 = 7.6398e-04
Loss = 1.0958e-03, PNorm = 42.5266, GNorm = 1.1213, lr_0 = 7.6319e-04
Loss = 1.1603e-03, PNorm = 42.5466, GNorm = 0.7447, lr_0 = 7.6239e-04
Loss = 1.0422e-03, PNorm = 42.5705, GNorm = 0.4825, lr_0 = 7.6159e-04
Loss = 1.1863e-03, PNorm = 42.5919, GNorm = 0.7388, lr_0 = 7.6080e-04
Loss = 1.5003e-03, PNorm = 42.6088, GNorm = 0.9147, lr_0 = 7.6000e-04
Loss = 1.1679e-03, PNorm = 42.6329, GNorm = 0.7571, lr_0 = 7.5921e-04
Loss = 1.1019e-03, PNorm = 42.6586, GNorm = 0.8184, lr_0 = 7.5842e-04
Loss = 1.0876e-03, PNorm = 42.6785, GNorm = 0.5443, lr_0 = 7.5763e-04
Loss = 1.1760e-03, PNorm = 42.6931, GNorm = 0.4639, lr_0 = 7.5684e-04
Loss = 9.6726e-04, PNorm = 42.7130, GNorm = 0.7494, lr_0 = 7.5605e-04
Loss = 1.3761e-03, PNorm = 42.7434, GNorm = 0.9508, lr_0 = 7.5526e-04
Loss = 1.2535e-03, PNorm = 42.7651, GNorm = 0.6583, lr_0 = 7.5447e-04
Loss = 1.1460e-03, PNorm = 42.7906, GNorm = 0.5074, lr_0 = 7.5368e-04
Validation rmse logD = 0.655549
Validation R2 logD = 0.699446
Validation rmse logP = 0.487615
Validation R2 logP = 0.934547
Epoch 14
Train function
Loss = 6.6587e-04, PNorm = 42.8182, GNorm = 0.6405, lr_0 = 7.5282e-04
Loss = 9.3246e-04, PNorm = 42.8436, GNorm = 0.5876, lr_0 = 7.5203e-04
Loss = 9.8021e-04, PNorm = 42.8639, GNorm = 0.4065, lr_0 = 7.5125e-04
Loss = 8.6732e-04, PNorm = 42.8787, GNorm = 0.7288, lr_0 = 7.5046e-04
Loss = 1.2643e-03, PNorm = 42.8951, GNorm = 1.1569, lr_0 = 7.4968e-04
Loss = 1.3784e-03, PNorm = 42.9174, GNorm = 1.1889, lr_0 = 7.4890e-04
Loss = 1.0188e-03, PNorm = 42.9407, GNorm = 0.6671, lr_0 = 7.4811e-04
Loss = 1.1796e-03, PNorm = 42.9678, GNorm = 1.1645, lr_0 = 7.4733e-04
Loss = 1.0115e-03, PNorm = 42.9912, GNorm = 0.9592, lr_0 = 7.4655e-04
Loss = 1.0844e-03, PNorm = 43.0062, GNorm = 0.4922, lr_0 = 7.4577e-04
Loss = 1.3220e-03, PNorm = 43.0202, GNorm = 1.2869, lr_0 = 7.4500e-04
Loss = 1.1136e-03, PNorm = 43.0471, GNorm = 1.6421, lr_0 = 7.4422e-04
Loss = 1.1186e-03, PNorm = 43.0722, GNorm = 0.4621, lr_0 = 7.4344e-04
Loss = 9.6763e-04, PNorm = 43.0978, GNorm = 0.6477, lr_0 = 7.4267e-04
Loss = 9.6806e-04, PNorm = 43.1157, GNorm = 1.1017, lr_0 = 7.4189e-04
Loss = 9.3137e-04, PNorm = 43.1390, GNorm = 0.9774, lr_0 = 7.4112e-04
Loss = 1.0708e-03, PNorm = 43.1655, GNorm = 0.6503, lr_0 = 7.4034e-04
Loss = 7.6975e-04, PNorm = 43.1892, GNorm = 1.0308, lr_0 = 7.3957e-04
Loss = 9.5974e-04, PNorm = 43.2002, GNorm = 0.5479, lr_0 = 7.3880e-04
Loss = 9.0013e-04, PNorm = 43.2159, GNorm = 1.0128, lr_0 = 7.3803e-04
Loss = 8.0839e-04, PNorm = 43.2270, GNorm = 0.4005, lr_0 = 7.3726e-04
Loss = 9.9025e-04, PNorm = 43.2392, GNorm = 0.9986, lr_0 = 7.3649e-04
Loss = 9.6846e-04, PNorm = 43.2552, GNorm = 0.6047, lr_0 = 7.3572e-04
Validation rmse logD = 0.643053
Validation R2 logD = 0.710795
Validation rmse logP = 0.487234
Validation R2 logP = 0.934649
Epoch 15
Train function
Loss = 7.5805e-04, PNorm = 43.2711, GNorm = 0.9447, lr_0 = 7.3495e-04
Loss = 8.7945e-04, PNorm = 43.2792, GNorm = 0.9091, lr_0 = 7.3418e-04
Loss = 7.8140e-04, PNorm = 43.2962, GNorm = 0.8293, lr_0 = 7.3342e-04
Loss = 1.0177e-03, PNorm = 43.3104, GNorm = 0.4398, lr_0 = 7.3265e-04
Loss = 9.6188e-04, PNorm = 43.3383, GNorm = 0.4967, lr_0 = 7.3189e-04
Loss = 9.5274e-04, PNorm = 43.3576, GNorm = 0.4857, lr_0 = 7.3112e-04
Loss = 9.4593e-04, PNorm = 43.3779, GNorm = 0.7763, lr_0 = 7.3036e-04
Loss = 8.1888e-04, PNorm = 43.3975, GNorm = 0.6449, lr_0 = 7.2960e-04
Loss = 8.7284e-04, PNorm = 43.4184, GNorm = 0.6124, lr_0 = 7.2884e-04
Loss = 7.2416e-04, PNorm = 43.4338, GNorm = 0.6025, lr_0 = 7.2808e-04
Loss = 8.9644e-04, PNorm = 43.4495, GNorm = 0.6958, lr_0 = 7.2732e-04
Loss = 7.9102e-04, PNorm = 43.4618, GNorm = 0.5783, lr_0 = 7.2656e-04
Loss = 9.6176e-04, PNorm = 43.4789, GNorm = 1.2970, lr_0 = 7.2580e-04
Loss = 8.8066e-04, PNorm = 43.4886, GNorm = 0.5975, lr_0 = 7.2504e-04
Loss = 8.8670e-04, PNorm = 43.5076, GNorm = 0.6222, lr_0 = 7.2428e-04
Loss = 8.1105e-04, PNorm = 43.5209, GNorm = 0.4339, lr_0 = 7.2353e-04
Loss = 8.6664e-04, PNorm = 43.5400, GNorm = 0.5044, lr_0 = 7.2277e-04
Loss = 8.1607e-04, PNorm = 43.5522, GNorm = 0.2981, lr_0 = 7.2202e-04
Loss = 9.4491e-04, PNorm = 43.5641, GNorm = 1.0245, lr_0 = 7.2127e-04
Loss = 1.0671e-03, PNorm = 43.5891, GNorm = 0.6156, lr_0 = 7.2051e-04
Loss = 1.0001e-03, PNorm = 43.6098, GNorm = 0.7419, lr_0 = 7.1976e-04
Loss = 9.6079e-04, PNorm = 43.6262, GNorm = 0.5125, lr_0 = 7.1901e-04
Validation rmse logD = 0.655943
Validation R2 logD = 0.699085
Validation rmse logP = 0.499952
Validation R2 logP = 0.931193
Epoch 16
Train function
Loss = 6.8623e-04, PNorm = 43.6476, GNorm = 0.5544, lr_0 = 7.1818e-04
Loss = 5.7685e-04, PNorm = 43.6620, GNorm = 0.4348, lr_0 = 7.1743e-04
Loss = 7.7859e-04, PNorm = 43.6793, GNorm = 0.7022, lr_0 = 7.1669e-04
Loss = 8.0402e-04, PNorm = 43.6892, GNorm = 0.9156, lr_0 = 7.1594e-04
Loss = 7.8123e-04, PNorm = 43.7093, GNorm = 0.7191, lr_0 = 7.1519e-04
Loss = 9.0066e-04, PNorm = 43.7275, GNorm = 1.0781, lr_0 = 7.1444e-04
Loss = 9.0366e-04, PNorm = 43.7404, GNorm = 0.4510, lr_0 = 7.1370e-04
Loss = 9.2458e-04, PNorm = 43.7632, GNorm = 0.5169, lr_0 = 7.1295e-04
Loss = 9.9226e-04, PNorm = 43.7838, GNorm = 1.0018, lr_0 = 7.1221e-04
Loss = 7.9525e-04, PNorm = 43.7986, GNorm = 0.7029, lr_0 = 7.1147e-04
Loss = 8.1016e-04, PNorm = 43.8170, GNorm = 0.5073, lr_0 = 7.1072e-04
Loss = 1.0050e-03, PNorm = 43.8356, GNorm = 0.5870, lr_0 = 7.0998e-04
Loss = 9.3854e-04, PNorm = 43.8593, GNorm = 1.3374, lr_0 = 7.0924e-04
Loss = 1.1099e-03, PNorm = 43.8875, GNorm = 0.7790, lr_0 = 7.0850e-04
Loss = 9.6431e-04, PNorm = 43.9123, GNorm = 0.5750, lr_0 = 7.0776e-04
Loss = 1.0838e-03, PNorm = 43.9296, GNorm = 1.6291, lr_0 = 7.0702e-04
Loss = 9.5448e-04, PNorm = 43.9537, GNorm = 1.2088, lr_0 = 7.0628e-04
Loss = 1.0661e-03, PNorm = 43.9685, GNorm = 1.0932, lr_0 = 7.0555e-04
Loss = 1.0614e-03, PNorm = 43.9867, GNorm = 0.9094, lr_0 = 7.0481e-04
Loss = 1.0246e-03, PNorm = 44.0182, GNorm = 1.2691, lr_0 = 7.0408e-04
Loss = 8.8451e-04, PNorm = 44.0484, GNorm = 0.4305, lr_0 = 7.0334e-04
Loss = 7.5209e-04, PNorm = 44.0680, GNorm = 0.7650, lr_0 = 7.0261e-04
Loss = 7.8880e-04, PNorm = 44.0853, GNorm = 0.6526, lr_0 = 7.0187e-04
Validation rmse logD = 0.656234
Validation R2 logD = 0.698817
Validation rmse logP = 0.481769
Validation R2 logP = 0.936107
Epoch 17
Train function
Loss = 7.0742e-04, PNorm = 44.1019, GNorm = 0.7175, lr_0 = 7.0114e-04
Loss = 7.3873e-04, PNorm = 44.1234, GNorm = 0.5549, lr_0 = 7.0041e-04
Loss = 7.1603e-04, PNorm = 44.1399, GNorm = 0.4100, lr_0 = 6.9968e-04
Loss = 8.4234e-04, PNorm = 44.1504, GNorm = 0.4501, lr_0 = 6.9895e-04
Loss = 1.0329e-03, PNorm = 44.1726, GNorm = 0.4907, lr_0 = 6.9822e-04
Loss = 7.9919e-04, PNorm = 44.1931, GNorm = 0.7108, lr_0 = 6.9749e-04
Loss = 6.3988e-04, PNorm = 44.2092, GNorm = 0.7880, lr_0 = 6.9676e-04
Loss = 7.6076e-04, PNorm = 44.2238, GNorm = 0.7204, lr_0 = 6.9603e-04
Loss = 7.3267e-04, PNorm = 44.2338, GNorm = 0.4523, lr_0 = 6.9531e-04
Loss = 7.2898e-04, PNorm = 44.2496, GNorm = 0.8226, lr_0 = 6.9458e-04
Loss = 7.3552e-04, PNorm = 44.2616, GNorm = 0.7456, lr_0 = 6.9386e-04
Loss = 8.0937e-04, PNorm = 44.2760, GNorm = 0.6147, lr_0 = 6.9313e-04
Loss = 8.2779e-04, PNorm = 44.2918, GNorm = 0.5342, lr_0 = 6.9241e-04
Loss = 7.7700e-04, PNorm = 44.3026, GNorm = 0.5142, lr_0 = 6.9169e-04
Loss = 8.7422e-04, PNorm = 44.3166, GNorm = 0.5579, lr_0 = 6.9096e-04
Loss = 7.6080e-04, PNorm = 44.3368, GNorm = 0.4808, lr_0 = 6.9024e-04
Loss = 7.6699e-04, PNorm = 44.3514, GNorm = 0.5968, lr_0 = 6.8952e-04
Loss = 8.3159e-04, PNorm = 44.3723, GNorm = 0.4237, lr_0 = 6.8880e-04
Loss = 7.2105e-04, PNorm = 44.3892, GNorm = 0.6253, lr_0 = 6.8808e-04
Loss = 6.8458e-04, PNorm = 44.4018, GNorm = 0.6552, lr_0 = 6.8737e-04
Loss = 6.6437e-04, PNorm = 44.4138, GNorm = 0.5469, lr_0 = 6.8665e-04
Loss = 7.3932e-04, PNorm = 44.4245, GNorm = 0.3309, lr_0 = 6.8593e-04
Validation rmse logD = 0.646970
Validation R2 logD = 0.707261
Validation rmse logP = 0.472845
Validation R2 logP = 0.938452
Epoch 18
Train function
Loss = 6.3652e-04, PNorm = 44.4445, GNorm = 0.3890, lr_0 = 6.8514e-04
Loss = 5.3410e-04, PNorm = 44.4621, GNorm = 0.3841, lr_0 = 6.8443e-04
Loss = 5.9116e-04, PNorm = 44.4729, GNorm = 0.3932, lr_0 = 6.8372e-04
Loss = 5.1579e-04, PNorm = 44.4875, GNorm = 0.4919, lr_0 = 6.8300e-04
Loss = 7.5353e-04, PNorm = 44.5016, GNorm = 0.6422, lr_0 = 6.8229e-04
Loss = 7.1860e-04, PNorm = 44.5138, GNorm = 0.7980, lr_0 = 6.8158e-04
Loss = 6.8881e-04, PNorm = 44.5240, GNorm = 0.6963, lr_0 = 6.8087e-04
Loss = 7.7490e-04, PNorm = 44.5461, GNorm = 0.4079, lr_0 = 6.8015e-04
Loss = 6.1747e-04, PNorm = 44.5609, GNorm = 0.9554, lr_0 = 6.7944e-04
Loss = 6.7496e-04, PNorm = 44.5743, GNorm = 0.9051, lr_0 = 6.7874e-04
Loss = 7.8081e-04, PNorm = 44.5880, GNorm = 0.6084, lr_0 = 6.7803e-04
Loss = 8.3494e-04, PNorm = 44.5986, GNorm = 1.0464, lr_0 = 6.7732e-04
Loss = 7.6837e-04, PNorm = 44.6123, GNorm = 0.5473, lr_0 = 6.7661e-04
Loss = 7.1896e-04, PNorm = 44.6297, GNorm = 0.8660, lr_0 = 6.7591e-04
Loss = 6.2340e-04, PNorm = 44.6466, GNorm = 0.4334, lr_0 = 6.7520e-04
Loss = 7.0937e-04, PNorm = 44.6652, GNorm = 0.6054, lr_0 = 6.7450e-04
Loss = 7.2011e-04, PNorm = 44.6800, GNorm = 0.5872, lr_0 = 6.7379e-04
Loss = 6.5764e-04, PNorm = 44.6935, GNorm = 0.4243, lr_0 = 6.7309e-04
Loss = 6.8294e-04, PNorm = 44.7079, GNorm = 0.5579, lr_0 = 6.7239e-04
Loss = 8.8069e-04, PNorm = 44.7246, GNorm = 0.8958, lr_0 = 6.7168e-04
Loss = 8.5572e-04, PNorm = 44.7432, GNorm = 0.7675, lr_0 = 6.7098e-04
Loss = 7.8789e-04, PNorm = 44.7645, GNorm = 0.7480, lr_0 = 6.7028e-04
Loss = 9.8140e-04, PNorm = 44.7798, GNorm = 1.2652, lr_0 = 6.6958e-04
Validation rmse logD = 0.644513
Validation R2 logD = 0.709480
Validation rmse logP = 0.530784
Validation R2 logP = 0.922445
Epoch 19
Train function
Loss = 6.4000e-04, PNorm = 44.7957, GNorm = 0.5969, lr_0 = 6.6889e-04
Loss = 6.3082e-04, PNorm = 44.8143, GNorm = 0.6368, lr_0 = 6.6819e-04
Loss = 6.6694e-04, PNorm = 44.8337, GNorm = 0.5125, lr_0 = 6.6749e-04
Loss = 6.0463e-04, PNorm = 44.8480, GNorm = 0.5369, lr_0 = 6.6679e-04
Loss = 7.5614e-04, PNorm = 44.8573, GNorm = 0.8142, lr_0 = 6.6610e-04
Loss = 6.3847e-04, PNorm = 44.8718, GNorm = 1.1988, lr_0 = 6.6540e-04
Loss = 8.3334e-04, PNorm = 44.8850, GNorm = 0.5159, lr_0 = 6.6471e-04
Loss = 7.8949e-04, PNorm = 44.9110, GNorm = 0.6845, lr_0 = 6.6401e-04
Loss = 7.5881e-04, PNorm = 44.9314, GNorm = 0.4813, lr_0 = 6.6332e-04
Loss = 7.4680e-04, PNorm = 44.9491, GNorm = 0.9024, lr_0 = 6.6263e-04
Loss = 6.2566e-04, PNorm = 44.9635, GNorm = 0.5728, lr_0 = 6.6194e-04
Loss = 5.4451e-04, PNorm = 44.9772, GNorm = 0.5316, lr_0 = 6.6125e-04
Loss = 5.6796e-04, PNorm = 44.9874, GNorm = 0.7531, lr_0 = 6.6056e-04
Loss = 7.2511e-04, PNorm = 44.9966, GNorm = 0.3127, lr_0 = 6.5987e-04
Loss = 5.9268e-04, PNorm = 45.0076, GNorm = 0.5796, lr_0 = 6.5918e-04
Loss = 6.1665e-04, PNorm = 45.0190, GNorm = 0.7357, lr_0 = 6.5849e-04
Loss = 7.1816e-04, PNorm = 45.0314, GNorm = 0.9348, lr_0 = 6.5780e-04
Loss = 6.6561e-04, PNorm = 45.0516, GNorm = 0.3600, lr_0 = 6.5712e-04
Loss = 6.7516e-04, PNorm = 45.0634, GNorm = 0.4237, lr_0 = 6.5643e-04
Loss = 6.3745e-04, PNorm = 45.0783, GNorm = 0.5958, lr_0 = 6.5574e-04
Loss = 7.8347e-04, PNorm = 45.0976, GNorm = 0.6250, lr_0 = 6.5506e-04
Loss = 7.0458e-04, PNorm = 45.1152, GNorm = 0.5712, lr_0 = 6.5438e-04
Validation rmse logD = 0.666950
Validation R2 logD = 0.688901
Validation rmse logP = 0.470599
Validation R2 logP = 0.939036
Epoch 20
Train function
Loss = 5.7435e-04, PNorm = 45.1318, GNorm = 0.8311, lr_0 = 6.5363e-04
Loss = 5.1641e-04, PNorm = 45.1450, GNorm = 0.4620, lr_0 = 6.5294e-04
Loss = 5.5633e-04, PNorm = 45.1609, GNorm = 0.6039, lr_0 = 6.5226e-04
Loss = 5.4984e-04, PNorm = 45.1751, GNorm = 0.6782, lr_0 = 6.5158e-04
Loss = 4.4015e-04, PNorm = 45.1868, GNorm = 0.4851, lr_0 = 6.5090e-04
Loss = 6.3275e-04, PNorm = 45.1979, GNorm = 0.6246, lr_0 = 6.5022e-04
Loss = 5.8327e-04, PNorm = 45.2131, GNorm = 0.4279, lr_0 = 6.4954e-04
Loss = 6.6681e-04, PNorm = 45.2277, GNorm = 0.7611, lr_0 = 6.4886e-04
Loss = 5.3747e-04, PNorm = 45.2447, GNorm = 0.6605, lr_0 = 6.4819e-04
Loss = 6.3328e-04, PNorm = 45.2640, GNorm = 0.5013, lr_0 = 6.4751e-04
Loss = 6.4033e-04, PNorm = 45.2835, GNorm = 0.8953, lr_0 = 6.4684e-04
Loss = 5.9383e-04, PNorm = 45.3012, GNorm = 0.4933, lr_0 = 6.4616e-04
Loss = 7.2522e-04, PNorm = 45.3127, GNorm = 1.0175, lr_0 = 6.4549e-04
Loss = 7.3087e-04, PNorm = 45.3302, GNorm = 1.3861, lr_0 = 6.4481e-04
Loss = 6.2157e-04, PNorm = 45.3463, GNorm = 0.9281, lr_0 = 6.4414e-04
Loss = 8.3683e-04, PNorm = 45.3665, GNorm = 0.8846, lr_0 = 6.4347e-04
Loss = 6.2662e-04, PNorm = 45.3819, GNorm = 0.7557, lr_0 = 6.4280e-04
Loss = 8.2713e-04, PNorm = 45.3971, GNorm = 0.8443, lr_0 = 6.4212e-04
Loss = 7.2201e-04, PNorm = 45.4146, GNorm = 0.4015, lr_0 = 6.4145e-04
Loss = 8.5954e-04, PNorm = 45.4325, GNorm = 0.7904, lr_0 = 6.4078e-04
Loss = 7.0028e-04, PNorm = 45.4542, GNorm = 1.1263, lr_0 = 6.4012e-04
Loss = 6.6530e-04, PNorm = 45.4708, GNorm = 0.3324, lr_0 = 6.3945e-04
Loss = 5.6556e-04, PNorm = 45.4847, GNorm = 0.5584, lr_0 = 6.3878e-04
Validation rmse logD = 0.646029
Validation R2 logD = 0.708112
Validation rmse logP = 0.472561
Validation R2 logP = 0.938526
Epoch 21
Train function
Loss = 5.0931e-04, PNorm = 45.4990, GNorm = 0.4049, lr_0 = 6.3811e-04
Loss = 5.9007e-04, PNorm = 45.5152, GNorm = 0.3197, lr_0 = 6.3745e-04
Loss = 6.5883e-04, PNorm = 45.5340, GNorm = 0.7976, lr_0 = 6.3678e-04
Loss = 4.5973e-04, PNorm = 45.5474, GNorm = 0.3753, lr_0 = 6.3612e-04
Loss = 5.7441e-04, PNorm = 45.5562, GNorm = 0.4369, lr_0 = 6.3545e-04
Loss = 4.7902e-04, PNorm = 45.5707, GNorm = 0.5142, lr_0 = 6.3479e-04
Loss = 4.8837e-04, PNorm = 45.5817, GNorm = 0.6257, lr_0 = 6.3413e-04
Loss = 5.2894e-04, PNorm = 45.5930, GNorm = 0.7251, lr_0 = 6.3347e-04
Loss = 4.9128e-04, PNorm = 45.6050, GNorm = 0.4316, lr_0 = 6.3280e-04
Loss = 4.6828e-04, PNorm = 45.6153, GNorm = 0.7165, lr_0 = 6.3214e-04
Loss = 5.3094e-04, PNorm = 45.6292, GNorm = 0.4209, lr_0 = 6.3148e-04
Loss = 6.6354e-04, PNorm = 45.6451, GNorm = 0.5908, lr_0 = 6.3083e-04
Loss = 7.0777e-04, PNorm = 45.6618, GNorm = 1.2764, lr_0 = 6.3017e-04
Loss = 5.7899e-04, PNorm = 45.6714, GNorm = 0.6170, lr_0 = 6.2951e-04
Loss = 5.1098e-04, PNorm = 45.6869, GNorm = 0.3419, lr_0 = 6.2885e-04
Loss = 5.4723e-04, PNorm = 45.6998, GNorm = 0.3627, lr_0 = 6.2820e-04
Loss = 6.1214e-04, PNorm = 45.7119, GNorm = 0.4994, lr_0 = 6.2754e-04
Loss = 6.2605e-04, PNorm = 45.7257, GNorm = 0.6019, lr_0 = 6.2689e-04
Loss = 4.8131e-04, PNorm = 45.7449, GNorm = 0.3238, lr_0 = 6.2623e-04
Loss = 5.6557e-04, PNorm = 45.7565, GNorm = 0.7606, lr_0 = 6.2558e-04
Loss = 5.5317e-04, PNorm = 45.7645, GNorm = 0.5972, lr_0 = 6.2492e-04
Loss = 7.4447e-04, PNorm = 45.7729, GNorm = 0.9068, lr_0 = 6.2427e-04
Loss = 6.8920e-04, PNorm = 45.7867, GNorm = 0.5781, lr_0 = 6.2362e-04
Loss = 1.9912e-03, PNorm = 45.7889, GNorm = 1.2789, lr_0 = 6.2356e-04
Validation rmse logD = 0.655944
Validation R2 logD = 0.699084
Validation rmse logP = 0.469600
Validation R2 logP = 0.939294
Epoch 22
Train function
Loss = 6.5275e-04, PNorm = 45.8064, GNorm = 0.7535, lr_0 = 6.2290e-04
Loss = 5.8625e-04, PNorm = 45.8239, GNorm = 0.6753, lr_0 = 6.2225e-04
Loss = 5.1399e-04, PNorm = 45.8397, GNorm = 0.7295, lr_0 = 6.2161e-04
Loss = 5.3393e-04, PNorm = 45.8534, GNorm = 0.6087, lr_0 = 6.2096e-04
Loss = 6.7863e-04, PNorm = 45.8701, GNorm = 0.6466, lr_0 = 6.2031e-04
Loss = 6.0525e-04, PNorm = 45.8831, GNorm = 0.6143, lr_0 = 6.1966e-04
Loss = 5.5645e-04, PNorm = 45.9011, GNorm = 0.5871, lr_0 = 6.1901e-04
Loss = 4.5634e-04, PNorm = 45.9117, GNorm = 0.5028, lr_0 = 6.1837e-04
Loss = 4.8860e-04, PNorm = 45.9209, GNorm = 0.3807, lr_0 = 6.1772e-04
Loss = 4.3636e-04, PNorm = 45.9354, GNorm = 0.6246, lr_0 = 6.1708e-04
Loss = 5.8708e-04, PNorm = 45.9456, GNorm = 0.6191, lr_0 = 6.1643e-04
Loss = 6.9765e-04, PNorm = 45.9634, GNorm = 1.2652, lr_0 = 6.1579e-04
Loss = 8.5439e-04, PNorm = 45.9873, GNorm = 1.1126, lr_0 = 6.1515e-04
Loss = 8.8393e-04, PNorm = 46.0031, GNorm = 1.0196, lr_0 = 6.1451e-04
Loss = 7.1425e-04, PNorm = 46.0204, GNorm = 0.3240, lr_0 = 6.1386e-04
Loss = 7.7848e-04, PNorm = 46.0421, GNorm = 0.8067, lr_0 = 6.1322e-04
Loss = 8.7192e-04, PNorm = 46.0610, GNorm = 0.7589, lr_0 = 6.1258e-04
Loss = 6.9828e-04, PNorm = 46.0803, GNorm = 0.4448, lr_0 = 6.1194e-04
Loss = 5.2361e-04, PNorm = 46.0937, GNorm = 0.4233, lr_0 = 6.1131e-04
Loss = 5.7881e-04, PNorm = 46.1112, GNorm = 0.4461, lr_0 = 6.1067e-04
Loss = 4.8798e-04, PNorm = 46.1244, GNorm = 0.4605, lr_0 = 6.1003e-04
Loss = 6.1819e-04, PNorm = 46.1369, GNorm = 0.9646, lr_0 = 6.0939e-04
Validation rmse logD = 0.629033
Validation R2 logD = 0.723268
Validation rmse logP = 0.466401
Validation R2 logP = 0.940118
Epoch 23
Train function
Loss = 5.4000e-04, PNorm = 46.1475, GNorm = 0.9424, lr_0 = 6.0876e-04
Loss = 5.7004e-04, PNorm = 46.1604, GNorm = 0.4148, lr_0 = 6.0812e-04
Loss = 4.4710e-04, PNorm = 46.1734, GNorm = 0.4648, lr_0 = 6.0749e-04
Loss = 4.3779e-04, PNorm = 46.1836, GNorm = 0.4476, lr_0 = 6.0685e-04
Loss = 4.2561e-04, PNorm = 46.1959, GNorm = 0.5767, lr_0 = 6.0622e-04
Loss = 5.4672e-04, PNorm = 46.2091, GNorm = 0.4055, lr_0 = 6.0559e-04
Loss = 3.9967e-04, PNorm = 46.2247, GNorm = 0.5020, lr_0 = 6.0496e-04
Loss = 3.9448e-04, PNorm = 46.2336, GNorm = 0.4005, lr_0 = 6.0432e-04
Loss = 4.9794e-04, PNorm = 46.2493, GNorm = 0.5588, lr_0 = 6.0369e-04
Loss = 6.5193e-04, PNorm = 46.2573, GNorm = 0.8032, lr_0 = 6.0306e-04
Loss = 5.5990e-04, PNorm = 46.2708, GNorm = 0.4912, lr_0 = 6.0243e-04
Loss = 5.3669e-04, PNorm = 46.2826, GNorm = 0.6995, lr_0 = 6.0180e-04
Loss = 5.3635e-04, PNorm = 46.2988, GNorm = 0.3453, lr_0 = 6.0118e-04
Loss = 4.7287e-04, PNorm = 46.3120, GNorm = 0.6675, lr_0 = 6.0055e-04
Loss = 4.6447e-04, PNorm = 46.3269, GNorm = 0.3957, lr_0 = 5.9992e-04
Loss = 5.1184e-04, PNorm = 46.3350, GNorm = 1.0161, lr_0 = 5.9930e-04
Loss = 4.3617e-04, PNorm = 46.3459, GNorm = 0.5817, lr_0 = 5.9867e-04
Loss = 4.5858e-04, PNorm = 46.3567, GNorm = 0.2803, lr_0 = 5.9805e-04
Loss = 5.7654e-04, PNorm = 46.3732, GNorm = 0.4434, lr_0 = 5.9742e-04
Loss = 7.2072e-04, PNorm = 46.3823, GNorm = 0.8644, lr_0 = 5.9680e-04
Loss = 6.0157e-04, PNorm = 46.3998, GNorm = 0.3387, lr_0 = 5.9618e-04
Loss = 5.3292e-04, PNorm = 46.4181, GNorm = 0.6001, lr_0 = 5.9555e-04
Loss = 4.9691e-04, PNorm = 46.4332, GNorm = 0.5137, lr_0 = 5.9493e-04
Validation rmse logD = 0.629377
Validation R2 logD = 0.722965
Validation rmse logP = 0.470611
Validation R2 logP = 0.939032
Epoch 24
Train function
Loss = 5.0081e-04, PNorm = 46.4485, GNorm = 0.7124, lr_0 = 5.9425e-04
Loss = 4.2115e-04, PNorm = 46.4614, GNorm = 0.4577, lr_0 = 5.9363e-04
Loss = 4.6935e-04, PNorm = 46.4669, GNorm = 0.5369, lr_0 = 5.9301e-04
Loss = 4.0408e-04, PNorm = 46.4747, GNorm = 0.6283, lr_0 = 5.9239e-04
Loss = 4.0007e-04, PNorm = 46.4892, GNorm = 0.5523, lr_0 = 5.9177e-04
Loss = 4.2875e-04, PNorm = 46.5055, GNorm = 0.3115, lr_0 = 5.9115e-04
Loss = 5.5566e-04, PNorm = 46.5173, GNorm = 0.4219, lr_0 = 5.9054e-04
Loss = 3.9709e-04, PNorm = 46.5296, GNorm = 0.3574, lr_0 = 5.8992e-04
Loss = 4.5880e-04, PNorm = 46.5440, GNorm = 0.6004, lr_0 = 5.8931e-04
Loss = 6.4473e-04, PNorm = 46.5556, GNorm = 0.6175, lr_0 = 5.8869e-04
Loss = 4.2990e-04, PNorm = 46.5712, GNorm = 0.4088, lr_0 = 5.8808e-04
Loss = 5.2777e-04, PNorm = 46.5803, GNorm = 0.4174, lr_0 = 5.8746e-04
Loss = 4.3504e-04, PNorm = 46.5926, GNorm = 0.3198, lr_0 = 5.8685e-04
Loss = 4.3135e-04, PNorm = 46.6009, GNorm = 0.3593, lr_0 = 5.8624e-04
Loss = 4.1173e-04, PNorm = 46.6078, GNorm = 0.4065, lr_0 = 5.8562e-04
Loss = 4.8587e-04, PNorm = 46.6182, GNorm = 0.6606, lr_0 = 5.8501e-04
Loss = 4.8797e-04, PNorm = 46.6253, GNorm = 0.4537, lr_0 = 5.8440e-04
Loss = 4.9012e-04, PNorm = 46.6435, GNorm = 0.6205, lr_0 = 5.8379e-04
Loss = 4.6600e-04, PNorm = 46.6479, GNorm = 0.8132, lr_0 = 5.8318e-04
Loss = 5.5061e-04, PNorm = 46.6561, GNorm = 0.3454, lr_0 = 5.8257e-04
Loss = 4.3559e-04, PNorm = 46.6664, GNorm = 0.2348, lr_0 = 5.8197e-04
Loss = 5.1968e-04, PNorm = 46.6806, GNorm = 0.4744, lr_0 = 5.8136e-04
Validation rmse logD = 0.645652
Validation R2 logD = 0.708453
Validation rmse logP = 0.469347
Validation R2 logP = 0.939360
Epoch 25
Train function
Loss = 3.3416e-04, PNorm = 46.6950, GNorm = 0.3610, lr_0 = 5.8075e-04
Loss = 4.2246e-04, PNorm = 46.7092, GNorm = 0.2936, lr_0 = 5.8015e-04
Loss = 4.4205e-04, PNorm = 46.7231, GNorm = 1.0361, lr_0 = 5.7954e-04
Loss = 4.5719e-04, PNorm = 46.7362, GNorm = 0.5098, lr_0 = 5.7894e-04
Loss = 4.3591e-04, PNorm = 46.7542, GNorm = 0.6621, lr_0 = 5.7833e-04
Loss = 4.4766e-04, PNorm = 46.7663, GNorm = 0.4488, lr_0 = 5.7773e-04
Loss = 4.5704e-04, PNorm = 46.7803, GNorm = 0.3816, lr_0 = 5.7712e-04
Loss = 3.4940e-04, PNorm = 46.7892, GNorm = 0.3219, lr_0 = 5.7652e-04
Loss = 4.1333e-04, PNorm = 46.7992, GNorm = 0.3154, lr_0 = 5.7592e-04
Loss = 2.9067e-04, PNorm = 46.8049, GNorm = 0.3534, lr_0 = 5.7532e-04
Loss = 3.9753e-04, PNorm = 46.8160, GNorm = 0.3303, lr_0 = 5.7472e-04
Loss = 3.9384e-04, PNorm = 46.8241, GNorm = 0.5300, lr_0 = 5.7412e-04
Loss = 4.2403e-04, PNorm = 46.8331, GNorm = 0.3035, lr_0 = 5.7352e-04
Loss = 6.0023e-04, PNorm = 46.8461, GNorm = 0.5540, lr_0 = 5.7292e-04
Loss = 5.5971e-04, PNorm = 46.8634, GNorm = 0.4549, lr_0 = 5.7232e-04
Loss = 3.9204e-04, PNorm = 46.8760, GNorm = 0.5342, lr_0 = 5.7173e-04
Loss = 4.8331e-04, PNorm = 46.8855, GNorm = 0.6026, lr_0 = 5.7113e-04
Loss = 5.4371e-04, PNorm = 46.8910, GNorm = 0.5562, lr_0 = 5.7053e-04
Loss = 5.1783e-04, PNorm = 46.9052, GNorm = 0.3233, lr_0 = 5.6994e-04
Loss = 5.3959e-04, PNorm = 46.9220, GNorm = 0.4868, lr_0 = 5.6934e-04
Loss = 4.9261e-04, PNorm = 46.9366, GNorm = 0.7129, lr_0 = 5.6875e-04
Loss = 4.6247e-04, PNorm = 46.9469, GNorm = 0.7481, lr_0 = 5.6816e-04
Loss = 5.5934e-04, PNorm = 46.9576, GNorm = 0.6337, lr_0 = 5.6756e-04
Validation rmse logD = 0.640481
Validation R2 logD = 0.713104
Validation rmse logP = 0.471971
Validation R2 logP = 0.938680
Epoch 26
Train function
Loss = 4.7700e-04, PNorm = 46.9745, GNorm = 0.4607, lr_0 = 5.6691e-04
Loss = 4.6509e-04, PNorm = 46.9926, GNorm = 0.5101, lr_0 = 5.6632e-04
Loss = 3.3228e-04, PNorm = 47.0030, GNorm = 0.2280, lr_0 = 5.6573e-04
Loss = 3.6828e-04, PNorm = 47.0139, GNorm = 0.4145, lr_0 = 5.6514e-04
Loss = 3.4144e-04, PNorm = 47.0228, GNorm = 0.4274, lr_0 = 5.6455e-04
Loss = 4.3918e-04, PNorm = 47.0306, GNorm = 0.4055, lr_0 = 5.6396e-04
Loss = 4.4270e-04, PNorm = 47.0444, GNorm = 0.5136, lr_0 = 5.6337e-04
Loss = 3.6322e-04, PNorm = 47.0590, GNorm = 0.3832, lr_0 = 5.6278e-04
Loss = 3.9387e-04, PNorm = 47.0683, GNorm = 0.7920, lr_0 = 5.6219e-04
Loss = 4.1192e-04, PNorm = 47.0763, GNorm = 0.4364, lr_0 = 5.6161e-04
Loss = 4.2446e-04, PNorm = 47.0889, GNorm = 0.4073, lr_0 = 5.6102e-04
Loss = 4.4295e-04, PNorm = 47.0996, GNorm = 0.6197, lr_0 = 5.6044e-04
Loss = 4.7854e-04, PNorm = 47.1113, GNorm = 0.5519, lr_0 = 5.5985e-04
Loss = 4.4699e-04, PNorm = 47.1260, GNorm = 0.3913, lr_0 = 5.5927e-04
Loss = 4.7376e-04, PNorm = 47.1369, GNorm = 0.3950, lr_0 = 5.5868e-04
Loss = 4.0281e-04, PNorm = 47.1514, GNorm = 0.2930, lr_0 = 5.5810e-04
Loss = 4.6188e-04, PNorm = 47.1605, GNorm = 0.4008, lr_0 = 5.5752e-04
Loss = 3.8955e-04, PNorm = 47.1715, GNorm = 0.5121, lr_0 = 5.5694e-04
Loss = 3.9947e-04, PNorm = 47.1806, GNorm = 0.5396, lr_0 = 5.5635e-04
Loss = 4.1422e-04, PNorm = 47.1913, GNorm = 0.2995, lr_0 = 5.5577e-04
Loss = 4.1675e-04, PNorm = 47.2047, GNorm = 0.7440, lr_0 = 5.5519e-04
Loss = 3.7997e-04, PNorm = 47.2146, GNorm = 0.3102, lr_0 = 5.5461e-04
Validation rmse logD = 0.646409
Validation R2 logD = 0.707769
Validation rmse logP = 0.487668
Validation R2 logP = 0.934533
Epoch 27
Train function
Loss = 5.8637e-04, PNorm = 47.2232, GNorm = 0.5166, lr_0 = 5.5398e-04
Loss = 4.4814e-04, PNorm = 47.2326, GNorm = 0.3525, lr_0 = 5.5340e-04
Loss = 5.1538e-04, PNorm = 47.2395, GNorm = 0.7655, lr_0 = 5.5282e-04
Loss = 4.4060e-04, PNorm = 47.2517, GNorm = 1.1292, lr_0 = 5.5224e-04
Loss = 3.6307e-04, PNorm = 47.2650, GNorm = 0.6351, lr_0 = 5.5167e-04
Loss = 3.8585e-04, PNorm = 47.2749, GNorm = 0.2948, lr_0 = 5.5109e-04
Loss = 4.2800e-04, PNorm = 47.2912, GNorm = 0.7211, lr_0 = 5.5052e-04
Loss = 3.5952e-04, PNorm = 47.3056, GNorm = 0.9314, lr_0 = 5.4994e-04
Loss = 3.3988e-04, PNorm = 47.3131, GNorm = 0.3017, lr_0 = 5.4937e-04
Loss = 3.9940e-04, PNorm = 47.3191, GNorm = 0.5021, lr_0 = 5.4880e-04
Loss = 3.6544e-04, PNorm = 47.3292, GNorm = 0.3111, lr_0 = 5.4822e-04
Loss = 3.6709e-04, PNorm = 47.3385, GNorm = 0.6548, lr_0 = 5.4765e-04
Loss = 3.3837e-04, PNorm = 47.3494, GNorm = 0.5188, lr_0 = 5.4708e-04
Loss = 4.5628e-04, PNorm = 47.3580, GNorm = 0.7394, lr_0 = 5.4651e-04
Loss = 3.7192e-04, PNorm = 47.3711, GNorm = 0.3242, lr_0 = 5.4594e-04
Loss = 3.7263e-04, PNorm = 47.3858, GNorm = 0.4226, lr_0 = 5.4537e-04
Loss = 3.7865e-04, PNorm = 47.3941, GNorm = 0.5416, lr_0 = 5.4480e-04
Loss = 3.8521e-04, PNorm = 47.4003, GNorm = 0.5797, lr_0 = 5.4423e-04
Loss = 4.4388e-04, PNorm = 47.4097, GNorm = 0.6521, lr_0 = 5.4366e-04
Loss = 2.8531e-04, PNorm = 47.4171, GNorm = 0.5762, lr_0 = 5.4309e-04
Loss = 3.8592e-04, PNorm = 47.4254, GNorm = 0.3042, lr_0 = 5.4253e-04
Loss = 4.0323e-04, PNorm = 47.4326, GNorm = 0.3104, lr_0 = 5.4196e-04
Loss = 3.2845e-04, PNorm = 47.4361, GNorm = 0.3060, lr_0 = 5.4140e-04
Validation rmse logD = 0.616766
Validation R2 logD = 0.733956
Validation rmse logP = 0.460672
Validation R2 logP = 0.941581
Epoch 28
Train function
Loss = 2.8715e-04, PNorm = 47.4441, GNorm = 0.4455, lr_0 = 5.4083e-04
Loss = 3.0371e-04, PNorm = 47.4507, GNorm = 0.2355, lr_0 = 5.4027e-04
Loss = 3.1923e-04, PNorm = 47.4636, GNorm = 0.3942, lr_0 = 5.3970e-04
Loss = 2.9242e-04, PNorm = 47.4759, GNorm = 0.4858, lr_0 = 5.3914e-04
Loss = 2.9656e-04, PNorm = 47.4830, GNorm = 0.4147, lr_0 = 5.3858e-04
Loss = 2.7552e-04, PNorm = 47.4880, GNorm = 0.3369, lr_0 = 5.3801e-04
Loss = 2.8052e-04, PNorm = 47.4980, GNorm = 0.3446, lr_0 = 5.3745e-04
Loss = 3.1107e-04, PNorm = 47.5064, GNorm = 0.5229, lr_0 = 5.3689e-04
Loss = 3.5306e-04, PNorm = 47.5188, GNorm = 0.4354, lr_0 = 5.3633e-04
Loss = 3.0758e-04, PNorm = 47.5257, GNorm = 0.3475, lr_0 = 5.3577e-04
Loss = 3.9792e-04, PNorm = 47.5382, GNorm = 0.3113, lr_0 = 5.3521e-04
Loss = 3.1961e-04, PNorm = 47.5515, GNorm = 0.3582, lr_0 = 5.3465e-04
Loss = 3.7162e-04, PNorm = 47.5614, GNorm = 0.6864, lr_0 = 5.3410e-04
Loss = 4.3651e-04, PNorm = 47.5716, GNorm = 0.9855, lr_0 = 5.3354e-04
Loss = 4.9801e-04, PNorm = 47.5873, GNorm = 0.2921, lr_0 = 5.3298e-04
Loss = 4.4905e-04, PNorm = 47.5992, GNorm = 0.4003, lr_0 = 5.3243e-04
Loss = 3.8785e-04, PNorm = 47.6084, GNorm = 0.4053, lr_0 = 5.3187e-04
Loss = 4.7604e-04, PNorm = 47.6191, GNorm = 0.3631, lr_0 = 5.3131e-04
Loss = 3.7227e-04, PNorm = 47.6310, GNorm = 0.5162, lr_0 = 5.3076e-04
Loss = 3.3904e-04, PNorm = 47.6378, GNorm = 0.3784, lr_0 = 5.3021e-04
Loss = 3.1621e-04, PNorm = 47.6482, GNorm = 0.2993, lr_0 = 5.2965e-04
Loss = 3.7768e-04, PNorm = 47.6540, GNorm = 0.2336, lr_0 = 5.2910e-04
Validation rmse logD = 0.631403
Validation R2 logD = 0.721179
Validation rmse logP = 0.461950
Validation R2 logP = 0.941256
Epoch 29
Train function
Loss = 2.7955e-04, PNorm = 47.6576, GNorm = 0.2747, lr_0 = 5.2849e-04
Loss = 3.0041e-04, PNorm = 47.6706, GNorm = 0.2325, lr_0 = 5.2794e-04
Loss = 3.3261e-04, PNorm = 47.6787, GNorm = 0.2903, lr_0 = 5.2739e-04
Loss = 2.9932e-04, PNorm = 47.6852, GNorm = 0.3926, lr_0 = 5.2684e-04
Loss = 3.4300e-04, PNorm = 47.6928, GNorm = 0.4028, lr_0 = 5.2629e-04
Loss = 3.1997e-04, PNorm = 47.7034, GNorm = 0.3908, lr_0 = 5.2574e-04
Loss = 3.5544e-04, PNorm = 47.7141, GNorm = 0.4960, lr_0 = 5.2519e-04
Loss = 2.9719e-04, PNorm = 47.7227, GNorm = 0.3554, lr_0 = 5.2464e-04
Loss = 3.3245e-04, PNorm = 47.7348, GNorm = 0.5452, lr_0 = 5.2410e-04
Loss = 2.7621e-04, PNorm = 47.7415, GNorm = 0.7316, lr_0 = 5.2355e-04
Loss = 3.4029e-04, PNorm = 47.7509, GNorm = 0.5128, lr_0 = 5.2300e-04
Loss = 3.0394e-04, PNorm = 47.7619, GNorm = 0.4163, lr_0 = 5.2246e-04
Loss = 2.8256e-04, PNorm = 47.7741, GNorm = 0.2971, lr_0 = 5.2191e-04
Loss = 2.6228e-04, PNorm = 47.7837, GNorm = 0.2977, lr_0 = 5.2137e-04
Loss = 2.9528e-04, PNorm = 47.7962, GNorm = 0.4953, lr_0 = 5.2082e-04
Loss = 3.1199e-04, PNorm = 47.8020, GNorm = 0.3485, lr_0 = 5.2028e-04
Loss = 3.4882e-04, PNorm = 47.8091, GNorm = 0.3791, lr_0 = 5.1974e-04
Loss = 3.3609e-04, PNorm = 47.8191, GNorm = 0.3273, lr_0 = 5.1919e-04
Loss = 3.2193e-04, PNorm = 47.8284, GNorm = 0.3711, lr_0 = 5.1865e-04
Loss = 4.3790e-04, PNorm = 47.8388, GNorm = 0.3887, lr_0 = 5.1811e-04
Loss = 3.6376e-04, PNorm = 47.8520, GNorm = 0.2667, lr_0 = 5.1757e-04
Loss = 3.0940e-04, PNorm = 47.8606, GNorm = 0.2996, lr_0 = 5.1703e-04
Loss = 2.9169e-04, PNorm = 47.8697, GNorm = 0.3691, lr_0 = 5.1649e-04
Validation rmse logD = 0.637102
Validation R2 logD = 0.716123
Validation rmse logP = 0.464576
Validation R2 logP = 0.940586
Epoch 30
Train function
Loss = 2.4019e-04, PNorm = 47.8781, GNorm = 0.3964, lr_0 = 5.1595e-04
Loss = 2.4544e-04, PNorm = 47.8842, GNorm = 0.3472, lr_0 = 5.1541e-04
Loss = 2.5901e-04, PNorm = 47.8893, GNorm = 0.4587, lr_0 = 5.1487e-04
Loss = 3.6665e-04, PNorm = 47.8957, GNorm = 0.3729, lr_0 = 5.1434e-04
Loss = 2.4188e-04, PNorm = 47.9080, GNorm = 0.3002, lr_0 = 5.1380e-04
Loss = 3.3318e-04, PNorm = 47.9164, GNorm = 0.3523, lr_0 = 5.1326e-04
Loss = 3.4067e-04, PNorm = 47.9242, GNorm = 0.7956, lr_0 = 5.1273e-04
Loss = 3.5099e-04, PNorm = 47.9324, GNorm = 0.6902, lr_0 = 5.1219e-04
Loss = 2.7633e-04, PNorm = 47.9416, GNorm = 0.5857, lr_0 = 5.1166e-04
Loss = 2.9667e-04, PNorm = 47.9587, GNorm = 0.3695, lr_0 = 5.1112e-04
Loss = 3.5464e-04, PNorm = 47.9644, GNorm = 0.4979, lr_0 = 5.1059e-04
Loss = 3.9276e-04, PNorm = 47.9770, GNorm = 0.6517, lr_0 = 5.1006e-04
Loss = 3.1636e-04, PNorm = 47.9861, GNorm = 0.4767, lr_0 = 5.0953e-04
Loss = 3.2734e-04, PNorm = 47.9984, GNorm = 0.6691, lr_0 = 5.0899e-04
Loss = 3.4909e-04, PNorm = 48.0092, GNorm = 0.3197, lr_0 = 5.0846e-04
Loss = 3.3787e-04, PNorm = 48.0168, GNorm = 0.4573, lr_0 = 5.0793e-04
Loss = 3.0216e-04, PNorm = 48.0247, GNorm = 0.4481, lr_0 = 5.0740e-04
Loss = 2.9641e-04, PNorm = 48.0337, GNorm = 0.3836, lr_0 = 5.0687e-04
Loss = 3.0124e-04, PNorm = 48.0413, GNorm = 0.4095, lr_0 = 5.0634e-04
Loss = 2.9482e-04, PNorm = 48.0476, GNorm = 0.5304, lr_0 = 5.0581e-04
Loss = 3.3160e-04, PNorm = 48.0568, GNorm = 0.4287, lr_0 = 5.0529e-04
Loss = 2.9199e-04, PNorm = 48.0685, GNorm = 0.2598, lr_0 = 5.0476e-04
Validation rmse logD = 0.624161
Validation R2 logD = 0.727538
Validation rmse logP = 0.465322
Validation R2 logP = 0.940395
Epoch 31
Train function
Loss = 1.7836e-04, PNorm = 48.0834, GNorm = 0.2840, lr_0 = 5.0418e-04
Loss = 3.1600e-04, PNorm = 48.0946, GNorm = 0.4770, lr_0 = 5.0365e-04
Loss = 2.6980e-04, PNorm = 48.1082, GNorm = 0.2627, lr_0 = 5.0313e-04
Loss = 2.5331e-04, PNorm = 48.1138, GNorm = 0.4087, lr_0 = 5.0260e-04
Loss = 2.8987e-04, PNorm = 48.1192, GNorm = 0.1920, lr_0 = 5.0208e-04
Loss = 3.4311e-04, PNorm = 48.1324, GNorm = 0.5905, lr_0 = 5.0155e-04
Loss = 2.6931e-04, PNorm = 48.1409, GNorm = 0.4004, lr_0 = 5.0103e-04
Loss = 2.7146e-04, PNorm = 48.1515, GNorm = 0.2515, lr_0 = 5.0051e-04
Loss = 3.5838e-04, PNorm = 48.1594, GNorm = 0.2807, lr_0 = 4.9998e-04
Loss = 2.5327e-04, PNorm = 48.1684, GNorm = 0.3168, lr_0 = 4.9946e-04
Loss = 3.7122e-04, PNorm = 48.1751, GNorm = 0.6472, lr_0 = 4.9894e-04
Loss = 3.3686e-04, PNorm = 48.1862, GNorm = 0.2727, lr_0 = 4.9842e-04
Loss = 2.8143e-04, PNorm = 48.1967, GNorm = 0.3852, lr_0 = 4.9790e-04
Loss = 2.8294e-04, PNorm = 48.2050, GNorm = 0.4546, lr_0 = 4.9738e-04
Loss = 3.4585e-04, PNorm = 48.2115, GNorm = 0.4450, lr_0 = 4.9686e-04
Loss = 2.5647e-04, PNorm = 48.2200, GNorm = 0.5935, lr_0 = 4.9634e-04
Loss = 2.9750e-04, PNorm = 48.2297, GNorm = 0.2590, lr_0 = 4.9583e-04
Loss = 2.6287e-04, PNorm = 48.2420, GNorm = 0.3236, lr_0 = 4.9531e-04
Loss = 2.7856e-04, PNorm = 48.2483, GNorm = 0.2967, lr_0 = 4.9479e-04
Loss = 2.8271e-04, PNorm = 48.2536, GNorm = 0.3052, lr_0 = 4.9427e-04
Loss = 2.9322e-04, PNorm = 48.2575, GNorm = 0.2467, lr_0 = 4.9376e-04
Loss = 3.8365e-04, PNorm = 48.2653, GNorm = 0.2188, lr_0 = 4.9324e-04
Loss = 3.1156e-04, PNorm = 48.2758, GNorm = 0.4787, lr_0 = 4.9273e-04
Validation rmse logD = 0.615638
Validation R2 logD = 0.734929
Validation rmse logP = 0.455135
Validation R2 logP = 0.942976
Epoch 32
Train function
Loss = 2.0866e-04, PNorm = 48.2865, GNorm = 0.2162, lr_0 = 4.9221e-04
Loss = 2.6893e-04, PNorm = 48.2959, GNorm = 0.2701, lr_0 = 4.9170e-04
Loss = 2.2032e-04, PNorm = 48.3074, GNorm = 0.6014, lr_0 = 4.9119e-04
Loss = 2.5745e-04, PNorm = 48.3150, GNorm = 0.2773, lr_0 = 4.9067e-04
Loss = 2.4625e-04, PNorm = 48.3171, GNorm = 0.5710, lr_0 = 4.9016e-04
Loss = 2.0438e-04, PNorm = 48.3235, GNorm = 0.2735, lr_0 = 4.8965e-04
Loss = 1.9753e-04, PNorm = 48.3339, GNorm = 0.4486, lr_0 = 4.8914e-04
Loss = 3.4416e-04, PNorm = 48.3409, GNorm = 1.0845, lr_0 = 4.8863e-04
Loss = 3.0445e-04, PNorm = 48.3514, GNorm = 0.2416, lr_0 = 4.8812e-04
Loss = 3.4448e-04, PNorm = 48.3607, GNorm = 0.7166, lr_0 = 4.8761e-04
Loss = 2.6708e-04, PNorm = 48.3704, GNorm = 0.4206, lr_0 = 4.8710e-04
Loss = 3.1201e-04, PNorm = 48.3759, GNorm = 0.6683, lr_0 = 4.8659e-04
Loss = 3.0131e-04, PNorm = 48.3829, GNorm = 0.6608, lr_0 = 4.8608e-04
Loss = 2.5770e-04, PNorm = 48.3937, GNorm = 0.3080, lr_0 = 4.8558e-04
Loss = 2.6395e-04, PNorm = 48.4043, GNorm = 0.2998, lr_0 = 4.8507e-04
Loss = 2.5856e-04, PNorm = 48.4122, GNorm = 0.3118, lr_0 = 4.8456e-04
Loss = 2.6215e-04, PNorm = 48.4163, GNorm = 0.5224, lr_0 = 4.8406e-04
Loss = 2.8487e-04, PNorm = 48.4265, GNorm = 0.2987, lr_0 = 4.8355e-04
Loss = 3.1851e-04, PNorm = 48.4335, GNorm = 0.4384, lr_0 = 4.8305e-04
Loss = 2.6757e-04, PNorm = 48.4423, GNorm = 0.3403, lr_0 = 4.8254e-04
Loss = 3.0258e-04, PNorm = 48.4532, GNorm = 0.3644, lr_0 = 4.8204e-04
Loss = 3.7471e-04, PNorm = 48.4634, GNorm = 0.3321, lr_0 = 4.8154e-04
Loss = 2.9842e-04, PNorm = 48.4709, GNorm = 0.6801, lr_0 = 4.8104e-04
Loss = 6.9195e-04, PNorm = 48.4723, GNorm = 0.4363, lr_0 = 4.8098e-04
Validation rmse logD = 0.615305
Validation R2 logD = 0.735216
Validation rmse logP = 0.476051
Validation R2 logP = 0.937615
Epoch 33
Train function
Loss = 3.1643e-04, PNorm = 48.4816, GNorm = 0.1988, lr_0 = 4.8048e-04
Loss = 3.6865e-04, PNorm = 48.4937, GNorm = 0.3963, lr_0 = 4.7998e-04
Loss = 2.5575e-04, PNorm = 48.5014, GNorm = 0.2772, lr_0 = 4.7948e-04
Loss = 3.0812e-04, PNorm = 48.5113, GNorm = 0.2851, lr_0 = 4.7898e-04
Loss = 2.6607e-04, PNorm = 48.5206, GNorm = 0.5098, lr_0 = 4.7848e-04
Loss = 2.4783e-04, PNorm = 48.5289, GNorm = 0.3115, lr_0 = 4.7798e-04
Loss = 2.8101e-04, PNorm = 48.5354, GNorm = 0.2981, lr_0 = 4.7748e-04
Loss = 3.0344e-04, PNorm = 48.5425, GNorm = 0.3186, lr_0 = 4.7698e-04
Loss = 2.7960e-04, PNorm = 48.5478, GNorm = 0.3568, lr_0 = 4.7649e-04
Loss = 2.3857e-04, PNorm = 48.5573, GNorm = 0.2667, lr_0 = 4.7599e-04
Loss = 2.7849e-04, PNorm = 48.5645, GNorm = 0.6194, lr_0 = 4.7549e-04
Loss = 2.9107e-04, PNorm = 48.5749, GNorm = 0.4518, lr_0 = 4.7500e-04
Loss = 2.1076e-04, PNorm = 48.5821, GNorm = 0.2227, lr_0 = 4.7450e-04
Loss = 2.4757e-04, PNorm = 48.5888, GNorm = 0.3210, lr_0 = 4.7400e-04
Loss = 2.3490e-04, PNorm = 48.5970, GNorm = 0.3947, lr_0 = 4.7351e-04
Loss = 2.4942e-04, PNorm = 48.6050, GNorm = 0.3919, lr_0 = 4.7302e-04
Loss = 2.8498e-04, PNorm = 48.6128, GNorm = 0.7183, lr_0 = 4.7252e-04
Loss = 2.6237e-04, PNorm = 48.6155, GNorm = 0.4227, lr_0 = 4.7203e-04
Loss = 3.2191e-04, PNorm = 48.6277, GNorm = 0.3293, lr_0 = 4.7154e-04
Loss = 2.7468e-04, PNorm = 48.6347, GNorm = 0.6233, lr_0 = 4.7104e-04
Loss = 2.9807e-04, PNorm = 48.6418, GNorm = 0.6151, lr_0 = 4.7055e-04
Loss = 2.9334e-04, PNorm = 48.6517, GNorm = 0.3823, lr_0 = 4.7006e-04
Validation rmse logD = 0.631880
Validation R2 logD = 0.720757
Validation rmse logP = 0.458857
Validation R2 logP = 0.942040
Epoch 34
Train function
Loss = 1.9826e-04, PNorm = 48.6607, GNorm = 0.4278, lr_0 = 4.6957e-04
Loss = 2.6989e-04, PNorm = 48.6697, GNorm = 0.6812, lr_0 = 4.6908e-04
Loss = 2.5083e-04, PNorm = 48.6766, GNorm = 0.3074, lr_0 = 4.6859e-04
Loss = 2.3833e-04, PNorm = 48.6849, GNorm = 0.4278, lr_0 = 4.6810e-04
Loss = 2.8102e-04, PNorm = 48.6918, GNorm = 0.5155, lr_0 = 4.6761e-04
Loss = 2.8389e-04, PNorm = 48.7002, GNorm = 0.7114, lr_0 = 4.6712e-04
Loss = 3.0767e-04, PNorm = 48.7103, GNorm = 0.3673, lr_0 = 4.6664e-04
Loss = 1.7836e-04, PNorm = 48.7166, GNorm = 0.2266, lr_0 = 4.6615e-04
Loss = 2.7226e-04, PNorm = 48.7245, GNorm = 0.6073, lr_0 = 4.6566e-04
Loss = 2.4446e-04, PNorm = 48.7341, GNorm = 0.2087, lr_0 = 4.6518e-04
Loss = 2.6404e-04, PNorm = 48.7415, GNorm = 0.2426, lr_0 = 4.6469e-04
Loss = 3.6391e-04, PNorm = 48.7478, GNorm = 0.4453, lr_0 = 4.6421e-04
Loss = 2.9387e-04, PNorm = 48.7572, GNorm = 0.5270, lr_0 = 4.6372e-04
Loss = 2.3486e-04, PNorm = 48.7655, GNorm = 0.3660, lr_0 = 4.6324e-04
Loss = 2.6768e-04, PNorm = 48.7723, GNorm = 0.5410, lr_0 = 4.6276e-04
Loss = 2.8637e-04, PNorm = 48.7808, GNorm = 0.5376, lr_0 = 4.6227e-04
Loss = 2.5766e-04, PNorm = 48.7863, GNorm = 0.4649, lr_0 = 4.6179e-04
Loss = 3.3279e-04, PNorm = 48.7992, GNorm = 0.5806, lr_0 = 4.6131e-04
Loss = 3.2236e-04, PNorm = 48.8076, GNorm = 0.3178, lr_0 = 4.6083e-04
Loss = 2.5373e-04, PNorm = 48.8163, GNorm = 0.3449, lr_0 = 4.6035e-04
Loss = 3.1530e-04, PNorm = 48.8253, GNorm = 0.6748, lr_0 = 4.5987e-04
Loss = 3.3364e-04, PNorm = 48.8368, GNorm = 0.2823, lr_0 = 4.5939e-04
Loss = 3.2250e-04, PNorm = 48.8479, GNorm = 0.7266, lr_0 = 4.5891e-04
Validation rmse logD = 0.624362
Validation R2 logD = 0.727363
Validation rmse logP = 0.465928
Validation R2 logP = 0.940240
Epoch 35
Train function
Loss = 1.9748e-04, PNorm = 48.8598, GNorm = 0.3508, lr_0 = 4.5838e-04
Loss = 2.0004e-04, PNorm = 48.8694, GNorm = 0.3872, lr_0 = 4.5790e-04
Loss = 2.2482e-04, PNorm = 48.8814, GNorm = 0.3055, lr_0 = 4.5742e-04
Loss = 2.5048e-04, PNorm = 48.8910, GNorm = 0.4781, lr_0 = 4.5695e-04
Loss = 2.6795e-04, PNorm = 48.9000, GNorm = 0.2082, lr_0 = 4.5647e-04
Loss = 3.0326e-04, PNorm = 48.9120, GNorm = 0.5495, lr_0 = 4.5599e-04
Loss = 3.9558e-04, PNorm = 48.9229, GNorm = 0.5616, lr_0 = 4.5552e-04
Loss = 3.2234e-04, PNorm = 48.9353, GNorm = 0.4026, lr_0 = 4.5504e-04
Loss = 2.8728e-04, PNorm = 48.9425, GNorm = 0.3669, lr_0 = 4.5457e-04
Loss = 2.7672e-04, PNorm = 48.9520, GNorm = 0.5594, lr_0 = 4.5409e-04
Loss = 2.1545e-04, PNorm = 48.9590, GNorm = 0.3103, lr_0 = 4.5362e-04
Loss = 2.5862e-04, PNorm = 48.9665, GNorm = 0.3413, lr_0 = 4.5314e-04
Loss = 2.5282e-04, PNorm = 48.9714, GNorm = 0.2673, lr_0 = 4.5267e-04
Loss = 2.6235e-04, PNorm = 48.9800, GNorm = 0.3420, lr_0 = 4.5220e-04
Loss = 1.1076e-03, PNorm = 48.9845, GNorm = 0.3900, lr_0 = 4.5173e-04
Loss = 4.1928e-04, PNorm = 49.0067, GNorm = 0.2996, lr_0 = 4.5125e-04
Loss = 5.2532e-04, PNorm = 49.0321, GNorm = 1.1686, lr_0 = 4.5078e-04
Loss = 5.2527e-04, PNorm = 49.0551, GNorm = 0.3424, lr_0 = 4.5031e-04
Loss = 5.5147e-04, PNorm = 49.0757, GNorm = 0.3422, lr_0 = 4.4984e-04
Loss = 3.9930e-04, PNorm = 49.0882, GNorm = 0.5064, lr_0 = 4.4937e-04
Loss = 4.0017e-04, PNorm = 49.1020, GNorm = 0.5957, lr_0 = 4.4890e-04
Loss = 6.1904e-04, PNorm = 49.1167, GNorm = 0.4745, lr_0 = 4.4844e-04
Validation rmse logD = 0.634688
Validation R2 logD = 0.718270
Validation rmse logP = 0.476231
Validation R2 logP = 0.937568
Epoch 36
Train function
Loss = 5.3954e-04, PNorm = 49.1303, GNorm = 0.8330, lr_0 = 4.4797e-04
Loss = 4.2258e-04, PNorm = 49.1472, GNorm = 0.4082, lr_0 = 4.4750e-04
Loss = 3.8146e-04, PNorm = 49.1665, GNorm = 0.4770, lr_0 = 4.4703e-04
Loss = 3.4181e-04, PNorm = 49.1766, GNorm = 0.2769, lr_0 = 4.4657e-04
Loss = 3.0116e-04, PNorm = 49.1868, GNorm = 0.3116, lr_0 = 4.4610e-04
Loss = 3.0481e-04, PNorm = 49.1979, GNorm = 0.3639, lr_0 = 4.4564e-04
Loss = 3.0017e-04, PNorm = 49.2091, GNorm = 0.4132, lr_0 = 4.4517e-04
Loss = 3.1036e-04, PNorm = 49.2166, GNorm = 0.3311, lr_0 = 4.4471e-04
Loss = 3.4187e-04, PNorm = 49.2233, GNorm = 0.3733, lr_0 = 4.4424e-04
Loss = 2.9847e-04, PNorm = 49.2300, GNorm = 0.4908, lr_0 = 4.4378e-04
Loss = 2.7183e-04, PNorm = 49.2404, GNorm = 0.4123, lr_0 = 4.4331e-04
Loss = 2.1250e-04, PNorm = 49.2489, GNorm = 0.2181, lr_0 = 4.4285e-04
Loss = 2.8160e-04, PNorm = 49.2560, GNorm = 0.4051, lr_0 = 4.4239e-04
Loss = 2.5511e-04, PNorm = 49.2625, GNorm = 0.3105, lr_0 = 4.4193e-04
Loss = 2.7079e-04, PNorm = 49.2649, GNorm = 0.2819, lr_0 = 4.4147e-04
Loss = 2.9217e-04, PNorm = 49.2709, GNorm = 0.7235, lr_0 = 4.4101e-04
Loss = 2.9316e-04, PNorm = 49.2814, GNorm = 0.3544, lr_0 = 4.4055e-04
Loss = 3.1407e-04, PNorm = 49.2899, GNorm = 0.3007, lr_0 = 4.4009e-04
Loss = 2.6904e-04, PNorm = 49.2997, GNorm = 0.3002, lr_0 = 4.3963e-04
Loss = 2.8264e-04, PNorm = 49.3053, GNorm = 0.3204, lr_0 = 4.3917e-04
Loss = 3.0486e-04, PNorm = 49.3135, GNorm = 0.2841, lr_0 = 4.3871e-04
Loss = 2.2752e-04, PNorm = 49.3189, GNorm = 0.3298, lr_0 = 4.3825e-04
Loss = 2.5323e-04, PNorm = 49.3223, GNorm = 0.3190, lr_0 = 4.3779e-04
Validation rmse logD = 0.616136
Validation R2 logD = 0.734499
Validation rmse logP = 0.461250
Validation R2 logP = 0.941434
Epoch 37
Train function
Loss = 2.1435e-04, PNorm = 49.3334, GNorm = 0.3172, lr_0 = 4.3729e-04
Loss = 2.5307e-04, PNorm = 49.3387, GNorm = 0.5744, lr_0 = 4.3684e-04
Loss = 2.3729e-04, PNorm = 49.3436, GNorm = 0.8555, lr_0 = 4.3638e-04
Loss = 2.4512e-04, PNorm = 49.3529, GNorm = 0.4081, lr_0 = 4.3592e-04
Loss = 2.4270e-04, PNorm = 49.3636, GNorm = 0.2057, lr_0 = 4.3547e-04
Loss = 2.5640e-04, PNorm = 49.3720, GNorm = 0.6207, lr_0 = 4.3501e-04
Loss = 2.2411e-04, PNorm = 49.3757, GNorm = 0.5565, lr_0 = 4.3456e-04
Loss = 2.9346e-04, PNorm = 49.3818, GNorm = 0.3938, lr_0 = 4.3411e-04
Loss = 2.6546e-04, PNorm = 49.3884, GNorm = 0.6233, lr_0 = 4.3365e-04
Loss = 2.2276e-04, PNorm = 49.3924, GNorm = 0.3526, lr_0 = 4.3320e-04
Loss = 2.2797e-04, PNorm = 49.3995, GNorm = 0.2190, lr_0 = 4.3275e-04
Loss = 2.2350e-04, PNorm = 49.4076, GNorm = 0.4727, lr_0 = 4.3230e-04
Loss = 2.0091e-04, PNorm = 49.4121, GNorm = 0.3109, lr_0 = 4.3185e-04
Loss = 2.1615e-04, PNorm = 49.4192, GNorm = 0.3933, lr_0 = 4.3140e-04
Loss = 2.3527e-04, PNorm = 49.4299, GNorm = 0.3129, lr_0 = 4.3094e-04
Loss = 2.0746e-04, PNorm = 49.4377, GNorm = 0.2756, lr_0 = 4.3050e-04
Loss = 2.2862e-04, PNorm = 49.4442, GNorm = 0.2223, lr_0 = 4.3005e-04
Loss = 2.2422e-04, PNorm = 49.4484, GNorm = 0.3602, lr_0 = 4.2960e-04
Loss = 2.2440e-04, PNorm = 49.4546, GNorm = 0.5961, lr_0 = 4.2915e-04
Loss = 1.9205e-04, PNorm = 49.4612, GNorm = 0.2952, lr_0 = 4.2870e-04
Loss = 2.2058e-04, PNorm = 49.4673, GNorm = 0.3044, lr_0 = 4.2825e-04
Loss = 2.3215e-04, PNorm = 49.4735, GNorm = 0.5205, lr_0 = 4.2781e-04
Validation rmse logD = 0.606337
Validation R2 logD = 0.742878
Validation rmse logP = 0.456543
Validation R2 logP = 0.942623
Epoch 38
Train function
Loss = 1.6555e-04, PNorm = 49.4811, GNorm = 0.3384, lr_0 = 4.2736e-04
Loss = 1.9159e-04, PNorm = 49.4856, GNorm = 0.3343, lr_0 = 4.2691e-04
Loss = 1.8082e-04, PNorm = 49.4906, GNorm = 0.5353, lr_0 = 4.2647e-04
Loss = 1.9055e-04, PNorm = 49.4971, GNorm = 0.1865, lr_0 = 4.2602e-04
Loss = 1.8011e-04, PNorm = 49.5054, GNorm = 0.3832, lr_0 = 4.2558e-04
Loss = 2.2452e-04, PNorm = 49.5141, GNorm = 0.8514, lr_0 = 4.2513e-04
Loss = 2.5480e-04, PNorm = 49.5186, GNorm = 0.2548, lr_0 = 4.2469e-04
Loss = 2.3043e-04, PNorm = 49.5249, GNorm = 0.4640, lr_0 = 4.2425e-04
Loss = 2.1136e-04, PNorm = 49.5322, GNorm = 0.4256, lr_0 = 4.2380e-04
Loss = 2.1806e-04, PNorm = 49.5394, GNorm = 0.2298, lr_0 = 4.2336e-04
Loss = 2.2666e-04, PNorm = 49.5449, GNorm = 0.1909, lr_0 = 4.2292e-04
Loss = 2.3130e-04, PNorm = 49.5493, GNorm = 0.2809, lr_0 = 4.2248e-04
Loss = 2.2360e-04, PNorm = 49.5550, GNorm = 0.5655, lr_0 = 4.2204e-04
Loss = 2.0869e-04, PNorm = 49.5626, GNorm = 0.2676, lr_0 = 4.2160e-04
Loss = 1.8759e-04, PNorm = 49.5680, GNorm = 0.2737, lr_0 = 4.2116e-04
Loss = 1.6566e-04, PNorm = 49.5744, GNorm = 0.2835, lr_0 = 4.2072e-04
Loss = 1.8456e-04, PNorm = 49.5798, GNorm = 0.3576, lr_0 = 4.2028e-04
Loss = 1.7742e-04, PNorm = 49.5860, GNorm = 0.3722, lr_0 = 4.1984e-04
Loss = 2.1667e-04, PNorm = 49.5912, GNorm = 0.3275, lr_0 = 4.1940e-04
Loss = 2.5455e-04, PNorm = 49.5970, GNorm = 0.2566, lr_0 = 4.1896e-04
Loss = 1.9212e-04, PNorm = 49.6033, GNorm = 0.4455, lr_0 = 4.1853e-04
Loss = 2.2000e-04, PNorm = 49.6123, GNorm = 0.2453, lr_0 = 4.1809e-04
Loss = 2.0277e-04, PNorm = 49.6203, GNorm = 0.4010, lr_0 = 4.1765e-04
Validation rmse logD = 0.641074
Validation R2 logD = 0.712573
Validation rmse logP = 0.455586
Validation R2 logP = 0.942863
Epoch 39
Train function
Loss = 1.4503e-04, PNorm = 49.6287, GNorm = 0.2205, lr_0 = 4.1717e-04
Loss = 1.8835e-04, PNorm = 49.6347, GNorm = 0.3212, lr_0 = 4.1674e-04
Loss = 1.9360e-04, PNorm = 49.6404, GNorm = 0.3276, lr_0 = 4.1630e-04
Loss = 1.6274e-04, PNorm = 49.6467, GNorm = 0.2483, lr_0 = 4.1587e-04
Loss = 2.0676e-04, PNorm = 49.6469, GNorm = 0.4875, lr_0 = 4.1544e-04
Loss = 1.8098e-04, PNorm = 49.6511, GNorm = 0.2920, lr_0 = 4.1500e-04
Loss = 1.9245e-04, PNorm = 49.6600, GNorm = 0.3207, lr_0 = 4.1457e-04
Loss = 2.2612e-04, PNorm = 49.6669, GNorm = 0.3018, lr_0 = 4.1414e-04
Loss = 2.1151e-04, PNorm = 49.6741, GNorm = 0.3721, lr_0 = 4.1370e-04
Loss = 1.5290e-04, PNorm = 49.6808, GNorm = 0.2415, lr_0 = 4.1327e-04
Loss = 1.5534e-04, PNorm = 49.6879, GNorm = 0.3557, lr_0 = 4.1284e-04
Loss = 2.0920e-04, PNorm = 49.6928, GNorm = 0.3408, lr_0 = 4.1241e-04
Loss = 1.7059e-04, PNorm = 49.6976, GNorm = 0.4629, lr_0 = 4.1198e-04
Loss = 2.3105e-04, PNorm = 49.7037, GNorm = 0.3666, lr_0 = 4.1155e-04
Loss = 1.5957e-04, PNorm = 49.7066, GNorm = 0.2744, lr_0 = 4.1112e-04
Loss = 1.7902e-04, PNorm = 49.7101, GNorm = 0.4090, lr_0 = 4.1069e-04
Loss = 1.8331e-04, PNorm = 49.7145, GNorm = 0.3423, lr_0 = 4.1026e-04
Loss = 2.1748e-04, PNorm = 49.7222, GNorm = 0.2637, lr_0 = 4.0983e-04
Loss = 2.1301e-04, PNorm = 49.7293, GNorm = 0.3850, lr_0 = 4.0941e-04
Loss = 2.4640e-04, PNorm = 49.7333, GNorm = 0.3591, lr_0 = 4.0898e-04
Loss = 2.2241e-04, PNorm = 49.7439, GNorm = 0.6070, lr_0 = 4.0855e-04
Loss = 2.3242e-04, PNorm = 49.7505, GNorm = 0.2148, lr_0 = 4.0813e-04
Validation rmse logD = 0.618741
Validation R2 logD = 0.732249
Validation rmse logP = 0.467051
Validation R2 logP = 0.939951
Epoch 40
Train function
Loss = 1.1759e-04, PNorm = 49.7582, GNorm = 0.2754, lr_0 = 4.0770e-04
Loss = 1.6589e-04, PNorm = 49.7639, GNorm = 0.6127, lr_0 = 4.0727e-04
Loss = 1.8478e-04, PNorm = 49.7717, GNorm = 0.2063, lr_0 = 4.0685e-04
Loss = 1.7678e-04, PNorm = 49.7795, GNorm = 0.3564, lr_0 = 4.0642e-04
Loss = 1.7282e-04, PNorm = 49.7831, GNorm = 0.2450, lr_0 = 4.0600e-04
Loss = 1.6875e-04, PNorm = 49.7872, GNorm = 0.2073, lr_0 = 4.0558e-04
Loss = 1.2387e-04, PNorm = 49.7922, GNorm = 0.2150, lr_0 = 4.0515e-04
Loss = 1.4724e-04, PNorm = 49.7955, GNorm = 0.4118, lr_0 = 4.0473e-04
Loss = 1.7922e-04, PNorm = 49.7986, GNorm = 0.3113, lr_0 = 4.0431e-04
Loss = 1.7432e-04, PNorm = 49.8045, GNorm = 0.2481, lr_0 = 4.0389e-04
Loss = 1.5709e-04, PNorm = 49.8111, GNorm = 0.1817, lr_0 = 4.0346e-04
Loss = 1.5964e-04, PNorm = 49.8139, GNorm = 0.2784, lr_0 = 4.0304e-04
Loss = 1.7511e-04, PNorm = 49.8179, GNorm = 0.1770, lr_0 = 4.0262e-04
Loss = 1.6991e-04, PNorm = 49.8224, GNorm = 0.2506, lr_0 = 4.0220e-04
Loss = 1.6416e-04, PNorm = 49.8283, GNorm = 0.2802, lr_0 = 4.0178e-04
Loss = 1.7129e-04, PNorm = 49.8328, GNorm = 0.2862, lr_0 = 4.0136e-04
Loss = 2.2567e-04, PNorm = 49.8373, GNorm = 0.6784, lr_0 = 4.0094e-04
Loss = 1.8290e-04, PNorm = 49.8429, GNorm = 0.3806, lr_0 = 4.0053e-04
Loss = 1.8729e-04, PNorm = 49.8485, GNorm = 0.3639, lr_0 = 4.0011e-04
Loss = 1.7882e-04, PNorm = 49.8540, GNorm = 0.4327, lr_0 = 3.9969e-04
Loss = 1.7440e-04, PNorm = 49.8600, GNorm = 0.2666, lr_0 = 3.9927e-04
Loss = 1.5907e-04, PNorm = 49.8622, GNorm = 0.3695, lr_0 = 3.9886e-04
Loss = 2.1378e-04, PNorm = 49.8695, GNorm = 0.7160, lr_0 = 3.9844e-04
Validation rmse logD = 0.625318
Validation R2 logD = 0.726527
Validation rmse logP = 0.458589
Validation R2 logP = 0.942108
Epoch 41
Train function
Loss = 2.3257e-04, PNorm = 49.8761, GNorm = 0.3232, lr_0 = 3.9798e-04
Loss = 1.6643e-04, PNorm = 49.8831, GNorm = 0.3154, lr_0 = 3.9757e-04
Loss = 1.5650e-04, PNorm = 49.8883, GNorm = 0.3299, lr_0 = 3.9715e-04
Loss = 1.4506e-04, PNorm = 49.8914, GNorm = 0.2082, lr_0 = 3.9674e-04
Loss = 1.7915e-04, PNorm = 49.9003, GNorm = 0.2487, lr_0 = 3.9632e-04
Loss = 1.3396e-04, PNorm = 49.9058, GNorm = 0.1613, lr_0 = 3.9591e-04
Loss = 1.8524e-04, PNorm = 49.9104, GNorm = 0.4206, lr_0 = 3.9550e-04
Loss = 1.7588e-04, PNorm = 49.9178, GNorm = 0.2342, lr_0 = 3.9508e-04
Loss = 1.7741e-04, PNorm = 49.9221, GNorm = 0.5637, lr_0 = 3.9467e-04
Loss = 1.4161e-04, PNorm = 49.9265, GNorm = 0.2040, lr_0 = 3.9426e-04
Loss = 1.8903e-04, PNorm = 49.9330, GNorm = 0.6222, lr_0 = 3.9385e-04
Loss = 2.0675e-04, PNorm = 49.9401, GNorm = 0.3459, lr_0 = 3.9344e-04
Loss = 1.5984e-04, PNorm = 49.9439, GNorm = 0.3569, lr_0 = 3.9303e-04
Loss = 1.9424e-04, PNorm = 49.9522, GNorm = 0.4274, lr_0 = 3.9262e-04
Loss = 1.4301e-04, PNorm = 49.9567, GNorm = 0.2398, lr_0 = 3.9221e-04
Loss = 1.2707e-04, PNorm = 49.9601, GNorm = 0.2025, lr_0 = 3.9180e-04
Loss = 1.7957e-04, PNorm = 49.9650, GNorm = 0.2849, lr_0 = 3.9139e-04
Loss = 1.5118e-04, PNorm = 49.9717, GNorm = 0.3794, lr_0 = 3.9098e-04
Loss = 1.7837e-04, PNorm = 49.9750, GNorm = 0.2910, lr_0 = 3.9057e-04
Loss = 1.4047e-04, PNorm = 49.9826, GNorm = 0.2463, lr_0 = 3.9016e-04
Loss = 1.7240e-04, PNorm = 49.9890, GNorm = 0.4646, lr_0 = 3.8976e-04
Loss = 1.6958e-04, PNorm = 49.9943, GNorm = 0.2920, lr_0 = 3.8935e-04
Loss = 1.7941e-04, PNorm = 49.9999, GNorm = 0.3531, lr_0 = 3.8894e-04
Validation rmse logD = 0.605358
Validation R2 logD = 0.743707
Validation rmse logP = 0.454899
Validation R2 logP = 0.943036
Epoch 42
Train function
Loss = 1.5142e-04, PNorm = 50.0082, GNorm = 0.4011, lr_0 = 3.8854e-04
Loss = 1.3590e-04, PNorm = 50.0139, GNorm = 0.2665, lr_0 = 3.8813e-04
Loss = 1.3951e-04, PNorm = 50.0178, GNorm = 0.2087, lr_0 = 3.8773e-04
Loss = 1.7822e-04, PNorm = 50.0203, GNorm = 0.1878, lr_0 = 3.8732e-04
Loss = 1.5093e-04, PNorm = 50.0272, GNorm = 0.4202, lr_0 = 3.8692e-04
Loss = 2.0111e-04, PNorm = 50.0342, GNorm = 0.3441, lr_0 = 3.8651e-04
Loss = 1.5491e-04, PNorm = 50.0382, GNorm = 0.1577, lr_0 = 3.8611e-04
Loss = 1.7273e-04, PNorm = 50.0469, GNorm = 0.2887, lr_0 = 3.8571e-04
Loss = 1.5283e-04, PNorm = 50.0528, GNorm = 0.2929, lr_0 = 3.8531e-04
Loss = 1.5506e-04, PNorm = 50.0559, GNorm = 0.2860, lr_0 = 3.8490e-04
Loss = 1.6902e-04, PNorm = 50.0612, GNorm = 0.2446, lr_0 = 3.8450e-04
Loss = 1.5561e-04, PNorm = 50.0636, GNorm = 0.2953, lr_0 = 3.8410e-04
Loss = 1.6390e-04, PNorm = 50.0680, GNorm = 0.2266, lr_0 = 3.8370e-04
Loss = 1.4647e-04, PNorm = 50.0742, GNorm = 0.3043, lr_0 = 3.8330e-04
Loss = 1.4826e-04, PNorm = 50.0788, GNorm = 0.2149, lr_0 = 3.8290e-04
Loss = 1.6862e-04, PNorm = 50.0819, GNorm = 0.2667, lr_0 = 3.8250e-04
Loss = 1.2650e-04, PNorm = 50.0854, GNorm = 0.1978, lr_0 = 3.8210e-04
Loss = 2.0390e-04, PNorm = 50.0925, GNorm = 0.5723, lr_0 = 3.8170e-04
Loss = 1.8450e-04, PNorm = 50.0982, GNorm = 0.3116, lr_0 = 3.8130e-04
Loss = 1.7949e-04, PNorm = 50.1039, GNorm = 0.4359, lr_0 = 3.8090e-04
Loss = 1.8464e-04, PNorm = 50.1111, GNorm = 0.3744, lr_0 = 3.8051e-04
Loss = 1.8352e-04, PNorm = 50.1154, GNorm = 0.2531, lr_0 = 3.8011e-04
Validation rmse logD = 0.609250
Validation R2 logD = 0.740401
Validation rmse logP = 0.458974
Validation R2 logP = 0.942010
Epoch 43
Train function
Loss = 1.1631e-04, PNorm = 50.1223, GNorm = 0.3090, lr_0 = 3.7967e-04
Loss = 1.5160e-04, PNorm = 50.1277, GNorm = 0.2011, lr_0 = 3.7928e-04
Loss = 1.4225e-04, PNorm = 50.1345, GNorm = 0.3101, lr_0 = 3.7888e-04
Loss = 1.3530e-04, PNorm = 50.1389, GNorm = 0.3239, lr_0 = 3.7849e-04
Loss = 1.2786e-04, PNorm = 50.1452, GNorm = 0.1940, lr_0 = 3.7809e-04
Loss = 1.2613e-04, PNorm = 50.1532, GNorm = 0.1670, lr_0 = 3.7770e-04
Loss = 1.5173e-04, PNorm = 50.1580, GNorm = 0.4337, lr_0 = 3.7730e-04
Loss = 1.2371e-04, PNorm = 50.1623, GNorm = 0.2605, lr_0 = 3.7691e-04
Loss = 1.3457e-04, PNorm = 50.1665, GNorm = 0.3162, lr_0 = 3.7652e-04
Loss = 1.7982e-04, PNorm = 50.1688, GNorm = 0.2371, lr_0 = 3.7612e-04
Loss = 1.5998e-04, PNorm = 50.1759, GNorm = 0.3276, lr_0 = 3.7573e-04
Loss = 1.4802e-04, PNorm = 50.1784, GNorm = 0.3634, lr_0 = 3.7534e-04
Loss = 1.6325e-04, PNorm = 50.1839, GNorm = 0.4917, lr_0 = 3.7495e-04
Loss = 1.9572e-04, PNorm = 50.1860, GNorm = 0.5084, lr_0 = 3.7455e-04
Loss = 2.3290e-04, PNorm = 50.1915, GNorm = 0.2718, lr_0 = 3.7416e-04
Loss = 2.1216e-04, PNorm = 50.1993, GNorm = 0.4943, lr_0 = 3.7377e-04
Loss = 1.8042e-04, PNorm = 50.2062, GNorm = 0.2292, lr_0 = 3.7338e-04
Loss = 1.5339e-04, PNorm = 50.2124, GNorm = 0.3055, lr_0 = 3.7299e-04
Loss = 1.2746e-04, PNorm = 50.2147, GNorm = 0.1808, lr_0 = 3.7260e-04
Loss = 1.5047e-04, PNorm = 50.2197, GNorm = 0.2746, lr_0 = 3.7221e-04
Loss = 1.8059e-04, PNorm = 50.2263, GNorm = 0.5288, lr_0 = 3.7183e-04
Loss = 1.9169e-04, PNorm = 50.2336, GNorm = 0.3709, lr_0 = 3.7144e-04
Loss = 1.7471e-04, PNorm = 50.2410, GNorm = 0.2761, lr_0 = 3.7105e-04
Validation rmse logD = 0.611683
Validation R2 logD = 0.738323
Validation rmse logP = 0.463753
Validation R2 logP = 0.940797
Epoch 44
Train function
Loss = 1.5110e-04, PNorm = 50.2463, GNorm = 0.3248, lr_0 = 3.7066e-04
Loss = 1.5122e-04, PNorm = 50.2544, GNorm = 0.2962, lr_0 = 3.7028e-04
Loss = 1.6815e-04, PNorm = 50.2604, GNorm = 0.5706, lr_0 = 3.6989e-04
Loss = 1.5800e-04, PNorm = 50.2676, GNorm = 0.6992, lr_0 = 3.6950e-04
Loss = 1.4790e-04, PNorm = 50.2748, GNorm = 0.2656, lr_0 = 3.6912e-04
Loss = 1.5635e-04, PNorm = 50.2795, GNorm = 0.7782, lr_0 = 3.6873e-04
Loss = 1.4784e-04, PNorm = 50.2839, GNorm = 0.2155, lr_0 = 3.6835e-04
Loss = 1.3089e-04, PNorm = 50.2899, GNorm = 0.3092, lr_0 = 3.6796e-04
Loss = 1.7446e-04, PNorm = 50.2946, GNorm = 0.4898, lr_0 = 3.6758e-04
Loss = 2.0741e-04, PNorm = 50.2991, GNorm = 0.2465, lr_0 = 3.6720e-04
Loss = 1.4522e-04, PNorm = 50.3014, GNorm = 0.2996, lr_0 = 3.6681e-04
Loss = 1.2184e-04, PNorm = 50.3077, GNorm = 0.2338, lr_0 = 3.6643e-04
Loss = 1.3044e-04, PNorm = 50.3126, GNorm = 0.2537, lr_0 = 3.6605e-04
Loss = 1.4147e-04, PNorm = 50.3158, GNorm = 0.2494, lr_0 = 3.6567e-04
Loss = 1.4165e-04, PNorm = 50.3207, GNorm = 0.6130, lr_0 = 3.6528e-04
Loss = 1.9255e-04, PNorm = 50.3254, GNorm = 0.4139, lr_0 = 3.6490e-04
Loss = 1.6896e-04, PNorm = 50.3324, GNorm = 0.4234, lr_0 = 3.6452e-04
Loss = 1.3871e-04, PNorm = 50.3334, GNorm = 0.4724, lr_0 = 3.6414e-04
Loss = 1.5009e-04, PNorm = 50.3390, GNorm = 0.2768, lr_0 = 3.6376e-04
Loss = 1.4708e-04, PNorm = 50.3442, GNorm = 0.3251, lr_0 = 3.6338e-04
Loss = 1.7911e-04, PNorm = 50.3494, GNorm = 0.2833, lr_0 = 3.6300e-04
Loss = 1.5121e-04, PNorm = 50.3531, GNorm = 0.2680, lr_0 = 3.6262e-04
Validation rmse logD = 0.603238
Validation R2 logD = 0.745499
Validation rmse logP = 0.452111
Validation R2 logP = 0.943732
Epoch 45
Train function
Loss = 8.4260e-05, PNorm = 50.3542, GNorm = 0.1550, lr_0 = 3.6221e-04
Loss = 1.3633e-04, PNorm = 50.3592, GNorm = 0.4551, lr_0 = 3.6183e-04
Loss = 1.6751e-04, PNorm = 50.3650, GNorm = 0.3447, lr_0 = 3.6145e-04
Loss = 1.1637e-04, PNorm = 50.3680, GNorm = 0.3634, lr_0 = 3.6107e-04
Loss = 1.3686e-04, PNorm = 50.3727, GNorm = 0.5214, lr_0 = 3.6070e-04
Loss = 1.7376e-04, PNorm = 50.3807, GNorm = 0.2743, lr_0 = 3.6032e-04
Loss = 1.6520e-04, PNorm = 50.3870, GNorm = 0.7072, lr_0 = 3.5994e-04
Loss = 1.4464e-04, PNorm = 50.3932, GNorm = 0.1974, lr_0 = 3.5957e-04
Loss = 1.9338e-04, PNorm = 50.3974, GNorm = 0.2492, lr_0 = 3.5919e-04
Loss = 1.5231e-04, PNorm = 50.4023, GNorm = 0.4789, lr_0 = 3.5882e-04
Loss = 1.1167e-04, PNorm = 50.4077, GNorm = 0.1675, lr_0 = 3.5844e-04
Loss = 1.0353e-04, PNorm = 50.4125, GNorm = 0.1385, lr_0 = 3.5807e-04
Loss = 1.0707e-04, PNorm = 50.4175, GNorm = 0.2360, lr_0 = 3.5770e-04
Loss = 1.2795e-04, PNorm = 50.4217, GNorm = 0.3010, lr_0 = 3.5732e-04
Loss = 1.1937e-04, PNorm = 50.4280, GNorm = 0.4334, lr_0 = 3.5695e-04
Loss = 1.3058e-04, PNorm = 50.4328, GNorm = 0.2476, lr_0 = 3.5658e-04
Loss = 1.4283e-04, PNorm = 50.4397, GNorm = 0.2002, lr_0 = 3.5621e-04
Loss = 1.3600e-04, PNorm = 50.4434, GNorm = 0.2845, lr_0 = 3.5583e-04
Loss = 1.6755e-04, PNorm = 50.4505, GNorm = 0.4803, lr_0 = 3.5546e-04
Loss = 1.2574e-04, PNorm = 50.4555, GNorm = 0.2336, lr_0 = 3.5509e-04
Loss = 1.4552e-04, PNorm = 50.4608, GNorm = 0.2701, lr_0 = 3.5472e-04
Loss = 1.8081e-04, PNorm = 50.4633, GNorm = 0.3215, lr_0 = 3.5435e-04
Loss = 1.2607e-04, PNorm = 50.4673, GNorm = 0.2546, lr_0 = 3.5398e-04
Validation rmse logD = 0.630672
Validation R2 logD = 0.721825
Validation rmse logP = 0.451166
Validation R2 logP = 0.943967
Epoch 46
Train function
Loss = 1.4951e-04, PNorm = 50.4738, GNorm = 0.3749, lr_0 = 3.5361e-04
Loss = 1.4625e-04, PNorm = 50.4762, GNorm = 0.3622, lr_0 = 3.5324e-04
Loss = 1.6942e-04, PNorm = 50.4798, GNorm = 0.3065, lr_0 = 3.5287e-04
Loss = 1.6107e-04, PNorm = 50.4872, GNorm = 0.2541, lr_0 = 3.5251e-04
Loss = 1.4886e-04, PNorm = 50.4922, GNorm = 0.5177, lr_0 = 3.5214e-04
Loss = 1.3978e-04, PNorm = 50.4967, GNorm = 0.1691, lr_0 = 3.5177e-04
Loss = 1.4824e-04, PNorm = 50.5029, GNorm = 0.3664, lr_0 = 3.5140e-04
Loss = 2.1082e-04, PNorm = 50.5085, GNorm = 0.4726, lr_0 = 3.5104e-04
Loss = 1.7083e-04, PNorm = 50.5136, GNorm = 0.2145, lr_0 = 3.5067e-04
Loss = 1.5435e-04, PNorm = 50.5182, GNorm = 0.2008, lr_0 = 3.5030e-04
Loss = 1.3950e-04, PNorm = 50.5220, GNorm = 0.3612, lr_0 = 3.4994e-04
Loss = 1.3155e-04, PNorm = 50.5258, GNorm = 0.3206, lr_0 = 3.4957e-04
Loss = 1.3973e-04, PNorm = 50.5351, GNorm = 0.2420, lr_0 = 3.4921e-04
Loss = 1.2810e-04, PNorm = 50.5392, GNorm = 0.2852, lr_0 = 3.4884e-04
Loss = 1.3771e-04, PNorm = 50.5426, GNorm = 0.2208, lr_0 = 3.4848e-04
Loss = 1.4648e-04, PNorm = 50.5465, GNorm = 0.3508, lr_0 = 3.4812e-04
Loss = 1.5999e-04, PNorm = 50.5556, GNorm = 0.2327, lr_0 = 3.4775e-04
Loss = 1.0714e-04, PNorm = 50.5601, GNorm = 0.1911, lr_0 = 3.4739e-04
Loss = 1.1975e-04, PNorm = 50.5635, GNorm = 0.1571, lr_0 = 3.4703e-04
Loss = 1.2677e-04, PNorm = 50.5690, GNorm = 0.3299, lr_0 = 3.4666e-04
Loss = 1.5637e-04, PNorm = 50.5743, GNorm = 0.1977, lr_0 = 3.4630e-04
Loss = 1.4130e-04, PNorm = 50.5818, GNorm = 0.3279, lr_0 = 3.4594e-04
Validation rmse logD = 0.612129
Validation R2 logD = 0.737942
Validation rmse logP = 0.459838
Validation R2 logP = 0.941792
Epoch 47
Train function
Loss = 1.0344e-04, PNorm = 50.5887, GNorm = 0.1999, lr_0 = 3.4554e-04
Loss = 1.2310e-04, PNorm = 50.5924, GNorm = 0.2627, lr_0 = 3.4518e-04
Loss = 7.7526e-05, PNorm = 50.5942, GNorm = 0.1410, lr_0 = 3.4482e-04
Loss = 1.2384e-04, PNorm = 50.5987, GNorm = 0.2024, lr_0 = 3.4446e-04
Loss = 1.3260e-04, PNorm = 50.6051, GNorm = 0.1963, lr_0 = 3.4410e-04
Loss = 1.0190e-04, PNorm = 50.6074, GNorm = 0.3458, lr_0 = 3.4374e-04
Loss = 1.1326e-04, PNorm = 50.6099, GNorm = 0.2629, lr_0 = 3.4339e-04
Loss = 9.5450e-05, PNorm = 50.6148, GNorm = 0.1902, lr_0 = 3.4303e-04
Loss = 1.0816e-04, PNorm = 50.6212, GNorm = 0.2449, lr_0 = 3.4267e-04
Loss = 9.9357e-05, PNorm = 50.6223, GNorm = 0.2696, lr_0 = 3.4231e-04
Loss = 9.9918e-05, PNorm = 50.6248, GNorm = 0.1853, lr_0 = 3.4195e-04
Loss = 1.2018e-04, PNorm = 50.6277, GNorm = 0.4152, lr_0 = 3.4160e-04
Loss = 1.1363e-04, PNorm = 50.6306, GNorm = 0.1754, lr_0 = 3.4124e-04
Loss = 1.3098e-04, PNorm = 50.6305, GNorm = 0.2209, lr_0 = 3.4088e-04
Loss = 1.3444e-04, PNorm = 50.6307, GNorm = 0.2060, lr_0 = 3.4053e-04
Loss = 1.3679e-04, PNorm = 50.6365, GNorm = 0.2825, lr_0 = 3.4017e-04
Loss = 1.2681e-04, PNorm = 50.6403, GNorm = 0.1959, lr_0 = 3.3982e-04
Loss = 1.3790e-04, PNorm = 50.6455, GNorm = 0.2890, lr_0 = 3.3946e-04
Loss = 1.0745e-04, PNorm = 50.6513, GNorm = 0.2239, lr_0 = 3.3911e-04
Loss = 1.1463e-04, PNorm = 50.6554, GNorm = 0.2441, lr_0 = 3.3876e-04
Loss = 1.1530e-04, PNorm = 50.6588, GNorm = 0.2026, lr_0 = 3.3840e-04
Loss = 1.4860e-04, PNorm = 50.6623, GNorm = 0.5801, lr_0 = 3.3805e-04
Loss = 1.4862e-04, PNorm = 50.6696, GNorm = 0.1833, lr_0 = 3.3770e-04
Validation rmse logD = 0.604371
Validation R2 logD = 0.744542
Validation rmse logP = 0.462868
Validation R2 logP = 0.941022
Epoch 48
Train function
Loss = 1.1316e-04, PNorm = 50.6722, GNorm = 0.3212, lr_0 = 3.3734e-04
Loss = 9.9317e-05, PNorm = 50.6756, GNorm = 0.2722, lr_0 = 3.3699e-04
Loss = 1.0076e-04, PNorm = 50.6797, GNorm = 0.2159, lr_0 = 3.3664e-04
Loss = 8.9297e-05, PNorm = 50.6834, GNorm = 0.1560, lr_0 = 3.3629e-04
Loss = 9.2218e-05, PNorm = 50.6879, GNorm = 0.3458, lr_0 = 3.3594e-04
Loss = 1.0477e-04, PNorm = 50.6912, GNorm = 0.3368, lr_0 = 3.3559e-04
Loss = 1.4557e-04, PNorm = 50.6966, GNorm = 0.1632, lr_0 = 3.3524e-04
Loss = 1.2640e-04, PNorm = 50.7033, GNorm = 0.2728, lr_0 = 3.3489e-04
Loss = 1.0463e-04, PNorm = 50.7089, GNorm = 0.1789, lr_0 = 3.3454e-04
Loss = 1.1994e-04, PNorm = 50.7141, GNorm = 0.2208, lr_0 = 3.3419e-04
Loss = 1.0991e-04, PNorm = 50.7200, GNorm = 0.1556, lr_0 = 3.3384e-04
Loss = 9.6650e-05, PNorm = 50.7236, GNorm = 0.1802, lr_0 = 3.3349e-04
Loss = 1.1904e-04, PNorm = 50.7277, GNorm = 0.3433, lr_0 = 3.3314e-04
Loss = 1.1573e-04, PNorm = 50.7331, GNorm = 0.2520, lr_0 = 3.3280e-04
Loss = 1.1391e-04, PNorm = 50.7356, GNorm = 0.1801, lr_0 = 3.3245e-04
Loss = 1.2986e-04, PNorm = 50.7417, GNorm = 0.2954, lr_0 = 3.3210e-04
Loss = 1.1437e-04, PNorm = 50.7467, GNorm = 0.2502, lr_0 = 3.3175e-04
Loss = 8.9380e-05, PNorm = 50.7477, GNorm = 0.1774, lr_0 = 3.3141e-04
Loss = 8.4359e-05, PNorm = 50.7512, GNorm = 0.1487, lr_0 = 3.3106e-04
Loss = 1.0304e-04, PNorm = 50.7535, GNorm = 0.2726, lr_0 = 3.3072e-04
Loss = 1.3496e-04, PNorm = 50.7578, GNorm = 0.1951, lr_0 = 3.3037e-04
Loss = 1.4923e-04, PNorm = 50.7605, GNorm = 0.5644, lr_0 = 3.3003e-04
Validation rmse logD = 0.620482
Validation R2 logD = 0.730741
Validation rmse logP = 0.459176
Validation R2 logP = 0.941959
Epoch 49
Train function
Loss = 1.7804e-04, PNorm = 50.7629, GNorm = 0.2962, lr_0 = 3.2965e-04
Loss = 1.5777e-04, PNorm = 50.7680, GNorm = 0.3881, lr_0 = 3.2930e-04
Loss = 1.5174e-04, PNorm = 50.7731, GNorm = 0.3411, lr_0 = 3.2896e-04
Loss = 1.5410e-04, PNorm = 50.7805, GNorm = 0.3546, lr_0 = 3.2862e-04
Loss = 1.2429e-04, PNorm = 50.7839, GNorm = 0.3531, lr_0 = 3.2827e-04
Loss = 1.2359e-04, PNorm = 50.7849, GNorm = 0.3221, lr_0 = 3.2793e-04
Loss = 1.1749e-04, PNorm = 50.7881, GNorm = 0.2746, lr_0 = 3.2759e-04
Loss = 1.3322e-04, PNorm = 50.7952, GNorm = 0.4455, lr_0 = 3.2725e-04
Loss = 1.4072e-04, PNorm = 50.8032, GNorm = 0.3034, lr_0 = 3.2691e-04
Loss = 1.3957e-04, PNorm = 50.8089, GNorm = 0.5024, lr_0 = 3.2656e-04
Loss = 1.6669e-04, PNorm = 50.8145, GNorm = 0.2449, lr_0 = 3.2622e-04
Loss = 1.4577e-04, PNorm = 50.8180, GNorm = 0.2445, lr_0 = 3.2588e-04
Loss = 1.1520e-04, PNorm = 50.8226, GNorm = 0.1399, lr_0 = 3.2554e-04
Loss = 1.1638e-04, PNorm = 50.8258, GNorm = 0.2438, lr_0 = 3.2520e-04
Loss = 1.3930e-04, PNorm = 50.8295, GNorm = 0.2964, lr_0 = 3.2486e-04
Loss = 1.4788e-04, PNorm = 50.8373, GNorm = 0.4043, lr_0 = 3.2452e-04
Loss = 1.2283e-04, PNorm = 50.8398, GNorm = 0.1924, lr_0 = 3.2419e-04
Loss = 1.0493e-04, PNorm = 50.8431, GNorm = 0.2711, lr_0 = 3.2385e-04
Loss = 1.1746e-04, PNorm = 50.8473, GNorm = 0.2491, lr_0 = 3.2351e-04
Loss = 1.0121e-04, PNorm = 50.8500, GNorm = 0.1724, lr_0 = 3.2317e-04
Loss = 1.1964e-04, PNorm = 50.8556, GNorm = 0.4742, lr_0 = 3.2283e-04
Loss = 1.1733e-04, PNorm = 50.8602, GNorm = 0.2244, lr_0 = 3.2250e-04
Loss = 1.3309e-04, PNorm = 50.8678, GNorm = 0.4412, lr_0 = 3.2216e-04
Validation rmse logD = 0.612188
Validation R2 logD = 0.737891
Validation rmse logP = 0.455377
Validation R2 logP = 0.942916
Epoch 50
Train function
Loss = 1.5808e-04, PNorm = 50.8716, GNorm = 0.3684, lr_0 = 3.2182e-04
Loss = 1.4198e-04, PNorm = 50.8743, GNorm = 0.1986, lr_0 = 3.2149e-04
Loss = 1.0853e-04, PNorm = 50.8806, GNorm = 0.2021, lr_0 = 3.2115e-04
Loss = 1.0982e-04, PNorm = 50.8857, GNorm = 0.2867, lr_0 = 3.2082e-04
Loss = 9.5835e-05, PNorm = 50.8907, GNorm = 0.2832, lr_0 = 3.2048e-04
Loss = 1.4312e-04, PNorm = 50.8939, GNorm = 0.3524, lr_0 = 3.2015e-04
Loss = 1.2997e-04, PNorm = 50.8971, GNorm = 0.1805, lr_0 = 3.1981e-04
Loss = 1.3647e-04, PNorm = 50.9028, GNorm = 0.2259, lr_0 = 3.1948e-04
Loss = 1.1079e-04, PNorm = 50.9066, GNorm = 0.1843, lr_0 = 3.1915e-04
Loss = 1.2893e-04, PNorm = 50.9100, GNorm = 0.2997, lr_0 = 3.1881e-04
Loss = 1.0953e-04, PNorm = 50.9153, GNorm = 0.3497, lr_0 = 3.1848e-04
Loss = 1.0532e-04, PNorm = 50.9200, GNorm = 0.3300, lr_0 = 3.1815e-04
Loss = 9.5852e-05, PNorm = 50.9226, GNorm = 0.2527, lr_0 = 3.1782e-04
Loss = 1.2170e-04, PNorm = 50.9257, GNorm = 0.1584, lr_0 = 3.1749e-04
Loss = 1.1325e-04, PNorm = 50.9278, GNorm = 0.2070, lr_0 = 3.1715e-04
Loss = 1.2330e-04, PNorm = 50.9318, GNorm = 0.1625, lr_0 = 3.1682e-04
Loss = 1.1767e-04, PNorm = 50.9351, GNorm = 0.4640, lr_0 = 3.1649e-04
Loss = 1.2560e-04, PNorm = 50.9387, GNorm = 0.2931, lr_0 = 3.1616e-04
Loss = 1.3313e-04, PNorm = 50.9465, GNorm = 0.2214, lr_0 = 3.1583e-04
Loss = 1.0016e-04, PNorm = 50.9529, GNorm = 0.2215, lr_0 = 3.1550e-04
Loss = 1.3042e-04, PNorm = 50.9587, GNorm = 0.2825, lr_0 = 3.1517e-04
Loss = 9.9983e-05, PNorm = 50.9630, GNorm = 0.2548, lr_0 = 3.1484e-04
Validation rmse logD = 0.601321
Validation R2 logD = 0.747114
Validation rmse logP = 0.456267
Validation R2 logP = 0.942692
Epoch 51
Train function
Loss = 1.3930e-04, PNorm = 50.9668, GNorm = 0.3450, lr_0 = 3.1448e-04
Loss = 9.4389e-05, PNorm = 50.9686, GNorm = 0.2708, lr_0 = 3.1415e-04
Loss = 1.0587e-04, PNorm = 50.9708, GNorm = 0.3910, lr_0 = 3.1383e-04
Loss = 1.2097e-04, PNorm = 50.9764, GNorm = 0.1896, lr_0 = 3.1350e-04
Loss = 9.6514e-05, PNorm = 50.9797, GNorm = 0.1886, lr_0 = 3.1317e-04
Loss = 1.1703e-04, PNorm = 50.9842, GNorm = 0.2060, lr_0 = 3.1284e-04
Loss = 1.0031e-04, PNorm = 50.9879, GNorm = 0.2846, lr_0 = 3.1252e-04
Loss = 1.1015e-04, PNorm = 50.9941, GNorm = 0.2080, lr_0 = 3.1219e-04
Loss = 1.0447e-04, PNorm = 51.0000, GNorm = 0.2627, lr_0 = 3.1187e-04
Loss = 9.8271e-05, PNorm = 51.0020, GNorm = 0.2484, lr_0 = 3.1154e-04
Loss = 1.1967e-04, PNorm = 51.0023, GNorm = 0.5012, lr_0 = 3.1122e-04
Loss = 1.3763e-04, PNorm = 51.0066, GNorm = 0.5506, lr_0 = 3.1089e-04
Loss = 1.3685e-04, PNorm = 51.0127, GNorm = 0.3233, lr_0 = 3.1057e-04
Loss = 1.1935e-04, PNorm = 51.0162, GNorm = 0.2745, lr_0 = 3.1024e-04
Loss = 1.1154e-04, PNorm = 51.0233, GNorm = 0.2228, lr_0 = 3.0992e-04
Loss = 1.0862e-04, PNorm = 51.0301, GNorm = 0.2909, lr_0 = 3.0959e-04
Loss = 8.4981e-05, PNorm = 51.0325, GNorm = 0.1857, lr_0 = 3.0927e-04
Loss = 7.9471e-05, PNorm = 51.0356, GNorm = 0.1564, lr_0 = 3.0895e-04
Loss = 8.9595e-05, PNorm = 51.0383, GNorm = 0.2431, lr_0 = 3.0863e-04
Loss = 8.2908e-05, PNorm = 51.0390, GNorm = 0.1740, lr_0 = 3.0830e-04
Loss = 9.7863e-05, PNorm = 51.0406, GNorm = 0.3155, lr_0 = 3.0798e-04
Loss = 1.1770e-04, PNorm = 51.0434, GNorm = 0.2441, lr_0 = 3.0766e-04
Loss = 1.0536e-04, PNorm = 51.0458, GNorm = 0.2239, lr_0 = 3.0734e-04
Validation rmse logD = 0.608918
Validation R2 logD = 0.740684
Validation rmse logP = 0.458616
Validation R2 logP = 0.942101
Epoch 52
Train function
Loss = 1.0800e-04, PNorm = 51.0491, GNorm = 0.2976, lr_0 = 3.0699e-04
Loss = 9.2925e-05, PNorm = 51.0520, GNorm = 0.2807, lr_0 = 3.0667e-04
Loss = 8.9054e-05, PNorm = 51.0577, GNorm = 0.2727, lr_0 = 3.0635e-04
Loss = 7.6242e-05, PNorm = 51.0613, GNorm = 0.2615, lr_0 = 3.0603e-04
Loss = 9.0438e-05, PNorm = 51.0647, GNorm = 0.2265, lr_0 = 3.0571e-04
Loss = 8.5830e-05, PNorm = 51.0674, GNorm = 0.3734, lr_0 = 3.0539e-04
Loss = 8.8066e-05, PNorm = 51.0700, GNorm = 0.2652, lr_0 = 3.0507e-04
Loss = 8.2496e-05, PNorm = 51.0732, GNorm = 0.1685, lr_0 = 3.0475e-04
Loss = 9.8531e-05, PNorm = 51.0789, GNorm = 0.2780, lr_0 = 3.0443e-04
Loss = 1.2031e-04, PNorm = 51.0817, GNorm = 0.3504, lr_0 = 3.0412e-04
Loss = 9.2753e-05, PNorm = 51.0844, GNorm = 0.1810, lr_0 = 3.0380e-04
Loss = 1.0429e-04, PNorm = 51.0900, GNorm = 0.2114, lr_0 = 3.0348e-04
Loss = 1.0352e-04, PNorm = 51.0965, GNorm = 0.4051, lr_0 = 3.0316e-04
Loss = 9.5918e-05, PNorm = 51.1010, GNorm = 0.1817, lr_0 = 3.0285e-04
Loss = 9.3840e-05, PNorm = 51.1061, GNorm = 0.1979, lr_0 = 3.0253e-04
Loss = 1.0592e-04, PNorm = 51.1108, GNorm = 0.2859, lr_0 = 3.0222e-04
Loss = 8.8677e-05, PNorm = 51.1157, GNorm = 0.1615, lr_0 = 3.0190e-04
Loss = 1.2305e-04, PNorm = 51.1203, GNorm = 0.1594, lr_0 = 3.0159e-04
Loss = 1.1963e-04, PNorm = 51.1242, GNorm = 0.3396, lr_0 = 3.0127e-04
Loss = 1.1019e-04, PNorm = 51.1256, GNorm = 0.1992, lr_0 = 3.0096e-04
Loss = 1.7719e-04, PNorm = 51.1289, GNorm = 0.5403, lr_0 = 3.0064e-04
Loss = 1.1282e-04, PNorm = 51.1347, GNorm = 0.2199, lr_0 = 3.0033e-04
Loss = 1.1034e-04, PNorm = 51.1408, GNorm = 0.3084, lr_0 = 3.0001e-04
Validation rmse logD = 0.607159
Validation R2 logD = 0.742180
Validation rmse logP = 0.457041
Validation R2 logP = 0.942498
Epoch 53
Train function
Loss = 9.9209e-05, PNorm = 51.1417, GNorm = 0.1991, lr_0 = 2.9970e-04
Loss = 8.3130e-05, PNorm = 51.1468, GNorm = 0.1401, lr_0 = 2.9939e-04
Loss = 9.0884e-05, PNorm = 51.1495, GNorm = 0.2288, lr_0 = 2.9908e-04
Loss = 9.8468e-05, PNorm = 51.1514, GNorm = 0.1869, lr_0 = 2.9876e-04
Loss = 1.0065e-04, PNorm = 51.1550, GNorm = 0.3850, lr_0 = 2.9845e-04
Loss = 8.4208e-05, PNorm = 51.1580, GNorm = 0.1733, lr_0 = 2.9814e-04
Loss = 6.6255e-05, PNorm = 51.1592, GNorm = 0.1709, lr_0 = 2.9783e-04
Loss = 7.0522e-05, PNorm = 51.1626, GNorm = 0.1218, lr_0 = 2.9752e-04
Loss = 7.7376e-05, PNorm = 51.1641, GNorm = 0.2291, lr_0 = 2.9721e-04
Loss = 1.0462e-04, PNorm = 51.1657, GNorm = 0.2392, lr_0 = 2.9690e-04
Loss = 9.1915e-05, PNorm = 51.1665, GNorm = 0.2952, lr_0 = 2.9659e-04
Loss = 9.1430e-05, PNorm = 51.1711, GNorm = 0.3042, lr_0 = 2.9628e-04
Loss = 8.1555e-05, PNorm = 51.1746, GNorm = 0.1214, lr_0 = 2.9597e-04
Loss = 9.1718e-05, PNorm = 51.1780, GNorm = 0.2238, lr_0 = 2.9566e-04
Loss = 1.0341e-04, PNorm = 51.1826, GNorm = 0.2373, lr_0 = 2.9535e-04
Loss = 1.2489e-04, PNorm = 51.1891, GNorm = 0.3710, lr_0 = 2.9504e-04
Loss = 1.3110e-04, PNorm = 51.1924, GNorm = 0.1973, lr_0 = 2.9474e-04
Loss = 1.1099e-04, PNorm = 51.1969, GNorm = 0.2513, lr_0 = 2.9443e-04
Loss = 8.4128e-05, PNorm = 51.1999, GNorm = 0.2695, lr_0 = 2.9412e-04
Loss = 1.1491e-04, PNorm = 51.2026, GNorm = 0.2878, lr_0 = 2.9381e-04
Loss = 8.6877e-05, PNorm = 51.2074, GNorm = 0.1425, lr_0 = 2.9351e-04
Loss = 8.9153e-05, PNorm = 51.2118, GNorm = 0.1457, lr_0 = 2.9320e-04
Validation rmse logD = 0.604076
Validation R2 logD = 0.744791
Validation rmse logP = 0.456401
Validation R2 logP = 0.942659
Epoch 54
Train function
Loss = 7.3421e-05, PNorm = 51.2127, GNorm = 0.2398, lr_0 = 2.9286e-04
Loss = 7.1453e-05, PNorm = 51.2150, GNorm = 0.1775, lr_0 = 2.9256e-04
Loss = 7.4051e-05, PNorm = 51.2161, GNorm = 0.1470, lr_0 = 2.9225e-04
Loss = 7.3522e-05, PNorm = 51.2179, GNorm = 0.1779, lr_0 = 2.9195e-04
Loss = 8.7727e-05, PNorm = 51.2208, GNorm = 0.1503, lr_0 = 2.9164e-04
Loss = 7.0609e-05, PNorm = 51.2249, GNorm = 0.1587, lr_0 = 2.9134e-04
Loss = 7.1198e-05, PNorm = 51.2290, GNorm = 0.1501, lr_0 = 2.9104e-04
Loss = 7.3301e-05, PNorm = 51.2305, GNorm = 0.1624, lr_0 = 2.9073e-04
Loss = 6.6416e-05, PNorm = 51.2331, GNorm = 0.2632, lr_0 = 2.9043e-04
Loss = 7.0772e-05, PNorm = 51.2361, GNorm = 0.1875, lr_0 = 2.9012e-04
Loss = 6.1909e-05, PNorm = 51.2386, GNorm = 0.1698, lr_0 = 2.8982e-04
Loss = 7.2889e-05, PNorm = 51.2406, GNorm = 0.1252, lr_0 = 2.8952e-04
Loss = 6.7122e-05, PNorm = 51.2429, GNorm = 0.1625, lr_0 = 2.8922e-04
Loss = 6.6523e-05, PNorm = 51.2455, GNorm = 0.1316, lr_0 = 2.8892e-04
Loss = 7.2344e-05, PNorm = 51.2492, GNorm = 0.2619, lr_0 = 2.8861e-04
Loss = 8.5975e-05, PNorm = 51.2559, GNorm = 0.4001, lr_0 = 2.8831e-04
Loss = 1.0066e-04, PNorm = 51.2604, GNorm = 0.2724, lr_0 = 2.8801e-04
Loss = 9.9831e-05, PNorm = 51.2640, GNorm = 0.2006, lr_0 = 2.8771e-04
Loss = 7.8195e-05, PNorm = 51.2662, GNorm = 0.2779, lr_0 = 2.8741e-04
Loss = 9.6784e-05, PNorm = 51.2677, GNorm = 0.2021, lr_0 = 2.8711e-04
Loss = 9.3781e-05, PNorm = 51.2728, GNorm = 0.1715, lr_0 = 2.8681e-04
Loss = 9.2867e-05, PNorm = 51.2768, GNorm = 0.1684, lr_0 = 2.8651e-04
Loss = 1.2365e-04, PNorm = 51.2811, GNorm = 0.1422, lr_0 = 2.8621e-04
Validation rmse logD = 0.611646
Validation R2 logD = 0.738355
Validation rmse logP = 0.455455
Validation R2 logP = 0.942896
Epoch 55
Train function
Loss = 8.3336e-05, PNorm = 51.2841, GNorm = 0.1934, lr_0 = 2.8591e-04
Loss = 7.9104e-05, PNorm = 51.2876, GNorm = 0.1956, lr_0 = 2.8562e-04
Loss = 5.8833e-05, PNorm = 51.2900, GNorm = 0.2019, lr_0 = 2.8532e-04
Loss = 7.7153e-05, PNorm = 51.2909, GNorm = 0.2079, lr_0 = 2.8502e-04
Loss = 6.2426e-05, PNorm = 51.2935, GNorm = 0.1354, lr_0 = 2.8472e-04
Loss = 7.3542e-05, PNorm = 51.2975, GNorm = 0.1330, lr_0 = 2.8443e-04
Loss = 6.4144e-05, PNorm = 51.2989, GNorm = 0.1634, lr_0 = 2.8413e-04
Loss = 6.5520e-05, PNorm = 51.3013, GNorm = 0.1453, lr_0 = 2.8383e-04
Loss = 6.8281e-05, PNorm = 51.3039, GNorm = 0.1008, lr_0 = 2.8354e-04
Loss = 8.1434e-05, PNorm = 51.3069, GNorm = 0.2122, lr_0 = 2.8324e-04
Loss = 7.3894e-05, PNorm = 51.3084, GNorm = 0.2128, lr_0 = 2.8294e-04
Loss = 6.6097e-05, PNorm = 51.3130, GNorm = 0.1127, lr_0 = 2.8265e-04
Loss = 6.6722e-05, PNorm = 51.3148, GNorm = 0.1552, lr_0 = 2.8235e-04
Loss = 7.5572e-05, PNorm = 51.3179, GNorm = 0.2083, lr_0 = 2.8206e-04
Loss = 9.2360e-05, PNorm = 51.3209, GNorm = 0.1908, lr_0 = 2.8176e-04
Loss = 7.1414e-05, PNorm = 51.3250, GNorm = 0.1642, lr_0 = 2.8147e-04
Loss = 8.7180e-05, PNorm = 51.3287, GNorm = 0.1548, lr_0 = 2.8118e-04
Loss = 8.7056e-05, PNorm = 51.3316, GNorm = 0.2937, lr_0 = 2.8088e-04
Loss = 7.4309e-05, PNorm = 51.3337, GNorm = 0.1302, lr_0 = 2.8059e-04
Loss = 8.2729e-05, PNorm = 51.3377, GNorm = 0.1706, lr_0 = 2.8030e-04
Loss = 1.0112e-04, PNorm = 51.3417, GNorm = 0.2573, lr_0 = 2.8000e-04
Loss = 8.4318e-05, PNorm = 51.3447, GNorm = 0.1529, lr_0 = 2.7971e-04
Validation rmse logD = 0.604778
Validation R2 logD = 0.744198
Validation rmse logP = 0.457111
Validation R2 logP = 0.942480
Epoch 56
Train function
Loss = 8.7716e-05, PNorm = 51.3456, GNorm = 0.2893, lr_0 = 2.7939e-04
Loss = 7.2917e-05, PNorm = 51.3493, GNorm = 0.1879, lr_0 = 2.7910e-04
Loss = 8.4415e-05, PNorm = 51.3514, GNorm = 0.1567, lr_0 = 2.7881e-04
Loss = 7.1075e-05, PNorm = 51.3545, GNorm = 0.3068, lr_0 = 2.7852e-04
Loss = 7.1168e-05, PNorm = 51.3586, GNorm = 0.1680, lr_0 = 2.7823e-04
Loss = 8.0585e-05, PNorm = 51.3605, GNorm = 0.3209, lr_0 = 2.7794e-04
Loss = 8.8131e-05, PNorm = 51.3644, GNorm = 0.4239, lr_0 = 2.7765e-04
Loss = 6.5919e-05, PNorm = 51.3678, GNorm = 0.2038, lr_0 = 2.7736e-04
Loss = 6.2048e-05, PNorm = 51.3727, GNorm = 0.2293, lr_0 = 2.7707e-04
Loss = 9.0444e-05, PNorm = 51.3753, GNorm = 0.3393, lr_0 = 2.7678e-04
Loss = 7.6385e-05, PNorm = 51.3768, GNorm = 0.1396, lr_0 = 2.7649e-04
Loss = 8.2085e-05, PNorm = 51.3772, GNorm = 0.1524, lr_0 = 2.7620e-04
Loss = 8.5377e-05, PNorm = 51.3797, GNorm = 0.1707, lr_0 = 2.7591e-04
Loss = 1.0131e-04, PNorm = 51.3849, GNorm = 0.2254, lr_0 = 2.7562e-04
Loss = 7.1925e-05, PNorm = 51.3882, GNorm = 0.2555, lr_0 = 2.7534e-04
Loss = 8.6525e-05, PNorm = 51.3930, GNorm = 0.2222, lr_0 = 2.7505e-04
Loss = 7.8202e-05, PNorm = 51.3943, GNorm = 0.3745, lr_0 = 2.7476e-04
Loss = 7.7995e-05, PNorm = 51.3970, GNorm = 0.3755, lr_0 = 2.7448e-04
Loss = 7.3110e-05, PNorm = 51.3988, GNorm = 0.0826, lr_0 = 2.7419e-04
Loss = 8.7387e-05, PNorm = 51.3996, GNorm = 0.2189, lr_0 = 2.7390e-04
Loss = 9.1594e-05, PNorm = 51.4039, GNorm = 0.1411, lr_0 = 2.7362e-04
Loss = 6.7782e-05, PNorm = 51.4067, GNorm = 0.1289, lr_0 = 2.7333e-04
Loss = 9.5507e-05, PNorm = 51.4085, GNorm = 0.3120, lr_0 = 2.7305e-04
Validation rmse logD = 0.613162
Validation R2 logD = 0.737056
Validation rmse logP = 0.454356
Validation R2 logP = 0.943172
Epoch 57
Train function
Loss = 8.6620e-05, PNorm = 51.4117, GNorm = 0.1976, lr_0 = 2.7276e-04
Loss = 9.8441e-05, PNorm = 51.4129, GNorm = 0.2794, lr_0 = 2.7248e-04
Loss = 8.7571e-05, PNorm = 51.4178, GNorm = 0.1873, lr_0 = 2.7219e-04
Loss = 7.3593e-05, PNorm = 51.4222, GNorm = 0.1342, lr_0 = 2.7191e-04
Loss = 6.7092e-05, PNorm = 51.4244, GNorm = 0.1905, lr_0 = 2.7162e-04
Loss = 7.9448e-05, PNorm = 51.4279, GNorm = 0.1830, lr_0 = 2.7134e-04
Loss = 8.6409e-05, PNorm = 51.4311, GNorm = 0.2742, lr_0 = 2.7106e-04
Loss = 8.0929e-05, PNorm = 51.4338, GNorm = 0.2791, lr_0 = 2.7077e-04
Loss = 7.4336e-05, PNorm = 51.4358, GNorm = 0.2091, lr_0 = 2.7049e-04
Loss = 8.6785e-05, PNorm = 51.4391, GNorm = 0.1744, lr_0 = 2.7021e-04
Loss = 7.5326e-05, PNorm = 51.4429, GNorm = 0.2431, lr_0 = 2.6993e-04
Loss = 8.6644e-05, PNorm = 51.4472, GNorm = 0.1324, lr_0 = 2.6965e-04
Loss = 9.5305e-05, PNorm = 51.4508, GNorm = 0.1777, lr_0 = 2.6936e-04
Loss = 8.1018e-05, PNorm = 51.4550, GNorm = 0.2374, lr_0 = 2.6908e-04
Loss = 6.1932e-05, PNorm = 51.4576, GNorm = 0.1853, lr_0 = 2.6880e-04
Loss = 7.5940e-05, PNorm = 51.4602, GNorm = 0.2121, lr_0 = 2.6852e-04
Loss = 7.8499e-05, PNorm = 51.4621, GNorm = 0.3211, lr_0 = 2.6824e-04
Loss = 6.5648e-05, PNorm = 51.4648, GNorm = 0.1719, lr_0 = 2.6796e-04
Loss = 7.1841e-05, PNorm = 51.4677, GNorm = 0.1742, lr_0 = 2.6768e-04
Loss = 6.9612e-05, PNorm = 51.4696, GNorm = 0.1999, lr_0 = 2.6740e-04
Loss = 6.2418e-05, PNorm = 51.4724, GNorm = 0.1893, lr_0 = 2.6712e-04
Loss = 8.4588e-05, PNorm = 51.4745, GNorm = 0.4417, lr_0 = 2.6684e-04
Validation rmse logD = 0.610060
Validation R2 logD = 0.739710
Validation rmse logP = 0.460437
Validation R2 logP = 0.941640
Epoch 58
Train function
Loss = 7.1364e-05, PNorm = 51.4795, GNorm = 0.1998, lr_0 = 2.6654e-04
Loss = 7.1727e-05, PNorm = 51.4813, GNorm = 0.3151, lr_0 = 2.6626e-04
Loss = 7.5855e-05, PNorm = 51.4835, GNorm = 0.2290, lr_0 = 2.6598e-04
Loss = 6.8421e-05, PNorm = 51.4865, GNorm = 0.2558, lr_0 = 2.6570e-04
Loss = 6.1582e-05, PNorm = 51.4914, GNorm = 0.1219, lr_0 = 2.6543e-04
Loss = 5.6551e-05, PNorm = 51.4945, GNorm = 0.1181, lr_0 = 2.6515e-04
Loss = 5.9427e-05, PNorm = 51.4969, GNorm = 0.1344, lr_0 = 2.6487e-04
Loss = 5.4836e-05, PNorm = 51.4998, GNorm = 0.1913, lr_0 = 2.6460e-04
Loss = 5.9324e-05, PNorm = 51.5006, GNorm = 0.1232, lr_0 = 2.6432e-04
Loss = 5.1937e-05, PNorm = 51.5029, GNorm = 0.1090, lr_0 = 2.6405e-04
Loss = 6.1736e-05, PNorm = 51.5055, GNorm = 0.1438, lr_0 = 2.6377e-04
Loss = 5.3823e-05, PNorm = 51.5083, GNorm = 0.2151, lr_0 = 2.6349e-04
Loss = 5.9543e-05, PNorm = 51.5113, GNorm = 0.1194, lr_0 = 2.6322e-04
Loss = 7.6401e-05, PNorm = 51.5148, GNorm = 0.1835, lr_0 = 2.6294e-04
Loss = 6.6830e-05, PNorm = 51.5196, GNorm = 0.1663, lr_0 = 2.6267e-04
Loss = 8.0381e-05, PNorm = 51.5212, GNorm = 0.2625, lr_0 = 2.6240e-04
Loss = 7.4180e-05, PNorm = 51.5245, GNorm = 0.2296, lr_0 = 2.6212e-04
Loss = 8.0874e-05, PNorm = 51.5272, GNorm = 0.2685, lr_0 = 2.6185e-04
Loss = 8.4764e-05, PNorm = 51.5295, GNorm = 0.2699, lr_0 = 2.6158e-04
Loss = 8.4155e-05, PNorm = 51.5321, GNorm = 0.1508, lr_0 = 2.6130e-04
Loss = 6.5114e-05, PNorm = 51.5355, GNorm = 0.1853, lr_0 = 2.6103e-04
Loss = 7.3002e-05, PNorm = 51.5380, GNorm = 0.1326, lr_0 = 2.6076e-04
Loss = 8.0927e-05, PNorm = 51.5410, GNorm = 0.2298, lr_0 = 2.6048e-04
Validation rmse logD = 0.613066
Validation R2 logD = 0.737139
Validation rmse logP = 0.456552
Validation R2 logP = 0.942621
Epoch 59
Train function
Loss = 6.5617e-05, PNorm = 51.5435, GNorm = 0.2062, lr_0 = 2.6021e-04
Loss = 7.5327e-05, PNorm = 51.5463, GNorm = 0.1470, lr_0 = 2.5994e-04
Loss = 6.5717e-05, PNorm = 51.5481, GNorm = 0.1541, lr_0 = 2.5967e-04
Loss = 7.7056e-05, PNorm = 51.5514, GNorm = 0.2403, lr_0 = 2.5940e-04
Loss = 7.7989e-05, PNorm = 51.5565, GNorm = 0.2346, lr_0 = 2.5913e-04
Loss = 5.8981e-05, PNorm = 51.5599, GNorm = 0.1631, lr_0 = 2.5886e-04
Loss = 6.0895e-05, PNorm = 51.5613, GNorm = 0.1879, lr_0 = 2.5859e-04
Loss = 6.9334e-05, PNorm = 51.5640, GNorm = 0.2461, lr_0 = 2.5832e-04
Loss = 6.6634e-05, PNorm = 51.5678, GNorm = 0.1951, lr_0 = 2.5805e-04
Loss = 8.7989e-05, PNorm = 51.5715, GNorm = 0.2232, lr_0 = 2.5778e-04
Loss = 7.0991e-05, PNorm = 51.5740, GNorm = 0.2684, lr_0 = 2.5751e-04
Loss = 1.0341e-04, PNorm = 51.5773, GNorm = 0.2861, lr_0 = 2.5724e-04
Loss = 1.0699e-04, PNorm = 51.5806, GNorm = 0.4069, lr_0 = 2.5697e-04
Loss = 9.2409e-05, PNorm = 51.5824, GNorm = 0.1843, lr_0 = 2.5670e-04
Loss = 7.8177e-05, PNorm = 51.5846, GNorm = 0.1266, lr_0 = 2.5644e-04
Loss = 6.3782e-05, PNorm = 51.5857, GNorm = 0.2695, lr_0 = 2.5617e-04
Loss = 6.0483e-05, PNorm = 51.5887, GNorm = 0.1261, lr_0 = 2.5590e-04
Loss = 6.2419e-05, PNorm = 51.5929, GNorm = 0.1905, lr_0 = 2.5563e-04
Loss = 8.2079e-05, PNorm = 51.5951, GNorm = 0.3796, lr_0 = 2.5537e-04
Loss = 8.3869e-05, PNorm = 51.5986, GNorm = 0.2293, lr_0 = 2.5510e-04
Loss = 7.4656e-05, PNorm = 51.6048, GNorm = 0.3463, lr_0 = 2.5483e-04
Loss = 7.5101e-05, PNorm = 51.6078, GNorm = 0.1861, lr_0 = 2.5457e-04
Validation rmse logD = 0.605138
Validation R2 logD = 0.743893
Validation rmse logP = 0.458886
Validation R2 logP = 0.942033
Epoch 60
Train function
Loss = 4.8696e-05, PNorm = 51.6127, GNorm = 0.1613, lr_0 = 2.5428e-04
Loss = 6.6463e-05, PNorm = 51.6145, GNorm = 0.2891, lr_0 = 2.5401e-04
Loss = 6.0807e-05, PNorm = 51.6178, GNorm = 0.1554, lr_0 = 2.5375e-04
Loss = 6.0883e-05, PNorm = 51.6207, GNorm = 0.1361, lr_0 = 2.5348e-04
Loss = 6.1880e-05, PNorm = 51.6238, GNorm = 0.2885, lr_0 = 2.5322e-04
Loss = 6.9225e-05, PNorm = 51.6263, GNorm = 0.2482, lr_0 = 2.5295e-04
Loss = 6.2338e-05, PNorm = 51.6293, GNorm = 0.2330, lr_0 = 2.5269e-04
Loss = 7.8545e-05, PNorm = 51.6331, GNorm = 0.4475, lr_0 = 2.5242e-04
Loss = 9.0369e-05, PNorm = 51.6358, GNorm = 0.3140, lr_0 = 2.5216e-04
Loss = 8.0415e-05, PNorm = 51.6407, GNorm = 0.1123, lr_0 = 2.5190e-04
Loss = 8.5409e-05, PNorm = 51.6448, GNorm = 0.2458, lr_0 = 2.5163e-04
Loss = 8.1312e-05, PNorm = 51.6474, GNorm = 0.2924, lr_0 = 2.5137e-04
Loss = 7.0375e-05, PNorm = 51.6504, GNorm = 0.1348, lr_0 = 2.5111e-04
Loss = 7.0123e-05, PNorm = 51.6533, GNorm = 0.1947, lr_0 = 2.5085e-04
Loss = 7.8085e-05, PNorm = 51.6572, GNorm = 0.2112, lr_0 = 2.5059e-04
Loss = 6.0307e-05, PNorm = 51.6613, GNorm = 0.2461, lr_0 = 2.5032e-04
Loss = 7.0424e-05, PNorm = 51.6637, GNorm = 0.1678, lr_0 = 2.5006e-04
Loss = 8.0805e-05, PNorm = 51.6652, GNorm = 0.2690, lr_0 = 2.4980e-04
Loss = 7.2237e-05, PNorm = 51.6667, GNorm = 0.1480, lr_0 = 2.4954e-04
Loss = 6.9214e-05, PNorm = 51.6700, GNorm = 0.1472, lr_0 = 2.4928e-04
Loss = 6.4512e-05, PNorm = 51.6730, GNorm = 0.1097, lr_0 = 2.4902e-04
Loss = 5.3236e-05, PNorm = 51.6738, GNorm = 0.1101, lr_0 = 2.4876e-04
Loss = 7.0209e-05, PNorm = 51.6762, GNorm = 0.1617, lr_0 = 2.4850e-04
Validation rmse logD = 0.612843
Validation R2 logD = 0.737330
Validation rmse logP = 0.458428
Validation R2 logP = 0.942148
Epoch 61
Train function
Loss = 5.8753e-05, PNorm = 51.6779, GNorm = 0.3230, lr_0 = 2.4824e-04
Loss = 7.0089e-05, PNorm = 51.6810, GNorm = 0.1090, lr_0 = 2.4798e-04
Loss = 6.6486e-05, PNorm = 51.6841, GNorm = 0.2012, lr_0 = 2.4772e-04
Loss = 5.3198e-05, PNorm = 51.6884, GNorm = 0.1730, lr_0 = 2.4747e-04
Loss = 4.9073e-05, PNorm = 51.6911, GNorm = 0.1830, lr_0 = 2.4721e-04
Loss = 4.8863e-05, PNorm = 51.6921, GNorm = 0.0947, lr_0 = 2.4695e-04
Loss = 4.4475e-05, PNorm = 51.6925, GNorm = 0.1182, lr_0 = 2.4669e-04
Loss = 5.0899e-05, PNorm = 51.6939, GNorm = 0.1575, lr_0 = 2.4643e-04
Loss = 5.9467e-05, PNorm = 51.6975, GNorm = 0.3004, lr_0 = 2.4618e-04
Loss = 5.0879e-05, PNorm = 51.7006, GNorm = 0.1611, lr_0 = 2.4592e-04
Loss = 6.0049e-05, PNorm = 51.7035, GNorm = 0.1785, lr_0 = 2.4566e-04
Loss = 5.8439e-05, PNorm = 51.7060, GNorm = 0.1628, lr_0 = 2.4541e-04
Loss = 5.0027e-05, PNorm = 51.7077, GNorm = 0.1103, lr_0 = 2.4515e-04
Loss = 5.5169e-05, PNorm = 51.7088, GNorm = 0.1817, lr_0 = 2.4489e-04
Loss = 6.0098e-05, PNorm = 51.7106, GNorm = 0.3224, lr_0 = 2.4464e-04
Loss = 6.4193e-05, PNorm = 51.7117, GNorm = 0.2260, lr_0 = 2.4438e-04
Loss = 6.8209e-05, PNorm = 51.7156, GNorm = 0.1844, lr_0 = 2.4413e-04
Loss = 5.1182e-05, PNorm = 51.7192, GNorm = 0.1586, lr_0 = 2.4387e-04
Loss = 5.2442e-05, PNorm = 51.7199, GNorm = 0.1342, lr_0 = 2.4362e-04
Loss = 6.6200e-05, PNorm = 51.7219, GNorm = 0.2115, lr_0 = 2.4337e-04
Loss = 6.0485e-05, PNorm = 51.7250, GNorm = 0.1392, lr_0 = 2.4311e-04
Loss = 7.1919e-05, PNorm = 51.7275, GNorm = 0.2395, lr_0 = 2.4286e-04
Validation rmse logD = 0.610223
Validation R2 logD = 0.739571
Validation rmse logP = 0.456748
Validation R2 logP = 0.942572
Epoch 62
Train function
Loss = 6.0058e-05, PNorm = 51.7319, GNorm = 0.2020, lr_0 = 2.4258e-04
Loss = 4.7100e-05, PNorm = 51.7352, GNorm = 0.1082, lr_0 = 2.4233e-04
Loss = 5.1982e-05, PNorm = 51.7376, GNorm = 0.1367, lr_0 = 2.4207e-04
Loss = 4.1640e-05, PNorm = 51.7402, GNorm = 0.1199, lr_0 = 2.4182e-04
Loss = 5.0694e-05, PNorm = 51.7435, GNorm = 0.2618, lr_0 = 2.4157e-04
Loss = 5.5205e-05, PNorm = 51.7455, GNorm = 0.1309, lr_0 = 2.4132e-04
Loss = 5.2227e-05, PNorm = 51.7474, GNorm = 0.2276, lr_0 = 2.4106e-04
Loss = 4.4950e-05, PNorm = 51.7479, GNorm = 0.1415, lr_0 = 2.4081e-04
Loss = 3.9043e-05, PNorm = 51.7497, GNorm = 0.1738, lr_0 = 2.4056e-04
Loss = 4.5234e-05, PNorm = 51.7516, GNorm = 0.1105, lr_0 = 2.4031e-04
Loss = 4.5746e-05, PNorm = 51.7541, GNorm = 0.0801, lr_0 = 2.4006e-04
Loss = 5.3035e-05, PNorm = 51.7559, GNorm = 0.1597, lr_0 = 2.3981e-04
Loss = 4.6567e-05, PNorm = 51.7564, GNorm = 0.1844, lr_0 = 2.3956e-04
Loss = 6.9680e-05, PNorm = 51.7589, GNorm = 0.1453, lr_0 = 2.3931e-04
Loss = 6.0815e-05, PNorm = 51.7613, GNorm = 0.1583, lr_0 = 2.3906e-04
Loss = 4.5803e-05, PNorm = 51.7621, GNorm = 0.2452, lr_0 = 2.3881e-04
Loss = 5.8092e-05, PNorm = 51.7648, GNorm = 0.2137, lr_0 = 2.3856e-04
Loss = 5.9355e-05, PNorm = 51.7680, GNorm = 0.1939, lr_0 = 2.3831e-04
Loss = 5.6355e-05, PNorm = 51.7708, GNorm = 0.1522, lr_0 = 2.3806e-04
Loss = 5.5588e-05, PNorm = 51.7722, GNorm = 0.2220, lr_0 = 2.3781e-04
Loss = 6.2608e-05, PNorm = 51.7753, GNorm = 0.3306, lr_0 = 2.3756e-04
Loss = 7.3046e-05, PNorm = 51.7781, GNorm = 0.1024, lr_0 = 2.3732e-04
Loss = 7.0269e-05, PNorm = 51.7802, GNorm = 0.3525, lr_0 = 2.3707e-04
Validation rmse logD = 0.616830
Validation R2 logD = 0.733901
Validation rmse logP = 0.458000
Validation R2 logP = 0.942256
Epoch 63
Train function
Loss = 6.1348e-05, PNorm = 51.7823, GNorm = 0.2663, lr_0 = 2.3682e-04
Loss = 7.5893e-05, PNorm = 51.7869, GNorm = 0.2904, lr_0 = 2.3657e-04
Loss = 8.0140e-05, PNorm = 51.7906, GNorm = 0.2780, lr_0 = 2.3633e-04
Loss = 6.5882e-05, PNorm = 51.7947, GNorm = 0.1707, lr_0 = 2.3608e-04
Loss = 6.0804e-05, PNorm = 51.7976, GNorm = 0.2472, lr_0 = 2.3583e-04
Loss = 6.2867e-05, PNorm = 51.7997, GNorm = 0.1965, lr_0 = 2.3559e-04
Loss = 7.2348e-05, PNorm = 51.8029, GNorm = 0.3645, lr_0 = 2.3534e-04
Loss = 6.0095e-05, PNorm = 51.8041, GNorm = 0.3296, lr_0 = 2.3510e-04
Loss = 7.3189e-05, PNorm = 51.8067, GNorm = 0.1504, lr_0 = 2.3485e-04
Loss = 6.8240e-05, PNorm = 51.8094, GNorm = 0.3506, lr_0 = 2.3461e-04
Loss = 6.7952e-05, PNorm = 51.8128, GNorm = 0.1316, lr_0 = 2.3436e-04
Loss = 5.9651e-05, PNorm = 51.8142, GNorm = 0.2114, lr_0 = 2.3412e-04
Loss = 5.9994e-05, PNorm = 51.8157, GNorm = 0.1289, lr_0 = 2.3387e-04
Loss = 6.1351e-05, PNorm = 51.8182, GNorm = 0.1904, lr_0 = 2.3363e-04
Loss = 5.5275e-05, PNorm = 51.8205, GNorm = 0.1572, lr_0 = 2.3338e-04
Loss = 5.0064e-05, PNorm = 51.8234, GNorm = 0.2454, lr_0 = 2.3314e-04
Loss = 4.9283e-05, PNorm = 51.8248, GNorm = 0.0879, lr_0 = 2.3290e-04
Loss = 6.6434e-05, PNorm = 51.8262, GNorm = 0.4072, lr_0 = 2.3265e-04
Loss = 5.5062e-05, PNorm = 51.8276, GNorm = 0.1468, lr_0 = 2.3241e-04
Loss = 6.9147e-05, PNorm = 51.8305, GNorm = 0.2385, lr_0 = 2.3217e-04
Loss = 5.5864e-05, PNorm = 51.8334, GNorm = 0.1480, lr_0 = 2.3193e-04
Loss = 6.8619e-05, PNorm = 51.8388, GNorm = 0.1165, lr_0 = 2.3169e-04
Loss = 5.1234e-05, PNorm = 51.8414, GNorm = 0.0977, lr_0 = 2.3144e-04
Loss = 1.0286e-04, PNorm = 51.8414, GNorm = 0.2801, lr_0 = 2.3142e-04
Validation rmse logD = 0.614188
Validation R2 logD = 0.736176
Validation rmse logP = 0.457920
Validation R2 logP = 0.942276
Epoch 64
Train function
Loss = 5.3874e-05, PNorm = 51.8415, GNorm = 0.1954, lr_0 = 2.3118e-04
Loss = 5.5264e-05, PNorm = 51.8420, GNorm = 0.2514, lr_0 = 2.3094e-04
Loss = 5.5779e-05, PNorm = 51.8467, GNorm = 0.1171, lr_0 = 2.3070e-04
Loss = 5.3750e-05, PNorm = 51.8496, GNorm = 0.0917, lr_0 = 2.3045e-04
Loss = 6.5386e-05, PNorm = 51.8518, GNorm = 0.1009, lr_0 = 2.3021e-04
Loss = 4.9193e-05, PNorm = 51.8526, GNorm = 0.1779, lr_0 = 2.2997e-04
Loss = 6.0010e-05, PNorm = 51.8543, GNorm = 0.1629, lr_0 = 2.2973e-04
Loss = 6.1564e-05, PNorm = 51.8568, GNorm = 0.2272, lr_0 = 2.2949e-04
Loss = 4.6429e-05, PNorm = 51.8588, GNorm = 0.1654, lr_0 = 2.2925e-04
Loss = 6.1734e-05, PNorm = 51.8619, GNorm = 0.2093, lr_0 = 2.2902e-04
Loss = 5.4138e-05, PNorm = 51.8644, GNorm = 0.1789, lr_0 = 2.2878e-04
Loss = 7.4452e-05, PNorm = 51.8684, GNorm = 0.2063, lr_0 = 2.2854e-04
Loss = 6.6918e-05, PNorm = 51.8709, GNorm = 0.2965, lr_0 = 2.2830e-04
Loss = 6.1660e-05, PNorm = 51.8745, GNorm = 0.0990, lr_0 = 2.2806e-04
Loss = 4.8357e-05, PNorm = 51.8779, GNorm = 0.0929, lr_0 = 2.2782e-04
Loss = 6.5064e-05, PNorm = 51.8783, GNorm = 0.2596, lr_0 = 2.2758e-04
Loss = 5.4651e-05, PNorm = 51.8803, GNorm = 0.1023, lr_0 = 2.2735e-04
Loss = 4.5367e-05, PNorm = 51.8836, GNorm = 0.2329, lr_0 = 2.2711e-04
Loss = 5.0210e-05, PNorm = 51.8853, GNorm = 0.2422, lr_0 = 2.2687e-04
Loss = 5.1523e-05, PNorm = 51.8884, GNorm = 0.1327, lr_0 = 2.2664e-04
Loss = 5.2841e-05, PNorm = 51.8900, GNorm = 0.1972, lr_0 = 2.2640e-04
Loss = 5.5560e-05, PNorm = 51.8931, GNorm = 0.2193, lr_0 = 2.2616e-04
Validation rmse logD = 0.612347
Validation R2 logD = 0.737755
Validation rmse logP = 0.454356
Validation R2 logP = 0.943172
Epoch 65
Train function
Loss = 4.2138e-05, PNorm = 51.8947, GNorm = 0.1850, lr_0 = 2.2593e-04
Loss = 4.0176e-05, PNorm = 51.8954, GNorm = 0.1587, lr_0 = 2.2569e-04
Loss = 4.3332e-05, PNorm = 51.8962, GNorm = 0.1633, lr_0 = 2.2546e-04
Loss = 4.8206e-05, PNorm = 51.8965, GNorm = 0.2608, lr_0 = 2.2522e-04
Loss = 4.4891e-05, PNorm = 51.8983, GNorm = 0.1395, lr_0 = 2.2499e-04
Loss = 4.8062e-05, PNorm = 51.9024, GNorm = 0.1394, lr_0 = 2.2475e-04
Loss = 4.8266e-05, PNorm = 51.9037, GNorm = 0.1346, lr_0 = 2.2452e-04
Loss = 5.8758e-05, PNorm = 51.9051, GNorm = 0.1347, lr_0 = 2.2428e-04
Loss = 6.6337e-05, PNorm = 51.9066, GNorm = 0.1623, lr_0 = 2.2405e-04
Loss = 6.1650e-05, PNorm = 51.9104, GNorm = 0.3569, lr_0 = 2.2381e-04
Loss = 5.1964e-05, PNorm = 51.9109, GNorm = 0.2505, lr_0 = 2.2358e-04
Loss = 5.0472e-05, PNorm = 51.9132, GNorm = 0.2408, lr_0 = 2.2335e-04
Loss = 5.1107e-05, PNorm = 51.9161, GNorm = 0.1179, lr_0 = 2.2311e-04
Loss = 3.8892e-05, PNorm = 51.9184, GNorm = 0.2639, lr_0 = 2.2288e-04
Loss = 5.0685e-05, PNorm = 51.9203, GNorm = 0.2085, lr_0 = 2.2265e-04
Loss = 4.3836e-05, PNorm = 51.9220, GNorm = 0.1095, lr_0 = 2.2242e-04
Loss = 6.0538e-05, PNorm = 51.9233, GNorm = 0.1977, lr_0 = 2.2218e-04
Loss = 7.3086e-05, PNorm = 51.9241, GNorm = 0.1666, lr_0 = 2.2195e-04
Loss = 6.0776e-05, PNorm = 51.9268, GNorm = 0.2323, lr_0 = 2.2172e-04
Loss = 7.5164e-05, PNorm = 51.9287, GNorm = 0.1180, lr_0 = 2.2149e-04
Loss = 5.5137e-05, PNorm = 51.9300, GNorm = 0.2019, lr_0 = 2.2126e-04
Loss = 8.1336e-05, PNorm = 51.9351, GNorm = 0.2017, lr_0 = 2.2103e-04
Loss = 7.1579e-05, PNorm = 51.9375, GNorm = 0.4734, lr_0 = 2.2080e-04
Validation rmse logD = 0.612113
Validation R2 logD = 0.737956
Validation rmse logP = 0.467776
Validation R2 logP = 0.939765
Epoch 66
Train function
Loss = 6.3352e-05, PNorm = 51.9397, GNorm = 0.2518, lr_0 = 2.2054e-04
Loss = 6.1441e-05, PNorm = 51.9420, GNorm = 0.1389, lr_0 = 2.2031e-04
Loss = 5.3079e-05, PNorm = 51.9447, GNorm = 0.1607, lr_0 = 2.2008e-04
Loss = 4.1287e-05, PNorm = 51.9474, GNorm = 0.1277, lr_0 = 2.1985e-04
Loss = 5.7153e-05, PNorm = 51.9508, GNorm = 0.1722, lr_0 = 2.1962e-04
Loss = 5.2227e-05, PNorm = 51.9535, GNorm = 0.1840, lr_0 = 2.1939e-04
Loss = 3.9302e-05, PNorm = 51.9534, GNorm = 0.1031, lr_0 = 2.1916e-04
Loss = 4.4342e-05, PNorm = 51.9554, GNorm = 0.1103, lr_0 = 2.1894e-04
Loss = 4.5492e-05, PNorm = 51.9565, GNorm = 0.1110, lr_0 = 2.1871e-04
Loss = 4.7453e-05, PNorm = 51.9587, GNorm = 0.1365, lr_0 = 2.1848e-04
Loss = 4.6420e-05, PNorm = 51.9613, GNorm = 0.1850, lr_0 = 2.1825e-04
Loss = 4.3182e-05, PNorm = 51.9650, GNorm = 0.1350, lr_0 = 2.1802e-04
Loss = 4.6500e-05, PNorm = 51.9672, GNorm = 0.0935, lr_0 = 2.1780e-04
Loss = 4.2362e-05, PNorm = 51.9690, GNorm = 0.1983, lr_0 = 2.1757e-04
Loss = 4.1024e-05, PNorm = 51.9707, GNorm = 0.1419, lr_0 = 2.1734e-04
Loss = 4.8927e-05, PNorm = 51.9725, GNorm = 0.1550, lr_0 = 2.1711e-04
Loss = 5.3921e-05, PNorm = 51.9742, GNorm = 0.1598, lr_0 = 2.1689e-04
Loss = 5.5260e-05, PNorm = 51.9773, GNorm = 0.1247, lr_0 = 2.1666e-04
Loss = 4.8685e-05, PNorm = 51.9791, GNorm = 0.1397, lr_0 = 2.1644e-04
Loss = 4.0060e-05, PNorm = 51.9813, GNorm = 0.1088, lr_0 = 2.1621e-04
Loss = 5.0748e-05, PNorm = 51.9840, GNorm = 0.1741, lr_0 = 2.1598e-04
Loss = 4.7970e-05, PNorm = 51.9863, GNorm = 0.1861, lr_0 = 2.1576e-04
Validation rmse logD = 0.610599
Validation R2 logD = 0.739250
Validation rmse logP = 0.457282
Validation R2 logP = 0.942437
Epoch 67
Train function
Loss = 3.4436e-05, PNorm = 51.9873, GNorm = 0.1120, lr_0 = 2.1553e-04
Loss = 3.7391e-05, PNorm = 51.9888, GNorm = 0.1431, lr_0 = 2.1531e-04
Loss = 4.0999e-05, PNorm = 51.9909, GNorm = 0.1273, lr_0 = 2.1508e-04
Loss = 5.4985e-05, PNorm = 51.9928, GNorm = 0.2054, lr_0 = 2.1486e-04
Loss = 4.3915e-05, PNorm = 51.9953, GNorm = 0.1255, lr_0 = 2.1464e-04
Loss = 3.8939e-05, PNorm = 51.9963, GNorm = 0.1019, lr_0 = 2.1441e-04
Loss = 4.0084e-05, PNorm = 51.9977, GNorm = 0.1432, lr_0 = 2.1419e-04
Loss = 4.2735e-05, PNorm = 52.0011, GNorm = 0.2378, lr_0 = 2.1396e-04
Loss = 4.7818e-05, PNorm = 52.0037, GNorm = 0.1943, lr_0 = 2.1374e-04
Loss = 5.3525e-05, PNorm = 52.0053, GNorm = 0.2239, lr_0 = 2.1352e-04
Loss = 4.8882e-05, PNorm = 52.0078, GNorm = 0.0871, lr_0 = 2.1329e-04
Loss = 4.8451e-05, PNorm = 52.0119, GNorm = 0.1345, lr_0 = 2.1307e-04
Loss = 3.7368e-05, PNorm = 52.0134, GNorm = 0.1315, lr_0 = 2.1285e-04
Loss = 3.4993e-05, PNorm = 52.0144, GNorm = 0.1929, lr_0 = 2.1263e-04
Loss = 5.6446e-05, PNorm = 52.0161, GNorm = 0.2376, lr_0 = 2.1241e-04
Loss = 4.7517e-05, PNorm = 52.0170, GNorm = 0.0906, lr_0 = 2.1218e-04
Loss = 4.8330e-05, PNorm = 52.0198, GNorm = 0.2210, lr_0 = 2.1196e-04
Loss = 4.8541e-05, PNorm = 52.0224, GNorm = 0.1176, lr_0 = 2.1174e-04
Loss = 3.4682e-05, PNorm = 52.0245, GNorm = 0.1265, lr_0 = 2.1152e-04
Loss = 4.0666e-05, PNorm = 52.0259, GNorm = 0.0719, lr_0 = 2.1130e-04
Loss = 4.0121e-05, PNorm = 52.0269, GNorm = 0.1593, lr_0 = 2.1108e-04
Loss = 4.8218e-05, PNorm = 52.0273, GNorm = 0.1548, lr_0 = 2.1086e-04
Loss = 3.7564e-05, PNorm = 52.0293, GNorm = 0.1421, lr_0 = 2.1064e-04
Validation rmse logD = 0.605695
Validation R2 logD = 0.743421
Validation rmse logP = 0.460165
Validation R2 logP = 0.941709
Epoch 68
Train function
Loss = 3.7619e-05, PNorm = 52.0330, GNorm = 0.1064, lr_0 = 2.1040e-04
Loss = 3.9659e-05, PNorm = 52.0351, GNorm = 0.2397, lr_0 = 2.1018e-04
Loss = 4.3490e-05, PNorm = 52.0368, GNorm = 0.2342, lr_0 = 2.0996e-04
Loss = 3.9631e-05, PNorm = 52.0390, GNorm = 0.1094, lr_0 = 2.0974e-04
Loss = 3.7038e-05, PNorm = 52.0410, GNorm = 0.1091, lr_0 = 2.0952e-04
Loss = 4.2226e-05, PNorm = 52.0430, GNorm = 0.1659, lr_0 = 2.0930e-04
Loss = 4.6260e-05, PNorm = 52.0445, GNorm = 0.1148, lr_0 = 2.0908e-04
Loss = 4.2534e-05, PNorm = 52.0471, GNorm = 0.1528, lr_0 = 2.0886e-04
Loss = 3.9006e-05, PNorm = 52.0485, GNorm = 0.1285, lr_0 = 2.0865e-04
Loss = 5.8690e-05, PNorm = 52.0500, GNorm = 0.1850, lr_0 = 2.0843e-04
Loss = 4.4128e-05, PNorm = 52.0517, GNorm = 0.1479, lr_0 = 2.0821e-04
Loss = 5.3397e-05, PNorm = 52.0543, GNorm = 0.1881, lr_0 = 2.0799e-04
Loss = 4.7847e-05, PNorm = 52.0557, GNorm = 0.1379, lr_0 = 2.0778e-04
Loss = 4.5343e-05, PNorm = 52.0560, GNorm = 0.1289, lr_0 = 2.0756e-04
Loss = 3.6841e-05, PNorm = 52.0579, GNorm = 0.0789, lr_0 = 2.0734e-04
Loss = 4.4998e-05, PNorm = 52.0583, GNorm = 0.1114, lr_0 = 2.0713e-04
Loss = 6.3648e-05, PNorm = 52.0611, GNorm = 0.3763, lr_0 = 2.0691e-04
Loss = 6.2937e-05, PNorm = 52.0613, GNorm = 0.1802, lr_0 = 2.0669e-04
Loss = 4.9530e-05, PNorm = 52.0629, GNorm = 0.1264, lr_0 = 2.0648e-04
Loss = 5.1448e-05, PNorm = 52.0648, GNorm = 0.2847, lr_0 = 2.0626e-04
Loss = 5.0221e-05, PNorm = 52.0661, GNorm = 0.2121, lr_0 = 2.0605e-04
Loss = 4.3273e-05, PNorm = 52.0702, GNorm = 0.1251, lr_0 = 2.0583e-04
Validation rmse logD = 0.612436
Validation R2 logD = 0.737678
Validation rmse logP = 0.458058
Validation R2 logP = 0.942242
Epoch 69
Train function
Loss = 4.4579e-05, PNorm = 52.0718, GNorm = 0.1624, lr_0 = 2.0562e-04
Loss = 3.9118e-05, PNorm = 52.0732, GNorm = 0.1031, lr_0 = 2.0540e-04
Loss = 3.3370e-05, PNorm = 52.0741, GNorm = 0.1356, lr_0 = 2.0519e-04
Loss = 3.6207e-05, PNorm = 52.0761, GNorm = 0.1726, lr_0 = 2.0497e-04
Loss = 3.6731e-05, PNorm = 52.0786, GNorm = 0.1274, lr_0 = 2.0476e-04
Loss = 3.0693e-05, PNorm = 52.0808, GNorm = 0.1349, lr_0 = 2.0455e-04
Loss = 4.2417e-05, PNorm = 52.0817, GNorm = 0.1512, lr_0 = 2.0433e-04
Loss = 4.6099e-05, PNorm = 52.0842, GNorm = 0.2123, lr_0 = 2.0412e-04
Loss = 3.5944e-05, PNorm = 52.0858, GNorm = 0.1796, lr_0 = 2.0391e-04
Loss = 5.2735e-05, PNorm = 52.0892, GNorm = 0.1113, lr_0 = 2.0369e-04
Loss = 4.2797e-05, PNorm = 52.0913, GNorm = 0.2303, lr_0 = 2.0348e-04
Loss = 3.8669e-05, PNorm = 52.0926, GNorm = 0.0874, lr_0 = 2.0327e-04
Loss = 5.1309e-05, PNorm = 52.0935, GNorm = 0.1992, lr_0 = 2.0306e-04
Loss = 4.2984e-05, PNorm = 52.0938, GNorm = 0.1213, lr_0 = 2.0285e-04
Loss = 3.7146e-05, PNorm = 52.0945, GNorm = 0.1341, lr_0 = 2.0263e-04
Loss = 3.6706e-05, PNorm = 52.0966, GNorm = 0.2238, lr_0 = 2.0242e-04
Loss = 4.4529e-05, PNorm = 52.0979, GNorm = 0.1527, lr_0 = 2.0221e-04
Loss = 4.0231e-05, PNorm = 52.0995, GNorm = 0.2099, lr_0 = 2.0200e-04
Loss = 4.4675e-05, PNorm = 52.1009, GNorm = 0.1240, lr_0 = 2.0179e-04
Loss = 6.2034e-05, PNorm = 52.1047, GNorm = 0.1528, lr_0 = 2.0158e-04
Loss = 7.4540e-05, PNorm = 52.1091, GNorm = 0.1877, lr_0 = 2.0137e-04
Loss = 6.6566e-05, PNorm = 52.1132, GNorm = 0.4701, lr_0 = 2.0116e-04
Loss = 5.8962e-05, PNorm = 52.1153, GNorm = 0.2162, lr_0 = 2.0095e-04
Validation rmse logD = 0.610292
Validation R2 logD = 0.739512
Validation rmse logP = 0.456609
Validation R2 logP = 0.942606
Epoch 70
Train function
Loss = 4.6980e-05, PNorm = 52.1175, GNorm = 0.1089, lr_0 = 2.0072e-04
Loss = 4.2694e-05, PNorm = 52.1187, GNorm = 0.2552, lr_0 = 2.0051e-04
Loss = 3.7121e-05, PNorm = 52.1196, GNorm = 0.1089, lr_0 = 2.0030e-04
Loss = 4.3580e-05, PNorm = 52.1213, GNorm = 0.1248, lr_0 = 2.0009e-04
Loss = 3.9583e-05, PNorm = 52.1229, GNorm = 0.2139, lr_0 = 1.9988e-04
Loss = 3.2054e-05, PNorm = 52.1223, GNorm = 0.1147, lr_0 = 1.9967e-04
Loss = 4.0404e-05, PNorm = 52.1231, GNorm = 0.2171, lr_0 = 1.9946e-04
Loss = 4.5672e-05, PNorm = 52.1253, GNorm = 0.2142, lr_0 = 1.9926e-04
Loss = 4.3004e-05, PNorm = 52.1272, GNorm = 0.1111, lr_0 = 1.9905e-04
Loss = 5.4281e-05, PNorm = 52.1278, GNorm = 0.1981, lr_0 = 1.9884e-04
Loss = 4.5552e-05, PNorm = 52.1283, GNorm = 0.1655, lr_0 = 1.9863e-04
Loss = 4.2293e-05, PNorm = 52.1296, GNorm = 0.1109, lr_0 = 1.9842e-04
Loss = 3.7862e-05, PNorm = 52.1313, GNorm = 0.2211, lr_0 = 1.9822e-04
Loss = 4.2536e-05, PNorm = 52.1343, GNorm = 0.2158, lr_0 = 1.9801e-04
Loss = 5.0803e-05, PNorm = 52.1383, GNorm = 0.1489, lr_0 = 1.9780e-04
Loss = 3.5958e-05, PNorm = 52.1386, GNorm = 0.0939, lr_0 = 1.9760e-04
Loss = 4.1600e-05, PNorm = 52.1404, GNorm = 0.1557, lr_0 = 1.9739e-04
Loss = 5.5077e-05, PNorm = 52.1429, GNorm = 0.2246, lr_0 = 1.9719e-04
Loss = 6.1028e-05, PNorm = 52.1459, GNorm = 0.2530, lr_0 = 1.9698e-04
Loss = 4.3834e-05, PNorm = 52.1499, GNorm = 0.1339, lr_0 = 1.9677e-04
Loss = 4.1675e-05, PNorm = 52.1523, GNorm = 0.1138, lr_0 = 1.9657e-04
Loss = 5.3848e-05, PNorm = 52.1571, GNorm = 0.0994, lr_0 = 1.9636e-04
Validation rmse logD = 0.612186
Validation R2 logD = 0.737893
Validation rmse logP = 0.459144
Validation R2 logP = 0.941967
Epoch 71
Train function
Loss = 3.4058e-05, PNorm = 52.1586, GNorm = 0.0992, lr_0 = 1.9616e-04
Loss = 3.1360e-05, PNorm = 52.1587, GNorm = 0.1040, lr_0 = 1.9595e-04
Loss = 2.9798e-05, PNorm = 52.1586, GNorm = 0.1089, lr_0 = 1.9575e-04
Loss = 3.0501e-05, PNorm = 52.1587, GNorm = 0.0977, lr_0 = 1.9555e-04
Loss = 3.2995e-05, PNorm = 52.1596, GNorm = 0.1299, lr_0 = 1.9534e-04
Loss = 3.3773e-05, PNorm = 52.1621, GNorm = 0.1454, lr_0 = 1.9514e-04
Loss = 3.1852e-05, PNorm = 52.1627, GNorm = 0.1525, lr_0 = 1.9493e-04
Loss = 4.1025e-05, PNorm = 52.1630, GNorm = 0.1456, lr_0 = 1.9473e-04
Loss = 3.6745e-05, PNorm = 52.1637, GNorm = 0.1738, lr_0 = 1.9453e-04
Loss = 4.4549e-05, PNorm = 52.1669, GNorm = 0.1458, lr_0 = 1.9432e-04
Loss = 4.9004e-05, PNorm = 52.1675, GNorm = 0.1303, lr_0 = 1.9412e-04
Loss = 3.4525e-05, PNorm = 52.1698, GNorm = 0.1942, lr_0 = 1.9392e-04
Loss = 3.0180e-05, PNorm = 52.1712, GNorm = 0.1211, lr_0 = 1.9372e-04
Loss = 3.2392e-05, PNorm = 52.1730, GNorm = 0.0960, lr_0 = 1.9351e-04
Loss = 5.0093e-05, PNorm = 52.1751, GNorm = 0.1978, lr_0 = 1.9331e-04
Loss = 4.7182e-05, PNorm = 52.1760, GNorm = 0.2153, lr_0 = 1.9311e-04
Loss = 5.6451e-05, PNorm = 52.1777, GNorm = 0.2588, lr_0 = 1.9291e-04
Loss = 3.6968e-05, PNorm = 52.1797, GNorm = 0.1539, lr_0 = 1.9271e-04
Loss = 4.2219e-05, PNorm = 52.1815, GNorm = 0.1499, lr_0 = 1.9251e-04
Loss = 4.2529e-05, PNorm = 52.1838, GNorm = 0.2055, lr_0 = 1.9231e-04
Loss = 4.1227e-05, PNorm = 52.1868, GNorm = 0.2058, lr_0 = 1.9210e-04
Loss = 3.6508e-05, PNorm = 52.1883, GNorm = 0.1043, lr_0 = 1.9190e-04
Loss = 3.3192e-05, PNorm = 52.1905, GNorm = 0.1156, lr_0 = 1.9170e-04
Validation rmse logD = 0.607490
Validation R2 logD = 0.741899
Validation rmse logP = 0.457245
Validation R2 logP = 0.942446
Epoch 72
Train function
Loss = 2.9342e-05, PNorm = 52.1938, GNorm = 0.0807, lr_0 = 1.9148e-04
Loss = 3.7726e-05, PNorm = 52.1956, GNorm = 0.2627, lr_0 = 1.9128e-04
Loss = 3.4271e-05, PNorm = 52.1961, GNorm = 0.1355, lr_0 = 1.9108e-04
Loss = 3.9737e-05, PNorm = 52.1984, GNorm = 0.2492, lr_0 = 1.9088e-04
Loss = 4.0037e-05, PNorm = 52.2000, GNorm = 0.1182, lr_0 = 1.9069e-04
Loss = 4.5410e-05, PNorm = 52.2001, GNorm = 0.1644, lr_0 = 1.9049e-04
Loss = 4.9086e-05, PNorm = 52.2024, GNorm = 0.1541, lr_0 = 1.9029e-04
Loss = 3.9677e-05, PNorm = 52.2040, GNorm = 0.1722, lr_0 = 1.9009e-04
Loss = 3.5358e-05, PNorm = 52.2053, GNorm = 0.1242, lr_0 = 1.8989e-04
Loss = 3.5663e-05, PNorm = 52.2057, GNorm = 0.1424, lr_0 = 1.8969e-04
Loss = 3.5164e-05, PNorm = 52.2065, GNorm = 0.0966, lr_0 = 1.8949e-04
Loss = 2.9861e-05, PNorm = 52.2065, GNorm = 0.1163, lr_0 = 1.8930e-04
Loss = 2.9640e-05, PNorm = 52.2081, GNorm = 0.1417, lr_0 = 1.8910e-04
Loss = 4.1976e-05, PNorm = 52.2096, GNorm = 0.2795, lr_0 = 1.8890e-04
Loss = 4.4768e-05, PNorm = 52.2113, GNorm = 0.1345, lr_0 = 1.8870e-04
Loss = 6.1536e-05, PNorm = 52.2139, GNorm = 0.2830, lr_0 = 1.8851e-04
Loss = 4.5634e-05, PNorm = 52.2145, GNorm = 0.1777, lr_0 = 1.8831e-04
Loss = 5.1161e-05, PNorm = 52.2162, GNorm = 0.2060, lr_0 = 1.8811e-04
Loss = 4.2665e-05, PNorm = 52.2185, GNorm = 0.2182, lr_0 = 1.8792e-04
Loss = 5.0242e-05, PNorm = 52.2195, GNorm = 0.1711, lr_0 = 1.8772e-04
Loss = 3.8504e-05, PNorm = 52.2217, GNorm = 0.1988, lr_0 = 1.8753e-04
Loss = 4.1179e-05, PNorm = 52.2215, GNorm = 0.1426, lr_0 = 1.8733e-04
Loss = 4.9731e-05, PNorm = 52.2234, GNorm = 0.1443, lr_0 = 1.8713e-04
Validation rmse logD = 0.614594
Validation R2 logD = 0.735827
Validation rmse logP = 0.459917
Validation R2 logP = 0.941772
Epoch 73
Train function
Loss = 5.2554e-05, PNorm = 52.2263, GNorm = 0.1496, lr_0 = 1.8694e-04
Loss = 4.6580e-05, PNorm = 52.2285, GNorm = 0.2216, lr_0 = 1.8674e-04
Loss = 4.3263e-05, PNorm = 52.2307, GNorm = 0.1001, lr_0 = 1.8655e-04
Loss = 4.4500e-05, PNorm = 52.2314, GNorm = 0.2384, lr_0 = 1.8635e-04
Loss = 3.9600e-05, PNorm = 52.2326, GNorm = 0.1604, lr_0 = 1.8616e-04
Loss = 3.2229e-05, PNorm = 52.2341, GNorm = 0.1197, lr_0 = 1.8597e-04
Loss = 3.6968e-05, PNorm = 52.2357, GNorm = 0.0983, lr_0 = 1.8577e-04
Loss = 3.6896e-05, PNorm = 52.2378, GNorm = 0.1347, lr_0 = 1.8558e-04
Loss = 3.5206e-05, PNorm = 52.2398, GNorm = 0.1589, lr_0 = 1.8538e-04
Loss = 3.3463e-05, PNorm = 52.2415, GNorm = 0.1216, lr_0 = 1.8519e-04
Loss = 3.6847e-05, PNorm = 52.2431, GNorm = 0.0967, lr_0 = 1.8500e-04
Loss = 4.0594e-05, PNorm = 52.2452, GNorm = 0.1536, lr_0 = 1.8480e-04
Loss = 2.7278e-05, PNorm = 52.2476, GNorm = 0.1164, lr_0 = 1.8461e-04
Loss = 3.4391e-05, PNorm = 52.2496, GNorm = 0.1443, lr_0 = 1.8442e-04
Loss = 3.5474e-05, PNorm = 52.2513, GNorm = 0.2038, lr_0 = 1.8423e-04
Loss = 4.8213e-05, PNorm = 52.2543, GNorm = 0.2359, lr_0 = 1.8403e-04
Loss = 3.3573e-05, PNorm = 52.2574, GNorm = 0.1528, lr_0 = 1.8384e-04
Loss = 4.5300e-05, PNorm = 52.2596, GNorm = 0.1633, lr_0 = 1.8365e-04
Loss = 4.8033e-05, PNorm = 52.2611, GNorm = 0.1510, lr_0 = 1.8346e-04
Loss = 3.4566e-05, PNorm = 52.2628, GNorm = 0.1093, lr_0 = 1.8327e-04
Loss = 3.2804e-05, PNorm = 52.2639, GNorm = 0.2727, lr_0 = 1.8308e-04
Loss = 3.8206e-05, PNorm = 52.2671, GNorm = 0.1217, lr_0 = 1.8288e-04
Validation rmse logD = 0.607151
Validation R2 logD = 0.742186
Validation rmse logP = 0.459511
Validation R2 logP = 0.941875
Epoch 74
Train function
Loss = 2.8997e-05, PNorm = 52.2686, GNorm = 0.0706, lr_0 = 1.8267e-04
Loss = 3.6330e-05, PNorm = 52.2686, GNorm = 0.2274, lr_0 = 1.8248e-04
Loss = 3.0583e-05, PNorm = 52.2707, GNorm = 0.0985, lr_0 = 1.8229e-04
Loss = 2.7128e-05, PNorm = 52.2711, GNorm = 0.1465, lr_0 = 1.8210e-04
Loss = 2.6598e-05, PNorm = 52.2714, GNorm = 0.1237, lr_0 = 1.8191e-04
Loss = 2.5746e-05, PNorm = 52.2714, GNorm = 0.1129, lr_0 = 1.8172e-04
Loss = 3.3024e-05, PNorm = 52.2731, GNorm = 0.1020, lr_0 = 1.8153e-04
Loss = 2.6861e-05, PNorm = 52.2753, GNorm = 0.1789, lr_0 = 1.8134e-04
Loss = 3.4976e-05, PNorm = 52.2757, GNorm = 0.0720, lr_0 = 1.8115e-04
Loss = 3.3563e-05, PNorm = 52.2760, GNorm = 0.0928, lr_0 = 1.8097e-04
Loss = 3.7593e-05, PNorm = 52.2778, GNorm = 0.1705, lr_0 = 1.8078e-04
Loss = 3.3033e-05, PNorm = 52.2792, GNorm = 0.1799, lr_0 = 1.8059e-04
Loss = 3.0186e-05, PNorm = 52.2807, GNorm = 0.1121, lr_0 = 1.8040e-04
Loss = 3.4616e-05, PNorm = 52.2811, GNorm = 0.0786, lr_0 = 1.8021e-04
Loss = 2.8426e-05, PNorm = 52.2816, GNorm = 0.1153, lr_0 = 1.8002e-04
Loss = 2.9695e-05, PNorm = 52.2829, GNorm = 0.0996, lr_0 = 1.7984e-04
Loss = 2.4608e-05, PNorm = 52.2851, GNorm = 0.1486, lr_0 = 1.7965e-04
Loss = 2.8023e-05, PNorm = 52.2865, GNorm = 0.1127, lr_0 = 1.7946e-04
Loss = 3.1373e-05, PNorm = 52.2877, GNorm = 0.1270, lr_0 = 1.7927e-04
Loss = 4.3515e-05, PNorm = 52.2904, GNorm = 0.2220, lr_0 = 1.7909e-04
Loss = 3.8513e-05, PNorm = 52.2924, GNorm = 0.1256, lr_0 = 1.7890e-04
Loss = 5.4778e-05, PNorm = 52.2949, GNorm = 0.2573, lr_0 = 1.7871e-04
Loss = 3.8215e-05, PNorm = 52.2967, GNorm = 0.2125, lr_0 = 1.7853e-04
Validation rmse logD = 0.610931
Validation R2 logD = 0.738966
Validation rmse logP = 0.457441
Validation R2 logP = 0.942397
Epoch 75
Train function
Loss = 3.6825e-05, PNorm = 52.2985, GNorm = 0.0710, lr_0 = 1.7834e-04
Loss = 3.0247e-05, PNorm = 52.3007, GNorm = 0.1143, lr_0 = 1.7815e-04
Loss = 3.3217e-05, PNorm = 52.3030, GNorm = 0.1599, lr_0 = 1.7797e-04
Loss = 2.8099e-05, PNorm = 52.3049, GNorm = 0.1746, lr_0 = 1.7778e-04
Loss = 2.7291e-05, PNorm = 52.3046, GNorm = 0.1053, lr_0 = 1.7760e-04
Loss = 2.4633e-05, PNorm = 52.3055, GNorm = 0.1237, lr_0 = 1.7741e-04
Loss = 2.9210e-05, PNorm = 52.3068, GNorm = 0.0721, lr_0 = 1.7723e-04
Loss = 2.6574e-05, PNorm = 52.3073, GNorm = 0.1618, lr_0 = 1.7704e-04
Loss = 2.6812e-05, PNorm = 52.3088, GNorm = 0.0956, lr_0 = 1.7686e-04
Loss = 3.7089e-05, PNorm = 52.3094, GNorm = 0.2945, lr_0 = 1.7667e-04
Loss = 2.5927e-05, PNorm = 52.3113, GNorm = 0.1302, lr_0 = 1.7649e-04
Loss = 4.2224e-05, PNorm = 52.3137, GNorm = 0.1749, lr_0 = 1.7630e-04
Loss = 3.4645e-05, PNorm = 52.3152, GNorm = 0.1801, lr_0 = 1.7612e-04
Loss = 3.7938e-05, PNorm = 52.3172, GNorm = 0.1302, lr_0 = 1.7593e-04
Loss = 2.7686e-05, PNorm = 52.3180, GNorm = 0.0990, lr_0 = 1.7575e-04
Loss = 3.1094e-05, PNorm = 52.3198, GNorm = 0.1490, lr_0 = 1.7557e-04
Loss = 4.2845e-05, PNorm = 52.3209, GNorm = 0.2832, lr_0 = 1.7538e-04
Loss = 4.8659e-05, PNorm = 52.3231, GNorm = 0.1699, lr_0 = 1.7520e-04
Loss = 4.1029e-05, PNorm = 52.3257, GNorm = 0.1113, lr_0 = 1.7502e-04
Loss = 3.3217e-05, PNorm = 52.3284, GNorm = 0.1043, lr_0 = 1.7484e-04
Loss = 3.3843e-05, PNorm = 52.3302, GNorm = 0.1361, lr_0 = 1.7465e-04
Loss = 3.1855e-05, PNorm = 52.3319, GNorm = 0.0726, lr_0 = 1.7447e-04
Validation rmse logD = 0.609286
Validation R2 logD = 0.740370
Validation rmse logP = 0.457332
Validation R2 logP = 0.942425
Epoch 76
Train function
Loss = 2.7736e-05, PNorm = 52.3324, GNorm = 0.1145, lr_0 = 1.7427e-04
Loss = 2.4980e-05, PNorm = 52.3338, GNorm = 0.0829, lr_0 = 1.7409e-04
Loss = 3.3809e-05, PNorm = 52.3362, GNorm = 0.1360, lr_0 = 1.7391e-04
Loss = 3.5549e-05, PNorm = 52.3384, GNorm = 0.1953, lr_0 = 1.7373e-04
Loss = 3.2110e-05, PNorm = 52.3392, GNorm = 0.1365, lr_0 = 1.7354e-04
Loss = 3.5964e-05, PNorm = 52.3377, GNorm = 0.1450, lr_0 = 1.7336e-04
Loss = 3.9001e-05, PNorm = 52.3394, GNorm = 0.1782, lr_0 = 1.7318e-04
Loss = 3.5381e-05, PNorm = 52.3414, GNorm = 0.1406, lr_0 = 1.7300e-04
Loss = 3.6231e-05, PNorm = 52.3403, GNorm = 0.2444, lr_0 = 1.7282e-04
Loss = 5.4944e-05, PNorm = 52.3427, GNorm = 0.2267, lr_0 = 1.7264e-04
Loss = 4.8849e-05, PNorm = 52.3456, GNorm = 0.1901, lr_0 = 1.7246e-04
Loss = 4.4823e-05, PNorm = 52.3481, GNorm = 0.2238, lr_0 = 1.7228e-04
Loss = 4.8471e-05, PNorm = 52.3509, GNorm = 0.3471, lr_0 = 1.7210e-04
Loss = 4.2806e-05, PNorm = 52.3535, GNorm = 0.1099, lr_0 = 1.7192e-04
Loss = 3.6169e-05, PNorm = 52.3562, GNorm = 0.2208, lr_0 = 1.7174e-04
Loss = 2.7761e-05, PNorm = 52.3570, GNorm = 0.0769, lr_0 = 1.7156e-04
Loss = 3.6357e-05, PNorm = 52.3588, GNorm = 0.1642, lr_0 = 1.7138e-04
Loss = 3.0061e-05, PNorm = 52.3601, GNorm = 0.1481, lr_0 = 1.7120e-04
Loss = 2.9180e-05, PNorm = 52.3616, GNorm = 0.1027, lr_0 = 1.7103e-04
Loss = 3.6630e-05, PNorm = 52.3629, GNorm = 0.1744, lr_0 = 1.7085e-04
Loss = 3.2513e-05, PNorm = 52.3638, GNorm = 0.1086, lr_0 = 1.7067e-04
Loss = 4.6632e-05, PNorm = 52.3651, GNorm = 0.0961, lr_0 = 1.7049e-04
Loss = 3.9238e-05, PNorm = 52.3668, GNorm = 0.1302, lr_0 = 1.7031e-04
Validation rmse logD = 0.604965
Validation R2 logD = 0.744039
Validation rmse logP = 0.456584
Validation R2 logP = 0.942613
Epoch 77
Train function
Loss = 3.1919e-05, PNorm = 52.3690, GNorm = 0.1251, lr_0 = 1.7012e-04
Loss = 2.3416e-05, PNorm = 52.3695, GNorm = 0.1014, lr_0 = 1.6994e-04
Loss = 2.8432e-05, PNorm = 52.3699, GNorm = 0.1329, lr_0 = 1.6976e-04
Loss = 2.7893e-05, PNorm = 52.3717, GNorm = 0.1194, lr_0 = 1.6959e-04
Loss = 2.7668e-05, PNorm = 52.3721, GNorm = 0.1530, lr_0 = 1.6941e-04
Loss = 2.1623e-05, PNorm = 52.3730, GNorm = 0.1143, lr_0 = 1.6923e-04
Loss = 2.6933e-05, PNorm = 52.3744, GNorm = 0.1044, lr_0 = 1.6905e-04
Loss = 2.4230e-05, PNorm = 52.3761, GNorm = 0.0891, lr_0 = 1.6888e-04
Loss = 2.5282e-05, PNorm = 52.3772, GNorm = 0.1606, lr_0 = 1.6870e-04
Loss = 3.2455e-05, PNorm = 52.3788, GNorm = 0.1852, lr_0 = 1.6853e-04
Loss = 4.6031e-05, PNorm = 52.3791, GNorm = 0.1276, lr_0 = 1.6835e-04
Loss = 3.9675e-05, PNorm = 52.3797, GNorm = 0.2323, lr_0 = 1.6817e-04
Loss = 3.7674e-05, PNorm = 52.3807, GNorm = 0.1954, lr_0 = 1.6800e-04
Loss = 5.5685e-05, PNorm = 52.3822, GNorm = 0.1903, lr_0 = 1.6782e-04
Loss = 5.7834e-05, PNorm = 52.3837, GNorm = 0.2347, lr_0 = 1.6765e-04
Loss = 3.2926e-05, PNorm = 52.3840, GNorm = 0.1910, lr_0 = 1.6747e-04
Loss = 3.5913e-05, PNorm = 52.3857, GNorm = 0.1146, lr_0 = 1.6730e-04
Loss = 4.2476e-05, PNorm = 52.3877, GNorm = 0.1573, lr_0 = 1.6712e-04
Loss = 2.3840e-05, PNorm = 52.3898, GNorm = 0.1129, lr_0 = 1.6695e-04
Loss = 2.8135e-05, PNorm = 52.3906, GNorm = 0.0738, lr_0 = 1.6678e-04
Loss = 2.8092e-05, PNorm = 52.3912, GNorm = 0.1748, lr_0 = 1.6660e-04
Loss = 3.4706e-05, PNorm = 52.3927, GNorm = 0.1677, lr_0 = 1.6643e-04
Validation rmse logD = 0.611181
Validation R2 logD = 0.738753
Validation rmse logP = 0.458455
Validation R2 logP = 0.942141
Epoch 78
Train function
Loss = 3.4233e-05, PNorm = 52.3938, GNorm = 0.1767, lr_0 = 1.6625e-04
Loss = 4.2759e-05, PNorm = 52.3948, GNorm = 0.2554, lr_0 = 1.6608e-04
Loss = 2.8937e-05, PNorm = 52.3970, GNorm = 0.1225, lr_0 = 1.6591e-04
Loss = 2.6059e-05, PNorm = 52.3992, GNorm = 0.2095, lr_0 = 1.6573e-04
Loss = 2.8391e-05, PNorm = 52.4000, GNorm = 0.1442, lr_0 = 1.6556e-04
Loss = 2.8303e-05, PNorm = 52.4011, GNorm = 0.1210, lr_0 = 1.6539e-04
Loss = 2.3620e-05, PNorm = 52.4022, GNorm = 0.0716, lr_0 = 1.6522e-04
Loss = 2.2135e-05, PNorm = 52.4036, GNorm = 0.0713, lr_0 = 1.6504e-04
Loss = 2.2883e-05, PNorm = 52.4049, GNorm = 0.1524, lr_0 = 1.6487e-04
Loss = 2.8355e-05, PNorm = 52.4061, GNorm = 0.1793, lr_0 = 1.6470e-04
Loss = 2.3804e-05, PNorm = 52.4073, GNorm = 0.1585, lr_0 = 1.6453e-04
Loss = 2.4134e-05, PNorm = 52.4090, GNorm = 0.1650, lr_0 = 1.6435e-04
Loss = 4.1637e-05, PNorm = 52.4096, GNorm = 0.1500, lr_0 = 1.6418e-04
Loss = 3.4751e-05, PNorm = 52.4105, GNorm = 0.1516, lr_0 = 1.6401e-04
Loss = 4.1188e-05, PNorm = 52.4122, GNorm = 0.2917, lr_0 = 1.6384e-04
Loss = 3.3058e-05, PNorm = 52.4134, GNorm = 0.1435, lr_0 = 1.6367e-04
Loss = 2.9048e-05, PNorm = 52.4153, GNorm = 0.0778, lr_0 = 1.6350e-04
Loss = 3.2172e-05, PNorm = 52.4175, GNorm = 0.0911, lr_0 = 1.6333e-04
Loss = 2.4138e-05, PNorm = 52.4186, GNorm = 0.1695, lr_0 = 1.6316e-04
Loss = 3.1418e-05, PNorm = 52.4189, GNorm = 0.0921, lr_0 = 1.6299e-04
Loss = 3.3265e-05, PNorm = 52.4190, GNorm = 0.1280, lr_0 = 1.6282e-04
Loss = 3.5066e-05, PNorm = 52.4199, GNorm = 0.1689, lr_0 = 1.6265e-04
Loss = 3.7252e-05, PNorm = 52.4210, GNorm = 0.1360, lr_0 = 1.6248e-04
Validation rmse logD = 0.602048
Validation R2 logD = 0.746502
Validation rmse logP = 0.458025
Validation R2 logP = 0.942250
Epoch 79
Train function
Loss = 3.0480e-05, PNorm = 52.4229, GNorm = 0.1921, lr_0 = 1.6229e-04
Loss = 3.1142e-05, PNorm = 52.4235, GNorm = 0.0930, lr_0 = 1.6212e-04
Loss = 2.8406e-05, PNorm = 52.4258, GNorm = 0.1102, lr_0 = 1.6195e-04
Loss = 2.4729e-05, PNorm = 52.4281, GNorm = 0.0919, lr_0 = 1.6178e-04
Loss = 3.5593e-05, PNorm = 52.4312, GNorm = 0.1418, lr_0 = 1.6161e-04
Loss = 2.8426e-05, PNorm = 52.4315, GNorm = 0.1132, lr_0 = 1.6145e-04
Loss = 3.3492e-05, PNorm = 52.4322, GNorm = 0.1240, lr_0 = 1.6128e-04
Loss = 2.6847e-05, PNorm = 52.4351, GNorm = 0.2828, lr_0 = 1.6111e-04
Loss = 2.8602e-05, PNorm = 52.4359, GNorm = 0.1786, lr_0 = 1.6094e-04
Loss = 2.1693e-05, PNorm = 52.4373, GNorm = 0.1002, lr_0 = 1.6077e-04
Loss = 3.4080e-05, PNorm = 52.4374, GNorm = 0.1230, lr_0 = 1.6061e-04
Loss = 2.9002e-05, PNorm = 52.4389, GNorm = 0.1220, lr_0 = 1.6044e-04
Loss = 2.6268e-05, PNorm = 52.4400, GNorm = 0.0849, lr_0 = 1.6027e-04
Loss = 3.0727e-05, PNorm = 52.4403, GNorm = 0.1715, lr_0 = 1.6010e-04
Loss = 2.4018e-05, PNorm = 52.4423, GNorm = 0.0703, lr_0 = 1.5994e-04
Loss = 2.9676e-05, PNorm = 52.4443, GNorm = 0.1731, lr_0 = 1.5977e-04
Loss = 2.7342e-05, PNorm = 52.4459, GNorm = 0.0561, lr_0 = 1.5960e-04
Loss = 2.8584e-05, PNorm = 52.4466, GNorm = 0.1153, lr_0 = 1.5944e-04
Loss = 2.7915e-05, PNorm = 52.4473, GNorm = 0.0723, lr_0 = 1.5927e-04
Loss = 2.7818e-05, PNorm = 52.4492, GNorm = 0.0829, lr_0 = 1.5910e-04
Loss = 2.6890e-05, PNorm = 52.4500, GNorm = 0.1277, lr_0 = 1.5894e-04
Loss = 2.3306e-05, PNorm = 52.4509, GNorm = 0.0884, lr_0 = 1.5877e-04
Validation rmse logD = 0.603318
Validation R2 logD = 0.745431
Validation rmse logP = 0.458010
Validation R2 logP = 0.942254
Epoch 80
Train function
Loss = 4.0854e-05, PNorm = 52.4519, GNorm = 0.1299, lr_0 = 1.5861e-04
Loss = 2.4798e-05, PNorm = 52.4523, GNorm = 0.0901, lr_0 = 1.5844e-04
Loss = 1.8895e-05, PNorm = 52.4525, GNorm = 0.0780, lr_0 = 1.5827e-04
Loss = 2.0840e-05, PNorm = 52.4538, GNorm = 0.1095, lr_0 = 1.5811e-04
Loss = 2.1879e-05, PNorm = 52.4547, GNorm = 0.1236, lr_0 = 1.5794e-04
Loss = 2.8327e-05, PNorm = 52.4549, GNorm = 0.0896, lr_0 = 1.5778e-04
Loss = 2.4234e-05, PNorm = 52.4556, GNorm = 0.2028, lr_0 = 1.5761e-04
Loss = 2.3423e-05, PNorm = 52.4572, GNorm = 0.0556, lr_0 = 1.5745e-04
Loss = 2.4177e-05, PNorm = 52.4583, GNorm = 0.0912, lr_0 = 1.5729e-04
Loss = 2.8072e-05, PNorm = 52.4584, GNorm = 0.0793, lr_0 = 1.5712e-04
Loss = 2.3855e-05, PNorm = 52.4591, GNorm = 0.0989, lr_0 = 1.5696e-04
Loss = 2.7611e-05, PNorm = 52.4603, GNorm = 0.1441, lr_0 = 1.5679e-04
Loss = 2.5195e-05, PNorm = 52.4619, GNorm = 0.1689, lr_0 = 1.5663e-04
Loss = 2.2891e-05, PNorm = 52.4634, GNorm = 0.1309, lr_0 = 1.5647e-04
Loss = 2.2633e-05, PNorm = 52.4644, GNorm = 0.0881, lr_0 = 1.5630e-04
Loss = 2.8343e-05, PNorm = 52.4653, GNorm = 0.1849, lr_0 = 1.5614e-04
Loss = 2.7305e-05, PNorm = 52.4661, GNorm = 0.0903, lr_0 = 1.5598e-04
Loss = 2.3341e-05, PNorm = 52.4673, GNorm = 0.0531, lr_0 = 1.5581e-04
Loss = 2.8292e-05, PNorm = 52.4688, GNorm = 0.1669, lr_0 = 1.5565e-04
Loss = 3.0808e-05, PNorm = 52.4687, GNorm = 0.1623, lr_0 = 1.5549e-04
Loss = 2.4239e-05, PNorm = 52.4707, GNorm = 0.1266, lr_0 = 1.5533e-04
Loss = 2.5503e-05, PNorm = 52.4722, GNorm = 0.1297, lr_0 = 1.5516e-04
Loss = 2.4509e-05, PNorm = 52.4729, GNorm = 0.1037, lr_0 = 1.5500e-04
Validation rmse logD = 0.606121
Validation R2 logD = 0.743061
Validation rmse logP = 0.457568
Validation R2 logP = 0.942365
Epoch 81
Train function
Loss = 3.0529e-05, PNorm = 52.4758, GNorm = 0.1274, lr_0 = 1.5483e-04
Loss = 2.7712e-05, PNorm = 52.4774, GNorm = 0.1947, lr_0 = 1.5466e-04
Loss = 2.4991e-05, PNorm = 52.4777, GNorm = 0.0919, lr_0 = 1.5450e-04
Loss = 3.1182e-05, PNorm = 52.4786, GNorm = 0.0749, lr_0 = 1.5434e-04
Loss = 4.1487e-05, PNorm = 52.4806, GNorm = 0.1315, lr_0 = 1.5418e-04
Loss = 3.0660e-05, PNorm = 52.4821, GNorm = 0.1266, lr_0 = 1.5402e-04
Loss = 3.0633e-05, PNorm = 52.4844, GNorm = 0.1195, lr_0 = 1.5386e-04
Loss = 2.5427e-05, PNorm = 52.4851, GNorm = 0.1059, lr_0 = 1.5370e-04
Loss = 2.4148e-05, PNorm = 52.4845, GNorm = 0.1386, lr_0 = 1.5354e-04
Loss = 2.5879e-05, PNorm = 52.4863, GNorm = 0.2389, lr_0 = 1.5338e-04
Loss = 2.6457e-05, PNorm = 52.4881, GNorm = 0.2445, lr_0 = 1.5322e-04
Loss = 2.7635e-05, PNorm = 52.4898, GNorm = 0.0810, lr_0 = 1.5306e-04
Loss = 2.4005e-05, PNorm = 52.4914, GNorm = 0.1118, lr_0 = 1.5290e-04
Loss = 2.2794e-05, PNorm = 52.4922, GNorm = 0.1050, lr_0 = 1.5274e-04
Loss = 1.9142e-05, PNorm = 52.4931, GNorm = 0.1122, lr_0 = 1.5258e-04
Loss = 1.8335e-05, PNorm = 52.4930, GNorm = 0.0673, lr_0 = 1.5242e-04
Loss = 2.2402e-05, PNorm = 52.4941, GNorm = 0.0891, lr_0 = 1.5226e-04
Loss = 2.2738e-05, PNorm = 52.4944, GNorm = 0.1354, lr_0 = 1.5210e-04
Loss = 2.8523e-05, PNorm = 52.4955, GNorm = 0.1113, lr_0 = 1.5194e-04
Loss = 3.3506e-05, PNorm = 52.4962, GNorm = 0.1754, lr_0 = 1.5178e-04
Loss = 2.3205e-05, PNorm = 52.4979, GNorm = 0.1002, lr_0 = 1.5163e-04
Loss = 2.1098e-05, PNorm = 52.4993, GNorm = 0.1172, lr_0 = 1.5147e-04
Validation rmse logD = 0.605819
Validation R2 logD = 0.743317
Validation rmse logP = 0.458669
Validation R2 logP = 0.942087
Epoch 82
Train function
Loss = 2.7022e-05, PNorm = 52.5000, GNorm = 0.1380, lr_0 = 1.5131e-04
Loss = 2.8913e-05, PNorm = 52.5015, GNorm = 0.1260, lr_0 = 1.5115e-04
Loss = 2.7157e-05, PNorm = 52.5032, GNorm = 0.1031, lr_0 = 1.5099e-04
Loss = 2.5579e-05, PNorm = 52.5040, GNorm = 0.1133, lr_0 = 1.5084e-04
Loss = 2.7487e-05, PNorm = 52.5052, GNorm = 0.0871, lr_0 = 1.5068e-04
Loss = 2.3589e-05, PNorm = 52.5064, GNorm = 0.1055, lr_0 = 1.5052e-04
Loss = 2.4995e-05, PNorm = 52.5072, GNorm = 0.1423, lr_0 = 1.5036e-04
Loss = 2.7265e-05, PNorm = 52.5085, GNorm = 0.1022, lr_0 = 1.5021e-04
Loss = 2.1861e-05, PNorm = 52.5102, GNorm = 0.0858, lr_0 = 1.5005e-04
Loss = 2.2920e-05, PNorm = 52.5113, GNorm = 0.1542, lr_0 = 1.4989e-04
Loss = 3.3237e-05, PNorm = 52.5124, GNorm = 0.1056, lr_0 = 1.4974e-04
Loss = 2.9106e-05, PNorm = 52.5131, GNorm = 0.1372, lr_0 = 1.4958e-04
Loss = 2.5532e-05, PNorm = 52.5127, GNorm = 0.1853, lr_0 = 1.4942e-04
Loss = 2.5994e-05, PNorm = 52.5143, GNorm = 0.0875, lr_0 = 1.4927e-04
Loss = 1.9674e-05, PNorm = 52.5162, GNorm = 0.0804, lr_0 = 1.4911e-04
Loss = 2.7856e-05, PNorm = 52.5170, GNorm = 0.0922, lr_0 = 1.4896e-04
Loss = 2.8532e-05, PNorm = 52.5186, GNorm = 0.0732, lr_0 = 1.4880e-04
Loss = 2.5238e-05, PNorm = 52.5198, GNorm = 0.0898, lr_0 = 1.4865e-04
Loss = 2.8184e-05, PNorm = 52.5206, GNorm = 0.0838, lr_0 = 1.4849e-04
Loss = 2.9986e-05, PNorm = 52.5211, GNorm = 0.1822, lr_0 = 1.4834e-04
Loss = 2.3780e-05, PNorm = 52.5219, GNorm = 0.1196, lr_0 = 1.4818e-04
Loss = 2.2934e-05, PNorm = 52.5224, GNorm = 0.1463, lr_0 = 1.4803e-04
Loss = 3.1438e-05, PNorm = 52.5236, GNorm = 0.0972, lr_0 = 1.4787e-04
Validation rmse logD = 0.610582
Validation R2 logD = 0.739264
Validation rmse logP = 0.458883
Validation R2 logP = 0.942033
Epoch 83
Train function
Loss = 2.1983e-05, PNorm = 52.5268, GNorm = 0.0926, lr_0 = 1.4770e-04
Loss = 2.2535e-05, PNorm = 52.5279, GNorm = 0.1074, lr_0 = 1.4755e-04
Loss = 1.4906e-05, PNorm = 52.5284, GNorm = 0.0854, lr_0 = 1.4739e-04
Loss = 2.5317e-05, PNorm = 52.5286, GNorm = 0.1131, lr_0 = 1.4724e-04
Loss = 2.9727e-05, PNorm = 52.5287, GNorm = 0.1421, lr_0 = 1.4709e-04
Loss = 2.5469e-05, PNorm = 52.5298, GNorm = 0.1744, lr_0 = 1.4693e-04
Loss = 2.8973e-05, PNorm = 52.5311, GNorm = 0.1126, lr_0 = 1.4678e-04
Loss = 2.6582e-05, PNorm = 52.5321, GNorm = 0.2238, lr_0 = 1.4663e-04
Loss = 3.4130e-05, PNorm = 52.5343, GNorm = 0.1137, lr_0 = 1.4647e-04
Loss = 3.0247e-05, PNorm = 52.5357, GNorm = 0.1011, lr_0 = 1.4632e-04
Loss = 3.0645e-05, PNorm = 52.5366, GNorm = 0.1065, lr_0 = 1.4617e-04
Loss = 3.7122e-05, PNorm = 52.5372, GNorm = 0.1580, lr_0 = 1.4602e-04
Loss = 2.7905e-05, PNorm = 52.5390, GNorm = 0.1609, lr_0 = 1.4586e-04
Loss = 2.3059e-05, PNorm = 52.5412, GNorm = 0.1095, lr_0 = 1.4571e-04
Loss = 2.3817e-05, PNorm = 52.5423, GNorm = 0.0588, lr_0 = 1.4556e-04
Loss = 2.3996e-05, PNorm = 52.5427, GNorm = 0.1112, lr_0 = 1.4541e-04
Loss = 2.0679e-05, PNorm = 52.5440, GNorm = 0.0946, lr_0 = 1.4526e-04
Loss = 1.9877e-05, PNorm = 52.5449, GNorm = 0.1076, lr_0 = 1.4510e-04
Loss = 2.9702e-05, PNorm = 52.5454, GNorm = 0.1203, lr_0 = 1.4495e-04
Loss = 2.2480e-05, PNorm = 52.5469, GNorm = 0.0981, lr_0 = 1.4480e-04
Loss = 2.0470e-05, PNorm = 52.5484, GNorm = 0.0937, lr_0 = 1.4465e-04
Loss = 2.1088e-05, PNorm = 52.5495, GNorm = 0.0887, lr_0 = 1.4450e-04
Loss = 2.7772e-05, PNorm = 52.5496, GNorm = 0.2773, lr_0 = 1.4435e-04
Validation rmse logD = 0.609734
Validation R2 logD = 0.739988
Validation rmse logP = 0.456914
Validation R2 logP = 0.942530
Epoch 84
Train function
Loss = 2.3125e-05, PNorm = 52.5507, GNorm = 0.1480, lr_0 = 1.4420e-04
Loss = 3.1112e-05, PNorm = 52.5517, GNorm = 0.0953, lr_0 = 1.4405e-04
Loss = 2.0390e-05, PNorm = 52.5531, GNorm = 0.0829, lr_0 = 1.4390e-04
Loss = 2.1695e-05, PNorm = 52.5537, GNorm = 0.0760, lr_0 = 1.4375e-04
Loss = 2.5783e-05, PNorm = 52.5544, GNorm = 0.0860, lr_0 = 1.4360e-04
Loss = 1.5340e-05, PNorm = 52.5554, GNorm = 0.0694, lr_0 = 1.4345e-04
Loss = 1.7791e-05, PNorm = 52.5561, GNorm = 0.1128, lr_0 = 1.4330e-04
Loss = 1.8597e-05, PNorm = 52.5562, GNorm = 0.1461, lr_0 = 1.4315e-04
Loss = 1.5360e-05, PNorm = 52.5564, GNorm = 0.0735, lr_0 = 1.4300e-04
Loss = 1.9018e-05, PNorm = 52.5580, GNorm = 0.1151, lr_0 = 1.4285e-04
Loss = 2.2072e-05, PNorm = 52.5588, GNorm = 0.0717, lr_0 = 1.4270e-04
Loss = 1.9038e-05, PNorm = 52.5603, GNorm = 0.1890, lr_0 = 1.4255e-04
Loss = 1.4623e-05, PNorm = 52.5616, GNorm = 0.0944, lr_0 = 1.4240e-04
Loss = 2.2233e-05, PNorm = 52.5623, GNorm = 0.1054, lr_0 = 1.4225e-04
Loss = 2.1769e-05, PNorm = 52.5625, GNorm = 0.1036, lr_0 = 1.4210e-04
Loss = 2.2870e-05, PNorm = 52.5634, GNorm = 0.1154, lr_0 = 1.4196e-04
Loss = 2.4328e-05, PNorm = 52.5650, GNorm = 0.0944, lr_0 = 1.4181e-04
Loss = 2.1442e-05, PNorm = 52.5670, GNorm = 0.1210, lr_0 = 1.4166e-04
Loss = 2.4033e-05, PNorm = 52.5679, GNorm = 0.0777, lr_0 = 1.4151e-04
Loss = 2.0819e-05, PNorm = 52.5674, GNorm = 0.0965, lr_0 = 1.4136e-04
Loss = 2.0153e-05, PNorm = 52.5670, GNorm = 0.0694, lr_0 = 1.4122e-04
Loss = 1.9253e-05, PNorm = 52.5676, GNorm = 0.0660, lr_0 = 1.4107e-04
Validation rmse logD = 0.610693
Validation R2 logD = 0.739169
Validation rmse logP = 0.460337
Validation R2 logP = 0.941665
Epoch 85
Train function
Loss = 2.1757e-05, PNorm = 52.5687, GNorm = 0.0698, lr_0 = 1.4091e-04
Loss = 2.0558e-05, PNorm = 52.5695, GNorm = 0.1414, lr_0 = 1.4076e-04
Loss = 1.8256e-05, PNorm = 52.5697, GNorm = 0.1053, lr_0 = 1.4061e-04
Loss = 1.4326e-05, PNorm = 52.5711, GNorm = 0.0913, lr_0 = 1.4047e-04
Loss = 2.2146e-05, PNorm = 52.5715, GNorm = 0.0965, lr_0 = 1.4032e-04
Loss = 1.9371e-05, PNorm = 52.5720, GNorm = 0.1138, lr_0 = 1.4017e-04
Loss = 1.9226e-05, PNorm = 52.5727, GNorm = 0.1112, lr_0 = 1.4003e-04
Loss = 1.6169e-05, PNorm = 52.5733, GNorm = 0.0613, lr_0 = 1.3988e-04
Loss = 1.4145e-05, PNorm = 52.5736, GNorm = 0.0816, lr_0 = 1.3974e-04
Loss = 1.4261e-05, PNorm = 52.5743, GNorm = 0.0885, lr_0 = 1.3959e-04
Loss = 1.7898e-05, PNorm = 52.5746, GNorm = 0.0685, lr_0 = 1.3944e-04
Loss = 2.1312e-05, PNorm = 52.5747, GNorm = 0.1345, lr_0 = 1.3930e-04
Loss = 2.5581e-05, PNorm = 52.5756, GNorm = 0.0913, lr_0 = 1.3915e-04
Loss = 2.0236e-05, PNorm = 52.5762, GNorm = 0.0672, lr_0 = 1.3901e-04
Loss = 1.8325e-05, PNorm = 52.5759, GNorm = 0.0648, lr_0 = 1.3886e-04
Loss = 1.4499e-05, PNorm = 52.5764, GNorm = 0.0701, lr_0 = 1.3872e-04
Loss = 1.5521e-05, PNorm = 52.5782, GNorm = 0.0713, lr_0 = 1.3857e-04
Loss = 2.0260e-05, PNorm = 52.5794, GNorm = 0.1283, lr_0 = 1.3843e-04
Loss = 1.6732e-05, PNorm = 52.5804, GNorm = 0.0902, lr_0 = 1.3828e-04
Loss = 1.4478e-05, PNorm = 52.5812, GNorm = 0.0836, lr_0 = 1.3814e-04
Loss = 2.5246e-05, PNorm = 52.5827, GNorm = 0.1387, lr_0 = 1.3800e-04
Loss = 2.6284e-05, PNorm = 52.5847, GNorm = 0.1727, lr_0 = 1.3785e-04
Loss = 1.9626e-05, PNorm = 52.5857, GNorm = 0.0796, lr_0 = 1.3771e-04
Validation rmse logD = 0.611413
Validation R2 logD = 0.738554
Validation rmse logP = 0.456823
Validation R2 logP = 0.942553
Epoch 86
Train function
Loss = 2.0319e-05, PNorm = 52.5859, GNorm = 0.1338, lr_0 = 1.3756e-04
Loss = 1.7444e-05, PNorm = 52.5875, GNorm = 0.0851, lr_0 = 1.3742e-04
Loss = 1.5007e-05, PNorm = 52.5884, GNorm = 0.0592, lr_0 = 1.3728e-04
Loss = 1.6397e-05, PNorm = 52.5887, GNorm = 0.0971, lr_0 = 1.3713e-04
Loss = 1.7755e-05, PNorm = 52.5896, GNorm = 0.1100, lr_0 = 1.3699e-04
Loss = 1.8090e-05, PNorm = 52.5898, GNorm = 0.1328, lr_0 = 1.3685e-04
Loss = 2.4106e-05, PNorm = 52.5908, GNorm = 0.2020, lr_0 = 1.3670e-04
Loss = 1.6418e-05, PNorm = 52.5920, GNorm = 0.0540, lr_0 = 1.3656e-04
Loss = 2.1479e-05, PNorm = 52.5932, GNorm = 0.1850, lr_0 = 1.3642e-04
Loss = 2.6442e-05, PNorm = 52.5945, GNorm = 0.0987, lr_0 = 1.3628e-04
Loss = 2.1434e-05, PNorm = 52.5967, GNorm = 0.1073, lr_0 = 1.3613e-04
Loss = 2.0238e-05, PNorm = 52.5980, GNorm = 0.2276, lr_0 = 1.3599e-04
Loss = 2.0232e-05, PNorm = 52.5984, GNorm = 0.2002, lr_0 = 1.3585e-04
Loss = 1.8965e-05, PNorm = 52.6001, GNorm = 0.0801, lr_0 = 1.3571e-04
Loss = 2.1548e-05, PNorm = 52.6004, GNorm = 0.0905, lr_0 = 1.3557e-04
Loss = 2.1034e-05, PNorm = 52.6014, GNorm = 0.1459, lr_0 = 1.3543e-04
Loss = 1.9492e-05, PNorm = 52.6020, GNorm = 0.1312, lr_0 = 1.3528e-04
Loss = 1.4529e-05, PNorm = 52.6027, GNorm = 0.0919, lr_0 = 1.3514e-04
Loss = 1.9867e-05, PNorm = 52.6031, GNorm = 0.0568, lr_0 = 1.3500e-04
Loss = 2.0957e-05, PNorm = 52.6039, GNorm = 0.1835, lr_0 = 1.3486e-04
Loss = 1.9155e-05, PNorm = 52.6053, GNorm = 0.1019, lr_0 = 1.3472e-04
Loss = 2.6932e-05, PNorm = 52.6071, GNorm = 0.1752, lr_0 = 1.3458e-04
Validation rmse logD = 0.608302
Validation R2 logD = 0.741208
Validation rmse logP = 0.458070
Validation R2 logP = 0.942239
Epoch 87
Train function
Loss = 2.3394e-05, PNorm = 52.6082, GNorm = 0.1090, lr_0 = 1.3443e-04
Loss = 2.2980e-05, PNorm = 52.6083, GNorm = 0.1126, lr_0 = 1.3428e-04
Loss = 2.6495e-05, PNorm = 52.6080, GNorm = 0.1449, lr_0 = 1.3414e-04
Loss = 2.4484e-05, PNorm = 52.6090, GNorm = 0.1161, lr_0 = 1.3400e-04
Loss = 2.4739e-05, PNorm = 52.6098, GNorm = 0.1431, lr_0 = 1.3386e-04
Loss = 2.0032e-05, PNorm = 52.6112, GNorm = 0.1077, lr_0 = 1.3373e-04
Loss = 1.8021e-05, PNorm = 52.6121, GNorm = 0.0787, lr_0 = 1.3359e-04
Loss = 1.6962e-05, PNorm = 52.6132, GNorm = 0.1119, lr_0 = 1.3345e-04
Loss = 1.8070e-05, PNorm = 52.6141, GNorm = 0.1248, lr_0 = 1.3331e-04
Loss = 1.9807e-05, PNorm = 52.6156, GNorm = 0.1323, lr_0 = 1.3317e-04
Loss = 2.1890e-05, PNorm = 52.6167, GNorm = 0.1533, lr_0 = 1.3303e-04
Loss = 2.1727e-05, PNorm = 52.6176, GNorm = 0.1947, lr_0 = 1.3289e-04
Loss = 2.1123e-05, PNorm = 52.6191, GNorm = 0.1526, lr_0 = 1.3275e-04
Loss = 2.0756e-05, PNorm = 52.6205, GNorm = 0.1112, lr_0 = 1.3261e-04
Loss = 1.9769e-05, PNorm = 52.6218, GNorm = 0.0901, lr_0 = 1.3247e-04
Loss = 1.8030e-05, PNorm = 52.6226, GNorm = 0.0974, lr_0 = 1.3234e-04
Loss = 3.1099e-05, PNorm = 52.6236, GNorm = 0.1593, lr_0 = 1.3220e-04
Loss = 3.1987e-05, PNorm = 52.6241, GNorm = 0.2730, lr_0 = 1.3206e-04
Loss = 2.6466e-05, PNorm = 52.6250, GNorm = 0.1109, lr_0 = 1.3192e-04
Loss = 3.0749e-05, PNorm = 52.6255, GNorm = 0.1472, lr_0 = 1.3178e-04
Loss = 2.9354e-05, PNorm = 52.6269, GNorm = 0.1479, lr_0 = 1.3165e-04
Loss = 2.7020e-05, PNorm = 52.6277, GNorm = 0.1523, lr_0 = 1.3151e-04
Loss = 2.7905e-05, PNorm = 52.6296, GNorm = 0.1105, lr_0 = 1.3137e-04
Validation rmse logD = 0.609623
Validation R2 logD = 0.740083
Validation rmse logP = 0.456236
Validation R2 logP = 0.942700
Epoch 88
Train function
Loss = 2.7731e-05, PNorm = 52.6313, GNorm = 0.1278, lr_0 = 1.3124e-04
Loss = 2.8367e-05, PNorm = 52.6332, GNorm = 0.1423, lr_0 = 1.3110e-04
Loss = 2.1649e-05, PNorm = 52.6353, GNorm = 0.1008, lr_0 = 1.3096e-04
Loss = 2.5642e-05, PNorm = 52.6366, GNorm = 0.1441, lr_0 = 1.3082e-04
Loss = 1.8819e-05, PNorm = 52.6366, GNorm = 0.1287, lr_0 = 1.3069e-04
Loss = 1.7027e-05, PNorm = 52.6367, GNorm = 0.0593, lr_0 = 1.3055e-04
Loss = 1.3874e-05, PNorm = 52.6375, GNorm = 0.0720, lr_0 = 1.3042e-04
Loss = 1.8942e-05, PNorm = 52.6388, GNorm = 0.0739, lr_0 = 1.3028e-04
Loss = 1.8994e-05, PNorm = 52.6398, GNorm = 0.1058, lr_0 = 1.3014e-04
Loss = 2.6289e-05, PNorm = 52.6406, GNorm = 0.1303, lr_0 = 1.3001e-04
Loss = 3.1147e-05, PNorm = 52.6423, GNorm = 0.3410, lr_0 = 1.2987e-04
Loss = 2.7472e-05, PNorm = 52.6434, GNorm = 0.1491, lr_0 = 1.2974e-04
Loss = 2.7202e-05, PNorm = 52.6443, GNorm = 0.1349, lr_0 = 1.2960e-04
Loss = 2.3928e-05, PNorm = 52.6453, GNorm = 0.0981, lr_0 = 1.2947e-04
Loss = 2.2862e-05, PNorm = 52.6464, GNorm = 0.1083, lr_0 = 1.2933e-04
Loss = 3.1894e-05, PNorm = 52.6482, GNorm = 0.1521, lr_0 = 1.2920e-04
Loss = 3.2459e-05, PNorm = 52.6489, GNorm = 0.1070, lr_0 = 1.2906e-04
Loss = 3.1406e-05, PNorm = 52.6513, GNorm = 0.1556, lr_0 = 1.2893e-04
Loss = 2.4857e-05, PNorm = 52.6519, GNorm = 0.1275, lr_0 = 1.2879e-04
Loss = 2.0521e-05, PNorm = 52.6527, GNorm = 0.1081, lr_0 = 1.2866e-04
Loss = 2.2629e-05, PNorm = 52.6533, GNorm = 0.1071, lr_0 = 1.2852e-04
Loss = 2.2841e-05, PNorm = 52.6540, GNorm = 0.1312, lr_0 = 1.2839e-04
Validation rmse logD = 0.608293
Validation R2 logD = 0.741216
Validation rmse logP = 0.457810
Validation R2 logP = 0.942304
Epoch 89
Train function
Loss = 1.7541e-05, PNorm = 52.6555, GNorm = 0.0638, lr_0 = 1.2824e-04
Loss = 1.9002e-05, PNorm = 52.6559, GNorm = 0.0982, lr_0 = 1.2811e-04
Loss = 2.0525e-05, PNorm = 52.6561, GNorm = 0.1161, lr_0 = 1.2797e-04
Loss = 1.8403e-05, PNorm = 52.6572, GNorm = 0.0697, lr_0 = 1.2784e-04
Loss = 2.1487e-05, PNorm = 52.6577, GNorm = 0.0917, lr_0 = 1.2771e-04
Loss = 1.7922e-05, PNorm = 52.6571, GNorm = 0.0740, lr_0 = 1.2757e-04
Loss = 2.3246e-05, PNorm = 52.6584, GNorm = 0.0506, lr_0 = 1.2744e-04
Loss = 1.7623e-05, PNorm = 52.6602, GNorm = 0.1186, lr_0 = 1.2731e-04
Loss = 1.8347e-05, PNorm = 52.6604, GNorm = 0.1090, lr_0 = 1.2717e-04
Loss = 2.3329e-05, PNorm = 52.6615, GNorm = 0.0781, lr_0 = 1.2704e-04
Loss = 1.8088e-05, PNorm = 52.6615, GNorm = 0.1452, lr_0 = 1.2691e-04
Loss = 1.9585e-05, PNorm = 52.6624, GNorm = 0.1021, lr_0 = 1.2678e-04
Loss = 1.8641e-05, PNorm = 52.6629, GNorm = 0.0777, lr_0 = 1.2664e-04
Loss = 1.9502e-05, PNorm = 52.6643, GNorm = 0.0809, lr_0 = 1.2651e-04
Loss = 1.4816e-05, PNorm = 52.6659, GNorm = 0.0758, lr_0 = 1.2638e-04
Loss = 1.6300e-05, PNorm = 52.6673, GNorm = 0.1554, lr_0 = 1.2625e-04
Loss = 1.8952e-05, PNorm = 52.6680, GNorm = 0.0975, lr_0 = 1.2612e-04
Loss = 1.8561e-05, PNorm = 52.6684, GNorm = 0.1075, lr_0 = 1.2598e-04
Loss = 1.6280e-05, PNorm = 52.6693, GNorm = 0.1714, lr_0 = 1.2585e-04
Loss = 2.6499e-05, PNorm = 52.6714, GNorm = 0.1421, lr_0 = 1.2572e-04
Loss = 2.1988e-05, PNorm = 52.6718, GNorm = 0.1302, lr_0 = 1.2559e-04
Loss = 1.8902e-05, PNorm = 52.6722, GNorm = 0.1026, lr_0 = 1.2546e-04
Loss = 1.6849e-05, PNorm = 52.6734, GNorm = 0.1649, lr_0 = 1.2533e-04
Validation rmse logD = 0.609953
Validation R2 logD = 0.739801
Validation rmse logP = 0.459235
Validation R2 logP = 0.941944
Epoch 90
Train function
Loss = 1.2857e-05, PNorm = 52.6747, GNorm = 0.0812, lr_0 = 1.2520e-04
Loss = 1.6337e-05, PNorm = 52.6760, GNorm = 0.1602, lr_0 = 1.2507e-04
Loss = 1.6955e-05, PNorm = 52.6777, GNorm = 0.0598, lr_0 = 1.2494e-04
Loss = 1.4079e-05, PNorm = 52.6786, GNorm = 0.0903, lr_0 = 1.2481e-04
Loss = 1.1672e-05, PNorm = 52.6793, GNorm = 0.0690, lr_0 = 1.2468e-04
Loss = 1.0036e-05, PNorm = 52.6798, GNorm = 0.0527, lr_0 = 1.2455e-04
Loss = 1.4465e-05, PNorm = 52.6806, GNorm = 0.0563, lr_0 = 1.2442e-04
Loss = 1.2927e-05, PNorm = 52.6806, GNorm = 0.0831, lr_0 = 1.2429e-04
Loss = 1.4132e-05, PNorm = 52.6812, GNorm = 0.0954, lr_0 = 1.2416e-04
Loss = 2.0344e-05, PNorm = 52.6819, GNorm = 0.1506, lr_0 = 1.2403e-04
Loss = 1.3751e-05, PNorm = 52.6822, GNorm = 0.1260, lr_0 = 1.2390e-04
Loss = 1.3671e-05, PNorm = 52.6823, GNorm = 0.0636, lr_0 = 1.2377e-04
Loss = 2.1274e-05, PNorm = 52.6836, GNorm = 0.1181, lr_0 = 1.2364e-04
Loss = 1.4655e-05, PNorm = 52.6845, GNorm = 0.0934, lr_0 = 1.2351e-04
Loss = 1.8290e-05, PNorm = 52.6850, GNorm = 0.0999, lr_0 = 1.2338e-04
Loss = 1.6711e-05, PNorm = 52.6871, GNorm = 0.1352, lr_0 = 1.2325e-04
Loss = 1.3641e-05, PNorm = 52.6875, GNorm = 0.0826, lr_0 = 1.2312e-04
Loss = 2.0179e-05, PNorm = 52.6879, GNorm = 0.0763, lr_0 = 1.2299e-04
Loss = 1.7246e-05, PNorm = 52.6893, GNorm = 0.2335, lr_0 = 1.2287e-04
Loss = 1.6689e-05, PNorm = 52.6900, GNorm = 0.0615, lr_0 = 1.2274e-04
Loss = 2.0845e-05, PNorm = 52.6907, GNorm = 0.0690, lr_0 = 1.2261e-04
Loss = 2.1349e-05, PNorm = 52.6915, GNorm = 0.0811, lr_0 = 1.2248e-04
Validation rmse logD = 0.609467
Validation R2 logD = 0.740216
Validation rmse logP = 0.458323
Validation R2 logP = 0.942175
Epoch 91
Train function
Loss = 6.8964e-06, PNorm = 52.6915, GNorm = 0.0350, lr_0 = 1.2234e-04
Loss = 1.5687e-05, PNorm = 52.6930, GNorm = 0.0553, lr_0 = 1.2221e-04
Loss = 1.5416e-05, PNorm = 52.6937, GNorm = 0.1762, lr_0 = 1.2209e-04
Loss = 2.4758e-05, PNorm = 52.6945, GNorm = 0.1301, lr_0 = 1.2196e-04
Loss = 2.3554e-05, PNorm = 52.6951, GNorm = 0.1213, lr_0 = 1.2183e-04
Loss = 1.9646e-05, PNorm = 52.6965, GNorm = 0.0970, lr_0 = 1.2170e-04
Loss = 1.5824e-05, PNorm = 52.6981, GNorm = 0.1163, lr_0 = 1.2158e-04
Loss = 1.7886e-05, PNorm = 52.6983, GNorm = 0.1847, lr_0 = 1.2145e-04
Loss = 1.4715e-05, PNorm = 52.6985, GNorm = 0.0726, lr_0 = 1.2132e-04
Loss = 1.4153e-05, PNorm = 52.6989, GNorm = 0.1020, lr_0 = 1.2120e-04
Loss = 1.5591e-05, PNorm = 52.6986, GNorm = 0.1443, lr_0 = 1.2107e-04
Loss = 2.0390e-05, PNorm = 52.6991, GNorm = 0.0886, lr_0 = 1.2094e-04
Loss = 1.8222e-05, PNorm = 52.7001, GNorm = 0.1297, lr_0 = 1.2082e-04
Loss = 1.5656e-05, PNorm = 52.7007, GNorm = 0.0769, lr_0 = 1.2069e-04
Loss = 1.6544e-05, PNorm = 52.7013, GNorm = 0.1094, lr_0 = 1.2057e-04
Loss = 1.5872e-05, PNorm = 52.7019, GNorm = 0.0982, lr_0 = 1.2044e-04
Loss = 1.1053e-05, PNorm = 52.7017, GNorm = 0.0782, lr_0 = 1.2031e-04
Loss = 1.8482e-05, PNorm = 52.7027, GNorm = 0.0864, lr_0 = 1.2019e-04
Loss = 1.1861e-05, PNorm = 52.7042, GNorm = 0.0905, lr_0 = 1.2006e-04
Loss = 1.4523e-05, PNorm = 52.7051, GNorm = 0.0909, lr_0 = 1.1994e-04
Loss = 1.4228e-05, PNorm = 52.7056, GNorm = 0.1667, lr_0 = 1.1981e-04
Loss = 1.6456e-05, PNorm = 52.7065, GNorm = 0.1520, lr_0 = 1.1969e-04
Loss = 1.7019e-05, PNorm = 52.7075, GNorm = 0.0989, lr_0 = 1.1956e-04
Validation rmse logD = 0.607730
Validation R2 logD = 0.741694
Validation rmse logP = 0.460050
Validation R2 logP = 0.941738
Epoch 92
Train function
Loss = 1.7304e-05, PNorm = 52.7080, GNorm = 0.1147, lr_0 = 1.1944e-04
Loss = 2.0290e-05, PNorm = 52.7086, GNorm = 0.0950, lr_0 = 1.1931e-04
Loss = 1.9841e-05, PNorm = 52.7101, GNorm = 0.0354, lr_0 = 1.1919e-04
Loss = 2.0123e-05, PNorm = 52.7107, GNorm = 0.1381, lr_0 = 1.1906e-04
Loss = 2.0524e-05, PNorm = 52.7108, GNorm = 0.0779, lr_0 = 1.1894e-04
Loss = 1.6458e-05, PNorm = 52.7116, GNorm = 0.1575, lr_0 = 1.1882e-04
Loss = 1.4639e-05, PNorm = 52.7129, GNorm = 0.0706, lr_0 = 1.1869e-04
Loss = 2.1196e-05, PNorm = 52.7135, GNorm = 0.1692, lr_0 = 1.1857e-04
Loss = 1.6596e-05, PNorm = 52.7141, GNorm = 0.0808, lr_0 = 1.1844e-04
Loss = 2.0786e-05, PNorm = 52.7156, GNorm = 0.2161, lr_0 = 1.1832e-04
Loss = 2.0187e-05, PNorm = 52.7168, GNorm = 0.0946, lr_0 = 1.1820e-04
Loss = 1.6606e-05, PNorm = 52.7174, GNorm = 0.1068, lr_0 = 1.1807e-04
Loss = 1.6445e-05, PNorm = 52.7181, GNorm = 0.0826, lr_0 = 1.1795e-04
Loss = 1.2043e-05, PNorm = 52.7189, GNorm = 0.0850, lr_0 = 1.1783e-04
Loss = 1.0960e-05, PNorm = 52.7192, GNorm = 0.0618, lr_0 = 1.1770e-04
Loss = 1.5494e-05, PNorm = 52.7191, GNorm = 0.0623, lr_0 = 1.1758e-04
Loss = 1.0895e-05, PNorm = 52.7196, GNorm = 0.1326, lr_0 = 1.1746e-04
Loss = 1.3081e-05, PNorm = 52.7196, GNorm = 0.0927, lr_0 = 1.1734e-04
Loss = 1.3560e-05, PNorm = 52.7197, GNorm = 0.0831, lr_0 = 1.1721e-04
Loss = 2.0304e-05, PNorm = 52.7207, GNorm = 0.1164, lr_0 = 1.1709e-04
Loss = 1.3990e-05, PNorm = 52.7224, GNorm = 0.0859, lr_0 = 1.1697e-04
Loss = 2.1073e-05, PNorm = 52.7219, GNorm = 0.0533, lr_0 = 1.1685e-04
Validation rmse logD = 0.610007
Validation R2 logD = 0.739756
Validation rmse logP = 0.459388
Validation R2 logP = 0.941906
Epoch 93
Train function
Loss = 1.6125e-05, PNorm = 52.7228, GNorm = 0.1019, lr_0 = 1.1671e-04
Loss = 1.7327e-05, PNorm = 52.7234, GNorm = 0.1246, lr_0 = 1.1659e-04
Loss = 1.1676e-05, PNorm = 52.7239, GNorm = 0.1207, lr_0 = 1.1647e-04
Loss = 1.2496e-05, PNorm = 52.7255, GNorm = 0.0655, lr_0 = 1.1635e-04
Loss = 2.0260e-05, PNorm = 52.7267, GNorm = 0.0900, lr_0 = 1.1623e-04
Loss = 1.9315e-05, PNorm = 52.7275, GNorm = 0.0740, lr_0 = 1.1611e-04
Loss = 1.5824e-05, PNorm = 52.7281, GNorm = 0.0942, lr_0 = 1.1598e-04
Loss = 1.6882e-05, PNorm = 52.7282, GNorm = 0.1530, lr_0 = 1.1586e-04
Loss = 1.1747e-05, PNorm = 52.7284, GNorm = 0.0701, lr_0 = 1.1574e-04
Loss = 1.2479e-05, PNorm = 52.7285, GNorm = 0.0552, lr_0 = 1.1562e-04
Loss = 1.3878e-05, PNorm = 52.7286, GNorm = 0.0562, lr_0 = 1.1550e-04
Loss = 1.2871e-05, PNorm = 52.7293, GNorm = 0.0813, lr_0 = 1.1538e-04
Loss = 1.1681e-05, PNorm = 52.7304, GNorm = 0.0583, lr_0 = 1.1526e-04
Loss = 9.6104e-06, PNorm = 52.7299, GNorm = 0.0773, lr_0 = 1.1514e-04
Loss = 1.3978e-05, PNorm = 52.7302, GNorm = 0.0690, lr_0 = 1.1502e-04
Loss = 1.6271e-05, PNorm = 52.7312, GNorm = 0.0925, lr_0 = 1.1490e-04
Loss = 1.7250e-05, PNorm = 52.7319, GNorm = 0.0594, lr_0 = 1.1478e-04
Loss = 1.2154e-05, PNorm = 52.7327, GNorm = 0.0608, lr_0 = 1.1466e-04
Loss = 1.6489e-05, PNorm = 52.7340, GNorm = 0.0867, lr_0 = 1.1454e-04
Loss = 1.1966e-05, PNorm = 52.7348, GNorm = 0.0797, lr_0 = 1.1442e-04
Loss = 1.5017e-05, PNorm = 52.7353, GNorm = 0.1024, lr_0 = 1.1430e-04
Loss = 1.7822e-05, PNorm = 52.7359, GNorm = 0.1447, lr_0 = 1.1418e-04
Loss = 2.2343e-05, PNorm = 52.7367, GNorm = 0.1151, lr_0 = 1.1406e-04
Validation rmse logD = 0.610320
Validation R2 logD = 0.739488
Validation rmse logP = 0.459094
Validation R2 logP = 0.941980
Epoch 94
Train function
Loss = 1.5333e-05, PNorm = 52.7376, GNorm = 0.1165, lr_0 = 1.1394e-04
Loss = 1.5990e-05, PNorm = 52.7386, GNorm = 0.0691, lr_0 = 1.1382e-04
Loss = 1.3515e-05, PNorm = 52.7389, GNorm = 0.0752, lr_0 = 1.1371e-04
Loss = 1.2256e-05, PNorm = 52.7402, GNorm = 0.0806, lr_0 = 1.1359e-04
Loss = 1.0773e-05, PNorm = 52.7407, GNorm = 0.0764, lr_0 = 1.1347e-04
Loss = 9.0324e-06, PNorm = 52.7419, GNorm = 0.0720, lr_0 = 1.1335e-04
Loss = 1.2567e-05, PNorm = 52.7421, GNorm = 0.0604, lr_0 = 1.1323e-04
Loss = 2.3967e-05, PNorm = 52.7436, GNorm = 0.1513, lr_0 = 1.1311e-04
Loss = 1.7500e-05, PNorm = 52.7450, GNorm = 0.0903, lr_0 = 1.1300e-04
Loss = 1.7275e-05, PNorm = 52.7452, GNorm = 0.1703, lr_0 = 1.1288e-04
Loss = 1.3175e-05, PNorm = 52.7450, GNorm = 0.1144, lr_0 = 1.1276e-04
Loss = 1.3182e-05, PNorm = 52.7469, GNorm = 0.0610, lr_0 = 1.1264e-04
Loss = 1.7280e-05, PNorm = 52.7475, GNorm = 0.1885, lr_0 = 1.1252e-04
Loss = 1.8482e-05, PNorm = 52.7493, GNorm = 0.1689, lr_0 = 1.1241e-04
Loss = 1.4328e-05, PNorm = 52.7497, GNorm = 0.0618, lr_0 = 1.1229e-04
Loss = 1.7454e-05, PNorm = 52.7510, GNorm = 0.0787, lr_0 = 1.1217e-04
Loss = 1.6273e-05, PNorm = 52.7517, GNorm = 0.0655, lr_0 = 1.1206e-04
Loss = 1.1059e-05, PNorm = 52.7521, GNorm = 0.0913, lr_0 = 1.1194e-04
Loss = 1.4816e-05, PNorm = 52.7529, GNorm = 0.0736, lr_0 = 1.1182e-04
Loss = 1.1435e-05, PNorm = 52.7536, GNorm = 0.0527, lr_0 = 1.1170e-04
Loss = 1.4338e-05, PNorm = 52.7541, GNorm = 0.1437, lr_0 = 1.1159e-04
Loss = 1.7549e-05, PNorm = 52.7549, GNorm = 0.0809, lr_0 = 1.1147e-04
Loss = 1.6345e-05, PNorm = 52.7555, GNorm = 0.1812, lr_0 = 1.1136e-04
Loss = 9.6461e-06, PNorm = 52.7557, GNorm = 0.0673, lr_0 = 1.1134e-04
Validation rmse logD = 0.607059
Validation R2 logD = 0.742265
Validation rmse logP = 0.458538
Validation R2 logP = 0.942121
Epoch 95
Train function
Loss = 1.5856e-05, PNorm = 52.7569, GNorm = 0.1213, lr_0 = 1.1123e-04
Loss = 1.0256e-05, PNorm = 52.7571, GNorm = 0.0760, lr_0 = 1.1111e-04
Loss = 9.3913e-06, PNorm = 52.7575, GNorm = 0.0868, lr_0 = 1.1100e-04
Loss = 1.1692e-05, PNorm = 52.7572, GNorm = 0.0995, lr_0 = 1.1088e-04
Loss = 1.2188e-05, PNorm = 52.7574, GNorm = 0.0577, lr_0 = 1.1076e-04
Loss = 1.1116e-05, PNorm = 52.7583, GNorm = 0.1366, lr_0 = 1.1065e-04
Loss = 1.0930e-05, PNorm = 52.7582, GNorm = 0.0975, lr_0 = 1.1053e-04
Loss = 1.3195e-05, PNorm = 52.7588, GNorm = 0.0979, lr_0 = 1.1042e-04
Loss = 1.1030e-05, PNorm = 52.7588, GNorm = 0.0742, lr_0 = 1.1030e-04
Loss = 1.2249e-05, PNorm = 52.7588, GNorm = 0.0625, lr_0 = 1.1019e-04
Loss = 1.2126e-05, PNorm = 52.7596, GNorm = 0.0858, lr_0 = 1.1007e-04
Loss = 2.1729e-05, PNorm = 52.7606, GNorm = 0.0612, lr_0 = 1.0996e-04
Loss = 1.5289e-05, PNorm = 52.7620, GNorm = 0.1581, lr_0 = 1.0984e-04
Loss = 1.3772e-05, PNorm = 52.7622, GNorm = 0.0668, lr_0 = 1.0973e-04
Loss = 1.4433e-05, PNorm = 52.7620, GNorm = 0.1008, lr_0 = 1.0961e-04
Loss = 1.4009e-05, PNorm = 52.7629, GNorm = 0.0950, lr_0 = 1.0950e-04
Loss = 1.5193e-05, PNorm = 52.7641, GNorm = 0.1094, lr_0 = 1.0938e-04
Loss = 1.3231e-05, PNorm = 52.7650, GNorm = 0.1281, lr_0 = 1.0927e-04
Loss = 1.6999e-05, PNorm = 52.7653, GNorm = 0.1003, lr_0 = 1.0916e-04
Loss = 1.3947e-05, PNorm = 52.7663, GNorm = 0.0778, lr_0 = 1.0904e-04
Loss = 1.7532e-05, PNorm = 52.7681, GNorm = 0.1058, lr_0 = 1.0893e-04
Loss = 1.6240e-05, PNorm = 52.7695, GNorm = 0.1127, lr_0 = 1.0882e-04
Validation rmse logD = 0.609754
Validation R2 logD = 0.739971
Validation rmse logP = 0.458178
Validation R2 logP = 0.942211
Epoch 96
Train function
Loss = 1.2178e-05, PNorm = 52.7694, GNorm = 0.0750, lr_0 = 1.0870e-04
Loss = 1.1969e-05, PNorm = 52.7694, GNorm = 0.0937, lr_0 = 1.0859e-04
Loss = 1.0654e-05, PNorm = 52.7699, GNorm = 0.0564, lr_0 = 1.0847e-04
Loss = 1.2186e-05, PNorm = 52.7709, GNorm = 0.1814, lr_0 = 1.0836e-04
Loss = 1.6226e-05, PNorm = 52.7717, GNorm = 0.0969, lr_0 = 1.0825e-04
Loss = 1.9060e-05, PNorm = 52.7729, GNorm = 0.0635, lr_0 = 1.0814e-04
Loss = 1.3168e-05, PNorm = 52.7735, GNorm = 0.1165, lr_0 = 1.0802e-04
Loss = 1.1842e-05, PNorm = 52.7743, GNorm = 0.0645, lr_0 = 1.0791e-04
Loss = 1.2945e-05, PNorm = 52.7753, GNorm = 0.0636, lr_0 = 1.0780e-04
Loss = 1.0935e-05, PNorm = 52.7760, GNorm = 0.1429, lr_0 = 1.0768e-04
Loss = 1.3973e-05, PNorm = 52.7762, GNorm = 0.1425, lr_0 = 1.0757e-04
Loss = 1.7924e-05, PNorm = 52.7762, GNorm = 0.0619, lr_0 = 1.0746e-04
Loss = 1.4109e-05, PNorm = 52.7774, GNorm = 0.1104, lr_0 = 1.0735e-04
Loss = 1.2025e-05, PNorm = 52.7781, GNorm = 0.0555, lr_0 = 1.0724e-04
Loss = 1.1629e-05, PNorm = 52.7783, GNorm = 0.0912, lr_0 = 1.0712e-04
Loss = 1.8426e-05, PNorm = 52.7786, GNorm = 0.1234, lr_0 = 1.0701e-04
Loss = 1.9309e-05, PNorm = 52.7792, GNorm = 0.1305, lr_0 = 1.0690e-04
Loss = 1.3389e-05, PNorm = 52.7800, GNorm = 0.0817, lr_0 = 1.0679e-04
Loss = 1.4258e-05, PNorm = 52.7803, GNorm = 0.0901, lr_0 = 1.0668e-04
Loss = 1.4550e-05, PNorm = 52.7810, GNorm = 0.0889, lr_0 = 1.0657e-04
Loss = 1.6275e-05, PNorm = 52.7809, GNorm = 0.0843, lr_0 = 1.0645e-04
Loss = 1.8520e-05, PNorm = 52.7827, GNorm = 0.0954, lr_0 = 1.0634e-04
Loss = 1.2533e-05, PNorm = 52.7843, GNorm = 0.0754, lr_0 = 1.0623e-04
Validation rmse logD = 0.607416
Validation R2 logD = 0.741962
Validation rmse logP = 0.458700
Validation R2 logP = 0.942080
Epoch 97
Train function
Loss = 1.3634e-05, PNorm = 52.7853, GNorm = 0.0534, lr_0 = 1.0611e-04
Loss = 1.3433e-05, PNorm = 52.7849, GNorm = 0.1282, lr_0 = 1.0600e-04
Loss = 1.3458e-05, PNorm = 52.7848, GNorm = 0.1263, lr_0 = 1.0589e-04
Loss = 1.1830e-05, PNorm = 52.7853, GNorm = 0.1128, lr_0 = 1.0578e-04
Loss = 1.1581e-05, PNorm = 52.7852, GNorm = 0.0995, lr_0 = 1.0567e-04
Loss = 1.0783e-05, PNorm = 52.7855, GNorm = 0.0593, lr_0 = 1.0556e-04
Loss = 1.2215e-05, PNorm = 52.7863, GNorm = 0.0628, lr_0 = 1.0545e-04
Loss = 1.1053e-05, PNorm = 52.7866, GNorm = 0.1102, lr_0 = 1.0534e-04
Loss = 1.1843e-05, PNorm = 52.7874, GNorm = 0.0672, lr_0 = 1.0523e-04
Loss = 1.0333e-05, PNorm = 52.7879, GNorm = 0.0858, lr_0 = 1.0512e-04
Loss = 1.6225e-05, PNorm = 52.7879, GNorm = 0.0820, lr_0 = 1.0501e-04
Loss = 1.1632e-05, PNorm = 52.7888, GNorm = 0.1076, lr_0 = 1.0490e-04
Loss = 9.8001e-06, PNorm = 52.7887, GNorm = 0.0738, lr_0 = 1.0479e-04
Loss = 1.1605e-05, PNorm = 52.7892, GNorm = 0.0708, lr_0 = 1.0468e-04
Loss = 1.3875e-05, PNorm = 52.7902, GNorm = 0.0962, lr_0 = 1.0457e-04
Loss = 1.8069e-05, PNorm = 52.7915, GNorm = 0.1705, lr_0 = 1.0446e-04
Loss = 1.7384e-05, PNorm = 52.7921, GNorm = 0.1173, lr_0 = 1.0435e-04
Loss = 1.3030e-05, PNorm = 52.7924, GNorm = 0.0429, lr_0 = 1.0424e-04
Loss = 2.0199e-05, PNorm = 52.7933, GNorm = 0.1692, lr_0 = 1.0413e-04
Loss = 1.7728e-05, PNorm = 52.7942, GNorm = 0.1129, lr_0 = 1.0403e-04
Loss = 2.4411e-05, PNorm = 52.7945, GNorm = 0.1033, lr_0 = 1.0392e-04
Loss = 2.6657e-05, PNorm = 52.7956, GNorm = 0.1218, lr_0 = 1.0381e-04
Validation rmse logD = 0.606175
Validation R2 logD = 0.743015
Validation rmse logP = 0.458770
Validation R2 logP = 0.942062
Epoch 98
Train function
Loss = 1.7080e-05, PNorm = 52.7962, GNorm = 0.1437, lr_0 = 1.0370e-04
Loss = 1.2535e-05, PNorm = 52.7968, GNorm = 0.0615, lr_0 = 1.0359e-04
Loss = 1.3995e-05, PNorm = 52.7972, GNorm = 0.0570, lr_0 = 1.0348e-04
Loss = 1.3991e-05, PNorm = 52.7975, GNorm = 0.0950, lr_0 = 1.0338e-04
Loss = 1.2304e-05, PNorm = 52.7986, GNorm = 0.1199, lr_0 = 1.0327e-04
Loss = 1.4630e-05, PNorm = 52.7991, GNorm = 0.2013, lr_0 = 1.0316e-04
Loss = 9.5269e-06, PNorm = 52.7999, GNorm = 0.0549, lr_0 = 1.0305e-04
Loss = 1.4450e-05, PNorm = 52.8006, GNorm = 0.0577, lr_0 = 1.0295e-04
Loss = 1.0823e-05, PNorm = 52.8014, GNorm = 0.0671, lr_0 = 1.0284e-04
Loss = 1.5685e-05, PNorm = 52.8029, GNorm = 0.0672, lr_0 = 1.0273e-04
Loss = 1.4148e-05, PNorm = 52.8039, GNorm = 0.1123, lr_0 = 1.0262e-04
Loss = 1.7334e-05, PNorm = 52.8045, GNorm = 0.1360, lr_0 = 1.0252e-04
Loss = 1.5613e-05, PNorm = 52.8046, GNorm = 0.0782, lr_0 = 1.0241e-04
Loss = 1.9356e-05, PNorm = 52.8051, GNorm = 0.0534, lr_0 = 1.0230e-04
Loss = 1.4655e-05, PNorm = 52.8044, GNorm = 0.1616, lr_0 = 1.0220e-04
Loss = 1.6481e-05, PNorm = 52.8050, GNorm = 0.1437, lr_0 = 1.0209e-04
Loss = 1.0814e-05, PNorm = 52.8057, GNorm = 0.0691, lr_0 = 1.0198e-04
Loss = 1.3637e-05, PNorm = 52.8062, GNorm = 0.0663, lr_0 = 1.0188e-04
Loss = 1.2294e-05, PNorm = 52.8064, GNorm = 0.1250, lr_0 = 1.0177e-04
Loss = 1.1431e-05, PNorm = 52.8067, GNorm = 0.0474, lr_0 = 1.0166e-04
Loss = 9.7494e-06, PNorm = 52.8076, GNorm = 0.0644, lr_0 = 1.0156e-04
Loss = 1.4225e-05, PNorm = 52.8081, GNorm = 0.0828, lr_0 = 1.0145e-04
Loss = 1.7658e-05, PNorm = 52.8095, GNorm = 0.1451, lr_0 = 1.0135e-04
Validation rmse logD = 0.610077
Validation R2 logD = 0.739696
Validation rmse logP = 0.458345
Validation R2 logP = 0.942169
Epoch 99
Train function
Loss = 1.0591e-05, PNorm = 52.8108, GNorm = 0.0686, lr_0 = 1.0123e-04
Loss = 1.2504e-05, PNorm = 52.8120, GNorm = 0.0616, lr_0 = 1.0112e-04
Loss = 1.4513e-05, PNorm = 52.8132, GNorm = 0.1066, lr_0 = 1.0102e-04
Loss = 1.1523e-05, PNorm = 52.8134, GNorm = 0.1056, lr_0 = 1.0091e-04
Loss = 1.5657e-05, PNorm = 52.8143, GNorm = 0.1087, lr_0 = 1.0081e-04
Loss = 1.9446e-05, PNorm = 52.8155, GNorm = 0.0474, lr_0 = 1.0070e-04
Loss = 1.3982e-05, PNorm = 52.8161, GNorm = 0.0661, lr_0 = 1.0060e-04
Loss = 1.9274e-05, PNorm = 52.8168, GNorm = 0.1143, lr_0 = 1.0049e-04
Loss = 1.1256e-05, PNorm = 52.8169, GNorm = 0.0940, lr_0 = 1.0039e-04
Loss = 1.2140e-05, PNorm = 52.8170, GNorm = 0.0703, lr_0 = 1.0028e-04
Loss = 1.0969e-05, PNorm = 52.8171, GNorm = 0.0791, lr_0 = 1.0018e-04
Loss = 8.2144e-06, PNorm = 52.8175, GNorm = 0.0557, lr_0 = 1.0007e-04
Loss = 1.0290e-05, PNorm = 52.8186, GNorm = 0.0922, lr_0 = 1.0000e-04
Loss = 9.0756e-06, PNorm = 52.8187, GNorm = 0.0974, lr_0 = 1.0000e-04
Loss = 1.1347e-05, PNorm = 52.8198, GNorm = 0.0630, lr_0 = 1.0000e-04
Loss = 9.2603e-06, PNorm = 52.8202, GNorm = 0.0830, lr_0 = 1.0000e-04
Loss = 1.0715e-05, PNorm = 52.8206, GNorm = 0.0672, lr_0 = 1.0000e-04
Loss = 1.0072e-05, PNorm = 52.8210, GNorm = 0.1372, lr_0 = 1.0000e-04
Loss = 1.7022e-05, PNorm = 52.8220, GNorm = 0.0677, lr_0 = 1.0000e-04
Loss = 1.3527e-05, PNorm = 52.8229, GNorm = 0.0810, lr_0 = 1.0000e-04
Loss = 1.1583e-05, PNorm = 52.8238, GNorm = 0.1325, lr_0 = 1.0000e-04
Loss = 1.3501e-05, PNorm = 52.8245, GNorm = 0.0931, lr_0 = 1.0000e-04
Validation rmse logD = 0.607030
Validation R2 logD = 0.742289
Validation rmse logP = 0.459909
Validation R2 logP = 0.941774
Model 0 best validation rmse = 0.527675 on epoch 44
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logD = 0.569817
Model 0 test R2 logD = 0.794068
Model 0 test rmse logP = 0.441359
Model 0 test R2 logP = 0.944049
Ensemble test rmse  logD= 0.569817
Ensemble test R2  logD= 0.794068
Ensemble test rmse  logP= 0.441359
Ensemble test R2  logP= 0.944049
Fold 1
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_best'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 199,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_1',
 'save_smiles_splits': False,
 'seed': 1,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD', 'logP'],
 'task_names': ['logD', 'logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 11274,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 1
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=499, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 414,902
Moving model to cuda
Epoch 0
Train function
Loss = 1.8669e-02, PNorm = 35.0899, GNorm = 3.4243, lr_0 = 1.2200e-04
Loss = 1.7509e-02, PNorm = 35.0916, GNorm = 4.8485, lr_0 = 1.4200e-04
Loss = 1.4350e-02, PNorm = 35.0958, GNorm = 2.2642, lr_0 = 1.6200e-04
Loss = 1.2368e-02, PNorm = 35.1001, GNorm = 3.6023, lr_0 = 1.8200e-04
Loss = 1.2053e-02, PNorm = 35.1054, GNorm = 2.5574, lr_0 = 2.0200e-04
Loss = 9.6621e-03, PNorm = 35.1117, GNorm = 5.8210, lr_0 = 2.2200e-04
Loss = 1.1169e-02, PNorm = 35.1180, GNorm = 4.9913, lr_0 = 2.4200e-04
Loss = 1.0278e-02, PNorm = 35.1220, GNorm = 8.7372, lr_0 = 2.6200e-04
Loss = 1.0860e-02, PNorm = 35.1301, GNorm = 4.5087, lr_0 = 2.8200e-04
Loss = 9.8303e-03, PNorm = 35.1410, GNorm = 2.6989, lr_0 = 3.0200e-04
Loss = 7.9733e-03, PNorm = 35.1511, GNorm = 5.9913, lr_0 = 3.2200e-04
Loss = 9.0732e-03, PNorm = 35.1609, GNorm = 2.2750, lr_0 = 3.4200e-04
Loss = 7.0866e-03, PNorm = 35.1721, GNorm = 1.7374, lr_0 = 3.6200e-04
Loss = 7.9604e-03, PNorm = 35.1818, GNorm = 5.3364, lr_0 = 3.8200e-04
Loss = 7.2620e-03, PNorm = 35.1931, GNorm = 3.8980, lr_0 = 4.0200e-04
Loss = 7.6543e-03, PNorm = 35.2065, GNorm = 2.4880, lr_0 = 4.2200e-04
Loss = 6.1483e-03, PNorm = 35.2178, GNorm = 2.7920, lr_0 = 4.4200e-04
Loss = 6.5079e-03, PNorm = 35.2302, GNorm = 3.3080, lr_0 = 4.6200e-04
Loss = 7.3858e-03, PNorm = 35.2445, GNorm = 1.3501, lr_0 = 4.8200e-04
Loss = 6.9670e-03, PNorm = 35.2592, GNorm = 3.2604, lr_0 = 5.0200e-04
Loss = 6.5911e-03, PNorm = 35.2772, GNorm = 4.0236, lr_0 = 5.2200e-04
Loss = 5.4037e-03, PNorm = 35.2978, GNorm = 2.4853, lr_0 = 5.4200e-04
Validation rmse logD = 0.886086
Validation R2 logD = 0.426643
Validation rmse logP = 0.878693
Validation R2 logP = 0.775995
Epoch 1
Train function
Loss = 5.9469e-03, PNorm = 35.3169, GNorm = 3.2980, lr_0 = 5.6400e-04
Loss = 6.3087e-03, PNorm = 35.3348, GNorm = 3.7745, lr_0 = 5.8400e-04
Loss = 5.4867e-03, PNorm = 35.3525, GNorm = 3.1080, lr_0 = 6.0400e-04
Loss = 5.9063e-03, PNorm = 35.3690, GNorm = 2.2398, lr_0 = 6.2400e-04
Loss = 5.3560e-03, PNorm = 35.3918, GNorm = 2.1611, lr_0 = 6.4400e-04
Loss = 5.5200e-03, PNorm = 35.4128, GNorm = 3.4236, lr_0 = 6.6400e-04
Loss = 7.4679e-03, PNorm = 35.4376, GNorm = 2.6496, lr_0 = 6.8400e-04
Loss = 6.6477e-03, PNorm = 35.4655, GNorm = 2.6047, lr_0 = 7.0400e-04
Loss = 5.9262e-03, PNorm = 35.4954, GNorm = 2.2639, lr_0 = 7.2400e-04
Loss = 5.2626e-03, PNorm = 35.5180, GNorm = 1.4985, lr_0 = 7.4400e-04
Loss = 4.6970e-03, PNorm = 35.5457, GNorm = 2.2243, lr_0 = 7.6400e-04
Loss = 4.9441e-03, PNorm = 35.5709, GNorm = 1.5730, lr_0 = 7.8400e-04
Loss = 4.3067e-03, PNorm = 35.5975, GNorm = 2.0186, lr_0 = 8.0400e-04
Loss = 5.2884e-03, PNorm = 35.6287, GNorm = 1.3627, lr_0 = 8.2400e-04
Loss = 4.2265e-03, PNorm = 35.6517, GNorm = 2.8444, lr_0 = 8.4400e-04
Loss = 5.8791e-03, PNorm = 35.6676, GNorm = 4.4479, lr_0 = 8.6400e-04
Loss = 5.9432e-03, PNorm = 35.6939, GNorm = 2.1260, lr_0 = 8.8400e-04
Loss = 5.5560e-03, PNorm = 35.7252, GNorm = 1.7287, lr_0 = 9.0400e-04
Loss = 4.9717e-03, PNorm = 35.7621, GNorm = 1.7430, lr_0 = 9.2400e-04
Loss = 4.3483e-03, PNorm = 35.7965, GNorm = 2.1965, lr_0 = 9.4400e-04
Loss = 4.7684e-03, PNorm = 35.8358, GNorm = 1.1050, lr_0 = 9.6400e-04
Loss = 4.1529e-03, PNorm = 35.8639, GNorm = 3.0855, lr_0 = 9.8400e-04
Loss = 5.7482e-03, PNorm = 35.8978, GNorm = 3.5366, lr_0 = 9.9979e-04
Loss = 6.8379e-03, PNorm = 35.9015, GNorm = 2.2283, lr_0 = 9.9969e-04
Validation rmse logD = 0.804150
Validation R2 logD = 0.527776
Validation rmse logP = 0.798688
Validation R2 logP = 0.814929
Epoch 2
Train function
Loss = 4.0479e-03, PNorm = 35.9328, GNorm = 2.4430, lr_0 = 9.9864e-04
Loss = 4.6538e-03, PNorm = 35.9723, GNorm = 2.4228, lr_0 = 9.9760e-04
Loss = 5.0896e-03, PNorm = 36.0053, GNorm = 1.7841, lr_0 = 9.9656e-04
Loss = 3.9307e-03, PNorm = 36.0461, GNorm = 3.1120, lr_0 = 9.9552e-04
Loss = 3.9181e-03, PNorm = 36.0712, GNorm = 1.1963, lr_0 = 9.9448e-04
Loss = 4.5032e-03, PNorm = 36.1019, GNorm = 1.9780, lr_0 = 9.9344e-04
Loss = 3.9818e-03, PNorm = 36.1323, GNorm = 1.1290, lr_0 = 9.9241e-04
Loss = 4.4527e-03, PNorm = 36.1683, GNorm = 1.0106, lr_0 = 9.9137e-04
Loss = 5.2639e-03, PNorm = 36.2028, GNorm = 2.9577, lr_0 = 9.9034e-04
Loss = 4.2370e-03, PNorm = 36.2345, GNorm = 1.2261, lr_0 = 9.8930e-04
Loss = 4.4082e-03, PNorm = 36.2636, GNorm = 1.1754, lr_0 = 9.8827e-04
Loss = 4.2074e-03, PNorm = 36.2874, GNorm = 2.7010, lr_0 = 9.8724e-04
Loss = 3.9436e-03, PNorm = 36.3250, GNorm = 1.5188, lr_0 = 9.8621e-04
Loss = 4.5019e-03, PNorm = 36.3542, GNorm = 1.0640, lr_0 = 9.8518e-04
Loss = 3.4314e-03, PNorm = 36.3887, GNorm = 2.8797, lr_0 = 9.8415e-04
Loss = 3.7353e-03, PNorm = 36.4121, GNorm = 1.6185, lr_0 = 9.8312e-04
Loss = 4.0518e-03, PNorm = 36.4365, GNorm = 1.8041, lr_0 = 9.8210e-04
Loss = 4.6404e-03, PNorm = 36.4668, GNorm = 1.5484, lr_0 = 9.8107e-04
Loss = 3.7229e-03, PNorm = 36.4977, GNorm = 0.6332, lr_0 = 9.8005e-04
Loss = 2.8514e-03, PNorm = 36.5266, GNorm = 1.4178, lr_0 = 9.7902e-04
Loss = 3.8234e-03, PNorm = 36.5586, GNorm = 1.5203, lr_0 = 9.7800e-04
Loss = 4.2260e-03, PNorm = 36.5886, GNorm = 1.3696, lr_0 = 9.7698e-04
Validation rmse logD = 0.783231
Validation R2 logD = 0.552025
Validation rmse logP = 0.613872
Validation R2 logP = 0.890670
Epoch 3
Train function
Loss = 2.9979e-03, PNorm = 36.6203, GNorm = 1.2830, lr_0 = 9.7596e-04
Loss = 3.3850e-03, PNorm = 36.6628, GNorm = 1.3394, lr_0 = 9.7494e-04
Loss = 3.3566e-03, PNorm = 36.6976, GNorm = 1.7533, lr_0 = 9.7393e-04
Loss = 3.1830e-03, PNorm = 36.7249, GNorm = 1.5665, lr_0 = 9.7291e-04
Loss = 3.1340e-03, PNorm = 36.7576, GNorm = 1.7614, lr_0 = 9.7189e-04
Loss = 3.4473e-03, PNorm = 36.7835, GNorm = 1.3557, lr_0 = 9.7088e-04
Loss = 3.6522e-03, PNorm = 36.8177, GNorm = 2.9294, lr_0 = 9.6987e-04
Loss = 3.1732e-03, PNorm = 36.8536, GNorm = 3.2408, lr_0 = 9.6885e-04
Loss = 2.9748e-03, PNorm = 36.8876, GNorm = 0.9134, lr_0 = 9.6784e-04
Loss = 3.1258e-03, PNorm = 36.9194, GNorm = 1.5862, lr_0 = 9.6683e-04
Loss = 3.3207e-03, PNorm = 36.9602, GNorm = 1.4803, lr_0 = 9.6582e-04
Loss = 3.0394e-03, PNorm = 36.9913, GNorm = 1.0974, lr_0 = 9.6482e-04
Loss = 2.8341e-03, PNorm = 37.0193, GNorm = 0.7338, lr_0 = 9.6381e-04
Loss = 3.1434e-03, PNorm = 37.0479, GNorm = 2.0396, lr_0 = 9.6280e-04
Loss = 3.1784e-03, PNorm = 37.0728, GNorm = 2.5540, lr_0 = 9.6180e-04
Loss = 4.1529e-03, PNorm = 37.1066, GNorm = 2.0239, lr_0 = 9.6079e-04
Loss = 3.2595e-03, PNorm = 37.1485, GNorm = 1.4129, lr_0 = 9.5979e-04
Loss = 3.0152e-03, PNorm = 37.1781, GNorm = 0.9078, lr_0 = 9.5879e-04
Loss = 3.7713e-03, PNorm = 37.2089, GNorm = 2.3559, lr_0 = 9.5779e-04
Loss = 3.8309e-03, PNorm = 37.2277, GNorm = 1.6261, lr_0 = 9.5679e-04
Loss = 3.6501e-03, PNorm = 37.2502, GNorm = 1.8526, lr_0 = 9.5579e-04
Loss = 3.8857e-03, PNorm = 37.2802, GNorm = 2.0368, lr_0 = 9.5479e-04
Loss = 3.1161e-03, PNorm = 37.3009, GNorm = 1.2048, lr_0 = 9.5380e-04
Validation rmse logD = 0.744192
Validation R2 logD = 0.595570
Validation rmse logP = 0.593570
Validation R2 logP = 0.897782
Epoch 4
Train function
Loss = 2.9666e-03, PNorm = 37.3284, GNorm = 1.2617, lr_0 = 9.5270e-04
Loss = 2.7007e-03, PNorm = 37.3626, GNorm = 1.1183, lr_0 = 9.5171e-04
Loss = 3.2456e-03, PNorm = 37.3866, GNorm = 1.6630, lr_0 = 9.5071e-04
Loss = 2.8856e-03, PNorm = 37.4170, GNorm = 0.7450, lr_0 = 9.4972e-04
Loss = 2.8272e-03, PNorm = 37.4313, GNorm = 1.4566, lr_0 = 9.4873e-04
Loss = 2.5976e-03, PNorm = 37.4599, GNorm = 1.2908, lr_0 = 9.4774e-04
Loss = 2.9137e-03, PNorm = 37.4898, GNorm = 2.9049, lr_0 = 9.4675e-04
Loss = 3.1005e-03, PNorm = 37.5077, GNorm = 0.9550, lr_0 = 9.4576e-04
Loss = 2.8146e-03, PNorm = 37.5326, GNorm = 1.2040, lr_0 = 9.4478e-04
Loss = 2.5633e-03, PNorm = 37.5692, GNorm = 1.5219, lr_0 = 9.4379e-04
Loss = 3.4376e-03, PNorm = 37.6026, GNorm = 1.4448, lr_0 = 9.4280e-04
Loss = 2.7511e-03, PNorm = 37.6363, GNorm = 1.5621, lr_0 = 9.4182e-04
Loss = 3.0977e-03, PNorm = 37.6621, GNorm = 0.9066, lr_0 = 9.4084e-04
Loss = 2.3860e-03, PNorm = 37.6876, GNorm = 1.0017, lr_0 = 9.3986e-04
Loss = 2.9967e-03, PNorm = 37.7209, GNorm = 1.5010, lr_0 = 9.3887e-04
Loss = 2.7495e-03, PNorm = 37.7500, GNorm = 0.9888, lr_0 = 9.3789e-04
Loss = 2.6212e-03, PNorm = 37.7759, GNorm = 1.0679, lr_0 = 9.3692e-04
Loss = 2.5888e-03, PNorm = 37.8020, GNorm = 1.4432, lr_0 = 9.3594e-04
Loss = 2.4064e-03, PNorm = 37.8160, GNorm = 1.4665, lr_0 = 9.3496e-04
Loss = 2.5536e-03, PNorm = 37.8367, GNorm = 0.9396, lr_0 = 9.3399e-04
Loss = 3.1031e-03, PNorm = 37.8661, GNorm = 1.3248, lr_0 = 9.3301e-04
Loss = 2.8198e-03, PNorm = 37.8937, GNorm = 0.8642, lr_0 = 9.3204e-04
Validation rmse logD = 0.749434
Validation R2 logD = 0.589852
Validation rmse logP = 0.606437
Validation R2 logP = 0.893302
Epoch 5
Train function
Loss = 3.1662e-03, PNorm = 37.9267, GNorm = 1.2203, lr_0 = 9.3106e-04
Loss = 2.5906e-03, PNorm = 37.9525, GNorm = 0.9580, lr_0 = 9.3009e-04
Loss = 2.2388e-03, PNorm = 37.9708, GNorm = 0.9211, lr_0 = 9.2912e-04
Loss = 2.2924e-03, PNorm = 37.9937, GNorm = 1.0199, lr_0 = 9.2815e-04
Loss = 2.6910e-03, PNorm = 38.0204, GNorm = 1.7396, lr_0 = 9.2718e-04
Loss = 2.4113e-03, PNorm = 38.0518, GNorm = 0.8143, lr_0 = 9.2622e-04
Loss = 2.4720e-03, PNorm = 38.0778, GNorm = 0.9221, lr_0 = 9.2525e-04
Loss = 2.7898e-03, PNorm = 38.0992, GNorm = 2.3876, lr_0 = 9.2428e-04
Loss = 2.3597e-03, PNorm = 38.1326, GNorm = 1.2336, lr_0 = 9.2332e-04
Loss = 2.4236e-03, PNorm = 38.1580, GNorm = 1.1849, lr_0 = 9.2235e-04
Loss = 2.4384e-03, PNorm = 38.1928, GNorm = 1.5540, lr_0 = 9.2139e-04
Loss = 2.5712e-03, PNorm = 38.2146, GNorm = 1.8727, lr_0 = 9.2043e-04
Loss = 2.8415e-03, PNorm = 38.2430, GNorm = 1.5894, lr_0 = 9.1947e-04
Loss = 2.1629e-03, PNorm = 38.2732, GNorm = 1.2990, lr_0 = 9.1851e-04
Loss = 2.6324e-03, PNorm = 38.3025, GNorm = 1.6911, lr_0 = 9.1755e-04
Loss = 2.5328e-03, PNorm = 38.3383, GNorm = 1.1087, lr_0 = 9.1659e-04
Loss = 2.8823e-03, PNorm = 38.3687, GNorm = 1.6785, lr_0 = 9.1564e-04
Loss = 2.6626e-03, PNorm = 38.4040, GNorm = 1.0253, lr_0 = 9.1468e-04
Loss = 1.9367e-03, PNorm = 38.4351, GNorm = 0.5055, lr_0 = 9.1373e-04
Loss = 2.3650e-03, PNorm = 38.4526, GNorm = 0.9391, lr_0 = 9.1277e-04
Loss = 2.6604e-03, PNorm = 38.4761, GNorm = 1.4693, lr_0 = 9.1182e-04
Loss = 2.7358e-03, PNorm = 38.4999, GNorm = 1.4262, lr_0 = 9.1087e-04
Loss = 2.0299e-03, PNorm = 38.5270, GNorm = 0.7946, lr_0 = 9.0992e-04
Validation rmse logD = 0.643707
Validation R2 logD = 0.697413
Validation rmse logP = 0.533621
Validation R2 logP = 0.917387
Epoch 6
Train function
Loss = 1.7220e-03, PNorm = 38.5576, GNorm = 0.7412, lr_0 = 9.0887e-04
Loss = 1.9723e-03, PNorm = 38.5794, GNorm = 1.0819, lr_0 = 9.0792e-04
Loss = 2.1689e-03, PNorm = 38.6053, GNorm = 1.4482, lr_0 = 9.0698e-04
Loss = 2.3285e-03, PNorm = 38.6384, GNorm = 1.8499, lr_0 = 9.0603e-04
Loss = 2.4705e-03, PNorm = 38.6697, GNorm = 2.1298, lr_0 = 9.0508e-04
Loss = 1.8251e-03, PNorm = 38.6982, GNorm = 1.2173, lr_0 = 9.0414e-04
Loss = 2.1164e-03, PNorm = 38.7326, GNorm = 0.6605, lr_0 = 9.0320e-04
Loss = 2.1876e-03, PNorm = 38.7641, GNorm = 1.4718, lr_0 = 9.0225e-04
Loss = 1.8857e-03, PNorm = 38.7838, GNorm = 0.7528, lr_0 = 9.0131e-04
Loss = 2.2752e-03, PNorm = 38.8128, GNorm = 1.0584, lr_0 = 9.0037e-04
Loss = 2.0739e-03, PNorm = 38.8301, GNorm = 1.2615, lr_0 = 8.9943e-04
Loss = 2.1961e-03, PNorm = 38.8461, GNorm = 1.4014, lr_0 = 8.9849e-04
Loss = 2.3265e-03, PNorm = 38.8751, GNorm = 0.9133, lr_0 = 8.9756e-04
Loss = 2.3586e-03, PNorm = 38.9013, GNorm = 1.3023, lr_0 = 8.9662e-04
Loss = 2.0045e-03, PNorm = 38.9176, GNorm = 0.7902, lr_0 = 8.9568e-04
Loss = 2.3132e-03, PNorm = 38.9381, GNorm = 1.2388, lr_0 = 8.9475e-04
Loss = 2.4455e-03, PNorm = 38.9609, GNorm = 1.0665, lr_0 = 8.9381e-04
Loss = 2.0568e-03, PNorm = 38.9815, GNorm = 0.6293, lr_0 = 8.9288e-04
Loss = 2.3525e-03, PNorm = 39.0014, GNorm = 0.8042, lr_0 = 8.9195e-04
Loss = 2.5873e-03, PNorm = 39.0281, GNorm = 1.5845, lr_0 = 8.9102e-04
Loss = 2.5294e-03, PNorm = 39.0647, GNorm = 1.5737, lr_0 = 8.9009e-04
Loss = 2.0255e-03, PNorm = 39.0962, GNorm = 1.6803, lr_0 = 8.8916e-04
Validation rmse logD = 0.630747
Validation R2 logD = 0.709475
Validation rmse logP = 0.517915
Validation R2 logP = 0.922178
Epoch 7
Train function
Loss = 1.3997e-03, PNorm = 39.1210, GNorm = 0.8853, lr_0 = 8.8823e-04
Loss = 2.0052e-03, PNorm = 39.1466, GNorm = 1.5306, lr_0 = 8.8730e-04
Loss = 1.7829e-03, PNorm = 39.1713, GNorm = 1.1273, lr_0 = 8.8638e-04
Loss = 2.2425e-03, PNorm = 39.2019, GNorm = 1.3184, lr_0 = 8.8545e-04
Loss = 1.8307e-03, PNorm = 39.2278, GNorm = 1.0908, lr_0 = 8.8453e-04
Loss = 1.7090e-03, PNorm = 39.2498, GNorm = 0.4866, lr_0 = 8.8361e-04
Loss = 1.9652e-03, PNorm = 39.2690, GNorm = 1.0668, lr_0 = 8.8268e-04
Loss = 1.6713e-03, PNorm = 39.2867, GNorm = 0.5993, lr_0 = 8.8176e-04
Loss = 1.8727e-03, PNorm = 39.3116, GNorm = 1.4191, lr_0 = 8.8084e-04
Loss = 1.8283e-03, PNorm = 39.3358, GNorm = 0.7503, lr_0 = 8.7992e-04
Loss = 1.5640e-03, PNorm = 39.3477, GNorm = 0.5568, lr_0 = 8.7900e-04
Loss = 1.7886e-03, PNorm = 39.3648, GNorm = 1.0123, lr_0 = 8.7809e-04
Loss = 2.0708e-03, PNorm = 39.3887, GNorm = 3.1168, lr_0 = 8.7717e-04
Loss = 2.7169e-03, PNorm = 39.4198, GNorm = 1.8430, lr_0 = 8.7625e-04
Loss = 2.7764e-03, PNorm = 39.4654, GNorm = 2.0327, lr_0 = 8.7534e-04
Loss = 2.3626e-03, PNorm = 39.5017, GNorm = 0.7971, lr_0 = 8.7443e-04
Loss = 1.8020e-03, PNorm = 39.5283, GNorm = 1.3159, lr_0 = 8.7351e-04
Loss = 2.1646e-03, PNorm = 39.5519, GNorm = 0.9166, lr_0 = 8.7260e-04
Loss = 1.8830e-03, PNorm = 39.5773, GNorm = 1.2385, lr_0 = 8.7169e-04
Loss = 2.1805e-03, PNorm = 39.5967, GNorm = 1.6603, lr_0 = 8.7078e-04
Loss = 2.4646e-03, PNorm = 39.6251, GNorm = 1.6736, lr_0 = 8.6987e-04
Loss = 2.3308e-03, PNorm = 39.6625, GNorm = 0.6712, lr_0 = 8.6896e-04
Loss = 2.3827e-03, PNorm = 39.6921, GNorm = 0.8400, lr_0 = 8.6806e-04
Validation rmse logD = 0.645388
Validation R2 logD = 0.695831
Validation rmse logP = 0.534340
Validation R2 logP = 0.917164
Epoch 8
Train function
Loss = 1.8270e-03, PNorm = 39.7181, GNorm = 1.2742, lr_0 = 8.6706e-04
Loss = 1.6054e-03, PNorm = 39.7422, GNorm = 1.0318, lr_0 = 8.6616e-04
Loss = 1.2071e-03, PNorm = 39.7652, GNorm = 0.8587, lr_0 = 8.6525e-04
Loss = 1.2627e-03, PNorm = 39.7822, GNorm = 0.6756, lr_0 = 8.6435e-04
Loss = 1.6280e-03, PNorm = 39.8057, GNorm = 1.0825, lr_0 = 8.6345e-04
Loss = 1.5421e-03, PNorm = 39.8296, GNorm = 1.4745, lr_0 = 8.6255e-04
Loss = 1.7611e-03, PNorm = 39.8522, GNorm = 0.5480, lr_0 = 8.6165e-04
Loss = 1.7318e-03, PNorm = 39.8748, GNorm = 0.6252, lr_0 = 8.6075e-04
Loss = 1.7291e-03, PNorm = 39.8920, GNorm = 1.1823, lr_0 = 8.5985e-04
Loss = 1.6189e-03, PNorm = 39.9139, GNorm = 1.1597, lr_0 = 8.5895e-04
Loss = 1.6147e-03, PNorm = 39.9373, GNorm = 0.6208, lr_0 = 8.5805e-04
Loss = 1.8186e-03, PNorm = 39.9582, GNorm = 0.7053, lr_0 = 8.5716e-04
Loss = 1.5709e-03, PNorm = 39.9843, GNorm = 1.0268, lr_0 = 8.5626e-04
Loss = 1.6051e-03, PNorm = 40.0118, GNorm = 1.1656, lr_0 = 8.5537e-04
Loss = 1.6994e-03, PNorm = 40.0348, GNorm = 1.1061, lr_0 = 8.5448e-04
Loss = 1.6965e-03, PNorm = 40.0593, GNorm = 0.8383, lr_0 = 8.5359e-04
Loss = 2.0556e-03, PNorm = 40.0848, GNorm = 1.1667, lr_0 = 8.5269e-04
Loss = 1.7703e-03, PNorm = 40.1105, GNorm = 1.2437, lr_0 = 8.5180e-04
Loss = 1.9747e-03, PNorm = 40.1363, GNorm = 1.3593, lr_0 = 8.5092e-04
Loss = 2.2747e-03, PNorm = 40.1686, GNorm = 2.4133, lr_0 = 8.5003e-04
Loss = 1.9866e-03, PNorm = 40.1959, GNorm = 2.2092, lr_0 = 8.4914e-04
Loss = 2.0657e-03, PNorm = 40.2213, GNorm = 0.7414, lr_0 = 8.4825e-04
Validation rmse logD = 0.640848
Validation R2 logD = 0.700095
Validation rmse logP = 0.525225
Validation R2 logP = 0.919966
Epoch 9
Train function
Loss = 1.3146e-03, PNorm = 40.2439, GNorm = 0.6616, lr_0 = 8.4737e-04
Loss = 2.2010e-03, PNorm = 40.2668, GNorm = 1.1434, lr_0 = 8.4648e-04
Loss = 2.0575e-03, PNorm = 40.3032, GNorm = 1.1417, lr_0 = 8.4560e-04
Loss = 1.4927e-03, PNorm = 40.3268, GNorm = 0.9109, lr_0 = 8.4472e-04
Loss = 1.6114e-03, PNorm = 40.3491, GNorm = 0.8911, lr_0 = 8.4384e-04
Loss = 1.6845e-03, PNorm = 40.3703, GNorm = 0.9361, lr_0 = 8.4296e-04
Loss = 1.6696e-03, PNorm = 40.3884, GNorm = 0.7505, lr_0 = 8.4208e-04
Loss = 1.3114e-03, PNorm = 40.4127, GNorm = 0.9239, lr_0 = 8.4120e-04
Loss = 1.6919e-03, PNorm = 40.4423, GNorm = 1.7896, lr_0 = 8.4032e-04
Loss = 1.6064e-03, PNorm = 40.4672, GNorm = 0.5437, lr_0 = 8.3944e-04
Loss = 1.8146e-03, PNorm = 40.4854, GNorm = 0.7301, lr_0 = 8.3857e-04
Loss = 1.4276e-03, PNorm = 40.5103, GNorm = 0.5401, lr_0 = 8.3769e-04
Loss = 1.3700e-03, PNorm = 40.5351, GNorm = 0.5706, lr_0 = 8.3682e-04
Loss = 1.4574e-03, PNorm = 40.5581, GNorm = 0.8324, lr_0 = 8.3594e-04
Loss = 1.5320e-03, PNorm = 40.5791, GNorm = 0.7922, lr_0 = 8.3507e-04
Loss = 1.6570e-03, PNorm = 40.6012, GNorm = 0.6569, lr_0 = 8.3420e-04
Loss = 1.4958e-03, PNorm = 40.6229, GNorm = 2.0707, lr_0 = 8.3333e-04
Loss = 1.6325e-03, PNorm = 40.6332, GNorm = 1.0666, lr_0 = 8.3246e-04
Loss = 1.7529e-03, PNorm = 40.6525, GNorm = 0.5688, lr_0 = 8.3159e-04
Loss = 1.5690e-03, PNorm = 40.6799, GNorm = 1.7658, lr_0 = 8.3072e-04
Loss = 1.4912e-03, PNorm = 40.6984, GNorm = 0.4865, lr_0 = 8.2986e-04
Loss = 1.5558e-03, PNorm = 40.7189, GNorm = 1.7103, lr_0 = 8.2899e-04
Loss = 1.2686e-03, PNorm = 40.7453, GNorm = 0.5739, lr_0 = 8.2812e-04
Validation rmse logD = 0.593581
Validation R2 logD = 0.742704
Validation rmse logP = 0.481743
Validation R2 logP = 0.932669
Epoch 10
Train function
Loss = 1.3563e-03, PNorm = 40.7688, GNorm = 0.5674, lr_0 = 8.2717e-04
Loss = 1.3181e-03, PNorm = 40.7907, GNorm = 0.7626, lr_0 = 8.2631e-04
Loss = 1.6745e-03, PNorm = 40.8097, GNorm = 0.8959, lr_0 = 8.2545e-04
Loss = 1.2286e-03, PNorm = 40.8341, GNorm = 0.6721, lr_0 = 8.2459e-04
Loss = 1.2093e-03, PNorm = 40.8539, GNorm = 1.2870, lr_0 = 8.2373e-04
Loss = 1.3620e-03, PNorm = 40.8737, GNorm = 1.1093, lr_0 = 8.2287e-04
Loss = 1.4807e-03, PNorm = 40.8971, GNorm = 0.6917, lr_0 = 8.2201e-04
Loss = 1.3249e-03, PNorm = 40.9225, GNorm = 0.8203, lr_0 = 8.2115e-04
Loss = 1.2143e-03, PNorm = 40.9381, GNorm = 0.5820, lr_0 = 8.2029e-04
Loss = 1.5348e-03, PNorm = 40.9486, GNorm = 0.5303, lr_0 = 8.1944e-04
Loss = 1.4056e-03, PNorm = 40.9710, GNorm = 1.1314, lr_0 = 8.1858e-04
Loss = 1.1573e-03, PNorm = 40.9766, GNorm = 0.7030, lr_0 = 8.1773e-04
Loss = 1.5392e-03, PNorm = 40.9927, GNorm = 0.6348, lr_0 = 8.1687e-04
Loss = 1.2966e-03, PNorm = 41.0159, GNorm = 0.9104, lr_0 = 8.1602e-04
Loss = 1.6021e-03, PNorm = 41.0457, GNorm = 0.9915, lr_0 = 8.1517e-04
Loss = 1.5858e-03, PNorm = 41.0726, GNorm = 0.7219, lr_0 = 8.1432e-04
Loss = 1.7483e-03, PNorm = 41.0980, GNorm = 1.4111, lr_0 = 8.1347e-04
Loss = 1.3796e-03, PNorm = 41.1235, GNorm = 0.5606, lr_0 = 8.1262e-04
Loss = 1.2633e-03, PNorm = 41.1475, GNorm = 1.1000, lr_0 = 8.1177e-04
Loss = 1.9328e-03, PNorm = 41.1750, GNorm = 1.5403, lr_0 = 8.1092e-04
Loss = 1.6290e-03, PNorm = 41.1997, GNorm = 0.9188, lr_0 = 8.1008e-04
Loss = 1.7097e-03, PNorm = 41.2231, GNorm = 1.8825, lr_0 = 8.0923e-04
Loss = 1.7585e-03, PNorm = 41.2431, GNorm = 0.8643, lr_0 = 8.0839e-04
Validation rmse logD = 0.577587
Validation R2 logD = 0.756383
Validation rmse logP = 0.540155
Validation R2 logP = 0.915351
Epoch 11
Train function
Loss = 1.3559e-03, PNorm = 41.2720, GNorm = 1.1030, lr_0 = 8.0754e-04
Loss = 1.3140e-03, PNorm = 41.2937, GNorm = 1.2207, lr_0 = 8.0670e-04
Loss = 1.3934e-03, PNorm = 41.3189, GNorm = 1.1411, lr_0 = 8.0586e-04
Loss = 1.6929e-03, PNorm = 41.3446, GNorm = 1.3301, lr_0 = 8.0502e-04
Loss = 1.6508e-03, PNorm = 41.3704, GNorm = 1.0519, lr_0 = 8.0418e-04
Loss = 1.0798e-03, PNorm = 41.3959, GNorm = 0.5763, lr_0 = 8.0334e-04
Loss = 1.2393e-03, PNorm = 41.4124, GNorm = 0.9051, lr_0 = 8.0250e-04
Loss = 1.2153e-03, PNorm = 41.4345, GNorm = 0.8550, lr_0 = 8.0166e-04
Loss = 1.1696e-03, PNorm = 41.4498, GNorm = 0.7688, lr_0 = 8.0082e-04
Loss = 1.0437e-03, PNorm = 41.4730, GNorm = 1.1146, lr_0 = 7.9999e-04
Loss = 1.4194e-03, PNorm = 41.4987, GNorm = 1.0071, lr_0 = 7.9915e-04
Loss = 1.5947e-03, PNorm = 41.5202, GNorm = 1.2752, lr_0 = 7.9832e-04
Loss = 1.3149e-03, PNorm = 41.5378, GNorm = 1.4757, lr_0 = 7.9749e-04
Loss = 1.6916e-03, PNorm = 41.5560, GNorm = 1.1013, lr_0 = 7.9665e-04
Loss = 1.6063e-03, PNorm = 41.5800, GNorm = 0.8684, lr_0 = 7.9582e-04
Loss = 1.4726e-03, PNorm = 41.6074, GNorm = 1.3450, lr_0 = 7.9499e-04
Loss = 1.6272e-03, PNorm = 41.6272, GNorm = 1.0663, lr_0 = 7.9416e-04
Loss = 1.7743e-03, PNorm = 41.6513, GNorm = 0.7502, lr_0 = 7.9333e-04
Loss = 1.6833e-03, PNorm = 41.6696, GNorm = 0.6993, lr_0 = 7.9251e-04
Loss = 1.8113e-03, PNorm = 41.7042, GNorm = 1.8132, lr_0 = 7.9168e-04
Loss = 1.4835e-03, PNorm = 41.7322, GNorm = 1.5160, lr_0 = 7.9085e-04
Loss = 1.5998e-03, PNorm = 41.7633, GNorm = 1.2775, lr_0 = 7.9003e-04
Validation rmse logD = 0.619848
Validation R2 logD = 0.719428
Validation rmse logP = 0.538860
Validation R2 logP = 0.915756
Epoch 12
Train function
Loss = 1.0788e-03, PNorm = 41.7960, GNorm = 1.4155, lr_0 = 7.8912e-04
Loss = 1.1234e-03, PNorm = 41.8174, GNorm = 0.7820, lr_0 = 7.8830e-04
Loss = 1.1277e-03, PNorm = 41.8287, GNorm = 0.8752, lr_0 = 7.8747e-04
Loss = 1.3299e-03, PNorm = 41.8544, GNorm = 0.6907, lr_0 = 7.8665e-04
Loss = 1.1233e-03, PNorm = 41.8762, GNorm = 0.7012, lr_0 = 7.8583e-04
Loss = 1.4363e-03, PNorm = 41.9010, GNorm = 0.6948, lr_0 = 7.8501e-04
Loss = 1.3567e-03, PNorm = 41.9247, GNorm = 0.5666, lr_0 = 7.8419e-04
Loss = 9.7591e-04, PNorm = 41.9486, GNorm = 0.4053, lr_0 = 7.8337e-04
Loss = 1.1981e-03, PNorm = 41.9692, GNorm = 0.8841, lr_0 = 7.8255e-04
Loss = 1.3855e-03, PNorm = 41.9906, GNorm = 0.8802, lr_0 = 7.8174e-04
Loss = 1.1917e-03, PNorm = 42.0076, GNorm = 0.8379, lr_0 = 7.8092e-04
Loss = 1.2864e-03, PNorm = 42.0176, GNorm = 0.9403, lr_0 = 7.8011e-04
Loss = 1.5295e-03, PNorm = 42.0438, GNorm = 0.7967, lr_0 = 7.7929e-04
Loss = 1.0963e-03, PNorm = 42.0686, GNorm = 0.6682, lr_0 = 7.7848e-04
Loss = 1.1677e-03, PNorm = 42.0855, GNorm = 0.9057, lr_0 = 7.7767e-04
Loss = 1.0468e-03, PNorm = 42.1051, GNorm = 0.4805, lr_0 = 7.7686e-04
Loss = 1.1918e-03, PNorm = 42.1268, GNorm = 0.8664, lr_0 = 7.7604e-04
Loss = 1.3732e-03, PNorm = 42.1479, GNorm = 0.5342, lr_0 = 7.7523e-04
Loss = 1.3978e-03, PNorm = 42.1634, GNorm = 0.5869, lr_0 = 7.7443e-04
Loss = 1.2103e-03, PNorm = 42.1848, GNorm = 0.9136, lr_0 = 7.7362e-04
Loss = 1.1560e-03, PNorm = 42.2085, GNorm = 0.8359, lr_0 = 7.7281e-04
Loss = 1.1988e-03, PNorm = 42.2287, GNorm = 0.4948, lr_0 = 7.7200e-04
Loss = 1.1863e-03, PNorm = 42.2487, GNorm = 0.6771, lr_0 = 7.7120e-04
Validation rmse logD = 0.575094
Validation R2 logD = 0.758481
Validation rmse logP = 0.537104
Validation R2 logP = 0.916305
Epoch 13
Train function
Loss = 1.2164e-03, PNorm = 42.2819, GNorm = 0.8118, lr_0 = 7.7039e-04
Loss = 1.0464e-03, PNorm = 42.3013, GNorm = 0.5649, lr_0 = 7.6959e-04
Loss = 1.1021e-03, PNorm = 42.3187, GNorm = 0.9118, lr_0 = 7.6879e-04
Loss = 9.7024e-04, PNorm = 42.3387, GNorm = 0.8145, lr_0 = 7.6798e-04
Loss = 9.4960e-04, PNorm = 42.3582, GNorm = 0.6383, lr_0 = 7.6718e-04
Loss = 9.1059e-04, PNorm = 42.3697, GNorm = 0.8910, lr_0 = 7.6638e-04
Loss = 1.0364e-03, PNorm = 42.3863, GNorm = 0.5678, lr_0 = 7.6558e-04
Loss = 8.0955e-04, PNorm = 42.4015, GNorm = 0.4886, lr_0 = 7.6478e-04
Loss = 8.9980e-04, PNorm = 42.4173, GNorm = 0.5991, lr_0 = 7.6398e-04
Loss = 1.1576e-03, PNorm = 42.4332, GNorm = 0.6991, lr_0 = 7.6319e-04
Loss = 9.1360e-04, PNorm = 42.4479, GNorm = 0.6116, lr_0 = 7.6239e-04
Loss = 8.0543e-04, PNorm = 42.4567, GNorm = 0.6121, lr_0 = 7.6159e-04
Loss = 1.1482e-03, PNorm = 42.4760, GNorm = 0.4954, lr_0 = 7.6080e-04
Loss = 1.0075e-03, PNorm = 42.4948, GNorm = 0.6564, lr_0 = 7.6000e-04
Loss = 1.2496e-03, PNorm = 42.5160, GNorm = 0.8299, lr_0 = 7.5921e-04
Loss = 1.1697e-03, PNorm = 42.5342, GNorm = 0.8473, lr_0 = 7.5842e-04
Loss = 1.3277e-03, PNorm = 42.5571, GNorm = 0.6245, lr_0 = 7.5763e-04
Loss = 1.2310e-03, PNorm = 42.5737, GNorm = 0.7344, lr_0 = 7.5684e-04
Loss = 1.2267e-03, PNorm = 42.5965, GNorm = 0.8882, lr_0 = 7.5605e-04
Loss = 1.3343e-03, PNorm = 42.6190, GNorm = 1.4374, lr_0 = 7.5526e-04
Loss = 1.3396e-03, PNorm = 42.6415, GNorm = 0.7205, lr_0 = 7.5447e-04
Loss = 1.3314e-03, PNorm = 42.6684, GNorm = 0.7525, lr_0 = 7.5368e-04
Validation rmse logD = 0.578412
Validation R2 logD = 0.755686
Validation rmse logP = 0.484158
Validation R2 logP = 0.931992
Epoch 14
Train function
Loss = 9.7504e-04, PNorm = 42.6993, GNorm = 0.3587, lr_0 = 7.5282e-04
Loss = 8.9620e-04, PNorm = 42.7202, GNorm = 0.6216, lr_0 = 7.5203e-04
Loss = 7.8017e-04, PNorm = 42.7414, GNorm = 0.6111, lr_0 = 7.5125e-04
Loss = 8.5839e-04, PNorm = 42.7575, GNorm = 0.7576, lr_0 = 7.5046e-04
Loss = 9.9896e-04, PNorm = 42.7720, GNorm = 1.4928, lr_0 = 7.4968e-04
Loss = 7.8014e-04, PNorm = 42.7842, GNorm = 0.4902, lr_0 = 7.4890e-04
Loss = 1.0229e-03, PNorm = 42.8023, GNorm = 0.7736, lr_0 = 7.4811e-04
Loss = 7.9364e-04, PNorm = 42.8181, GNorm = 0.6116, lr_0 = 7.4733e-04
Loss = 8.9590e-04, PNorm = 42.8358, GNorm = 0.4934, lr_0 = 7.4655e-04
Loss = 8.3396e-04, PNorm = 42.8512, GNorm = 0.4697, lr_0 = 7.4577e-04
Loss = 9.0446e-04, PNorm = 42.8640, GNorm = 0.8389, lr_0 = 7.4500e-04
Loss = 1.0329e-03, PNorm = 42.8835, GNorm = 0.4906, lr_0 = 7.4422e-04
Loss = 1.3027e-03, PNorm = 42.9031, GNorm = 1.6973, lr_0 = 7.4344e-04
Loss = 1.1479e-03, PNorm = 42.9152, GNorm = 0.6620, lr_0 = 7.4267e-04
Loss = 1.2025e-03, PNorm = 42.9365, GNorm = 0.9473, lr_0 = 7.4189e-04
Loss = 1.2528e-03, PNorm = 42.9674, GNorm = 1.2092, lr_0 = 7.4112e-04
Loss = 1.2598e-03, PNorm = 42.9911, GNorm = 1.1269, lr_0 = 7.4034e-04
Loss = 8.9630e-04, PNorm = 43.0101, GNorm = 0.6661, lr_0 = 7.3957e-04
Loss = 1.1713e-03, PNorm = 43.0224, GNorm = 1.2888, lr_0 = 7.3880e-04
Loss = 9.9941e-04, PNorm = 43.0432, GNorm = 0.6250, lr_0 = 7.3803e-04
Loss = 1.0327e-03, PNorm = 43.0609, GNorm = 0.4246, lr_0 = 7.3726e-04
Loss = 9.1237e-04, PNorm = 43.0815, GNorm = 0.7595, lr_0 = 7.3649e-04
Loss = 7.9605e-04, PNorm = 43.0975, GNorm = 0.4790, lr_0 = 7.3572e-04
Validation rmse logD = 0.565910
Validation R2 logD = 0.766133
Validation rmse logP = 0.464793
Validation R2 logP = 0.937324
Epoch 15
Train function
Loss = 6.4093e-04, PNorm = 43.1145, GNorm = 0.5139, lr_0 = 7.3495e-04
Loss = 7.3467e-04, PNorm = 43.1205, GNorm = 0.4843, lr_0 = 7.3418e-04
Loss = 9.0541e-04, PNorm = 43.1402, GNorm = 0.7357, lr_0 = 7.3342e-04
Loss = 8.6092e-04, PNorm = 43.1625, GNorm = 0.5279, lr_0 = 7.3265e-04
Loss = 9.2990e-04, PNorm = 43.1835, GNorm = 0.8864, lr_0 = 7.3189e-04
Loss = 1.0744e-03, PNorm = 43.2011, GNorm = 0.5562, lr_0 = 7.3112e-04
Loss = 1.1390e-03, PNorm = 43.2277, GNorm = 1.4582, lr_0 = 7.3036e-04
Loss = 1.4000e-03, PNorm = 43.2512, GNorm = 1.0416, lr_0 = 7.2960e-04
Loss = 1.1455e-03, PNorm = 43.2820, GNorm = 0.8826, lr_0 = 7.2884e-04
Loss = 9.7469e-04, PNorm = 43.3021, GNorm = 1.0192, lr_0 = 7.2808e-04
Loss = 1.2945e-03, PNorm = 43.3245, GNorm = 1.0885, lr_0 = 7.2732e-04
Loss = 1.0961e-03, PNorm = 43.3514, GNorm = 0.5491, lr_0 = 7.2656e-04
Loss = 1.0763e-03, PNorm = 43.3740, GNorm = 0.6198, lr_0 = 7.2580e-04
Loss = 1.1773e-03, PNorm = 43.3977, GNorm = 1.2119, lr_0 = 7.2504e-04
Loss = 1.1714e-03, PNorm = 43.4183, GNorm = 0.9630, lr_0 = 7.2428e-04
Loss = 1.0939e-03, PNorm = 43.4440, GNorm = 0.8791, lr_0 = 7.2353e-04
Loss = 1.2291e-03, PNorm = 43.4668, GNorm = 0.8794, lr_0 = 7.2277e-04
Loss = 9.9589e-04, PNorm = 43.4889, GNorm = 0.6676, lr_0 = 7.2202e-04
Loss = 1.3514e-03, PNorm = 43.5128, GNorm = 0.8042, lr_0 = 7.2127e-04
Loss = 9.2373e-04, PNorm = 43.5312, GNorm = 0.8402, lr_0 = 7.2051e-04
Loss = 8.4739e-04, PNorm = 43.5502, GNorm = 0.4412, lr_0 = 7.1976e-04
Loss = 7.6626e-04, PNorm = 43.5653, GNorm = 0.3990, lr_0 = 7.1901e-04
Validation rmse logD = 0.583378
Validation R2 logD = 0.751473
Validation rmse logP = 0.479337
Validation R2 logP = 0.933340
Epoch 16
Train function
Loss = 7.4294e-04, PNorm = 43.5880, GNorm = 0.4176, lr_0 = 7.1818e-04
Loss = 8.1674e-04, PNorm = 43.6046, GNorm = 0.3925, lr_0 = 7.1743e-04
Loss = 7.3982e-04, PNorm = 43.6188, GNorm = 0.5146, lr_0 = 7.1669e-04
Loss = 7.5745e-04, PNorm = 43.6296, GNorm = 0.4267, lr_0 = 7.1594e-04
Loss = 6.8797e-04, PNorm = 43.6400, GNorm = 0.4247, lr_0 = 7.1519e-04
Loss = 9.7846e-04, PNorm = 43.6572, GNorm = 0.9314, lr_0 = 7.1444e-04
Loss = 6.5862e-04, PNorm = 43.6788, GNorm = 0.6040, lr_0 = 7.1370e-04
Loss = 8.7483e-04, PNorm = 43.6964, GNorm = 0.7843, lr_0 = 7.1295e-04
Loss = 1.0910e-03, PNorm = 43.7112, GNorm = 1.0120, lr_0 = 7.1221e-04
Loss = 7.3928e-04, PNorm = 43.7291, GNorm = 0.3815, lr_0 = 7.1147e-04
Loss = 9.0498e-04, PNorm = 43.7517, GNorm = 0.7687, lr_0 = 7.1072e-04
Loss = 7.8911e-04, PNorm = 43.7646, GNorm = 0.4376, lr_0 = 7.0998e-04
Loss = 8.5113e-04, PNorm = 43.7819, GNorm = 0.8828, lr_0 = 7.0924e-04
Loss = 9.6157e-04, PNorm = 43.8036, GNorm = 0.8627, lr_0 = 7.0850e-04
Loss = 7.5959e-04, PNorm = 43.8284, GNorm = 0.8391, lr_0 = 7.0776e-04
Loss = 8.5895e-04, PNorm = 43.8450, GNorm = 0.6010, lr_0 = 7.0702e-04
Loss = 1.1290e-03, PNorm = 43.8625, GNorm = 1.4224, lr_0 = 7.0628e-04
Loss = 1.0363e-03, PNorm = 43.8763, GNorm = 0.7675, lr_0 = 7.0555e-04
Loss = 1.1270e-03, PNorm = 43.8974, GNorm = 0.6002, lr_0 = 7.0481e-04
Loss = 9.5131e-04, PNorm = 43.9169, GNorm = 0.8028, lr_0 = 7.0408e-04
Loss = 7.5011e-04, PNorm = 43.9317, GNorm = 0.6317, lr_0 = 7.0334e-04
Loss = 1.0398e-03, PNorm = 43.9480, GNorm = 0.4208, lr_0 = 7.0261e-04
Loss = 8.7469e-04, PNorm = 43.9668, GNorm = 0.6585, lr_0 = 7.0187e-04
Validation rmse logD = 0.597404
Validation R2 logD = 0.739379
Validation rmse logP = 0.462088
Validation R2 logP = 0.938051
Epoch 17
Train function
Loss = 7.9885e-04, PNorm = 43.9892, GNorm = 0.4898, lr_0 = 7.0114e-04
Loss = 9.2561e-04, PNorm = 44.0060, GNorm = 0.6316, lr_0 = 7.0041e-04
Loss = 8.2488e-04, PNorm = 44.0255, GNorm = 0.4385, lr_0 = 6.9968e-04
Loss = 6.6992e-04, PNorm = 44.0454, GNorm = 0.2895, lr_0 = 6.9895e-04
Loss = 7.4520e-04, PNorm = 44.0632, GNorm = 0.6011, lr_0 = 6.9822e-04
Loss = 8.0383e-04, PNorm = 44.0844, GNorm = 0.8875, lr_0 = 6.9749e-04
Loss = 8.6689e-04, PNorm = 44.1028, GNorm = 0.5213, lr_0 = 6.9676e-04
Loss = 9.4271e-04, PNorm = 44.1182, GNorm = 1.1455, lr_0 = 6.9603e-04
Loss = 8.2070e-04, PNorm = 44.1333, GNorm = 0.5739, lr_0 = 6.9531e-04
Loss = 7.9207e-04, PNorm = 44.1532, GNorm = 0.3701, lr_0 = 6.9458e-04
Loss = 6.9317e-04, PNorm = 44.1653, GNorm = 0.5616, lr_0 = 6.9386e-04
Loss = 7.4964e-04, PNorm = 44.1783, GNorm = 0.4425, lr_0 = 6.9313e-04
Loss = 7.4919e-04, PNorm = 44.1903, GNorm = 0.5647, lr_0 = 6.9241e-04
Loss = 6.3829e-04, PNorm = 44.2046, GNorm = 0.3305, lr_0 = 6.9169e-04
Loss = 7.6508e-04, PNorm = 44.2193, GNorm = 0.5497, lr_0 = 6.9096e-04
Loss = 8.8707e-04, PNorm = 44.2332, GNorm = 0.3508, lr_0 = 6.9024e-04
Loss = 6.7621e-04, PNorm = 44.2502, GNorm = 0.4278, lr_0 = 6.8952e-04
Loss = 8.2162e-04, PNorm = 44.2689, GNorm = 0.7718, lr_0 = 6.8880e-04
Loss = 7.7863e-04, PNorm = 44.2892, GNorm = 0.4396, lr_0 = 6.8808e-04
Loss = 8.6711e-04, PNorm = 44.3065, GNorm = 0.8589, lr_0 = 6.8737e-04
Loss = 9.0859e-04, PNorm = 44.3257, GNorm = 0.4663, lr_0 = 6.8665e-04
Loss = 9.0303e-04, PNorm = 44.3460, GNorm = 0.3603, lr_0 = 6.8593e-04
Validation rmse logD = 0.604869
Validation R2 logD = 0.732825
Validation rmse logP = 0.477241
Validation R2 logP = 0.933921
Epoch 18
Train function
Loss = 5.5111e-04, PNorm = 44.3633, GNorm = 0.5556, lr_0 = 6.8514e-04
Loss = 8.0203e-04, PNorm = 44.3773, GNorm = 0.5459, lr_0 = 6.8443e-04
Loss = 6.7059e-04, PNorm = 44.3975, GNorm = 0.9054, lr_0 = 6.8372e-04
Loss = 7.3561e-04, PNorm = 44.4225, GNorm = 1.6295, lr_0 = 6.8300e-04
Loss = 6.2697e-04, PNorm = 44.4462, GNorm = 0.8853, lr_0 = 6.8229e-04
Loss = 8.9072e-04, PNorm = 44.4630, GNorm = 1.1493, lr_0 = 6.8158e-04
Loss = 7.9852e-04, PNorm = 44.4777, GNorm = 0.9102, lr_0 = 6.8087e-04
Loss = 8.3195e-04, PNorm = 44.4943, GNorm = 0.5466, lr_0 = 6.8015e-04
Loss = 6.2048e-04, PNorm = 44.5157, GNorm = 0.4542, lr_0 = 6.7944e-04
Loss = 7.2641e-04, PNorm = 44.5311, GNorm = 0.6368, lr_0 = 6.7874e-04
Loss = 7.7230e-04, PNorm = 44.5392, GNorm = 0.7962, lr_0 = 6.7803e-04
Loss = 8.0364e-04, PNorm = 44.5565, GNorm = 0.8584, lr_0 = 6.7732e-04
Loss = 7.7001e-04, PNorm = 44.5764, GNorm = 0.6210, lr_0 = 6.7661e-04
Loss = 8.6354e-04, PNorm = 44.5948, GNorm = 0.5217, lr_0 = 6.7591e-04
Loss = 6.3566e-04, PNorm = 44.6117, GNorm = 0.4803, lr_0 = 6.7520e-04
Loss = 7.6287e-04, PNorm = 44.6264, GNorm = 0.3542, lr_0 = 6.7450e-04
Loss = 9.2921e-04, PNorm = 44.6439, GNorm = 1.3114, lr_0 = 6.7379e-04
Loss = 6.9419e-04, PNorm = 44.6612, GNorm = 0.9537, lr_0 = 6.7309e-04
Loss = 7.7760e-04, PNorm = 44.6687, GNorm = 0.4664, lr_0 = 6.7239e-04
Loss = 8.7428e-04, PNorm = 44.6823, GNorm = 0.5783, lr_0 = 6.7168e-04
Loss = 9.1360e-04, PNorm = 44.7002, GNorm = 1.2783, lr_0 = 6.7098e-04
Loss = 5.4758e-04, PNorm = 44.7109, GNorm = 0.7797, lr_0 = 6.7028e-04
Loss = 7.6744e-04, PNorm = 44.7261, GNorm = 0.5477, lr_0 = 6.6958e-04
Validation rmse logD = 0.585390
Validation R2 logD = 0.749755
Validation rmse logP = 0.507338
Validation R2 logP = 0.925324
Epoch 19
Train function
Loss = 6.4705e-04, PNorm = 44.7429, GNorm = 0.9633, lr_0 = 6.6889e-04
Loss = 7.1005e-04, PNorm = 44.7662, GNorm = 0.5120, lr_0 = 6.6819e-04
Loss = 6.5236e-04, PNorm = 44.7853, GNorm = 0.4622, lr_0 = 6.6749e-04
Loss = 7.9989e-04, PNorm = 44.7998, GNorm = 1.3195, lr_0 = 6.6679e-04
Loss = 7.7553e-04, PNorm = 44.8186, GNorm = 0.9650, lr_0 = 6.6610e-04
Loss = 7.7543e-04, PNorm = 44.8354, GNorm = 0.7280, lr_0 = 6.6540e-04
Loss = 7.5981e-04, PNorm = 44.8528, GNorm = 0.3549, lr_0 = 6.6471e-04
Loss = 9.2756e-04, PNorm = 44.8675, GNorm = 1.3054, lr_0 = 6.6401e-04
Loss = 1.2835e-03, PNorm = 44.8913, GNorm = 1.6428, lr_0 = 6.6332e-04
Loss = 7.7119e-04, PNorm = 44.9162, GNorm = 0.6726, lr_0 = 6.6263e-04
Loss = 8.4075e-04, PNorm = 44.9320, GNorm = 0.4659, lr_0 = 6.6194e-04
Loss = 7.5132e-04, PNorm = 44.9441, GNorm = 0.4817, lr_0 = 6.6125e-04
Loss = 8.2956e-04, PNorm = 44.9602, GNorm = 0.7658, lr_0 = 6.6056e-04
Loss = 6.7258e-04, PNorm = 44.9808, GNorm = 0.6233, lr_0 = 6.5987e-04
Loss = 7.6987e-04, PNorm = 45.0078, GNorm = 0.3940, lr_0 = 6.5918e-04
Loss = 6.4719e-04, PNorm = 45.0268, GNorm = 0.4839, lr_0 = 6.5849e-04
Loss = 6.4290e-04, PNorm = 45.0375, GNorm = 0.6346, lr_0 = 6.5780e-04
Loss = 6.0609e-04, PNorm = 45.0484, GNorm = 0.3155, lr_0 = 6.5712e-04
Loss = 7.3465e-04, PNorm = 45.0612, GNorm = 0.9235, lr_0 = 6.5643e-04
Loss = 6.2480e-04, PNorm = 45.0729, GNorm = 0.8792, lr_0 = 6.5574e-04
Loss = 6.9972e-04, PNorm = 45.0858, GNorm = 0.3487, lr_0 = 6.5506e-04
Loss = 6.2828e-04, PNorm = 45.0981, GNorm = 0.5560, lr_0 = 6.5438e-04
Validation rmse logD = 0.565484
Validation R2 logD = 0.766485
Validation rmse logP = 0.456793
Validation R2 logP = 0.939463
Epoch 20
Train function
Loss = 5.6982e-04, PNorm = 45.1142, GNorm = 0.4146, lr_0 = 6.5363e-04
Loss = 5.5164e-04, PNorm = 45.1279, GNorm = 0.7633, lr_0 = 6.5294e-04
Loss = 5.7324e-04, PNorm = 45.1441, GNorm = 0.6731, lr_0 = 6.5226e-04
Loss = 4.8831e-04, PNorm = 45.1552, GNorm = 0.4676, lr_0 = 6.5158e-04
Loss = 6.1431e-04, PNorm = 45.1682, GNorm = 0.6212, lr_0 = 6.5090e-04
Loss = 6.9260e-04, PNorm = 45.1796, GNorm = 0.4831, lr_0 = 6.5022e-04
Loss = 6.1230e-04, PNorm = 45.1939, GNorm = 0.5617, lr_0 = 6.4954e-04
Loss = 6.5634e-04, PNorm = 45.2064, GNorm = 1.1449, lr_0 = 6.4886e-04
Loss = 5.4256e-04, PNorm = 45.2153, GNorm = 0.5337, lr_0 = 6.4819e-04
Loss = 8.5363e-04, PNorm = 45.2344, GNorm = 0.8526, lr_0 = 6.4751e-04
Loss = 8.7143e-04, PNorm = 45.2544, GNorm = 1.0950, lr_0 = 6.4684e-04
Loss = 9.1865e-04, PNorm = 45.2713, GNorm = 0.7446, lr_0 = 6.4616e-04
Loss = 1.0128e-03, PNorm = 45.2924, GNorm = 0.9339, lr_0 = 6.4549e-04
Loss = 7.5587e-04, PNorm = 45.3131, GNorm = 0.3065, lr_0 = 6.4481e-04
Loss = 7.1329e-04, PNorm = 45.3279, GNorm = 0.5263, lr_0 = 6.4414e-04
Loss = 8.4714e-04, PNorm = 45.3457, GNorm = 0.9347, lr_0 = 6.4347e-04
Loss = 6.5361e-04, PNorm = 45.3672, GNorm = 0.8975, lr_0 = 6.4280e-04
Loss = 6.0730e-04, PNorm = 45.3840, GNorm = 0.5580, lr_0 = 6.4212e-04
Loss = 5.6497e-04, PNorm = 45.3969, GNorm = 0.5624, lr_0 = 6.4145e-04
Loss = 6.2845e-04, PNorm = 45.4148, GNorm = 0.9100, lr_0 = 6.4078e-04
Loss = 6.4424e-04, PNorm = 45.4302, GNorm = 0.2577, lr_0 = 6.4012e-04
Loss = 7.1311e-04, PNorm = 45.4426, GNorm = 1.0644, lr_0 = 6.3945e-04
Loss = 6.6700e-04, PNorm = 45.4580, GNorm = 0.3589, lr_0 = 6.3878e-04
Validation rmse logD = 0.565977
Validation R2 logD = 0.766078
Validation rmse logP = 0.459306
Validation R2 logP = 0.938795
Epoch 21
Train function
Loss = 6.3691e-04, PNorm = 45.4715, GNorm = 0.5170, lr_0 = 6.3811e-04
Loss = 5.7732e-04, PNorm = 45.4861, GNorm = 0.4952, lr_0 = 6.3745e-04
Loss = 5.9500e-04, PNorm = 45.5003, GNorm = 0.7800, lr_0 = 6.3678e-04
Loss = 4.7466e-04, PNorm = 45.5122, GNorm = 0.6907, lr_0 = 6.3612e-04
Loss = 4.6948e-04, PNorm = 45.5225, GNorm = 0.4182, lr_0 = 6.3545e-04
Loss = 5.8276e-04, PNorm = 45.5335, GNorm = 0.3390, lr_0 = 6.3479e-04
Loss = 4.8949e-04, PNorm = 45.5463, GNorm = 0.2677, lr_0 = 6.3413e-04
Loss = 4.9630e-04, PNorm = 45.5567, GNorm = 0.4171, lr_0 = 6.3347e-04
Loss = 4.9822e-04, PNorm = 45.5703, GNorm = 0.4539, lr_0 = 6.3280e-04
Loss = 5.1871e-04, PNorm = 45.5835, GNorm = 0.4867, lr_0 = 6.3214e-04
Loss = 5.9644e-04, PNorm = 45.5950, GNorm = 0.7531, lr_0 = 6.3148e-04
Loss = 4.5048e-04, PNorm = 45.6081, GNorm = 0.6446, lr_0 = 6.3083e-04
Loss = 5.5135e-04, PNorm = 45.6212, GNorm = 0.4156, lr_0 = 6.3017e-04
Loss = 6.6339e-04, PNorm = 45.6352, GNorm = 0.4028, lr_0 = 6.2951e-04
Loss = 5.1064e-04, PNorm = 45.6462, GNorm = 0.3186, lr_0 = 6.2885e-04
Loss = 6.2199e-04, PNorm = 45.6608, GNorm = 0.4429, lr_0 = 6.2820e-04
Loss = 5.4580e-04, PNorm = 45.6726, GNorm = 0.4948, lr_0 = 6.2754e-04
Loss = 5.5028e-04, PNorm = 45.6858, GNorm = 0.6769, lr_0 = 6.2689e-04
Loss = 7.0878e-04, PNorm = 45.6969, GNorm = 0.5043, lr_0 = 6.2623e-04
Loss = 7.1810e-04, PNorm = 45.7098, GNorm = 1.0411, lr_0 = 6.2558e-04
Loss = 5.4889e-04, PNorm = 45.7258, GNorm = 0.4734, lr_0 = 6.2492e-04
Loss = 6.4367e-04, PNorm = 45.7476, GNorm = 0.4358, lr_0 = 6.2427e-04
Loss = 5.9158e-04, PNorm = 45.7655, GNorm = 0.4701, lr_0 = 6.2362e-04
Loss = 1.0167e-03, PNorm = 45.7670, GNorm = 0.7517, lr_0 = 6.2356e-04
Validation rmse logD = 0.555911
Validation R2 logD = 0.774325
Validation rmse logP = 0.466850
Validation R2 logP = 0.936768
Epoch 22
Train function
Loss = 4.2849e-04, PNorm = 45.7791, GNorm = 0.5876, lr_0 = 6.2290e-04
Loss = 5.3053e-04, PNorm = 45.7886, GNorm = 0.6457, lr_0 = 6.2225e-04
Loss = 4.9032e-04, PNorm = 45.8036, GNorm = 0.5747, lr_0 = 6.2161e-04
Loss = 5.0702e-04, PNorm = 45.8180, GNorm = 0.4417, lr_0 = 6.2096e-04
Loss = 5.2298e-04, PNorm = 45.8289, GNorm = 0.3973, lr_0 = 6.2031e-04
Loss = 5.8874e-04, PNorm = 45.8350, GNorm = 0.6584, lr_0 = 6.1966e-04
Loss = 4.7872e-04, PNorm = 45.8495, GNorm = 0.6058, lr_0 = 6.1901e-04
Loss = 5.8348e-04, PNorm = 45.8635, GNorm = 0.5759, lr_0 = 6.1837e-04
Loss = 5.7907e-04, PNorm = 45.8789, GNorm = 0.4117, lr_0 = 6.1772e-04
Loss = 6.3054e-04, PNorm = 45.8921, GNorm = 0.6773, lr_0 = 6.1708e-04
Loss = 6.6209e-04, PNorm = 45.9102, GNorm = 0.7497, lr_0 = 6.1643e-04
Loss = 5.7051e-04, PNorm = 45.9219, GNorm = 0.9252, lr_0 = 6.1579e-04
Loss = 5.5600e-04, PNorm = 45.9389, GNorm = 0.5026, lr_0 = 6.1515e-04
Loss = 7.6712e-04, PNorm = 45.9587, GNorm = 0.5430, lr_0 = 6.1451e-04
Loss = 7.9019e-04, PNorm = 45.9800, GNorm = 0.8102, lr_0 = 6.1386e-04
Loss = 6.0391e-04, PNorm = 45.9999, GNorm = 0.3825, lr_0 = 6.1322e-04
Loss = 6.0877e-04, PNorm = 46.0135, GNorm = 0.8503, lr_0 = 6.1258e-04
Loss = 5.6341e-04, PNorm = 46.0297, GNorm = 0.8144, lr_0 = 6.1194e-04
Loss = 5.7742e-04, PNorm = 46.0498, GNorm = 0.5611, lr_0 = 6.1131e-04
Loss = 7.4581e-04, PNorm = 46.0604, GNorm = 0.3453, lr_0 = 6.1067e-04
Loss = 6.1262e-04, PNorm = 46.0796, GNorm = 0.4935, lr_0 = 6.1003e-04
Loss = 6.1394e-04, PNorm = 46.0990, GNorm = 0.6843, lr_0 = 6.0939e-04
Validation rmse logD = 0.566761
Validation R2 logD = 0.765430
Validation rmse logP = 0.454983
Validation R2 logP = 0.939941
Epoch 23
Train function
Loss = 5.6955e-04, PNorm = 46.1138, GNorm = 0.9185, lr_0 = 6.0876e-04
Loss = 6.2902e-04, PNorm = 46.1312, GNorm = 0.3998, lr_0 = 6.0812e-04
Loss = 5.6213e-04, PNorm = 46.1450, GNorm = 0.8111, lr_0 = 6.0749e-04
Loss = 6.2517e-04, PNorm = 46.1589, GNorm = 0.4088, lr_0 = 6.0685e-04
Loss = 5.8117e-04, PNorm = 46.1783, GNorm = 0.9354, lr_0 = 6.0622e-04
Loss = 6.1467e-04, PNorm = 46.1943, GNorm = 0.6356, lr_0 = 6.0559e-04
Loss = 5.5327e-04, PNorm = 46.2102, GNorm = 0.5441, lr_0 = 6.0496e-04
Loss = 5.3743e-04, PNorm = 46.2271, GNorm = 0.4352, lr_0 = 6.0432e-04
Loss = 5.2559e-04, PNorm = 46.2441, GNorm = 0.6171, lr_0 = 6.0369e-04
Loss = 5.6182e-04, PNorm = 46.2565, GNorm = 0.6160, lr_0 = 6.0306e-04
Loss = 5.1596e-04, PNorm = 46.2676, GNorm = 0.6484, lr_0 = 6.0243e-04
Loss = 5.0934e-04, PNorm = 46.2789, GNorm = 0.6825, lr_0 = 6.0180e-04
Loss = 4.3603e-04, PNorm = 46.2865, GNorm = 0.5451, lr_0 = 6.0118e-04
Loss = 5.4392e-04, PNorm = 46.2961, GNorm = 0.4407, lr_0 = 6.0055e-04
Loss = 6.2733e-04, PNorm = 46.3081, GNorm = 0.6009, lr_0 = 5.9992e-04
Loss = 4.9818e-04, PNorm = 46.3220, GNorm = 0.5784, lr_0 = 5.9930e-04
Loss = 7.3361e-04, PNorm = 46.3376, GNorm = 0.4900, lr_0 = 5.9867e-04
Loss = 4.8306e-04, PNorm = 46.3509, GNorm = 0.3491, lr_0 = 5.9805e-04
Loss = 5.5446e-04, PNorm = 46.3631, GNorm = 0.4785, lr_0 = 5.9742e-04
Loss = 4.5082e-04, PNorm = 46.3771, GNorm = 0.3746, lr_0 = 5.9680e-04
Loss = 5.2849e-04, PNorm = 46.3911, GNorm = 0.4225, lr_0 = 5.9618e-04
Loss = 6.4877e-04, PNorm = 46.4029, GNorm = 0.6771, lr_0 = 5.9555e-04
Loss = 5.8807e-04, PNorm = 46.4198, GNorm = 0.7654, lr_0 = 5.9493e-04
Validation rmse logD = 0.570630
Validation R2 logD = 0.762216
Validation rmse logP = 0.462785
Validation R2 logP = 0.937864
Epoch 24
Train function
Loss = 4.8550e-04, PNorm = 46.4347, GNorm = 0.4461, lr_0 = 5.9425e-04
Loss = 5.8902e-04, PNorm = 46.4483, GNorm = 0.4955, lr_0 = 5.9363e-04
Loss = 4.5477e-04, PNorm = 46.4631, GNorm = 0.4765, lr_0 = 5.9301e-04
Loss = 4.4757e-04, PNorm = 46.4794, GNorm = 0.7157, lr_0 = 5.9239e-04
Loss = 4.6219e-04, PNorm = 46.4918, GNorm = 0.7068, lr_0 = 5.9177e-04
Loss = 4.5288e-04, PNorm = 46.5023, GNorm = 0.6800, lr_0 = 5.9115e-04
Loss = 4.4045e-04, PNorm = 46.5048, GNorm = 0.4460, lr_0 = 5.9054e-04
Loss = 5.6003e-04, PNorm = 46.5122, GNorm = 0.4624, lr_0 = 5.8992e-04
Loss = 5.2144e-04, PNorm = 46.5235, GNorm = 0.4897, lr_0 = 5.8931e-04
Loss = 4.4377e-04, PNorm = 46.5346, GNorm = 0.3422, lr_0 = 5.8869e-04
Loss = 4.4338e-04, PNorm = 46.5451, GNorm = 0.5778, lr_0 = 5.8808e-04
Loss = 4.9411e-04, PNorm = 46.5554, GNorm = 0.6900, lr_0 = 5.8746e-04
Loss = 4.3677e-04, PNorm = 46.5660, GNorm = 0.6162, lr_0 = 5.8685e-04
Loss = 4.2845e-04, PNorm = 46.5807, GNorm = 0.4254, lr_0 = 5.8624e-04
Loss = 4.5566e-04, PNorm = 46.5953, GNorm = 0.5103, lr_0 = 5.8562e-04
Loss = 5.4681e-04, PNorm = 46.6064, GNorm = 0.3554, lr_0 = 5.8501e-04
Loss = 5.4782e-04, PNorm = 46.6242, GNorm = 0.9310, lr_0 = 5.8440e-04
Loss = 8.5234e-04, PNorm = 46.6487, GNorm = 0.3518, lr_0 = 5.8379e-04
Loss = 6.8437e-04, PNorm = 46.6665, GNorm = 1.1104, lr_0 = 5.8318e-04
Loss = 6.0080e-04, PNorm = 46.6832, GNorm = 0.2964, lr_0 = 5.8257e-04
Loss = 6.0075e-04, PNorm = 46.6986, GNorm = 0.9989, lr_0 = 5.8197e-04
Loss = 6.6758e-04, PNorm = 46.7056, GNorm = 0.5939, lr_0 = 5.8136e-04
Validation rmse logD = 0.583469
Validation R2 logD = 0.751395
Validation rmse logP = 0.461638
Validation R2 logP = 0.938172
Epoch 25
Train function
Loss = 6.5582e-04, PNorm = 46.7214, GNorm = 0.6823, lr_0 = 5.8075e-04
Loss = 4.5702e-04, PNorm = 46.7342, GNorm = 0.4449, lr_0 = 5.8015e-04
Loss = 4.3491e-04, PNorm = 46.7441, GNorm = 0.4143, lr_0 = 5.7954e-04
Loss = 3.9264e-04, PNorm = 46.7571, GNorm = 0.5827, lr_0 = 5.7894e-04
Loss = 4.6804e-04, PNorm = 46.7661, GNorm = 0.8556, lr_0 = 5.7833e-04
Loss = 4.9916e-04, PNorm = 46.7809, GNorm = 0.5375, lr_0 = 5.7773e-04
Loss = 3.8662e-04, PNorm = 46.7909, GNorm = 0.3583, lr_0 = 5.7712e-04
Loss = 4.6138e-04, PNorm = 46.8012, GNorm = 0.8691, lr_0 = 5.7652e-04
Loss = 4.3254e-04, PNorm = 46.8187, GNorm = 0.4048, lr_0 = 5.7592e-04
Loss = 5.0187e-04, PNorm = 46.8319, GNorm = 0.2801, lr_0 = 5.7532e-04
Loss = 4.6206e-04, PNorm = 46.8421, GNorm = 0.6507, lr_0 = 5.7472e-04
Loss = 6.1721e-04, PNorm = 46.8573, GNorm = 0.6207, lr_0 = 5.7412e-04
Loss = 1.0754e-03, PNorm = 46.8700, GNorm = 1.2025, lr_0 = 5.7352e-04
Loss = 7.0080e-04, PNorm = 46.9010, GNorm = 0.5577, lr_0 = 5.7292e-04
Loss = 7.0296e-04, PNorm = 46.9317, GNorm = 0.4657, lr_0 = 5.7232e-04
Loss = 6.3231e-04, PNorm = 46.9575, GNorm = 0.4092, lr_0 = 5.7173e-04
Loss = 7.0104e-04, PNorm = 46.9766, GNorm = 0.6101, lr_0 = 5.7113e-04
Loss = 6.8283e-04, PNorm = 46.9999, GNorm = 0.8500, lr_0 = 5.7053e-04
Loss = 7.7226e-04, PNorm = 47.0166, GNorm = 0.6803, lr_0 = 5.6994e-04
Loss = 4.7254e-04, PNorm = 47.0325, GNorm = 0.3921, lr_0 = 5.6934e-04
Loss = 4.8828e-04, PNorm = 47.0463, GNorm = 0.3780, lr_0 = 5.6875e-04
Loss = 4.8394e-04, PNorm = 47.0602, GNorm = 0.4236, lr_0 = 5.6816e-04
Loss = 6.1973e-04, PNorm = 47.0658, GNorm = 0.3526, lr_0 = 5.6756e-04
Validation rmse logD = 0.556873
Validation R2 logD = 0.773543
Validation rmse logP = 0.461864
Validation R2 logP = 0.938111
Epoch 26
Train function
Loss = 3.6969e-04, PNorm = 47.0824, GNorm = 0.4572, lr_0 = 5.6691e-04
Loss = 4.1747e-04, PNorm = 47.0908, GNorm = 0.3743, lr_0 = 5.6632e-04
Loss = 3.9983e-04, PNorm = 47.0978, GNorm = 0.4875, lr_0 = 5.6573e-04
Loss = 4.5414e-04, PNorm = 47.1130, GNorm = 0.5335, lr_0 = 5.6514e-04
Loss = 3.4903e-04, PNorm = 47.1273, GNorm = 0.3267, lr_0 = 5.6455e-04
Loss = 4.6954e-04, PNorm = 47.1372, GNorm = 0.2433, lr_0 = 5.6396e-04
Loss = 3.8681e-04, PNorm = 47.1471, GNorm = 0.4703, lr_0 = 5.6337e-04
Loss = 3.5215e-04, PNorm = 47.1588, GNorm = 0.5408, lr_0 = 5.6278e-04
Loss = 4.6929e-04, PNorm = 47.1701, GNorm = 1.1344, lr_0 = 5.6219e-04
Loss = 4.5842e-04, PNorm = 47.1826, GNorm = 0.2680, lr_0 = 5.6161e-04
Loss = 4.9589e-04, PNorm = 47.1907, GNorm = 0.5256, lr_0 = 5.6102e-04
Loss = 4.2755e-04, PNorm = 47.1995, GNorm = 0.3307, lr_0 = 5.6044e-04
Loss = 5.9809e-04, PNorm = 47.2089, GNorm = 0.7146, lr_0 = 5.5985e-04
Loss = 5.1819e-04, PNorm = 47.2175, GNorm = 0.3890, lr_0 = 5.5927e-04
Loss = 5.6677e-04, PNorm = 47.2301, GNorm = 0.6424, lr_0 = 5.5868e-04
Loss = 4.4081e-04, PNorm = 47.2463, GNorm = 0.4909, lr_0 = 5.5810e-04
Loss = 4.4465e-04, PNorm = 47.2591, GNorm = 0.4743, lr_0 = 5.5752e-04
Loss = 4.9313e-04, PNorm = 47.2740, GNorm = 0.3740, lr_0 = 5.5694e-04
Loss = 5.9726e-04, PNorm = 47.2866, GNorm = 0.7149, lr_0 = 5.5635e-04
Loss = 5.4034e-04, PNorm = 47.3050, GNorm = 0.5866, lr_0 = 5.5577e-04
Loss = 5.1586e-04, PNorm = 47.3213, GNorm = 0.6045, lr_0 = 5.5519e-04
Loss = 4.4037e-04, PNorm = 47.3324, GNorm = 0.6493, lr_0 = 5.5461e-04
Validation rmse logD = 0.557519
Validation R2 logD = 0.773017
Validation rmse logP = 0.464185
Validation R2 logP = 0.937487
Epoch 27
Train function
Loss = 6.2377e-04, PNorm = 47.3469, GNorm = 0.8825, lr_0 = 5.5398e-04
Loss = 4.0870e-04, PNorm = 47.3554, GNorm = 0.3919, lr_0 = 5.5340e-04
Loss = 3.9149e-04, PNorm = 47.3676, GNorm = 0.6758, lr_0 = 5.5282e-04
Loss = 3.8958e-04, PNorm = 47.3782, GNorm = 0.2420, lr_0 = 5.5224e-04
Loss = 3.9756e-04, PNorm = 47.3855, GNorm = 0.3618, lr_0 = 5.5167e-04
Loss = 4.5291e-04, PNorm = 47.3939, GNorm = 0.3635, lr_0 = 5.5109e-04
Loss = 4.1220e-04, PNorm = 47.4063, GNorm = 0.4454, lr_0 = 5.5052e-04
Loss = 3.6678e-04, PNorm = 47.4180, GNorm = 0.4004, lr_0 = 5.4994e-04
Loss = 4.7184e-04, PNorm = 47.4268, GNorm = 0.4579, lr_0 = 5.4937e-04
Loss = 4.3058e-04, PNorm = 47.4351, GNorm = 0.7229, lr_0 = 5.4880e-04
Loss = 5.8061e-04, PNorm = 47.4510, GNorm = 0.4571, lr_0 = 5.4822e-04
Loss = 4.4482e-04, PNorm = 47.4669, GNorm = 0.5929, lr_0 = 5.4765e-04
Loss = 3.8454e-04, PNorm = 47.4821, GNorm = 0.3559, lr_0 = 5.4708e-04
Loss = 4.8683e-04, PNorm = 47.4924, GNorm = 0.3535, lr_0 = 5.4651e-04
Loss = 4.0984e-04, PNorm = 47.5026, GNorm = 0.3869, lr_0 = 5.4594e-04
Loss = 4.9016e-04, PNorm = 47.5137, GNorm = 0.7105, lr_0 = 5.4537e-04
Loss = 4.8981e-04, PNorm = 47.5210, GNorm = 0.7111, lr_0 = 5.4480e-04
Loss = 4.3350e-04, PNorm = 47.5280, GNorm = 0.4925, lr_0 = 5.4423e-04
Loss = 3.7827e-04, PNorm = 47.5419, GNorm = 0.6276, lr_0 = 5.4366e-04
Loss = 5.3285e-04, PNorm = 47.5497, GNorm = 0.4519, lr_0 = 5.4309e-04
Loss = 4.0107e-04, PNorm = 47.5598, GNorm = 0.2665, lr_0 = 5.4253e-04
Loss = 4.0635e-04, PNorm = 47.5711, GNorm = 0.4629, lr_0 = 5.4196e-04
Loss = 4.2691e-04, PNorm = 47.5824, GNorm = 0.5830, lr_0 = 5.4140e-04
Validation rmse logD = 0.562818
Validation R2 logD = 0.768682
Validation rmse logP = 0.464670
Validation R2 logP = 0.937357
Epoch 28
Train function
Loss = 3.4167e-04, PNorm = 47.5950, GNorm = 0.2054, lr_0 = 5.4083e-04
Loss = 3.5408e-04, PNorm = 47.6073, GNorm = 0.5123, lr_0 = 5.4027e-04
Loss = 3.2586e-04, PNorm = 47.6120, GNorm = 0.3469, lr_0 = 5.3970e-04
Loss = 3.1931e-04, PNorm = 47.6210, GNorm = 0.3669, lr_0 = 5.3914e-04
Loss = 4.0320e-04, PNorm = 47.6295, GNorm = 0.3714, lr_0 = 5.3858e-04
Loss = 4.2028e-04, PNorm = 47.6408, GNorm = 0.5855, lr_0 = 5.3801e-04
Loss = 3.9589e-04, PNorm = 47.6516, GNorm = 0.8704, lr_0 = 5.3745e-04
Loss = 3.3565e-04, PNorm = 47.6583, GNorm = 0.3034, lr_0 = 5.3689e-04
Loss = 3.2693e-04, PNorm = 47.6682, GNorm = 0.3831, lr_0 = 5.3633e-04
Loss = 3.5459e-04, PNorm = 47.6791, GNorm = 0.3785, lr_0 = 5.3577e-04
Loss = 4.0781e-04, PNorm = 47.6861, GNorm = 0.3125, lr_0 = 5.3521e-04
Loss = 3.8356e-04, PNorm = 47.6973, GNorm = 0.7835, lr_0 = 5.3465e-04
Loss = 3.7714e-04, PNorm = 47.7065, GNorm = 0.3050, lr_0 = 5.3410e-04
Loss = 3.8700e-04, PNorm = 47.7172, GNorm = 0.2900, lr_0 = 5.3354e-04
Loss = 3.3426e-04, PNorm = 47.7257, GNorm = 0.3385, lr_0 = 5.3298e-04
Loss = 4.2957e-04, PNorm = 47.7358, GNorm = 0.3990, lr_0 = 5.3243e-04
Loss = 3.2413e-04, PNorm = 47.7445, GNorm = 0.3497, lr_0 = 5.3187e-04
Loss = 3.5361e-04, PNorm = 47.7567, GNorm = 0.3542, lr_0 = 5.3131e-04
Loss = 3.5142e-04, PNorm = 47.7702, GNorm = 0.3212, lr_0 = 5.3076e-04
Loss = 4.6706e-04, PNorm = 47.7792, GNorm = 0.3726, lr_0 = 5.3021e-04
Loss = 3.6749e-04, PNorm = 47.7866, GNorm = 0.3830, lr_0 = 5.2965e-04
Loss = 3.6239e-04, PNorm = 47.7961, GNorm = 0.2953, lr_0 = 5.2910e-04
Validation rmse logD = 0.562811
Validation R2 logD = 0.768688
Validation rmse logP = 0.458827
Validation R2 logP = 0.938922
Epoch 29
Train function
Loss = 3.2937e-04, PNorm = 47.8066, GNorm = 0.5039, lr_0 = 5.2849e-04
Loss = 2.6925e-04, PNorm = 47.8138, GNorm = 0.3190, lr_0 = 5.2794e-04
Loss = 3.7908e-04, PNorm = 47.8203, GNorm = 0.5824, lr_0 = 5.2739e-04
Loss = 2.7753e-04, PNorm = 47.8301, GNorm = 0.3475, lr_0 = 5.2684e-04
Loss = 3.3415e-04, PNorm = 47.8369, GNorm = 0.2627, lr_0 = 5.2629e-04
Loss = 3.1223e-04, PNorm = 47.8460, GNorm = 0.4116, lr_0 = 5.2574e-04
Loss = 2.8368e-04, PNorm = 47.8549, GNorm = 0.2429, lr_0 = 5.2519e-04
Loss = 2.4616e-04, PNorm = 47.8612, GNorm = 0.4669, lr_0 = 5.2464e-04
Loss = 2.8216e-04, PNorm = 47.8658, GNorm = 0.3198, lr_0 = 5.2410e-04
Loss = 2.8620e-04, PNorm = 47.8721, GNorm = 0.2504, lr_0 = 5.2355e-04
Loss = 3.4240e-04, PNorm = 47.8781, GNorm = 0.5476, lr_0 = 5.2300e-04
Loss = 3.2185e-04, PNorm = 47.8900, GNorm = 0.3081, lr_0 = 5.2246e-04
Loss = 2.8971e-04, PNorm = 47.9006, GNorm = 0.4873, lr_0 = 5.2191e-04
Loss = 3.8019e-04, PNorm = 47.9159, GNorm = 0.3621, lr_0 = 5.2137e-04
Loss = 3.8940e-04, PNorm = 47.9289, GNorm = 0.6892, lr_0 = 5.2082e-04
Loss = 3.3516e-04, PNorm = 47.9395, GNorm = 0.3755, lr_0 = 5.2028e-04
Loss = 3.9145e-04, PNorm = 47.9479, GNorm = 0.4761, lr_0 = 5.1974e-04
Loss = 4.3439e-04, PNorm = 47.9585, GNorm = 0.5402, lr_0 = 5.1919e-04
Loss = 3.4838e-04, PNorm = 47.9684, GNorm = 0.3458, lr_0 = 5.1865e-04
Loss = 3.8321e-04, PNorm = 47.9771, GNorm = 0.7251, lr_0 = 5.1811e-04
Loss = 3.2766e-04, PNorm = 47.9824, GNorm = 0.3506, lr_0 = 5.1757e-04
Loss = 2.5798e-04, PNorm = 47.9882, GNorm = 0.3960, lr_0 = 5.1703e-04
Loss = 3.0786e-04, PNorm = 47.9929, GNorm = 0.4255, lr_0 = 5.1649e-04
Validation rmse logD = 0.556027
Validation R2 logD = 0.774231
Validation rmse logP = 0.451450
Validation R2 logP = 0.940871
Epoch 30
Train function
Loss = 2.7904e-04, PNorm = 48.0030, GNorm = 0.2663, lr_0 = 5.1595e-04
Loss = 2.9190e-04, PNorm = 48.0142, GNorm = 0.2936, lr_0 = 5.1541e-04
Loss = 3.3513e-04, PNorm = 48.0217, GNorm = 0.4405, lr_0 = 5.1487e-04
Loss = 2.6634e-04, PNorm = 48.0297, GNorm = 0.3924, lr_0 = 5.1434e-04
Loss = 3.7388e-04, PNorm = 48.0362, GNorm = 0.5640, lr_0 = 5.1380e-04
Loss = 2.8255e-04, PNorm = 48.0458, GNorm = 0.3669, lr_0 = 5.1326e-04
Loss = 2.7241e-04, PNorm = 48.0559, GNorm = 0.4671, lr_0 = 5.1273e-04
Loss = 2.8834e-04, PNorm = 48.0658, GNorm = 0.3803, lr_0 = 5.1219e-04
Loss = 3.5259e-04, PNorm = 48.0724, GNorm = 0.4105, lr_0 = 5.1166e-04
Loss = 2.8831e-04, PNorm = 48.0851, GNorm = 0.4938, lr_0 = 5.1112e-04
Loss = 3.6077e-04, PNorm = 48.0928, GNorm = 0.2557, lr_0 = 5.1059e-04
Loss = 3.0039e-04, PNorm = 48.1035, GNorm = 0.3939, lr_0 = 5.1006e-04
Loss = 2.8437e-04, PNorm = 48.1138, GNorm = 0.3846, lr_0 = 5.0953e-04
Loss = 3.6318e-04, PNorm = 48.1215, GNorm = 0.3944, lr_0 = 5.0899e-04
Loss = 3.1286e-04, PNorm = 48.1259, GNorm = 0.3824, lr_0 = 5.0846e-04
Loss = 3.1010e-04, PNorm = 48.1333, GNorm = 0.7584, lr_0 = 5.0793e-04
Loss = 2.6751e-04, PNorm = 48.1376, GNorm = 0.2799, lr_0 = 5.0740e-04
Loss = 3.7337e-04, PNorm = 48.1470, GNorm = 0.4887, lr_0 = 5.0687e-04
Loss = 4.2331e-04, PNorm = 48.1584, GNorm = 0.5218, lr_0 = 5.0634e-04
Loss = 4.3802e-04, PNorm = 48.1710, GNorm = 1.0436, lr_0 = 5.0581e-04
Loss = 3.9184e-04, PNorm = 48.1815, GNorm = 0.4348, lr_0 = 5.0529e-04
Loss = 3.2468e-04, PNorm = 48.1911, GNorm = 0.5078, lr_0 = 5.0476e-04
Validation rmse logD = 0.560904
Validation R2 logD = 0.770253
Validation rmse logP = 0.454507
Validation R2 logP = 0.940067
Epoch 31
Train function
Loss = 4.8797e-04, PNorm = 48.1969, GNorm = 0.8243, lr_0 = 5.0418e-04
Loss = 2.7157e-04, PNorm = 48.2007, GNorm = 0.6318, lr_0 = 5.0365e-04
Loss = 3.0986e-04, PNorm = 48.2136, GNorm = 0.3055, lr_0 = 5.0313e-04
Loss = 2.9570e-04, PNorm = 48.2220, GNorm = 0.4897, lr_0 = 5.0260e-04
Loss = 2.7922e-04, PNorm = 48.2293, GNorm = 0.2900, lr_0 = 5.0208e-04
Loss = 3.4242e-04, PNorm = 48.2396, GNorm = 0.2711, lr_0 = 5.0155e-04
Loss = 3.4826e-04, PNorm = 48.2463, GNorm = 0.3881, lr_0 = 5.0103e-04
Loss = 3.3095e-04, PNorm = 48.2554, GNorm = 0.2674, lr_0 = 5.0051e-04
Loss = 2.7726e-04, PNorm = 48.2670, GNorm = 0.3333, lr_0 = 4.9998e-04
Loss = 2.6787e-04, PNorm = 48.2792, GNorm = 0.4386, lr_0 = 4.9946e-04
Loss = 3.4501e-04, PNorm = 48.2925, GNorm = 1.0106, lr_0 = 4.9894e-04
Loss = 4.2564e-04, PNorm = 48.3047, GNorm = 0.7555, lr_0 = 4.9842e-04
Loss = 3.4908e-04, PNorm = 48.3101, GNorm = 0.5263, lr_0 = 4.9790e-04
Loss = 3.0491e-04, PNorm = 48.3232, GNorm = 0.4070, lr_0 = 4.9738e-04
Loss = 2.7455e-04, PNorm = 48.3339, GNorm = 0.6384, lr_0 = 4.9686e-04
Loss = 3.2998e-04, PNorm = 48.3444, GNorm = 0.3322, lr_0 = 4.9634e-04
Loss = 2.5528e-04, PNorm = 48.3519, GNorm = 0.2742, lr_0 = 4.9583e-04
Loss = 3.1149e-04, PNorm = 48.3593, GNorm = 0.5518, lr_0 = 4.9531e-04
Loss = 2.8000e-04, PNorm = 48.3710, GNorm = 0.4179, lr_0 = 4.9479e-04
Loss = 2.7325e-04, PNorm = 48.3838, GNorm = 0.3310, lr_0 = 4.9427e-04
Loss = 2.8580e-04, PNorm = 48.3892, GNorm = 0.3320, lr_0 = 4.9376e-04
Loss = 3.9207e-04, PNorm = 48.3973, GNorm = 1.1207, lr_0 = 4.9324e-04
Loss = 3.3118e-04, PNorm = 48.4020, GNorm = 0.3888, lr_0 = 4.9273e-04
Validation rmse logD = 0.559476
Validation R2 logD = 0.771421
Validation rmse logP = 0.477436
Validation R2 logP = 0.933867
Epoch 32
Train function
Loss = 3.6898e-04, PNorm = 48.4153, GNorm = 0.8372, lr_0 = 4.9221e-04
Loss = 3.1486e-04, PNorm = 48.4278, GNorm = 0.8390, lr_0 = 4.9170e-04
Loss = 3.0138e-04, PNorm = 48.4368, GNorm = 0.5221, lr_0 = 4.9119e-04
Loss = 3.5353e-04, PNorm = 48.4476, GNorm = 0.3085, lr_0 = 4.9067e-04
Loss = 3.4934e-04, PNorm = 48.4566, GNorm = 0.5319, lr_0 = 4.9016e-04
Loss = 3.0323e-04, PNorm = 48.4695, GNorm = 0.3465, lr_0 = 4.8965e-04
Loss = 2.8765e-04, PNorm = 48.4791, GNorm = 0.5424, lr_0 = 4.8914e-04
Loss = 3.2114e-04, PNorm = 48.4864, GNorm = 0.3832, lr_0 = 4.8863e-04
Loss = 2.7919e-04, PNorm = 48.4920, GNorm = 0.3407, lr_0 = 4.8812e-04
Loss = 3.1262e-04, PNorm = 48.5033, GNorm = 0.4274, lr_0 = 4.8761e-04
Loss = 2.9515e-04, PNorm = 48.5100, GNorm = 0.3188, lr_0 = 4.8710e-04
Loss = 3.2927e-04, PNorm = 48.5221, GNorm = 0.4411, lr_0 = 4.8659e-04
Loss = 2.3178e-04, PNorm = 48.5309, GNorm = 0.4173, lr_0 = 4.8608e-04
Loss = 2.9200e-04, PNorm = 48.5416, GNorm = 0.3473, lr_0 = 4.8558e-04
Loss = 3.0208e-04, PNorm = 48.5547, GNorm = 0.5157, lr_0 = 4.8507e-04
Loss = 3.3862e-04, PNorm = 48.5634, GNorm = 0.6698, lr_0 = 4.8456e-04
Loss = 3.6038e-04, PNorm = 48.5729, GNorm = 0.3571, lr_0 = 4.8406e-04
Loss = 2.8214e-04, PNorm = 48.5790, GNorm = 0.5276, lr_0 = 4.8355e-04
Loss = 3.5812e-04, PNorm = 48.5894, GNorm = 0.4282, lr_0 = 4.8305e-04
Loss = 3.4237e-04, PNorm = 48.5981, GNorm = 0.6174, lr_0 = 4.8254e-04
Loss = 3.1816e-04, PNorm = 48.6049, GNorm = 0.5754, lr_0 = 4.8204e-04
Loss = 3.3580e-04, PNorm = 48.6122, GNorm = 0.3348, lr_0 = 4.8154e-04
Loss = 3.2972e-04, PNorm = 48.6241, GNorm = 0.2860, lr_0 = 4.8104e-04
Loss = 8.1541e-04, PNorm = 48.6250, GNorm = 0.5659, lr_0 = 4.8098e-04
Validation rmse logD = 0.558508
Validation R2 logD = 0.772211
Validation rmse logP = 0.460327
Validation R2 logP = 0.938522
Epoch 33
Train function
Loss = 2.5541e-04, PNorm = 48.6346, GNorm = 0.2475, lr_0 = 4.8048e-04
Loss = 2.3562e-04, PNorm = 48.6429, GNorm = 0.3300, lr_0 = 4.7998e-04
Loss = 2.8802e-04, PNorm = 48.6522, GNorm = 0.6291, lr_0 = 4.7948e-04
Loss = 2.5569e-04, PNorm = 48.6569, GNorm = 0.2550, lr_0 = 4.7898e-04
Loss = 2.9914e-04, PNorm = 48.6626, GNorm = 0.2905, lr_0 = 4.7848e-04
Loss = 2.2474e-04, PNorm = 48.6699, GNorm = 0.5782, lr_0 = 4.7798e-04
Loss = 2.5618e-04, PNorm = 48.6790, GNorm = 0.5150, lr_0 = 4.7748e-04
Loss = 2.5143e-04, PNorm = 48.6898, GNorm = 0.3101, lr_0 = 4.7698e-04
Loss = 3.4219e-04, PNorm = 48.6968, GNorm = 0.4078, lr_0 = 4.7649e-04
Loss = 2.3615e-04, PNorm = 48.7037, GNorm = 0.3001, lr_0 = 4.7599e-04
Loss = 2.7492e-04, PNorm = 48.7087, GNorm = 0.5537, lr_0 = 4.7549e-04
Loss = 2.8263e-04, PNorm = 48.7201, GNorm = 0.6851, lr_0 = 4.7500e-04
Loss = 2.8769e-04, PNorm = 48.7290, GNorm = 0.4436, lr_0 = 4.7450e-04
Loss = 2.4456e-04, PNorm = 48.7343, GNorm = 0.4962, lr_0 = 4.7400e-04
Loss = 4.4207e-04, PNorm = 48.7475, GNorm = 0.4657, lr_0 = 4.7351e-04
Loss = 3.6291e-04, PNorm = 48.7620, GNorm = 0.6212, lr_0 = 4.7302e-04
Loss = 3.4965e-04, PNorm = 48.7709, GNorm = 0.4504, lr_0 = 4.7252e-04
Loss = 3.2779e-04, PNorm = 48.7783, GNorm = 0.6415, lr_0 = 4.7203e-04
Loss = 2.9687e-04, PNorm = 48.7899, GNorm = 0.3127, lr_0 = 4.7154e-04
Loss = 2.8522e-04, PNorm = 48.8006, GNorm = 0.4536, lr_0 = 4.7104e-04
Loss = 2.9415e-04, PNorm = 48.8092, GNorm = 0.3013, lr_0 = 4.7055e-04
Loss = 3.1288e-04, PNorm = 48.8182, GNorm = 0.6124, lr_0 = 4.7006e-04
Validation rmse logD = 0.563421
Validation R2 logD = 0.768186
Validation rmse logP = 0.452613
Validation R2 logP = 0.940566
Epoch 34
Train function
Loss = 2.4292e-04, PNorm = 48.8277, GNorm = 0.4407, lr_0 = 4.6957e-04
Loss = 2.2351e-04, PNorm = 48.8361, GNorm = 0.4774, lr_0 = 4.6908e-04
Loss = 2.5240e-04, PNorm = 48.8465, GNorm = 0.4152, lr_0 = 4.6859e-04
Loss = 2.5220e-04, PNorm = 48.8513, GNorm = 0.2424, lr_0 = 4.6810e-04
Loss = 2.2848e-04, PNorm = 48.8595, GNorm = 0.2054, lr_0 = 4.6761e-04
Loss = 2.2937e-04, PNorm = 48.8649, GNorm = 0.3088, lr_0 = 4.6712e-04
Loss = 2.3824e-04, PNorm = 48.8752, GNorm = 0.2775, lr_0 = 4.6664e-04
Loss = 2.4357e-04, PNorm = 48.8819, GNorm = 0.6663, lr_0 = 4.6615e-04
Loss = 2.3380e-04, PNorm = 48.8866, GNorm = 0.5537, lr_0 = 4.6566e-04
Loss = 2.9141e-04, PNorm = 48.8964, GNorm = 0.5704, lr_0 = 4.6518e-04
Loss = 2.8166e-04, PNorm = 48.9023, GNorm = 0.3962, lr_0 = 4.6469e-04
Loss = 2.7758e-04, PNorm = 48.9093, GNorm = 0.3644, lr_0 = 4.6421e-04
Loss = 2.6536e-04, PNorm = 48.9162, GNorm = 0.3653, lr_0 = 4.6372e-04
Loss = 2.6620e-04, PNorm = 48.9198, GNorm = 0.4125, lr_0 = 4.6324e-04
Loss = 2.9612e-04, PNorm = 48.9241, GNorm = 0.7510, lr_0 = 4.6276e-04
Loss = 2.1990e-04, PNorm = 48.9301, GNorm = 0.3316, lr_0 = 4.6227e-04
Loss = 2.8649e-04, PNorm = 48.9345, GNorm = 0.3504, lr_0 = 4.6179e-04
Loss = 3.1700e-04, PNorm = 48.9435, GNorm = 0.2601, lr_0 = 4.6131e-04
Loss = 2.6112e-04, PNorm = 48.9527, GNorm = 0.2546, lr_0 = 4.6083e-04
Loss = 2.1256e-04, PNorm = 48.9624, GNorm = 0.4486, lr_0 = 4.6035e-04
Loss = 3.7073e-04, PNorm = 48.9720, GNorm = 0.7750, lr_0 = 4.5987e-04
Loss = 2.5382e-04, PNorm = 48.9813, GNorm = 0.5128, lr_0 = 4.5939e-04
Loss = 2.6255e-04, PNorm = 48.9879, GNorm = 0.4116, lr_0 = 4.5891e-04
Validation rmse logD = 0.555993
Validation R2 logD = 0.774258
Validation rmse logP = 0.455710
Validation R2 logP = 0.939749
Epoch 35
Train function
Loss = 2.4745e-04, PNorm = 48.9972, GNorm = 0.3133, lr_0 = 4.5838e-04
Loss = 3.6610e-04, PNorm = 49.0078, GNorm = 0.4545, lr_0 = 4.5790e-04
Loss = 3.3087e-04, PNorm = 49.0192, GNorm = 0.8710, lr_0 = 4.5742e-04
Loss = 3.0811e-04, PNorm = 49.0260, GNorm = 0.4846, lr_0 = 4.5695e-04
Loss = 2.8873e-04, PNorm = 49.0369, GNorm = 0.3764, lr_0 = 4.5647e-04
Loss = 3.0879e-04, PNorm = 49.0435, GNorm = 0.3073, lr_0 = 4.5599e-04
Loss = 3.0293e-04, PNorm = 49.0544, GNorm = 0.5512, lr_0 = 4.5552e-04
Loss = 2.8862e-04, PNorm = 49.0624, GNorm = 0.6567, lr_0 = 4.5504e-04
Loss = 1.9521e-04, PNorm = 49.0719, GNorm = 0.3319, lr_0 = 4.5457e-04
Loss = 2.2784e-04, PNorm = 49.0832, GNorm = 0.4007, lr_0 = 4.5409e-04
Loss = 2.6245e-04, PNorm = 49.0925, GNorm = 0.4593, lr_0 = 4.5362e-04
Loss = 2.8814e-04, PNorm = 49.0973, GNorm = 0.5721, lr_0 = 4.5314e-04
Loss = 2.6551e-04, PNorm = 49.1076, GNorm = 0.4682, lr_0 = 4.5267e-04
Loss = 2.7483e-04, PNorm = 49.1154, GNorm = 0.4195, lr_0 = 4.5220e-04
Loss = 2.4611e-04, PNorm = 49.1238, GNorm = 0.2860, lr_0 = 4.5173e-04
Loss = 2.5865e-04, PNorm = 49.1328, GNorm = 0.3340, lr_0 = 4.5125e-04
Loss = 2.6441e-04, PNorm = 49.1369, GNorm = 0.4070, lr_0 = 4.5078e-04
Loss = 2.8535e-04, PNorm = 49.1423, GNorm = 0.4901, lr_0 = 4.5031e-04
Loss = 2.2441e-04, PNorm = 49.1546, GNorm = 0.4385, lr_0 = 4.4984e-04
Loss = 2.9438e-04, PNorm = 49.1617, GNorm = 0.5262, lr_0 = 4.4937e-04
Loss = 3.3355e-04, PNorm = 49.1705, GNorm = 0.6595, lr_0 = 4.4890e-04
Loss = 2.7898e-04, PNorm = 49.1837, GNorm = 0.4723, lr_0 = 4.4844e-04
Validation rmse logD = 0.560479
Validation R2 logD = 0.770600
Validation rmse logP = 0.472323
Validation R2 logP = 0.935276
Epoch 36
Train function
Loss = 4.1315e-04, PNorm = 49.1933, GNorm = 0.3326, lr_0 = 4.4797e-04
Loss = 2.3394e-04, PNorm = 49.2059, GNorm = 0.3305, lr_0 = 4.4750e-04
Loss = 3.0227e-04, PNorm = 49.2145, GNorm = 0.4454, lr_0 = 4.4703e-04
Loss = 2.0077e-04, PNorm = 49.2179, GNorm = 0.3470, lr_0 = 4.4657e-04
Loss = 2.3179e-04, PNorm = 49.2216, GNorm = 0.3991, lr_0 = 4.4610e-04
Loss = 2.7683e-04, PNorm = 49.2298, GNorm = 0.2946, lr_0 = 4.4564e-04
Loss = 2.2535e-04, PNorm = 49.2394, GNorm = 0.4736, lr_0 = 4.4517e-04
Loss = 2.4472e-04, PNorm = 49.2471, GNorm = 0.2797, lr_0 = 4.4471e-04
Loss = 2.2265e-04, PNorm = 49.2537, GNorm = 0.1948, lr_0 = 4.4424e-04
Loss = 2.7897e-04, PNorm = 49.2626, GNorm = 0.2779, lr_0 = 4.4378e-04
Loss = 2.6543e-04, PNorm = 49.2704, GNorm = 0.6762, lr_0 = 4.4331e-04
Loss = 2.9233e-04, PNorm = 49.2786, GNorm = 0.2746, lr_0 = 4.4285e-04
Loss = 2.0751e-04, PNorm = 49.2850, GNorm = 0.3017, lr_0 = 4.4239e-04
Loss = 1.8818e-04, PNorm = 49.2925, GNorm = 0.2691, lr_0 = 4.4193e-04
Loss = 1.7517e-04, PNorm = 49.2973, GNorm = 0.3639, lr_0 = 4.4147e-04
Loss = 2.3618e-04, PNorm = 49.3082, GNorm = 0.3264, lr_0 = 4.4101e-04
Loss = 2.4631e-04, PNorm = 49.3174, GNorm = 0.6257, lr_0 = 4.4055e-04
Loss = 2.6113e-04, PNorm = 49.3255, GNorm = 0.2244, lr_0 = 4.4009e-04
Loss = 2.4195e-04, PNorm = 49.3308, GNorm = 0.3246, lr_0 = 4.3963e-04
Loss = 2.1673e-04, PNorm = 49.3353, GNorm = 0.2883, lr_0 = 4.3917e-04
Loss = 2.8830e-04, PNorm = 49.3427, GNorm = 0.3535, lr_0 = 4.3871e-04
Loss = 1.9689e-04, PNorm = 49.3494, GNorm = 0.5926, lr_0 = 4.3825e-04
Loss = 2.5334e-04, PNorm = 49.3583, GNorm = 0.3035, lr_0 = 4.3779e-04
Validation rmse logD = 0.549612
Validation R2 logD = 0.779410
Validation rmse logP = 0.454802
Validation R2 logP = 0.939989
Epoch 37
Train function
Loss = 1.8957e-04, PNorm = 49.3682, GNorm = 0.2249, lr_0 = 4.3729e-04
Loss = 2.8401e-04, PNorm = 49.3744, GNorm = 0.3171, lr_0 = 4.3684e-04
Loss = 1.9632e-04, PNorm = 49.3833, GNorm = 0.2217, lr_0 = 4.3638e-04
Loss = 1.9632e-04, PNorm = 49.3865, GNorm = 0.2701, lr_0 = 4.3592e-04
Loss = 1.6075e-04, PNorm = 49.3923, GNorm = 0.2520, lr_0 = 4.3547e-04
Loss = 1.9905e-04, PNorm = 49.3967, GNorm = 0.2593, lr_0 = 4.3501e-04
Loss = 2.0697e-04, PNorm = 49.4027, GNorm = 0.3891, lr_0 = 4.3456e-04
Loss = 2.3274e-04, PNorm = 49.4115, GNorm = 0.5520, lr_0 = 4.3411e-04
Loss = 2.1276e-04, PNorm = 49.4177, GNorm = 0.3026, lr_0 = 4.3365e-04
Loss = 2.0014e-04, PNorm = 49.4265, GNorm = 0.3362, lr_0 = 4.3320e-04
Loss = 2.1730e-04, PNorm = 49.4315, GNorm = 0.1734, lr_0 = 4.3275e-04
Loss = 2.0256e-04, PNorm = 49.4379, GNorm = 0.2696, lr_0 = 4.3230e-04
Loss = 1.8975e-04, PNorm = 49.4438, GNorm = 0.3651, lr_0 = 4.3185e-04
Loss = 1.8841e-04, PNorm = 49.4511, GNorm = 0.3351, lr_0 = 4.3140e-04
Loss = 2.1069e-04, PNorm = 49.4556, GNorm = 0.3046, lr_0 = 4.3094e-04
Loss = 1.7020e-04, PNorm = 49.4600, GNorm = 0.2877, lr_0 = 4.3050e-04
Loss = 2.1603e-04, PNorm = 49.4643, GNorm = 0.2060, lr_0 = 4.3005e-04
Loss = 2.1758e-04, PNorm = 49.4690, GNorm = 0.3679, lr_0 = 4.2960e-04
Loss = 2.2590e-04, PNorm = 49.4731, GNorm = 0.3184, lr_0 = 4.2915e-04
Loss = 1.9885e-04, PNorm = 49.4813, GNorm = 0.2648, lr_0 = 4.2870e-04
Loss = 2.1964e-04, PNorm = 49.4904, GNorm = 0.5472, lr_0 = 4.2825e-04
Loss = 2.3991e-04, PNorm = 49.4988, GNorm = 0.4606, lr_0 = 4.2781e-04
Validation rmse logD = 0.565775
Validation R2 logD = 0.766245
Validation rmse logP = 0.469513
Validation R2 logP = 0.936044
Epoch 38
Train function
Loss = 3.0001e-04, PNorm = 49.5069, GNorm = 0.5234, lr_0 = 4.2736e-04
Loss = 2.4444e-04, PNorm = 49.5146, GNorm = 0.7369, lr_0 = 4.2691e-04
Loss = 2.5405e-04, PNorm = 49.5256, GNorm = 0.3785, lr_0 = 4.2647e-04
Loss = 2.1166e-04, PNorm = 49.5323, GNorm = 0.2859, lr_0 = 4.2602e-04
Loss = 2.0161e-04, PNorm = 49.5388, GNorm = 0.1914, lr_0 = 4.2558e-04
Loss = 1.8915e-04, PNorm = 49.5430, GNorm = 0.2433, lr_0 = 4.2513e-04
Loss = 1.9020e-04, PNorm = 49.5503, GNorm = 0.3009, lr_0 = 4.2469e-04
Loss = 2.4860e-04, PNorm = 49.5558, GNorm = 0.2371, lr_0 = 4.2425e-04
Loss = 1.9847e-04, PNorm = 49.5620, GNorm = 0.3819, lr_0 = 4.2380e-04
Loss = 2.2142e-04, PNorm = 49.5689, GNorm = 0.1698, lr_0 = 4.2336e-04
Loss = 1.9464e-04, PNorm = 49.5736, GNorm = 0.3157, lr_0 = 4.2292e-04
Loss = 1.5360e-04, PNorm = 49.5810, GNorm = 0.1950, lr_0 = 4.2248e-04
Loss = 1.9694e-04, PNorm = 49.5884, GNorm = 0.2442, lr_0 = 4.2204e-04
Loss = 2.8804e-04, PNorm = 49.5942, GNorm = 0.2132, lr_0 = 4.2160e-04
Loss = 2.1195e-04, PNorm = 49.6003, GNorm = 0.6633, lr_0 = 4.2116e-04
Loss = 2.2208e-04, PNorm = 49.6097, GNorm = 0.5164, lr_0 = 4.2072e-04
Loss = 3.2103e-04, PNorm = 49.6214, GNorm = 0.8183, lr_0 = 4.2028e-04
Loss = 2.7983e-04, PNorm = 49.6263, GNorm = 0.4296, lr_0 = 4.1984e-04
Loss = 2.6903e-04, PNorm = 49.6327, GNorm = 0.2256, lr_0 = 4.1940e-04
Loss = 2.7978e-04, PNorm = 49.6419, GNorm = 0.3789, lr_0 = 4.1896e-04
Loss = 2.5677e-04, PNorm = 49.6503, GNorm = 0.2804, lr_0 = 4.1853e-04
Loss = 2.9945e-04, PNorm = 49.6625, GNorm = 0.6763, lr_0 = 4.1809e-04
Loss = 2.6523e-04, PNorm = 49.6712, GNorm = 0.3405, lr_0 = 4.1765e-04
Validation rmse logD = 0.553676
Validation R2 logD = 0.776135
Validation rmse logP = 0.453784
Validation R2 logP = 0.940258
Epoch 39
Train function
Loss = 1.9667e-04, PNorm = 49.6840, GNorm = 0.2273, lr_0 = 4.1717e-04
Loss = 2.4052e-04, PNorm = 49.6892, GNorm = 0.4935, lr_0 = 4.1674e-04
Loss = 2.1011e-04, PNorm = 49.6982, GNorm = 0.3903, lr_0 = 4.1630e-04
Loss = 2.1609e-04, PNorm = 49.7081, GNorm = 0.5757, lr_0 = 4.1587e-04
Loss = 2.1084e-04, PNorm = 49.7142, GNorm = 0.2150, lr_0 = 4.1544e-04
Loss = 2.0120e-04, PNorm = 49.7207, GNorm = 0.4937, lr_0 = 4.1500e-04
Loss = 2.7784e-04, PNorm = 49.7320, GNorm = 0.4955, lr_0 = 4.1457e-04
Loss = 2.7949e-04, PNorm = 49.7403, GNorm = 0.3687, lr_0 = 4.1414e-04
Loss = 1.8529e-04, PNorm = 49.7458, GNorm = 0.2754, lr_0 = 4.1370e-04
Loss = 2.1208e-04, PNorm = 49.7532, GNorm = 0.1939, lr_0 = 4.1327e-04
Loss = 1.7494e-04, PNorm = 49.7601, GNorm = 0.2765, lr_0 = 4.1284e-04
Loss = 1.6625e-04, PNorm = 49.7649, GNorm = 0.1506, lr_0 = 4.1241e-04
Loss = 1.9375e-04, PNorm = 49.7688, GNorm = 0.4399, lr_0 = 4.1198e-04
Loss = 1.7213e-04, PNorm = 49.7731, GNorm = 0.1659, lr_0 = 4.1155e-04
Loss = 2.1132e-04, PNorm = 49.7787, GNorm = 0.2879, lr_0 = 4.1112e-04
Loss = 1.8500e-04, PNorm = 49.7852, GNorm = 0.2768, lr_0 = 4.1069e-04
Loss = 1.7402e-04, PNorm = 49.7898, GNorm = 0.2977, lr_0 = 4.1026e-04
Loss = 1.8190e-04, PNorm = 49.7944, GNorm = 0.5122, lr_0 = 4.0983e-04
Loss = 2.0631e-04, PNorm = 49.8008, GNorm = 0.4141, lr_0 = 4.0941e-04
Loss = 1.7232e-04, PNorm = 49.8050, GNorm = 0.3581, lr_0 = 4.0898e-04
Loss = 2.0720e-04, PNorm = 49.8106, GNorm = 0.4531, lr_0 = 4.0855e-04
Loss = 3.1175e-04, PNorm = 49.8136, GNorm = 0.6233, lr_0 = 4.0813e-04
Validation rmse logD = 0.555917
Validation R2 logD = 0.774320
Validation rmse logP = 0.456010
Validation R2 logP = 0.939670
Epoch 40
Train function
Loss = 1.8010e-04, PNorm = 49.8231, GNorm = 0.2720, lr_0 = 4.0770e-04
Loss = 1.6169e-04, PNorm = 49.8294, GNorm = 0.3294, lr_0 = 4.0727e-04
Loss = 2.0703e-04, PNorm = 49.8386, GNorm = 0.4036, lr_0 = 4.0685e-04
Loss = 1.6312e-04, PNorm = 49.8470, GNorm = 0.2066, lr_0 = 4.0642e-04
Loss = 1.7364e-04, PNorm = 49.8506, GNorm = 0.3279, lr_0 = 4.0600e-04
Loss = 2.3812e-04, PNorm = 49.8590, GNorm = 0.3409, lr_0 = 4.0558e-04
Loss = 2.0689e-04, PNorm = 49.8653, GNorm = 0.2617, lr_0 = 4.0515e-04
Loss = 1.8394e-04, PNorm = 49.8691, GNorm = 0.2170, lr_0 = 4.0473e-04
Loss = 1.8369e-04, PNorm = 49.8749, GNorm = 0.3885, lr_0 = 4.0431e-04
Loss = 2.2192e-04, PNorm = 49.8828, GNorm = 0.4505, lr_0 = 4.0389e-04
Loss = 2.4433e-04, PNorm = 49.8921, GNorm = 0.3664, lr_0 = 4.0346e-04
Loss = 1.9698e-04, PNorm = 49.9017, GNorm = 0.3685, lr_0 = 4.0304e-04
Loss = 2.4464e-04, PNorm = 49.9119, GNorm = 0.4738, lr_0 = 4.0262e-04
Loss = 1.7881e-04, PNorm = 49.9172, GNorm = 0.5203, lr_0 = 4.0220e-04
Loss = 2.0858e-04, PNorm = 49.9228, GNorm = 0.4316, lr_0 = 4.0178e-04
Loss = 1.8391e-04, PNorm = 49.9306, GNorm = 0.3278, lr_0 = 4.0136e-04
Loss = 1.9181e-04, PNorm = 49.9374, GNorm = 0.5568, lr_0 = 4.0094e-04
Loss = 2.2838e-04, PNorm = 49.9426, GNorm = 0.4030, lr_0 = 4.0053e-04
Loss = 1.7690e-04, PNorm = 49.9498, GNorm = 0.3078, lr_0 = 4.0011e-04
Loss = 1.5940e-04, PNorm = 49.9547, GNorm = 0.2348, lr_0 = 3.9969e-04
Loss = 1.6102e-04, PNorm = 49.9596, GNorm = 0.2090, lr_0 = 3.9927e-04
Loss = 1.5745e-04, PNorm = 49.9648, GNorm = 0.3086, lr_0 = 3.9886e-04
Loss = 1.9066e-04, PNorm = 49.9693, GNorm = 0.2407, lr_0 = 3.9844e-04
Validation rmse logD = 0.560128
Validation R2 logD = 0.770888
Validation rmse logP = 0.453210
Validation R2 logP = 0.940409
Epoch 41
Train function
Loss = 1.5358e-04, PNorm = 49.9725, GNorm = 0.2694, lr_0 = 3.9798e-04
Loss = 1.9426e-04, PNorm = 49.9742, GNorm = 0.2378, lr_0 = 3.9757e-04
Loss = 1.5127e-04, PNorm = 49.9790, GNorm = 0.1953, lr_0 = 3.9715e-04
Loss = 1.8203e-04, PNorm = 49.9850, GNorm = 0.3005, lr_0 = 3.9674e-04
Loss = 1.3644e-04, PNorm = 49.9943, GNorm = 0.2049, lr_0 = 3.9632e-04
Loss = 1.4739e-04, PNorm = 49.9990, GNorm = 0.3581, lr_0 = 3.9591e-04
Loss = 1.2657e-04, PNorm = 50.0059, GNorm = 0.3095, lr_0 = 3.9550e-04
Loss = 1.2177e-04, PNorm = 50.0114, GNorm = 0.1995, lr_0 = 3.9508e-04
Loss = 1.6408e-04, PNorm = 50.0139, GNorm = 0.2910, lr_0 = 3.9467e-04
Loss = 1.7266e-04, PNorm = 50.0201, GNorm = 0.1797, lr_0 = 3.9426e-04
Loss = 1.4305e-04, PNorm = 50.0254, GNorm = 0.1763, lr_0 = 3.9385e-04
Loss = 1.4825e-04, PNorm = 50.0316, GNorm = 0.2772, lr_0 = 3.9344e-04
Loss = 1.4717e-04, PNorm = 50.0377, GNorm = 0.1457, lr_0 = 3.9303e-04
Loss = 1.5916e-04, PNorm = 50.0428, GNorm = 0.5308, lr_0 = 3.9262e-04
Loss = 1.5530e-04, PNorm = 50.0495, GNorm = 0.2969, lr_0 = 3.9221e-04
Loss = 1.5347e-04, PNorm = 50.0542, GNorm = 0.3732, lr_0 = 3.9180e-04
Loss = 2.5198e-04, PNorm = 50.0597, GNorm = 0.4885, lr_0 = 3.9139e-04
Loss = 2.0748e-04, PNorm = 50.0668, GNorm = 0.4739, lr_0 = 3.9098e-04
Loss = 1.5437e-04, PNorm = 50.0724, GNorm = 0.2179, lr_0 = 3.9057e-04
Loss = 1.5656e-04, PNorm = 50.0779, GNorm = 0.2736, lr_0 = 3.9016e-04
Loss = 2.6718e-04, PNorm = 50.0864, GNorm = 0.4179, lr_0 = 3.8976e-04
Loss = 1.9566e-04, PNorm = 50.0977, GNorm = 0.2650, lr_0 = 3.8935e-04
Loss = 1.7180e-04, PNorm = 50.1047, GNorm = 0.2828, lr_0 = 3.8894e-04
Validation rmse logD = 0.564632
Validation R2 logD = 0.767188
Validation rmse logP = 0.455255
Validation R2 logP = 0.939869
Epoch 42
Train function
Loss = 1.6189e-04, PNorm = 50.1100, GNorm = 0.1585, lr_0 = 3.8854e-04
Loss = 1.6367e-04, PNorm = 50.1149, GNorm = 0.3076, lr_0 = 3.8813e-04
Loss = 1.3615e-04, PNorm = 50.1202, GNorm = 0.2295, lr_0 = 3.8773e-04
Loss = 1.3560e-04, PNorm = 50.1254, GNorm = 0.3368, lr_0 = 3.8732e-04
Loss = 1.6376e-04, PNorm = 50.1300, GNorm = 0.1923, lr_0 = 3.8692e-04
Loss = 1.3655e-04, PNorm = 50.1342, GNorm = 0.2121, lr_0 = 3.8651e-04
Loss = 1.6932e-04, PNorm = 50.1415, GNorm = 0.3287, lr_0 = 3.8611e-04
Loss = 1.3912e-04, PNorm = 50.1459, GNorm = 0.2286, lr_0 = 3.8571e-04
Loss = 1.8656e-04, PNorm = 50.1507, GNorm = 0.1879, lr_0 = 3.8531e-04
Loss = 1.5231e-04, PNorm = 50.1569, GNorm = 0.3841, lr_0 = 3.8490e-04
Loss = 1.1979e-04, PNorm = 50.1608, GNorm = 0.2409, lr_0 = 3.8450e-04
Loss = 1.4648e-04, PNorm = 50.1666, GNorm = 0.3266, lr_0 = 3.8410e-04
Loss = 1.5387e-04, PNorm = 50.1730, GNorm = 0.3379, lr_0 = 3.8370e-04
Loss = 1.4370e-04, PNorm = 50.1773, GNorm = 0.3184, lr_0 = 3.8330e-04
Loss = 1.3116e-04, PNorm = 50.1819, GNorm = 0.2277, lr_0 = 3.8290e-04
Loss = 1.9242e-04, PNorm = 50.1864, GNorm = 0.2527, lr_0 = 3.8250e-04
Loss = 1.8447e-04, PNorm = 50.1910, GNorm = 0.6750, lr_0 = 3.8210e-04
Loss = 1.9639e-04, PNorm = 50.1972, GNorm = 0.6389, lr_0 = 3.8170e-04
Loss = 2.1703e-04, PNorm = 50.2015, GNorm = 0.6149, lr_0 = 3.8130e-04
Loss = 1.8102e-04, PNorm = 50.2061, GNorm = 0.1992, lr_0 = 3.8090e-04
Loss = 1.6793e-04, PNorm = 50.2128, GNorm = 0.3556, lr_0 = 3.8051e-04
Loss = 2.0935e-04, PNorm = 50.2218, GNorm = 0.2692, lr_0 = 3.8011e-04
Validation rmse logD = 0.558127
Validation R2 logD = 0.772522
Validation rmse logP = 0.449693
Validation R2 logP = 0.941330
Epoch 43
Train function
Loss = 1.6947e-04, PNorm = 50.2301, GNorm = 0.1998, lr_0 = 3.7967e-04
Loss = 1.5102e-04, PNorm = 50.2387, GNorm = 0.1717, lr_0 = 3.7928e-04
Loss = 1.8852e-04, PNorm = 50.2477, GNorm = 0.2686, lr_0 = 3.7888e-04
Loss = 1.5536e-04, PNorm = 50.2543, GNorm = 0.6656, lr_0 = 3.7849e-04
Loss = 1.8610e-04, PNorm = 50.2606, GNorm = 0.2168, lr_0 = 3.7809e-04
Loss = 1.2034e-04, PNorm = 50.2655, GNorm = 0.2193, lr_0 = 3.7770e-04
Loss = 1.3957e-04, PNorm = 50.2699, GNorm = 0.1569, lr_0 = 3.7730e-04
Loss = 1.5336e-04, PNorm = 50.2727, GNorm = 0.4333, lr_0 = 3.7691e-04
Loss = 1.4190e-04, PNorm = 50.2764, GNorm = 0.2821, lr_0 = 3.7652e-04
Loss = 1.4255e-04, PNorm = 50.2829, GNorm = 0.3056, lr_0 = 3.7612e-04
Loss = 1.3761e-04, PNorm = 50.2886, GNorm = 0.2165, lr_0 = 3.7573e-04
Loss = 1.4639e-04, PNorm = 50.2919, GNorm = 0.2295, lr_0 = 3.7534e-04
Loss = 1.4871e-04, PNorm = 50.2994, GNorm = 0.3940, lr_0 = 3.7495e-04
Loss = 1.3895e-04, PNorm = 50.3028, GNorm = 0.3599, lr_0 = 3.7455e-04
Loss = 1.5301e-04, PNorm = 50.3074, GNorm = 0.1394, lr_0 = 3.7416e-04
Loss = 1.8096e-04, PNorm = 50.3127, GNorm = 0.4152, lr_0 = 3.7377e-04
Loss = 1.7866e-04, PNorm = 50.3205, GNorm = 0.3014, lr_0 = 3.7338e-04
Loss = 1.5688e-04, PNorm = 50.3245, GNorm = 0.1481, lr_0 = 3.7299e-04
Loss = 1.6725e-04, PNorm = 50.3294, GNorm = 0.2892, lr_0 = 3.7260e-04
Loss = 1.8838e-04, PNorm = 50.3315, GNorm = 0.2529, lr_0 = 3.7221e-04
Loss = 1.6288e-04, PNorm = 50.3372, GNorm = 0.2417, lr_0 = 3.7183e-04
Loss = 1.8992e-04, PNorm = 50.3394, GNorm = 0.3188, lr_0 = 3.7144e-04
Loss = 1.7957e-04, PNorm = 50.3445, GNorm = 0.3624, lr_0 = 3.7105e-04
Validation rmse logD = 0.560171
Validation R2 logD = 0.770853
Validation rmse logP = 0.455551
Validation R2 logP = 0.939791
Epoch 44
Train function
Loss = 1.7801e-04, PNorm = 50.3520, GNorm = 0.3449, lr_0 = 3.7066e-04
Loss = 1.8371e-04, PNorm = 50.3614, GNorm = 0.4944, lr_0 = 3.7028e-04
Loss = 1.9875e-04, PNorm = 50.3692, GNorm = 0.3612, lr_0 = 3.6989e-04
Loss = 1.9540e-04, PNorm = 50.3747, GNorm = 0.2471, lr_0 = 3.6950e-04
Loss = 1.6845e-04, PNorm = 50.3834, GNorm = 0.2678, lr_0 = 3.6912e-04
Loss = 1.3619e-04, PNorm = 50.3860, GNorm = 0.2303, lr_0 = 3.6873e-04
Loss = 1.3646e-04, PNorm = 50.3900, GNorm = 0.2810, lr_0 = 3.6835e-04
Loss = 1.4312e-04, PNorm = 50.3962, GNorm = 0.2502, lr_0 = 3.6796e-04
Loss = 1.5457e-04, PNorm = 50.4023, GNorm = 0.3246, lr_0 = 3.6758e-04
Loss = 1.4190e-04, PNorm = 50.4085, GNorm = 0.3397, lr_0 = 3.6720e-04
Loss = 1.3864e-04, PNorm = 50.4150, GNorm = 0.2526, lr_0 = 3.6681e-04
Loss = 1.4471e-04, PNorm = 50.4216, GNorm = 0.2303, lr_0 = 3.6643e-04
Loss = 1.3924e-04, PNorm = 50.4264, GNorm = 0.2235, lr_0 = 3.6605e-04
Loss = 1.7958e-04, PNorm = 50.4311, GNorm = 0.2403, lr_0 = 3.6567e-04
Loss = 1.6653e-04, PNorm = 50.4375, GNorm = 0.2934, lr_0 = 3.6528e-04
Loss = 1.4059e-04, PNorm = 50.4470, GNorm = 0.1854, lr_0 = 3.6490e-04
Loss = 1.3252e-04, PNorm = 50.4542, GNorm = 0.2533, lr_0 = 3.6452e-04
Loss = 1.4075e-04, PNorm = 50.4581, GNorm = 0.4057, lr_0 = 3.6414e-04
Loss = 1.7964e-04, PNorm = 50.4641, GNorm = 0.4301, lr_0 = 3.6376e-04
Loss = 1.8769e-04, PNorm = 50.4693, GNorm = 0.3396, lr_0 = 3.6338e-04
Loss = 1.8043e-04, PNorm = 50.4748, GNorm = 0.3915, lr_0 = 3.6300e-04
Loss = 1.9144e-04, PNorm = 50.4828, GNorm = 0.3837, lr_0 = 3.6262e-04
Validation rmse logD = 0.564982
Validation R2 logD = 0.766900
Validation rmse logP = 0.455050
Validation R2 logP = 0.939924
Epoch 45
Train function
Loss = 1.3491e-04, PNorm = 50.4886, GNorm = 0.2741, lr_0 = 3.6221e-04
Loss = 1.4131e-04, PNorm = 50.4938, GNorm = 0.2396, lr_0 = 3.6183e-04
Loss = 1.1762e-04, PNorm = 50.4982, GNorm = 0.1640, lr_0 = 3.6145e-04
Loss = 1.3183e-04, PNorm = 50.5023, GNorm = 0.5011, lr_0 = 3.6107e-04
Loss = 1.1090e-04, PNorm = 50.5058, GNorm = 0.1434, lr_0 = 3.6070e-04
Loss = 1.2217e-04, PNorm = 50.5114, GNorm = 0.2378, lr_0 = 3.6032e-04
Loss = 1.3382e-04, PNorm = 50.5155, GNorm = 0.4653, lr_0 = 3.5994e-04
Loss = 1.4943e-04, PNorm = 50.5230, GNorm = 0.3321, lr_0 = 3.5957e-04
Loss = 2.0611e-04, PNorm = 50.5280, GNorm = 0.5003, lr_0 = 3.5919e-04
Loss = 1.8874e-04, PNorm = 50.5360, GNorm = 0.4525, lr_0 = 3.5882e-04
Loss = 1.5628e-04, PNorm = 50.5407, GNorm = 0.1875, lr_0 = 3.5844e-04
Loss = 1.5865e-04, PNorm = 50.5475, GNorm = 0.2930, lr_0 = 3.5807e-04
Loss = 1.2922e-04, PNorm = 50.5527, GNorm = 0.3291, lr_0 = 3.5770e-04
Loss = 1.4174e-04, PNorm = 50.5557, GNorm = 0.1572, lr_0 = 3.5732e-04
Loss = 1.3462e-04, PNorm = 50.5587, GNorm = 0.2610, lr_0 = 3.5695e-04
Loss = 1.8838e-04, PNorm = 50.5648, GNorm = 0.5183, lr_0 = 3.5658e-04
Loss = 1.4691e-04, PNorm = 50.5732, GNorm = 0.3205, lr_0 = 3.5621e-04
Loss = 1.3159e-04, PNorm = 50.5778, GNorm = 0.4039, lr_0 = 3.5583e-04
Loss = 1.4973e-04, PNorm = 50.5808, GNorm = 0.1662, lr_0 = 3.5546e-04
Loss = 1.4711e-04, PNorm = 50.5863, GNorm = 0.2561, lr_0 = 3.5509e-04
Loss = 1.2905e-04, PNorm = 50.5911, GNorm = 0.2136, lr_0 = 3.5472e-04
Loss = 1.5754e-04, PNorm = 50.5957, GNorm = 0.5697, lr_0 = 3.5435e-04
Loss = 1.8599e-04, PNorm = 50.6022, GNorm = 0.3356, lr_0 = 3.5398e-04
Validation rmse logD = 0.560461
Validation R2 logD = 0.770616
Validation rmse logP = 0.459222
Validation R2 logP = 0.938817
Epoch 46
Train function
Loss = 1.2620e-04, PNorm = 50.6062, GNorm = 0.1876, lr_0 = 3.5361e-04
Loss = 1.2142e-04, PNorm = 50.6101, GNorm = 0.2951, lr_0 = 3.5324e-04
Loss = 1.2598e-04, PNorm = 50.6155, GNorm = 0.2414, lr_0 = 3.5287e-04
Loss = 1.2132e-04, PNorm = 50.6213, GNorm = 0.2221, lr_0 = 3.5251e-04
Loss = 1.3541e-04, PNorm = 50.6257, GNorm = 0.3070, lr_0 = 3.5214e-04
Loss = 1.7374e-04, PNorm = 50.6286, GNorm = 0.2907, lr_0 = 3.5177e-04
Loss = 1.4124e-04, PNorm = 50.6316, GNorm = 0.2784, lr_0 = 3.5140e-04
Loss = 1.5592e-04, PNorm = 50.6372, GNorm = 0.1680, lr_0 = 3.5104e-04
Loss = 1.2758e-04, PNorm = 50.6429, GNorm = 0.2796, lr_0 = 3.5067e-04
Loss = 1.3575e-04, PNorm = 50.6458, GNorm = 0.3121, lr_0 = 3.5030e-04
Loss = 1.2188e-04, PNorm = 50.6527, GNorm = 0.3198, lr_0 = 3.4994e-04
Loss = 1.6552e-04, PNorm = 50.6587, GNorm = 0.2673, lr_0 = 3.4957e-04
Loss = 1.7749e-04, PNorm = 50.6655, GNorm = 0.4055, lr_0 = 3.4921e-04
Loss = 1.7980e-04, PNorm = 50.6722, GNorm = 0.4203, lr_0 = 3.4884e-04
Loss = 1.6831e-04, PNorm = 50.6768, GNorm = 0.3214, lr_0 = 3.4848e-04
Loss = 1.3167e-04, PNorm = 50.6817, GNorm = 0.1966, lr_0 = 3.4812e-04
Loss = 1.2067e-04, PNorm = 50.6881, GNorm = 0.3575, lr_0 = 3.4775e-04
Loss = 1.2320e-04, PNorm = 50.6938, GNorm = 0.2656, lr_0 = 3.4739e-04
Loss = 1.6309e-04, PNorm = 50.6994, GNorm = 0.3120, lr_0 = 3.4703e-04
Loss = 1.7037e-04, PNorm = 50.7052, GNorm = 0.2747, lr_0 = 3.4666e-04
Loss = 1.4386e-04, PNorm = 50.7125, GNorm = 0.2052, lr_0 = 3.4630e-04
Loss = 1.8973e-04, PNorm = 50.7173, GNorm = 0.2796, lr_0 = 3.4594e-04
Validation rmse logD = 0.559264
Validation R2 logD = 0.771594
Validation rmse logP = 0.451958
Validation R2 logP = 0.940738
Epoch 47
Train function
Loss = 1.5960e-04, PNorm = 50.7244, GNorm = 0.2001, lr_0 = 3.4554e-04
Loss = 1.2272e-04, PNorm = 50.7305, GNorm = 0.2854, lr_0 = 3.4518e-04
Loss = 1.2750e-04, PNorm = 50.7353, GNorm = 0.1618, lr_0 = 3.4482e-04
Loss = 1.0427e-04, PNorm = 50.7392, GNorm = 0.1487, lr_0 = 3.4446e-04
Loss = 1.1000e-04, PNorm = 50.7437, GNorm = 0.2738, lr_0 = 3.4410e-04
Loss = 1.1266e-04, PNorm = 50.7484, GNorm = 0.1700, lr_0 = 3.4374e-04
Loss = 1.1383e-04, PNorm = 50.7517, GNorm = 0.3367, lr_0 = 3.4339e-04
Loss = 9.2647e-05, PNorm = 50.7546, GNorm = 0.2264, lr_0 = 3.4303e-04
Loss = 1.2815e-04, PNorm = 50.7563, GNorm = 0.1942, lr_0 = 3.4267e-04
Loss = 1.2597e-04, PNorm = 50.7591, GNorm = 0.1968, lr_0 = 3.4231e-04
Loss = 1.2228e-04, PNorm = 50.7626, GNorm = 0.1863, lr_0 = 3.4195e-04
Loss = 1.1012e-04, PNorm = 50.7681, GNorm = 0.2257, lr_0 = 3.4160e-04
Loss = 1.1040e-04, PNorm = 50.7725, GNorm = 0.4026, lr_0 = 3.4124e-04
Loss = 1.4630e-04, PNorm = 50.7763, GNorm = 0.2584, lr_0 = 3.4088e-04
Loss = 2.0404e-04, PNorm = 50.7800, GNorm = 0.3227, lr_0 = 3.4053e-04
Loss = 1.5189e-04, PNorm = 50.7846, GNorm = 0.4778, lr_0 = 3.4017e-04
Loss = 1.2582e-04, PNorm = 50.7893, GNorm = 0.4156, lr_0 = 3.3982e-04
Loss = 1.4746e-04, PNorm = 50.7938, GNorm = 0.3443, lr_0 = 3.3946e-04
Loss = 1.5992e-04, PNorm = 50.8011, GNorm = 0.2994, lr_0 = 3.3911e-04
Loss = 1.2969e-04, PNorm = 50.8064, GNorm = 0.2240, lr_0 = 3.3876e-04
Loss = 1.1421e-04, PNorm = 50.8105, GNorm = 0.1842, lr_0 = 3.3840e-04
Loss = 1.3792e-04, PNorm = 50.8151, GNorm = 0.3938, lr_0 = 3.3805e-04
Loss = 1.2398e-04, PNorm = 50.8196, GNorm = 0.1981, lr_0 = 3.3770e-04
Validation rmse logD = 0.554313
Validation R2 logD = 0.775620
Validation rmse logP = 0.451239
Validation R2 logP = 0.940926
Epoch 48
Train function
Loss = 1.0679e-04, PNorm = 50.8239, GNorm = 0.1679, lr_0 = 3.3734e-04
Loss = 1.0616e-04, PNorm = 50.8286, GNorm = 0.1965, lr_0 = 3.3699e-04
Loss = 1.0398e-04, PNorm = 50.8316, GNorm = 0.2399, lr_0 = 3.3664e-04
Loss = 1.0201e-04, PNorm = 50.8351, GNorm = 0.3229, lr_0 = 3.3629e-04
Loss = 1.0770e-04, PNorm = 50.8410, GNorm = 0.1565, lr_0 = 3.3594e-04
Loss = 1.2160e-04, PNorm = 50.8460, GNorm = 0.3637, lr_0 = 3.3559e-04
Loss = 1.2009e-04, PNorm = 50.8511, GNorm = 0.2933, lr_0 = 3.3524e-04
Loss = 1.1435e-04, PNorm = 50.8543, GNorm = 0.3343, lr_0 = 3.3489e-04
Loss = 1.3452e-04, PNorm = 50.8591, GNorm = 0.2371, lr_0 = 3.3454e-04
Loss = 1.1371e-04, PNorm = 50.8656, GNorm = 0.1981, lr_0 = 3.3419e-04
Loss = 1.3309e-04, PNorm = 50.8729, GNorm = 0.2606, lr_0 = 3.3384e-04
Loss = 1.3361e-04, PNorm = 50.8775, GNorm = 0.3676, lr_0 = 3.3349e-04
Loss = 9.4959e-05, PNorm = 50.8816, GNorm = 0.2089, lr_0 = 3.3314e-04
Loss = 1.1932e-04, PNorm = 50.8837, GNorm = 0.1561, lr_0 = 3.3280e-04
Loss = 1.7197e-04, PNorm = 50.8882, GNorm = 0.3870, lr_0 = 3.3245e-04
Loss = 1.0964e-04, PNorm = 50.8899, GNorm = 0.1808, lr_0 = 3.3210e-04
Loss = 1.2417e-04, PNorm = 50.8959, GNorm = 0.2065, lr_0 = 3.3175e-04
Loss = 1.2784e-04, PNorm = 50.8980, GNorm = 0.3253, lr_0 = 3.3141e-04
Loss = 1.2554e-04, PNorm = 50.9028, GNorm = 0.2014, lr_0 = 3.3106e-04
Loss = 1.2761e-04, PNorm = 50.9064, GNorm = 0.2223, lr_0 = 3.3072e-04
Loss = 1.2762e-04, PNorm = 50.9136, GNorm = 0.3675, lr_0 = 3.3037e-04
Loss = 1.5602e-04, PNorm = 50.9223, GNorm = 0.2804, lr_0 = 3.3003e-04
Validation rmse logD = 0.562407
Validation R2 logD = 0.769019
Validation rmse logP = 0.451164
Validation R2 logP = 0.940945
Epoch 49
Train function
Loss = 1.1881e-04, PNorm = 50.9278, GNorm = 0.3215, lr_0 = 3.2965e-04
Loss = 1.2723e-04, PNorm = 50.9356, GNorm = 0.1317, lr_0 = 3.2930e-04
Loss = 1.3514e-04, PNorm = 50.9420, GNorm = 0.2023, lr_0 = 3.2896e-04
Loss = 1.1295e-04, PNorm = 50.9459, GNorm = 0.3698, lr_0 = 3.2862e-04
Loss = 1.0628e-04, PNorm = 50.9517, GNorm = 0.2207, lr_0 = 3.2827e-04
Loss = 9.7558e-05, PNorm = 50.9526, GNorm = 0.1979, lr_0 = 3.2793e-04
Loss = 1.0388e-04, PNorm = 50.9564, GNorm = 0.1843, lr_0 = 3.2759e-04
Loss = 9.9094e-05, PNorm = 50.9611, GNorm = 0.2182, lr_0 = 3.2725e-04
Loss = 8.9519e-05, PNorm = 50.9650, GNorm = 0.2650, lr_0 = 3.2691e-04
Loss = 1.1329e-04, PNorm = 50.9692, GNorm = 0.2221, lr_0 = 3.2656e-04
Loss = 1.2406e-04, PNorm = 50.9719, GNorm = 0.2077, lr_0 = 3.2622e-04
Loss = 1.0726e-04, PNorm = 50.9769, GNorm = 0.2672, lr_0 = 3.2588e-04
Loss = 1.1132e-04, PNorm = 50.9790, GNorm = 0.2512, lr_0 = 3.2554e-04
Loss = 1.1397e-04, PNorm = 50.9817, GNorm = 0.4028, lr_0 = 3.2520e-04
Loss = 9.9749e-05, PNorm = 50.9860, GNorm = 0.3522, lr_0 = 3.2486e-04
Loss = 9.4334e-05, PNorm = 50.9894, GNorm = 0.3528, lr_0 = 3.2452e-04
Loss = 1.2175e-04, PNorm = 50.9935, GNorm = 0.3206, lr_0 = 3.2419e-04
Loss = 1.2128e-04, PNorm = 50.9976, GNorm = 0.3200, lr_0 = 3.2385e-04
Loss = 1.7311e-04, PNorm = 51.0039, GNorm = 0.5529, lr_0 = 3.2351e-04
Loss = 1.4648e-04, PNorm = 51.0111, GNorm = 0.4102, lr_0 = 3.2317e-04
Loss = 2.0450e-04, PNorm = 51.0194, GNorm = 0.4115, lr_0 = 3.2283e-04
Loss = 1.5415e-04, PNorm = 51.0270, GNorm = 0.3177, lr_0 = 3.2250e-04
Loss = 2.0723e-04, PNorm = 51.0357, GNorm = 0.2005, lr_0 = 3.2216e-04
Validation rmse logD = 0.558551
Validation R2 logD = 0.772176
Validation rmse logP = 0.457827
Validation R2 logP = 0.939188
Epoch 50
Train function
Loss = 1.3988e-04, PNorm = 51.0421, GNorm = 0.3366, lr_0 = 3.2182e-04
Loss = 1.1276e-04, PNorm = 51.0467, GNorm = 0.2400, lr_0 = 3.2149e-04
Loss = 1.0651e-04, PNorm = 51.0515, GNorm = 0.1688, lr_0 = 3.2115e-04
Loss = 9.5789e-05, PNorm = 51.0544, GNorm = 0.2097, lr_0 = 3.2082e-04
Loss = 9.9248e-05, PNorm = 51.0539, GNorm = 0.2935, lr_0 = 3.2048e-04
Loss = 9.6756e-05, PNorm = 51.0563, GNorm = 0.2677, lr_0 = 3.2015e-04
Loss = 9.7232e-05, PNorm = 51.0582, GNorm = 0.2510, lr_0 = 3.1981e-04
Loss = 1.0049e-04, PNorm = 51.0626, GNorm = 0.1439, lr_0 = 3.1948e-04
Loss = 1.2504e-04, PNorm = 51.0682, GNorm = 0.4115, lr_0 = 3.1915e-04
Loss = 1.0287e-04, PNorm = 51.0729, GNorm = 0.1816, lr_0 = 3.1881e-04
Loss = 9.5616e-05, PNorm = 51.0751, GNorm = 0.1758, lr_0 = 3.1848e-04
Loss = 9.0904e-05, PNorm = 51.0772, GNorm = 0.2909, lr_0 = 3.1815e-04
Loss = 1.1020e-04, PNorm = 51.0807, GNorm = 0.2670, lr_0 = 3.1782e-04
Loss = 1.0153e-04, PNorm = 51.0856, GNorm = 0.2045, lr_0 = 3.1749e-04
Loss = 1.2069e-04, PNorm = 51.0901, GNorm = 0.1957, lr_0 = 3.1715e-04
Loss = 1.1675e-04, PNorm = 51.0950, GNorm = 0.2876, lr_0 = 3.1682e-04
Loss = 1.1231e-04, PNorm = 51.1013, GNorm = 0.2431, lr_0 = 3.1649e-04
Loss = 1.0881e-04, PNorm = 51.1039, GNorm = 0.2059, lr_0 = 3.1616e-04
Loss = 1.0052e-04, PNorm = 51.1067, GNorm = 0.1963, lr_0 = 3.1583e-04
Loss = 1.0995e-04, PNorm = 51.1125, GNorm = 0.2138, lr_0 = 3.1550e-04
Loss = 1.1048e-04, PNorm = 51.1190, GNorm = 0.2188, lr_0 = 3.1517e-04
Loss = 1.1079e-04, PNorm = 51.1235, GNorm = 0.3902, lr_0 = 3.1484e-04
Validation rmse logD = 0.552625
Validation R2 logD = 0.776985
Validation rmse logP = 0.449943
Validation R2 logP = 0.941265
Epoch 51
Train function
Loss = 1.0366e-04, PNorm = 51.1281, GNorm = 0.1838, lr_0 = 3.1448e-04
Loss = 7.7119e-05, PNorm = 51.1313, GNorm = 0.2280, lr_0 = 3.1415e-04
Loss = 9.7963e-05, PNorm = 51.1336, GNorm = 0.2108, lr_0 = 3.1383e-04
Loss = 1.1132e-04, PNorm = 51.1362, GNorm = 0.2369, lr_0 = 3.1350e-04
Loss = 1.3781e-04, PNorm = 51.1413, GNorm = 0.4371, lr_0 = 3.1317e-04
Loss = 1.3491e-04, PNorm = 51.1476, GNorm = 0.3208, lr_0 = 3.1284e-04
Loss = 1.2993e-04, PNorm = 51.1536, GNorm = 0.1720, lr_0 = 3.1252e-04
Loss = 1.1147e-04, PNorm = 51.1590, GNorm = 0.2969, lr_0 = 3.1219e-04
Loss = 1.5439e-04, PNorm = 51.1628, GNorm = 0.2789, lr_0 = 3.1187e-04
Loss = 1.2354e-04, PNorm = 51.1668, GNorm = 0.3356, lr_0 = 3.1154e-04
Loss = 1.4332e-04, PNorm = 51.1740, GNorm = 0.4390, lr_0 = 3.1122e-04
Loss = 1.2428e-04, PNorm = 51.1790, GNorm = 0.2963, lr_0 = 3.1089e-04
Loss = 1.2005e-04, PNorm = 51.1842, GNorm = 0.2307, lr_0 = 3.1057e-04
Loss = 1.2219e-04, PNorm = 51.1912, GNorm = 0.2189, lr_0 = 3.1024e-04
Loss = 1.1639e-04, PNorm = 51.1949, GNorm = 0.3321, lr_0 = 3.0992e-04
Loss = 1.0683e-04, PNorm = 51.2003, GNorm = 0.2112, lr_0 = 3.0959e-04
Loss = 1.0811e-04, PNorm = 51.2057, GNorm = 0.2393, lr_0 = 3.0927e-04
Loss = 9.0747e-05, PNorm = 51.2082, GNorm = 0.2821, lr_0 = 3.0895e-04
Loss = 1.1008e-04, PNorm = 51.2118, GNorm = 0.1924, lr_0 = 3.0863e-04
Loss = 1.0594e-04, PNorm = 51.2182, GNorm = 0.3593, lr_0 = 3.0830e-04
Loss = 1.3607e-04, PNorm = 51.2226, GNorm = 0.2633, lr_0 = 3.0798e-04
Loss = 1.3512e-04, PNorm = 51.2298, GNorm = 0.2561, lr_0 = 3.0766e-04
Loss = 1.5146e-04, PNorm = 51.2346, GNorm = 0.6157, lr_0 = 3.0734e-04
Validation rmse logD = 0.555371
Validation R2 logD = 0.774763
Validation rmse logP = 0.460928
Validation R2 logP = 0.938362
Epoch 52
Train function
Loss = 1.4149e-04, PNorm = 51.2398, GNorm = 0.3672, lr_0 = 3.0699e-04
Loss = 1.1917e-04, PNorm = 51.2443, GNorm = 0.2783, lr_0 = 3.0667e-04
Loss = 1.1852e-04, PNorm = 51.2492, GNorm = 0.2983, lr_0 = 3.0635e-04
Loss = 1.0012e-04, PNorm = 51.2538, GNorm = 0.1624, lr_0 = 3.0603e-04
Loss = 1.0243e-04, PNorm = 51.2581, GNorm = 0.2070, lr_0 = 3.0571e-04
Loss = 1.0095e-04, PNorm = 51.2601, GNorm = 0.2462, lr_0 = 3.0539e-04
Loss = 8.9865e-05, PNorm = 51.2626, GNorm = 0.1714, lr_0 = 3.0507e-04
Loss = 8.1513e-05, PNorm = 51.2646, GNorm = 0.2407, lr_0 = 3.0475e-04
Loss = 8.0592e-05, PNorm = 51.2691, GNorm = 0.1505, lr_0 = 3.0443e-04
Loss = 8.8447e-05, PNorm = 51.2736, GNorm = 0.2853, lr_0 = 3.0412e-04
Loss = 7.0208e-05, PNorm = 51.2775, GNorm = 0.1608, lr_0 = 3.0380e-04
Loss = 9.1096e-05, PNorm = 51.2802, GNorm = 0.1598, lr_0 = 3.0348e-04
Loss = 8.4843e-05, PNorm = 51.2821, GNorm = 0.2292, lr_0 = 3.0316e-04
Loss = 8.4231e-05, PNorm = 51.2865, GNorm = 0.3046, lr_0 = 3.0285e-04
Loss = 1.0421e-04, PNorm = 51.2905, GNorm = 0.1924, lr_0 = 3.0253e-04
Loss = 1.0132e-04, PNorm = 51.2962, GNorm = 0.2645, lr_0 = 3.0222e-04
Loss = 8.6818e-05, PNorm = 51.2992, GNorm = 0.1565, lr_0 = 3.0190e-04
Loss = 7.9561e-05, PNorm = 51.3029, GNorm = 0.2109, lr_0 = 3.0159e-04
Loss = 8.1853e-05, PNorm = 51.3064, GNorm = 0.1576, lr_0 = 3.0127e-04
Loss = 7.9807e-05, PNorm = 51.3103, GNorm = 0.2012, lr_0 = 3.0096e-04
Loss = 1.0360e-04, PNorm = 51.3120, GNorm = 0.1990, lr_0 = 3.0064e-04
Loss = 8.9434e-05, PNorm = 51.3171, GNorm = 0.2821, lr_0 = 3.0033e-04
Loss = 9.5762e-05, PNorm = 51.3191, GNorm = 0.2272, lr_0 = 3.0001e-04
Validation rmse logD = 0.552629
Validation R2 logD = 0.776981
Validation rmse logP = 0.447773
Validation R2 logP = 0.941830
Epoch 53
Train function
Loss = 8.5242e-05, PNorm = 51.3235, GNorm = 0.1349, lr_0 = 2.9970e-04
Loss = 6.4410e-05, PNorm = 51.3289, GNorm = 0.1587, lr_0 = 2.9939e-04
Loss = 7.7247e-05, PNorm = 51.3317, GNorm = 0.1762, lr_0 = 2.9908e-04
Loss = 6.6281e-05, PNorm = 51.3320, GNorm = 0.1768, lr_0 = 2.9876e-04
Loss = 7.3980e-05, PNorm = 51.3354, GNorm = 0.1539, lr_0 = 2.9845e-04
Loss = 8.2081e-05, PNorm = 51.3388, GNorm = 0.2097, lr_0 = 2.9814e-04
Loss = 8.4160e-05, PNorm = 51.3416, GNorm = 0.1885, lr_0 = 2.9783e-04
Loss = 7.9109e-05, PNorm = 51.3452, GNorm = 0.2738, lr_0 = 2.9752e-04
Loss = 8.3308e-05, PNorm = 51.3485, GNorm = 0.3061, lr_0 = 2.9721e-04
Loss = 7.4649e-05, PNorm = 51.3522, GNorm = 0.1028, lr_0 = 2.9690e-04
Loss = 9.0479e-05, PNorm = 51.3539, GNorm = 0.1358, lr_0 = 2.9659e-04
Loss = 9.1350e-05, PNorm = 51.3565, GNorm = 0.1877, lr_0 = 2.9628e-04
Loss = 1.0099e-04, PNorm = 51.3579, GNorm = 0.2312, lr_0 = 2.9597e-04
Loss = 7.2240e-05, PNorm = 51.3645, GNorm = 0.1070, lr_0 = 2.9566e-04
Loss = 7.4744e-05, PNorm = 51.3686, GNorm = 0.2597, lr_0 = 2.9535e-04
Loss = 8.9729e-05, PNorm = 51.3712, GNorm = 0.1766, lr_0 = 2.9504e-04
Loss = 7.5655e-05, PNorm = 51.3738, GNorm = 0.2213, lr_0 = 2.9474e-04
Loss = 8.8139e-05, PNorm = 51.3775, GNorm = 0.1978, lr_0 = 2.9443e-04
Loss = 1.0944e-04, PNorm = 51.3809, GNorm = 0.1426, lr_0 = 2.9412e-04
Loss = 1.1106e-04, PNorm = 51.3852, GNorm = 0.2717, lr_0 = 2.9381e-04
Loss = 9.6725e-05, PNorm = 51.3907, GNorm = 0.2505, lr_0 = 2.9351e-04
Loss = 8.7455e-05, PNorm = 51.3942, GNorm = 0.2200, lr_0 = 2.9320e-04
Validation rmse logD = 0.555565
Validation R2 logD = 0.774606
Validation rmse logP = 0.451887
Validation R2 logP = 0.940756
Epoch 54
Train function
Loss = 9.5780e-05, PNorm = 51.3975, GNorm = 0.1521, lr_0 = 2.9286e-04
Loss = 9.1968e-05, PNorm = 51.3991, GNorm = 0.2734, lr_0 = 2.9256e-04
Loss = 8.0209e-05, PNorm = 51.4023, GNorm = 0.2046, lr_0 = 2.9225e-04
Loss = 8.8624e-05, PNorm = 51.4053, GNorm = 0.1997, lr_0 = 2.9195e-04
Loss = 8.3984e-05, PNorm = 51.4096, GNorm = 0.1748, lr_0 = 2.9164e-04
Loss = 7.5806e-05, PNorm = 51.4141, GNorm = 0.1324, lr_0 = 2.9134e-04
Loss = 6.9605e-05, PNorm = 51.4177, GNorm = 0.1798, lr_0 = 2.9104e-04
Loss = 8.4543e-05, PNorm = 51.4196, GNorm = 0.3789, lr_0 = 2.9073e-04
Loss = 7.7469e-05, PNorm = 51.4220, GNorm = 0.2362, lr_0 = 2.9043e-04
Loss = 6.8760e-05, PNorm = 51.4255, GNorm = 0.1229, lr_0 = 2.9012e-04
Loss = 7.3676e-05, PNorm = 51.4299, GNorm = 0.2046, lr_0 = 2.8982e-04
Loss = 7.4491e-05, PNorm = 51.4336, GNorm = 0.1667, lr_0 = 2.8952e-04
Loss = 1.0001e-04, PNorm = 51.4359, GNorm = 0.1872, lr_0 = 2.8922e-04
Loss = 6.8313e-05, PNorm = 51.4370, GNorm = 0.2119, lr_0 = 2.8892e-04
Loss = 6.5783e-05, PNorm = 51.4403, GNorm = 0.2872, lr_0 = 2.8861e-04
Loss = 6.8829e-05, PNorm = 51.4421, GNorm = 0.1855, lr_0 = 2.8831e-04
Loss = 9.5894e-05, PNorm = 51.4421, GNorm = 0.1596, lr_0 = 2.8801e-04
Loss = 9.1218e-05, PNorm = 51.4433, GNorm = 0.2298, lr_0 = 2.8771e-04
Loss = 6.9410e-05, PNorm = 51.4474, GNorm = 0.1993, lr_0 = 2.8741e-04
Loss = 9.1797e-05, PNorm = 51.4501, GNorm = 0.3011, lr_0 = 2.8711e-04
Loss = 7.7709e-05, PNorm = 51.4545, GNorm = 0.2067, lr_0 = 2.8681e-04
Loss = 6.3764e-05, PNorm = 51.4585, GNorm = 0.2354, lr_0 = 2.8651e-04
Loss = 1.1367e-04, PNorm = 51.4640, GNorm = 0.3679, lr_0 = 2.8621e-04
Validation rmse logD = 0.554679
Validation R2 logD = 0.775324
Validation rmse logP = 0.450239
Validation R2 logP = 0.941187
Epoch 55
Train function
Loss = 8.3658e-05, PNorm = 51.4680, GNorm = 0.2650, lr_0 = 2.8591e-04
Loss = 7.5372e-05, PNorm = 51.4733, GNorm = 0.2280, lr_0 = 2.8562e-04
Loss = 6.3528e-05, PNorm = 51.4780, GNorm = 0.1680, lr_0 = 2.8532e-04
Loss = 5.8880e-05, PNorm = 51.4804, GNorm = 0.2171, lr_0 = 2.8502e-04
Loss = 7.7104e-05, PNorm = 51.4839, GNorm = 0.2101, lr_0 = 2.8472e-04
Loss = 7.7700e-05, PNorm = 51.4875, GNorm = 0.2569, lr_0 = 2.8443e-04
Loss = 7.7507e-05, PNorm = 51.4880, GNorm = 0.1914, lr_0 = 2.8413e-04
Loss = 8.4830e-05, PNorm = 51.4921, GNorm = 0.2155, lr_0 = 2.8383e-04
Loss = 1.0774e-04, PNorm = 51.4968, GNorm = 0.1986, lr_0 = 2.8354e-04
Loss = 8.3426e-05, PNorm = 51.5006, GNorm = 0.2296, lr_0 = 2.8324e-04
Loss = 1.0226e-04, PNorm = 51.5058, GNorm = 0.4000, lr_0 = 2.8294e-04
Loss = 6.8995e-05, PNorm = 51.5131, GNorm = 0.2047, lr_0 = 2.8265e-04
Loss = 9.3142e-05, PNorm = 51.5167, GNorm = 0.3532, lr_0 = 2.8235e-04
Loss = 9.7029e-05, PNorm = 51.5188, GNorm = 0.1993, lr_0 = 2.8206e-04
Loss = 7.5792e-05, PNorm = 51.5208, GNorm = 0.1419, lr_0 = 2.8176e-04
Loss = 6.1931e-05, PNorm = 51.5206, GNorm = 0.1830, lr_0 = 2.8147e-04
Loss = 8.7833e-05, PNorm = 51.5243, GNorm = 0.1172, lr_0 = 2.8118e-04
Loss = 7.8532e-05, PNorm = 51.5295, GNorm = 0.1258, lr_0 = 2.8088e-04
Loss = 7.2454e-05, PNorm = 51.5310, GNorm = 0.2177, lr_0 = 2.8059e-04
Loss = 6.6522e-05, PNorm = 51.5336, GNorm = 0.1972, lr_0 = 2.8030e-04
Loss = 7.1141e-05, PNorm = 51.5355, GNorm = 0.1257, lr_0 = 2.8000e-04
Loss = 8.3125e-05, PNorm = 51.5398, GNorm = 0.2212, lr_0 = 2.7971e-04
Validation rmse logD = 0.558916
Validation R2 logD = 0.771878
Validation rmse logP = 0.450893
Validation R2 logP = 0.941016
Epoch 56
Train function
Loss = 6.9511e-05, PNorm = 51.5439, GNorm = 0.1710, lr_0 = 2.7939e-04
Loss = 6.8329e-05, PNorm = 51.5451, GNorm = 0.1324, lr_0 = 2.7910e-04
Loss = 6.7905e-05, PNorm = 51.5466, GNorm = 0.1731, lr_0 = 2.7881e-04
Loss = 6.9652e-05, PNorm = 51.5492, GNorm = 0.3001, lr_0 = 2.7852e-04
Loss = 7.8528e-05, PNorm = 51.5533, GNorm = 0.1839, lr_0 = 2.7823e-04
Loss = 6.9481e-05, PNorm = 51.5557, GNorm = 0.1491, lr_0 = 2.7794e-04
Loss = 7.5869e-05, PNorm = 51.5579, GNorm = 0.2081, lr_0 = 2.7765e-04
Loss = 6.1641e-05, PNorm = 51.5595, GNorm = 0.1333, lr_0 = 2.7736e-04
Loss = 7.6187e-05, PNorm = 51.5636, GNorm = 0.3462, lr_0 = 2.7707e-04
Loss = 8.9291e-05, PNorm = 51.5689, GNorm = 0.2878, lr_0 = 2.7678e-04
Loss = 7.9170e-05, PNorm = 51.5738, GNorm = 0.2838, lr_0 = 2.7649e-04
Loss = 8.0092e-05, PNorm = 51.5761, GNorm = 0.2124, lr_0 = 2.7620e-04
Loss = 9.6122e-05, PNorm = 51.5794, GNorm = 0.1652, lr_0 = 2.7591e-04
Loss = 9.1880e-05, PNorm = 51.5813, GNorm = 0.1787, lr_0 = 2.7562e-04
Loss = 7.0029e-05, PNorm = 51.5852, GNorm = 0.1693, lr_0 = 2.7534e-04
Loss = 8.3514e-05, PNorm = 51.5893, GNorm = 0.1690, lr_0 = 2.7505e-04
Loss = 9.9471e-05, PNorm = 51.5930, GNorm = 0.1864, lr_0 = 2.7476e-04
Loss = 7.5045e-05, PNorm = 51.5977, GNorm = 0.2120, lr_0 = 2.7448e-04
Loss = 8.8012e-05, PNorm = 51.6026, GNorm = 0.1539, lr_0 = 2.7419e-04
Loss = 1.2295e-04, PNorm = 51.6070, GNorm = 0.3390, lr_0 = 2.7390e-04
Loss = 1.0492e-04, PNorm = 51.6096, GNorm = 0.3821, lr_0 = 2.7362e-04
Loss = 1.1584e-04, PNorm = 51.6135, GNorm = 0.2143, lr_0 = 2.7333e-04
Loss = 9.1636e-05, PNorm = 51.6169, GNorm = 0.1793, lr_0 = 2.7305e-04
Validation rmse logD = 0.553483
Validation R2 logD = 0.776292
Validation rmse logP = 0.450686
Validation R2 logP = 0.941070
Epoch 57
Train function
Loss = 7.7814e-05, PNorm = 51.6202, GNorm = 0.2947, lr_0 = 2.7276e-04
Loss = 7.1835e-05, PNorm = 51.6256, GNorm = 0.1483, lr_0 = 2.7248e-04
Loss = 1.1072e-04, PNorm = 51.6296, GNorm = 0.5898, lr_0 = 2.7219e-04
Loss = 8.1486e-05, PNorm = 51.6352, GNorm = 0.4811, lr_0 = 2.7191e-04
Loss = 7.3196e-05, PNorm = 51.6390, GNorm = 0.1470, lr_0 = 2.7162e-04
Loss = 6.6266e-05, PNorm = 51.6404, GNorm = 0.1622, lr_0 = 2.7134e-04
Loss = 5.7509e-05, PNorm = 51.6432, GNorm = 0.0816, lr_0 = 2.7106e-04
Loss = 7.3852e-05, PNorm = 51.6472, GNorm = 0.2184, lr_0 = 2.7077e-04
Loss = 8.0901e-05, PNorm = 51.6509, GNorm = 0.2135, lr_0 = 2.7049e-04
Loss = 6.3061e-05, PNorm = 51.6556, GNorm = 0.1295, lr_0 = 2.7021e-04
Loss = 7.2254e-05, PNorm = 51.6575, GNorm = 0.1259, lr_0 = 2.6993e-04
Loss = 8.0707e-05, PNorm = 51.6588, GNorm = 0.1996, lr_0 = 2.6965e-04
Loss = 8.6472e-05, PNorm = 51.6607, GNorm = 0.2825, lr_0 = 2.6936e-04
Loss = 7.3777e-05, PNorm = 51.6651, GNorm = 0.2121, lr_0 = 2.6908e-04
Loss = 8.7290e-05, PNorm = 51.6682, GNorm = 0.2262, lr_0 = 2.6880e-04
Loss = 7.0043e-05, PNorm = 51.6705, GNorm = 0.1244, lr_0 = 2.6852e-04
Loss = 8.0946e-05, PNorm = 51.6744, GNorm = 0.2165, lr_0 = 2.6824e-04
Loss = 6.5566e-05, PNorm = 51.6788, GNorm = 0.1694, lr_0 = 2.6796e-04
Loss = 6.8398e-05, PNorm = 51.6797, GNorm = 0.1672, lr_0 = 2.6768e-04
Loss = 7.5915e-05, PNorm = 51.6813, GNorm = 0.2732, lr_0 = 2.6740e-04
Loss = 9.8014e-05, PNorm = 51.6857, GNorm = 0.6215, lr_0 = 2.6712e-04
Loss = 9.4885e-05, PNorm = 51.6898, GNorm = 0.2908, lr_0 = 2.6684e-04
Validation rmse logD = 0.557028
Validation R2 logD = 0.773417
Validation rmse logP = 0.456744
Validation R2 logP = 0.939476
Epoch 58
Train function
Loss = 9.7396e-05, PNorm = 51.6939, GNorm = 0.2546, lr_0 = 2.6654e-04
Loss = 6.9473e-05, PNorm = 51.6993, GNorm = 0.2575, lr_0 = 2.6626e-04
Loss = 8.9049e-05, PNorm = 51.7042, GNorm = 0.2358, lr_0 = 2.6598e-04
Loss = 9.4803e-05, PNorm = 51.7070, GNorm = 0.3998, lr_0 = 2.6570e-04
Loss = 5.9825e-05, PNorm = 51.7073, GNorm = 0.1247, lr_0 = 2.6543e-04
Loss = 5.9475e-05, PNorm = 51.7090, GNorm = 0.2335, lr_0 = 2.6515e-04
Loss = 6.9501e-05, PNorm = 51.7127, GNorm = 0.1504, lr_0 = 2.6487e-04
Loss = 5.8628e-05, PNorm = 51.7146, GNorm = 0.1110, lr_0 = 2.6460e-04
Loss = 6.5317e-05, PNorm = 51.7153, GNorm = 0.2559, lr_0 = 2.6432e-04
Loss = 6.0998e-05, PNorm = 51.7170, GNorm = 0.1171, lr_0 = 2.6405e-04
Loss = 6.0083e-05, PNorm = 51.7193, GNorm = 0.1855, lr_0 = 2.6377e-04
Loss = 6.0402e-05, PNorm = 51.7242, GNorm = 0.2670, lr_0 = 2.6349e-04
Loss = 6.8394e-05, PNorm = 51.7277, GNorm = 0.1946, lr_0 = 2.6322e-04
Loss = 6.4796e-05, PNorm = 51.7314, GNorm = 0.1140, lr_0 = 2.6294e-04
Loss = 8.3009e-05, PNorm = 51.7329, GNorm = 0.3351, lr_0 = 2.6267e-04
Loss = 7.6892e-05, PNorm = 51.7360, GNorm = 0.3402, lr_0 = 2.6240e-04
Loss = 7.7306e-05, PNorm = 51.7392, GNorm = 0.2067, lr_0 = 2.6212e-04
Loss = 9.3949e-05, PNorm = 51.7442, GNorm = 0.4269, lr_0 = 2.6185e-04
Loss = 7.7720e-05, PNorm = 51.7492, GNorm = 0.2062, lr_0 = 2.6158e-04
Loss = 8.4050e-05, PNorm = 51.7536, GNorm = 0.2708, lr_0 = 2.6130e-04
Loss = 1.0419e-04, PNorm = 51.7558, GNorm = 0.2366, lr_0 = 2.6103e-04
Loss = 7.8167e-05, PNorm = 51.7600, GNorm = 0.2162, lr_0 = 2.6076e-04
Loss = 7.2005e-05, PNorm = 51.7631, GNorm = 0.1404, lr_0 = 2.6048e-04
Validation rmse logD = 0.556336
Validation R2 logD = 0.773980
Validation rmse logP = 0.448844
Validation R2 logP = 0.941551
Epoch 59
Train function
Loss = 6.7227e-05, PNorm = 51.7650, GNorm = 0.1939, lr_0 = 2.6021e-04
Loss = 8.0487e-05, PNorm = 51.7677, GNorm = 0.1798, lr_0 = 2.5994e-04
Loss = 6.8934e-05, PNorm = 51.7732, GNorm = 0.1709, lr_0 = 2.5967e-04
Loss = 5.7764e-05, PNorm = 51.7786, GNorm = 0.0941, lr_0 = 2.5940e-04
Loss = 6.4312e-05, PNorm = 51.7805, GNorm = 0.2854, lr_0 = 2.5913e-04
Loss = 5.4261e-05, PNorm = 51.7825, GNorm = 0.1265, lr_0 = 2.5886e-04
Loss = 6.8672e-05, PNorm = 51.7862, GNorm = 0.3124, lr_0 = 2.5859e-04
Loss = 7.6298e-05, PNorm = 51.7882, GNorm = 0.1875, lr_0 = 2.5832e-04
Loss = 7.3350e-05, PNorm = 51.7908, GNorm = 0.2127, lr_0 = 2.5805e-04
Loss = 7.8677e-05, PNorm = 51.7926, GNorm = 0.2338, lr_0 = 2.5778e-04
Loss = 6.9622e-05, PNorm = 51.7927, GNorm = 0.3532, lr_0 = 2.5751e-04
Loss = 5.6185e-05, PNorm = 51.7945, GNorm = 0.1191, lr_0 = 2.5724e-04
Loss = 5.7312e-05, PNorm = 51.7966, GNorm = 0.1968, lr_0 = 2.5697e-04
Loss = 5.8726e-05, PNorm = 51.8013, GNorm = 0.2283, lr_0 = 2.5670e-04
Loss = 7.3698e-05, PNorm = 51.8044, GNorm = 0.2220, lr_0 = 2.5644e-04
Loss = 7.7116e-05, PNorm = 51.8071, GNorm = 0.1867, lr_0 = 2.5617e-04
Loss = 7.8397e-05, PNorm = 51.8090, GNorm = 0.1660, lr_0 = 2.5590e-04
Loss = 6.7266e-05, PNorm = 51.8119, GNorm = 0.1804, lr_0 = 2.5563e-04
Loss = 5.4155e-05, PNorm = 51.8134, GNorm = 0.1268, lr_0 = 2.5537e-04
Loss = 6.2300e-05, PNorm = 51.8164, GNorm = 0.1160, lr_0 = 2.5510e-04
Loss = 7.1141e-05, PNorm = 51.8195, GNorm = 0.2524, lr_0 = 2.5483e-04
Loss = 6.7012e-05, PNorm = 51.8203, GNorm = 0.2279, lr_0 = 2.5457e-04
Validation rmse logD = 0.554443
Validation R2 logD = 0.775515
Validation rmse logP = 0.446725
Validation R2 logP = 0.942102
Epoch 60
Train function
Loss = 5.2404e-05, PNorm = 51.8241, GNorm = 0.1129, lr_0 = 2.5428e-04
Loss = 5.8664e-05, PNorm = 51.8271, GNorm = 0.1489, lr_0 = 2.5401e-04
Loss = 7.3140e-05, PNorm = 51.8302, GNorm = 0.3342, lr_0 = 2.5375e-04
Loss = 6.7458e-05, PNorm = 51.8332, GNorm = 0.1205, lr_0 = 2.5348e-04
Loss = 5.5794e-05, PNorm = 51.8366, GNorm = 0.1229, lr_0 = 2.5322e-04
Loss = 5.9763e-05, PNorm = 51.8401, GNorm = 0.1669, lr_0 = 2.5295e-04
Loss = 5.0907e-05, PNorm = 51.8426, GNorm = 0.0988, lr_0 = 2.5269e-04
Loss = 6.6350e-05, PNorm = 51.8450, GNorm = 0.1575, lr_0 = 2.5242e-04
Loss = 5.2305e-05, PNorm = 51.8478, GNorm = 0.1188, lr_0 = 2.5216e-04
Loss = 5.2409e-05, PNorm = 51.8498, GNorm = 0.1779, lr_0 = 2.5190e-04
Loss = 6.3905e-05, PNorm = 51.8489, GNorm = 0.1973, lr_0 = 2.5163e-04
Loss = 6.3732e-05, PNorm = 51.8514, GNorm = 0.3273, lr_0 = 2.5137e-04
Loss = 5.9738e-05, PNorm = 51.8534, GNorm = 0.1197, lr_0 = 2.5111e-04
Loss = 5.8464e-05, PNorm = 51.8559, GNorm = 0.1799, lr_0 = 2.5085e-04
Loss = 5.7296e-05, PNorm = 51.8595, GNorm = 0.1449, lr_0 = 2.5059e-04
Loss = 6.5673e-05, PNorm = 51.8602, GNorm = 0.1554, lr_0 = 2.5032e-04
Loss = 5.8618e-05, PNorm = 51.8639, GNorm = 0.2230, lr_0 = 2.5006e-04
Loss = 7.7457e-05, PNorm = 51.8677, GNorm = 0.2099, lr_0 = 2.4980e-04
Loss = 5.9919e-05, PNorm = 51.8730, GNorm = 0.1063, lr_0 = 2.4954e-04
Loss = 6.9131e-05, PNorm = 51.8777, GNorm = 0.2987, lr_0 = 2.4928e-04
Loss = 7.5153e-05, PNorm = 51.8802, GNorm = 0.1911, lr_0 = 2.4902e-04
Loss = 7.6885e-05, PNorm = 51.8841, GNorm = 0.2613, lr_0 = 2.4876e-04
Loss = 8.2731e-05, PNorm = 51.8890, GNorm = 0.1930, lr_0 = 2.4850e-04
Validation rmse logD = 0.564916
Validation R2 logD = 0.766954
Validation rmse logP = 0.457116
Validation R2 logP = 0.939377
Epoch 61
Train function
Loss = 9.6406e-05, PNorm = 51.8943, GNorm = 0.2055, lr_0 = 2.4824e-04
Loss = 9.0320e-05, PNorm = 51.8979, GNorm = 0.1756, lr_0 = 2.4798e-04
Loss = 6.3770e-05, PNorm = 51.9032, GNorm = 0.2803, lr_0 = 2.4772e-04
Loss = 6.2594e-05, PNorm = 51.9068, GNorm = 0.2625, lr_0 = 2.4747e-04
Loss = 5.7719e-05, PNorm = 51.9085, GNorm = 0.2120, lr_0 = 2.4721e-04
Loss = 5.1238e-05, PNorm = 51.9087, GNorm = 0.1620, lr_0 = 2.4695e-04
Loss = 4.4308e-05, PNorm = 51.9105, GNorm = 0.2023, lr_0 = 2.4669e-04
Loss = 7.6474e-05, PNorm = 51.9123, GNorm = 0.2435, lr_0 = 2.4643e-04
Loss = 6.5030e-05, PNorm = 51.9145, GNorm = 0.2993, lr_0 = 2.4618e-04
Loss = 6.1395e-05, PNorm = 51.9166, GNorm = 0.1477, lr_0 = 2.4592e-04
Loss = 5.9913e-05, PNorm = 51.9186, GNorm = 0.2319, lr_0 = 2.4566e-04
Loss = 4.9890e-05, PNorm = 51.9206, GNorm = 0.0982, lr_0 = 2.4541e-04
Loss = 5.9732e-05, PNorm = 51.9245, GNorm = 0.1878, lr_0 = 2.4515e-04
Loss = 7.0924e-05, PNorm = 51.9282, GNorm = 0.1717, lr_0 = 2.4489e-04
Loss = 5.8468e-05, PNorm = 51.9310, GNorm = 0.1213, lr_0 = 2.4464e-04
Loss = 5.7966e-05, PNorm = 51.9341, GNorm = 0.1577, lr_0 = 2.4438e-04
Loss = 6.0454e-05, PNorm = 51.9377, GNorm = 0.1832, lr_0 = 2.4413e-04
Loss = 5.4655e-05, PNorm = 51.9409, GNorm = 0.1665, lr_0 = 2.4387e-04
Loss = 6.2367e-05, PNorm = 51.9419, GNorm = 0.2269, lr_0 = 2.4362e-04
Loss = 6.2487e-05, PNorm = 51.9447, GNorm = 0.3692, lr_0 = 2.4337e-04
Loss = 8.1100e-05, PNorm = 51.9471, GNorm = 0.2181, lr_0 = 2.4311e-04
Loss = 8.1095e-05, PNorm = 51.9475, GNorm = 0.2821, lr_0 = 2.4286e-04
Validation rmse logD = 0.556116
Validation R2 logD = 0.774158
Validation rmse logP = 0.452796
Validation R2 logP = 0.940517
Epoch 62
Train function
Loss = 6.0083e-05, PNorm = 51.9506, GNorm = 0.2316, lr_0 = 2.4258e-04
Loss = 6.2037e-05, PNorm = 51.9532, GNorm = 0.1743, lr_0 = 2.4233e-04
Loss = 7.1456e-05, PNorm = 51.9564, GNorm = 0.2743, lr_0 = 2.4207e-04
Loss = 8.0697e-05, PNorm = 51.9600, GNorm = 0.1610, lr_0 = 2.4182e-04
Loss = 6.5242e-05, PNorm = 51.9623, GNorm = 0.1775, lr_0 = 2.4157e-04
Loss = 5.8358e-05, PNorm = 51.9653, GNorm = 0.1465, lr_0 = 2.4132e-04
Loss = 6.1252e-05, PNorm = 51.9698, GNorm = 0.2227, lr_0 = 2.4106e-04
Loss = 5.3978e-05, PNorm = 51.9728, GNorm = 0.2848, lr_0 = 2.4081e-04
Loss = 5.5825e-05, PNorm = 51.9758, GNorm = 0.1563, lr_0 = 2.4056e-04
Loss = 4.5593e-05, PNorm = 51.9782, GNorm = 0.0908, lr_0 = 2.4031e-04
Loss = 5.5965e-05, PNorm = 51.9802, GNorm = 0.1769, lr_0 = 2.4006e-04
Loss = 5.4586e-05, PNorm = 51.9845, GNorm = 0.2006, lr_0 = 2.3981e-04
Loss = 5.7030e-05, PNorm = 51.9878, GNorm = 0.1363, lr_0 = 2.3956e-04
Loss = 5.2202e-05, PNorm = 51.9876, GNorm = 0.1616, lr_0 = 2.3931e-04
Loss = 4.8863e-05, PNorm = 51.9904, GNorm = 0.2094, lr_0 = 2.3906e-04
Loss = 5.7916e-05, PNorm = 51.9933, GNorm = 0.1788, lr_0 = 2.3881e-04
Loss = 4.5999e-05, PNorm = 51.9960, GNorm = 0.1267, lr_0 = 2.3856e-04
Loss = 7.2379e-05, PNorm = 51.9984, GNorm = 0.1312, lr_0 = 2.3831e-04
Loss = 6.4395e-05, PNorm = 51.9995, GNorm = 0.3477, lr_0 = 2.3806e-04
Loss = 7.3496e-05, PNorm = 52.0008, GNorm = 0.1441, lr_0 = 2.3781e-04
Loss = 6.1447e-05, PNorm = 52.0052, GNorm = 0.3715, lr_0 = 2.3756e-04
Loss = 5.8813e-05, PNorm = 52.0080, GNorm = 0.1368, lr_0 = 2.3732e-04
Loss = 5.0087e-05, PNorm = 52.0099, GNorm = 0.1067, lr_0 = 2.3707e-04
Validation rmse logD = 0.558977
Validation R2 logD = 0.771829
Validation rmse logP = 0.449769
Validation R2 logP = 0.941310
Epoch 63
Train function
Loss = 5.8305e-05, PNorm = 52.0132, GNorm = 0.1548, lr_0 = 2.3682e-04
Loss = 6.7364e-05, PNorm = 52.0163, GNorm = 0.2932, lr_0 = 2.3657e-04
Loss = 5.6727e-05, PNorm = 52.0192, GNorm = 0.1468, lr_0 = 2.3633e-04
Loss = 5.0872e-05, PNorm = 52.0224, GNorm = 0.1900, lr_0 = 2.3608e-04
Loss = 6.7067e-05, PNorm = 52.0239, GNorm = 0.2645, lr_0 = 2.3583e-04
Loss = 4.8174e-05, PNorm = 52.0264, GNorm = 0.1553, lr_0 = 2.3559e-04
Loss = 5.5821e-05, PNorm = 52.0272, GNorm = 0.1582, lr_0 = 2.3534e-04
Loss = 5.1044e-05, PNorm = 52.0295, GNorm = 0.1399, lr_0 = 2.3510e-04
Loss = 5.3630e-05, PNorm = 52.0317, GNorm = 0.1664, lr_0 = 2.3485e-04
Loss = 4.4759e-05, PNorm = 52.0345, GNorm = 0.1306, lr_0 = 2.3461e-04
Loss = 4.5290e-05, PNorm = 52.0338, GNorm = 0.1299, lr_0 = 2.3436e-04
Loss = 5.4536e-05, PNorm = 52.0363, GNorm = 0.1796, lr_0 = 2.3412e-04
Loss = 5.0955e-05, PNorm = 52.0384, GNorm = 0.1789, lr_0 = 2.3387e-04
Loss = 5.0826e-05, PNorm = 52.0403, GNorm = 0.2318, lr_0 = 2.3363e-04
Loss = 6.2170e-05, PNorm = 52.0419, GNorm = 0.1693, lr_0 = 2.3338e-04
Loss = 4.8438e-05, PNorm = 52.0438, GNorm = 0.2093, lr_0 = 2.3314e-04
Loss = 4.5415e-05, PNorm = 52.0456, GNorm = 0.0992, lr_0 = 2.3290e-04
Loss = 5.3038e-05, PNorm = 52.0470, GNorm = 0.1382, lr_0 = 2.3265e-04
Loss = 4.6765e-05, PNorm = 52.0475, GNorm = 0.1422, lr_0 = 2.3241e-04
Loss = 5.1606e-05, PNorm = 52.0502, GNorm = 0.1357, lr_0 = 2.3217e-04
Loss = 5.6627e-05, PNorm = 52.0530, GNorm = 0.1117, lr_0 = 2.3193e-04
Loss = 5.0198e-05, PNorm = 52.0545, GNorm = 0.1817, lr_0 = 2.3169e-04
Loss = 5.2095e-05, PNorm = 52.0562, GNorm = 0.2911, lr_0 = 2.3144e-04
Loss = 1.1017e-04, PNorm = 52.0563, GNorm = 0.2223, lr_0 = 2.3142e-04
Validation rmse logD = 0.556390
Validation R2 logD = 0.773936
Validation rmse logP = 0.450551
Validation R2 logP = 0.941106
Epoch 64
Train function
Loss = 3.8322e-05, PNorm = 52.0595, GNorm = 0.1947, lr_0 = 2.3118e-04
Loss = 5.5815e-05, PNorm = 52.0622, GNorm = 0.1282, lr_0 = 2.3094e-04
Loss = 5.1669e-05, PNorm = 52.0642, GNorm = 0.1847, lr_0 = 2.3070e-04
Loss = 5.0200e-05, PNorm = 52.0673, GNorm = 0.1938, lr_0 = 2.3045e-04
Loss = 4.3578e-05, PNorm = 52.0701, GNorm = 0.1072, lr_0 = 2.3021e-04
Loss = 4.1327e-05, PNorm = 52.0724, GNorm = 0.1025, lr_0 = 2.2997e-04
Loss = 4.9444e-05, PNorm = 52.0759, GNorm = 0.1433, lr_0 = 2.2973e-04
Loss = 3.7046e-05, PNorm = 52.0796, GNorm = 0.1174, lr_0 = 2.2949e-04
Loss = 4.0235e-05, PNorm = 52.0814, GNorm = 0.1515, lr_0 = 2.2925e-04
Loss = 3.7469e-05, PNorm = 52.0829, GNorm = 0.1794, lr_0 = 2.2902e-04
Loss = 3.4733e-05, PNorm = 52.0856, GNorm = 0.2335, lr_0 = 2.2878e-04
Loss = 4.9238e-05, PNorm = 52.0864, GNorm = 0.1619, lr_0 = 2.2854e-04
Loss = 5.2285e-05, PNorm = 52.0884, GNorm = 0.3016, lr_0 = 2.2830e-04
Loss = 6.4609e-05, PNorm = 52.0906, GNorm = 0.1795, lr_0 = 2.2806e-04
Loss = 6.1588e-05, PNorm = 52.0943, GNorm = 0.3256, lr_0 = 2.2782e-04
Loss = 4.5580e-05, PNorm = 52.0982, GNorm = 0.1505, lr_0 = 2.2758e-04
Loss = 5.6010e-05, PNorm = 52.0984, GNorm = 0.2300, lr_0 = 2.2735e-04
Loss = 6.4654e-05, PNorm = 52.0986, GNorm = 0.2214, lr_0 = 2.2711e-04
Loss = 5.7849e-05, PNorm = 52.1014, GNorm = 0.2250, lr_0 = 2.2687e-04
Loss = 5.5890e-05, PNorm = 52.1026, GNorm = 0.1660, lr_0 = 2.2664e-04
Loss = 6.8303e-05, PNorm = 52.1048, GNorm = 0.2672, lr_0 = 2.2640e-04
Loss = 5.2068e-05, PNorm = 52.1053, GNorm = 0.1586, lr_0 = 2.2616e-04
Validation rmse logD = 0.555639
Validation R2 logD = 0.774545
Validation rmse logP = 0.448962
Validation R2 logP = 0.941521
Epoch 65
Train function
Loss = 3.8620e-05, PNorm = 52.1073, GNorm = 0.1176, lr_0 = 2.2593e-04
Loss = 5.7382e-05, PNorm = 52.1120, GNorm = 0.1013, lr_0 = 2.2569e-04
Loss = 4.5109e-05, PNorm = 52.1137, GNorm = 0.1296, lr_0 = 2.2546e-04
Loss = 4.4364e-05, PNorm = 52.1156, GNorm = 0.2487, lr_0 = 2.2522e-04
Loss = 5.5999e-05, PNorm = 52.1176, GNorm = 0.1835, lr_0 = 2.2499e-04
Loss = 5.5785e-05, PNorm = 52.1210, GNorm = 0.1858, lr_0 = 2.2475e-04
Loss = 4.6994e-05, PNorm = 52.1246, GNorm = 0.1334, lr_0 = 2.2452e-04
Loss = 4.7289e-05, PNorm = 52.1255, GNorm = 0.2138, lr_0 = 2.2428e-04
Loss = 4.8051e-05, PNorm = 52.1283, GNorm = 0.1322, lr_0 = 2.2405e-04
Loss = 4.1091e-05, PNorm = 52.1302, GNorm = 0.1581, lr_0 = 2.2381e-04
Loss = 5.0813e-05, PNorm = 52.1319, GNorm = 0.1046, lr_0 = 2.2358e-04
Loss = 4.3434e-05, PNorm = 52.1331, GNorm = 0.1634, lr_0 = 2.2335e-04
Loss = 4.1835e-05, PNorm = 52.1352, GNorm = 0.1529, lr_0 = 2.2311e-04
Loss = 4.7769e-05, PNorm = 52.1386, GNorm = 0.1122, lr_0 = 2.2288e-04
Loss = 4.7079e-05, PNorm = 52.1407, GNorm = 0.1715, lr_0 = 2.2265e-04
Loss = 4.8350e-05, PNorm = 52.1422, GNorm = 0.1729, lr_0 = 2.2242e-04
Loss = 4.5357e-05, PNorm = 52.1438, GNorm = 0.1610, lr_0 = 2.2218e-04
Loss = 6.2255e-05, PNorm = 52.1450, GNorm = 0.1397, lr_0 = 2.2195e-04
Loss = 5.3196e-05, PNorm = 52.1466, GNorm = 0.1492, lr_0 = 2.2172e-04
Loss = 5.2682e-05, PNorm = 52.1465, GNorm = 0.1907, lr_0 = 2.2149e-04
Loss = 4.7601e-05, PNorm = 52.1479, GNorm = 0.1955, lr_0 = 2.2126e-04
Loss = 4.7809e-05, PNorm = 52.1504, GNorm = 0.1744, lr_0 = 2.2103e-04
Loss = 4.4225e-05, PNorm = 52.1523, GNorm = 0.2057, lr_0 = 2.2080e-04
Validation rmse logD = 0.558872
Validation R2 logD = 0.771914
Validation rmse logP = 0.448509
Validation R2 logP = 0.941638
Epoch 66
Train function
Loss = 5.4220e-05, PNorm = 52.1566, GNorm = 0.1583, lr_0 = 2.2054e-04
Loss = 4.3599e-05, PNorm = 52.1583, GNorm = 0.1401, lr_0 = 2.2031e-04
Loss = 4.5962e-05, PNorm = 52.1624, GNorm = 0.1572, lr_0 = 2.2008e-04
Loss = 4.2228e-05, PNorm = 52.1657, GNorm = 0.1988, lr_0 = 2.1985e-04
Loss = 5.6635e-05, PNorm = 52.1682, GNorm = 0.1610, lr_0 = 2.1962e-04
Loss = 5.2201e-05, PNorm = 52.1704, GNorm = 0.2364, lr_0 = 2.1939e-04
Loss = 7.1810e-05, PNorm = 52.1726, GNorm = 0.3978, lr_0 = 2.1916e-04
Loss = 6.9655e-05, PNorm = 52.1739, GNorm = 0.3433, lr_0 = 2.1894e-04
Loss = 5.0822e-05, PNorm = 52.1747, GNorm = 0.1121, lr_0 = 2.1871e-04
Loss = 5.9149e-05, PNorm = 52.1766, GNorm = 0.3852, lr_0 = 2.1848e-04
Loss = 6.7322e-05, PNorm = 52.1802, GNorm = 0.2185, lr_0 = 2.1825e-04
Loss = 6.1175e-05, PNorm = 52.1820, GNorm = 0.2437, lr_0 = 2.1802e-04
Loss = 5.9014e-05, PNorm = 52.1841, GNorm = 0.1803, lr_0 = 2.1780e-04
Loss = 5.4202e-05, PNorm = 52.1871, GNorm = 0.1386, lr_0 = 2.1757e-04
Loss = 4.0862e-05, PNorm = 52.1895, GNorm = 0.1442, lr_0 = 2.1734e-04
Loss = 5.6032e-05, PNorm = 52.1909, GNorm = 0.1440, lr_0 = 2.1711e-04
Loss = 6.2897e-05, PNorm = 52.1928, GNorm = 0.3352, lr_0 = 2.1689e-04
Loss = 5.3525e-05, PNorm = 52.1956, GNorm = 0.2772, lr_0 = 2.1666e-04
Loss = 4.8421e-05, PNorm = 52.1981, GNorm = 0.1457, lr_0 = 2.1644e-04
Loss = 5.0166e-05, PNorm = 52.2009, GNorm = 0.1505, lr_0 = 2.1621e-04
Loss = 4.3034e-05, PNorm = 52.2034, GNorm = 0.1077, lr_0 = 2.1598e-04
Loss = 5.0317e-05, PNorm = 52.2063, GNorm = 0.0902, lr_0 = 2.1576e-04
Validation rmse logD = 0.557838
Validation R2 logD = 0.772757
Validation rmse logP = 0.448551
Validation R2 logP = 0.941628
Epoch 67
Train function
Loss = 3.7933e-05, PNorm = 52.2082, GNorm = 0.1678, lr_0 = 2.1553e-04
Loss = 5.1352e-05, PNorm = 52.2092, GNorm = 0.0958, lr_0 = 2.1531e-04
Loss = 4.1778e-05, PNorm = 52.2115, GNorm = 0.1524, lr_0 = 2.1508e-04
Loss = 4.0023e-05, PNorm = 52.2141, GNorm = 0.1365, lr_0 = 2.1486e-04
Loss = 5.9720e-05, PNorm = 52.2142, GNorm = 0.1037, lr_0 = 2.1464e-04
Loss = 4.1922e-05, PNorm = 52.2167, GNorm = 0.1953, lr_0 = 2.1441e-04
Loss = 4.7531e-05, PNorm = 52.2208, GNorm = 0.2307, lr_0 = 2.1419e-04
Loss = 5.8050e-05, PNorm = 52.2211, GNorm = 0.4213, lr_0 = 2.1396e-04
Loss = 5.9842e-05, PNorm = 52.2231, GNorm = 0.2134, lr_0 = 2.1374e-04
Loss = 4.5000e-05, PNorm = 52.2258, GNorm = 0.1176, lr_0 = 2.1352e-04
Loss = 6.1836e-05, PNorm = 52.2288, GNorm = 0.2423, lr_0 = 2.1329e-04
Loss = 5.1566e-05, PNorm = 52.2327, GNorm = 0.1630, lr_0 = 2.1307e-04
Loss = 4.2497e-05, PNorm = 52.2361, GNorm = 0.1073, lr_0 = 2.1285e-04
Loss = 4.3475e-05, PNorm = 52.2367, GNorm = 0.1844, lr_0 = 2.1263e-04
Loss = 4.6135e-05, PNorm = 52.2365, GNorm = 0.1899, lr_0 = 2.1241e-04
Loss = 4.7614e-05, PNorm = 52.2391, GNorm = 0.1726, lr_0 = 2.1218e-04
Loss = 4.2808e-05, PNorm = 52.2423, GNorm = 0.1314, lr_0 = 2.1196e-04
Loss = 4.9389e-05, PNorm = 52.2419, GNorm = 0.1681, lr_0 = 2.1174e-04
Loss = 5.7692e-05, PNorm = 52.2447, GNorm = 0.4023, lr_0 = 2.1152e-04
Loss = 4.5358e-05, PNorm = 52.2467, GNorm = 0.1490, lr_0 = 2.1130e-04
Loss = 6.7173e-05, PNorm = 52.2501, GNorm = 0.1819, lr_0 = 2.1108e-04
Loss = 6.0593e-05, PNorm = 52.2537, GNorm = 0.2078, lr_0 = 2.1086e-04
Loss = 6.6964e-05, PNorm = 52.2573, GNorm = 0.2245, lr_0 = 2.1064e-04
Validation rmse logD = 0.558157
Validation R2 logD = 0.772497
Validation rmse logP = 0.451952
Validation R2 logP = 0.940739
Epoch 68
Train function
Loss = 7.7754e-05, PNorm = 52.2622, GNorm = 0.2573, lr_0 = 2.1040e-04
Loss = 8.3051e-05, PNorm = 52.2663, GNorm = 0.2373, lr_0 = 2.1018e-04
Loss = 6.5468e-05, PNorm = 52.2707, GNorm = 0.2105, lr_0 = 2.0996e-04
Loss = 6.5894e-05, PNorm = 52.2735, GNorm = 0.2653, lr_0 = 2.0974e-04
Loss = 5.9969e-05, PNorm = 52.2755, GNorm = 0.1180, lr_0 = 2.0952e-04
Loss = 4.9113e-05, PNorm = 52.2777, GNorm = 0.1694, lr_0 = 2.0930e-04
Loss = 4.2696e-05, PNorm = 52.2802, GNorm = 0.2405, lr_0 = 2.0908e-04
Loss = 3.6902e-05, PNorm = 52.2812, GNorm = 0.1090, lr_0 = 2.0886e-04
Loss = 3.2458e-05, PNorm = 52.2815, GNorm = 0.1238, lr_0 = 2.0865e-04
Loss = 4.6716e-05, PNorm = 52.2832, GNorm = 0.1531, lr_0 = 2.0843e-04
Loss = 5.1793e-05, PNorm = 52.2854, GNorm = 0.1936, lr_0 = 2.0821e-04
Loss = 5.8845e-05, PNorm = 52.2881, GNorm = 0.2423, lr_0 = 2.0799e-04
Loss = 5.2854e-05, PNorm = 52.2888, GNorm = 0.1413, lr_0 = 2.0778e-04
Loss = 4.6751e-05, PNorm = 52.2914, GNorm = 0.2771, lr_0 = 2.0756e-04
Loss = 5.0188e-05, PNorm = 52.2940, GNorm = 0.3565, lr_0 = 2.0734e-04
Loss = 4.5691e-05, PNorm = 52.2965, GNorm = 0.0879, lr_0 = 2.0713e-04
Loss = 4.9330e-05, PNorm = 52.2992, GNorm = 0.1464, lr_0 = 2.0691e-04
Loss = 5.2330e-05, PNorm = 52.3000, GNorm = 0.1347, lr_0 = 2.0669e-04
Loss = 4.3233e-05, PNorm = 52.3026, GNorm = 0.1726, lr_0 = 2.0648e-04
Loss = 3.7922e-05, PNorm = 52.3044, GNorm = 0.1839, lr_0 = 2.0626e-04
Loss = 4.2489e-05, PNorm = 52.3058, GNorm = 0.1347, lr_0 = 2.0605e-04
Loss = 5.0050e-05, PNorm = 52.3083, GNorm = 0.1888, lr_0 = 2.0583e-04
Validation rmse logD = 0.558444
Validation R2 logD = 0.772264
Validation rmse logP = 0.449128
Validation R2 logP = 0.941477
Epoch 69
Train function
Loss = 4.9195e-05, PNorm = 52.3090, GNorm = 0.1898, lr_0 = 2.0562e-04
Loss = 4.5268e-05, PNorm = 52.3111, GNorm = 0.2335, lr_0 = 2.0540e-04
Loss = 4.9304e-05, PNorm = 52.3121, GNorm = 0.1512, lr_0 = 2.0519e-04
Loss = 3.5384e-05, PNorm = 52.3123, GNorm = 0.1281, lr_0 = 2.0497e-04
Loss = 4.1398e-05, PNorm = 52.3136, GNorm = 0.0782, lr_0 = 2.0476e-04
Loss = 3.1637e-05, PNorm = 52.3148, GNorm = 0.1274, lr_0 = 2.0455e-04
Loss = 4.7755e-05, PNorm = 52.3171, GNorm = 0.1759, lr_0 = 2.0433e-04
Loss = 3.5054e-05, PNorm = 52.3190, GNorm = 0.1251, lr_0 = 2.0412e-04
Loss = 4.6086e-05, PNorm = 52.3208, GNorm = 0.1475, lr_0 = 2.0391e-04
Loss = 3.0214e-05, PNorm = 52.3233, GNorm = 0.1159, lr_0 = 2.0369e-04
Loss = 3.0409e-05, PNorm = 52.3253, GNorm = 0.1166, lr_0 = 2.0348e-04
Loss = 4.2214e-05, PNorm = 52.3270, GNorm = 0.1991, lr_0 = 2.0327e-04
Loss = 3.3316e-05, PNorm = 52.3286, GNorm = 0.1973, lr_0 = 2.0306e-04
Loss = 3.8125e-05, PNorm = 52.3288, GNorm = 0.1297, lr_0 = 2.0285e-04
Loss = 3.6930e-05, PNorm = 52.3300, GNorm = 0.0923, lr_0 = 2.0263e-04
Loss = 3.4511e-05, PNorm = 52.3320, GNorm = 0.1578, lr_0 = 2.0242e-04
Loss = 3.8338e-05, PNorm = 52.3330, GNorm = 0.1545, lr_0 = 2.0221e-04
Loss = 4.5556e-05, PNorm = 52.3331, GNorm = 0.1539, lr_0 = 2.0200e-04
Loss = 4.0951e-05, PNorm = 52.3351, GNorm = 0.1832, lr_0 = 2.0179e-04
Loss = 5.3083e-05, PNorm = 52.3382, GNorm = 0.1235, lr_0 = 2.0158e-04
Loss = 4.4860e-05, PNorm = 52.3413, GNorm = 0.2187, lr_0 = 2.0137e-04
Loss = 4.2131e-05, PNorm = 52.3450, GNorm = 0.1385, lr_0 = 2.0116e-04
Loss = 3.6212e-05, PNorm = 52.3481, GNorm = 0.1277, lr_0 = 2.0095e-04
Validation rmse logD = 0.556934
Validation R2 logD = 0.773493
Validation rmse logP = 0.451213
Validation R2 logP = 0.940932
Epoch 70
Train function
Loss = 3.6521e-05, PNorm = 52.3505, GNorm = 0.1702, lr_0 = 2.0072e-04
Loss = 3.9352e-05, PNorm = 52.3516, GNorm = 0.2059, lr_0 = 2.0051e-04
Loss = 5.3110e-05, PNorm = 52.3519, GNorm = 0.1577, lr_0 = 2.0030e-04
Loss = 4.6329e-05, PNorm = 52.3541, GNorm = 0.2508, lr_0 = 2.0009e-04
Loss = 4.8156e-05, PNorm = 52.3564, GNorm = 0.1746, lr_0 = 1.9988e-04
Loss = 3.8387e-05, PNorm = 52.3595, GNorm = 0.1839, lr_0 = 1.9967e-04
Loss = 3.2945e-05, PNorm = 52.3607, GNorm = 0.1503, lr_0 = 1.9946e-04
Loss = 3.4920e-05, PNorm = 52.3632, GNorm = 0.2180, lr_0 = 1.9926e-04
Loss = 3.8256e-05, PNorm = 52.3643, GNorm = 0.1670, lr_0 = 1.9905e-04
Loss = 3.7856e-05, PNorm = 52.3657, GNorm = 0.1752, lr_0 = 1.9884e-04
Loss = 3.2925e-05, PNorm = 52.3679, GNorm = 0.1293, lr_0 = 1.9863e-04
Loss = 3.3746e-05, PNorm = 52.3699, GNorm = 0.1047, lr_0 = 1.9842e-04
Loss = 3.9546e-05, PNorm = 52.3716, GNorm = 0.1118, lr_0 = 1.9822e-04
Loss = 3.2518e-05, PNorm = 52.3733, GNorm = 0.1001, lr_0 = 1.9801e-04
Loss = 3.6542e-05, PNorm = 52.3727, GNorm = 0.1151, lr_0 = 1.9780e-04
Loss = 3.9442e-05, PNorm = 52.3739, GNorm = 0.1275, lr_0 = 1.9760e-04
Loss = 4.1404e-05, PNorm = 52.3759, GNorm = 0.1667, lr_0 = 1.9739e-04
Loss = 2.9651e-05, PNorm = 52.3775, GNorm = 0.1270, lr_0 = 1.9719e-04
Loss = 4.0526e-05, PNorm = 52.3793, GNorm = 0.0659, lr_0 = 1.9698e-04
Loss = 3.3367e-05, PNorm = 52.3795, GNorm = 0.0995, lr_0 = 1.9677e-04
Loss = 3.6510e-05, PNorm = 52.3796, GNorm = 0.1364, lr_0 = 1.9657e-04
Loss = 4.3987e-05, PNorm = 52.3817, GNorm = 0.1658, lr_0 = 1.9636e-04
Validation rmse logD = 0.558892
Validation R2 logD = 0.771898
Validation rmse logP = 0.449142
Validation R2 logP = 0.941474
Epoch 71
Train function
Loss = 2.0968e-05, PNorm = 52.3839, GNorm = 0.0883, lr_0 = 1.9616e-04
Loss = 4.0743e-05, PNorm = 52.3856, GNorm = 0.1774, lr_0 = 1.9595e-04
Loss = 4.8522e-05, PNorm = 52.3885, GNorm = 0.2342, lr_0 = 1.9575e-04
Loss = 3.8168e-05, PNorm = 52.3910, GNorm = 0.1085, lr_0 = 1.9555e-04
Loss = 4.2715e-05, PNorm = 52.3930, GNorm = 0.3037, lr_0 = 1.9534e-04
Loss = 4.4687e-05, PNorm = 52.3948, GNorm = 0.2209, lr_0 = 1.9514e-04
Loss = 3.7016e-05, PNorm = 52.3964, GNorm = 0.1783, lr_0 = 1.9493e-04
Loss = 3.5580e-05, PNorm = 52.3982, GNorm = 0.1462, lr_0 = 1.9473e-04
Loss = 3.2194e-05, PNorm = 52.3987, GNorm = 0.1425, lr_0 = 1.9453e-04
Loss = 3.0009e-05, PNorm = 52.4011, GNorm = 0.1370, lr_0 = 1.9432e-04
Loss = 2.9142e-05, PNorm = 52.4034, GNorm = 0.1773, lr_0 = 1.9412e-04
Loss = 4.3231e-05, PNorm = 52.4048, GNorm = 0.1736, lr_0 = 1.9392e-04
Loss = 3.7845e-05, PNorm = 52.4063, GNorm = 0.1713, lr_0 = 1.9372e-04
Loss = 2.7362e-05, PNorm = 52.4071, GNorm = 0.1421, lr_0 = 1.9351e-04
Loss = 2.8197e-05, PNorm = 52.4081, GNorm = 0.1037, lr_0 = 1.9331e-04
Loss = 3.7821e-05, PNorm = 52.4090, GNorm = 0.1125, lr_0 = 1.9311e-04
Loss = 3.2645e-05, PNorm = 52.4104, GNorm = 0.1371, lr_0 = 1.9291e-04
Loss = 3.1463e-05, PNorm = 52.4120, GNorm = 0.1225, lr_0 = 1.9271e-04
Loss = 3.2094e-05, PNorm = 52.4140, GNorm = 0.1733, lr_0 = 1.9251e-04
Loss = 3.2487e-05, PNorm = 52.4141, GNorm = 0.0826, lr_0 = 1.9231e-04
Loss = 3.1448e-05, PNorm = 52.4158, GNorm = 0.1177, lr_0 = 1.9210e-04
Loss = 3.4573e-05, PNorm = 52.4182, GNorm = 0.1328, lr_0 = 1.9190e-04
Loss = 2.6739e-05, PNorm = 52.4198, GNorm = 0.1096, lr_0 = 1.9170e-04
Validation rmse logD = 0.557670
Validation R2 logD = 0.772895
Validation rmse logP = 0.447105
Validation R2 logP = 0.942003
Epoch 72
Train function
Loss = 2.7700e-05, PNorm = 52.4225, GNorm = 0.0820, lr_0 = 1.9148e-04
Loss = 2.5385e-05, PNorm = 52.4240, GNorm = 0.0793, lr_0 = 1.9128e-04
Loss = 3.0328e-05, PNorm = 52.4256, GNorm = 0.1161, lr_0 = 1.9108e-04
Loss = 2.9861e-05, PNorm = 52.4283, GNorm = 0.1227, lr_0 = 1.9088e-04
Loss = 2.6741e-05, PNorm = 52.4303, GNorm = 0.1168, lr_0 = 1.9069e-04
Loss = 2.8836e-05, PNorm = 52.4310, GNorm = 0.1215, lr_0 = 1.9049e-04
Loss = 3.3940e-05, PNorm = 52.4330, GNorm = 0.1255, lr_0 = 1.9029e-04
Loss = 3.3733e-05, PNorm = 52.4329, GNorm = 0.1175, lr_0 = 1.9009e-04
Loss = 3.2879e-05, PNorm = 52.4345, GNorm = 0.0881, lr_0 = 1.8989e-04
Loss = 2.8903e-05, PNorm = 52.4349, GNorm = 0.1954, lr_0 = 1.8969e-04
Loss = 3.3892e-05, PNorm = 52.4394, GNorm = 0.1896, lr_0 = 1.8949e-04
Loss = 3.0129e-05, PNorm = 52.4414, GNorm = 0.1035, lr_0 = 1.8930e-04
Loss = 2.8308e-05, PNorm = 52.4431, GNorm = 0.1786, lr_0 = 1.8910e-04
Loss = 3.5947e-05, PNorm = 52.4447, GNorm = 0.1116, lr_0 = 1.8890e-04
Loss = 3.1014e-05, PNorm = 52.4465, GNorm = 0.0956, lr_0 = 1.8870e-04
Loss = 2.2874e-05, PNorm = 52.4483, GNorm = 0.0852, lr_0 = 1.8851e-04
Loss = 3.2287e-05, PNorm = 52.4493, GNorm = 0.1283, lr_0 = 1.8831e-04
Loss = 3.6726e-05, PNorm = 52.4519, GNorm = 0.1063, lr_0 = 1.8811e-04
Loss = 3.1834e-05, PNorm = 52.4530, GNorm = 0.1068, lr_0 = 1.8792e-04
Loss = 3.2707e-05, PNorm = 52.4542, GNorm = 0.0837, lr_0 = 1.8772e-04
Loss = 3.0568e-05, PNorm = 52.4549, GNorm = 0.1209, lr_0 = 1.8753e-04
Loss = 3.2778e-05, PNorm = 52.4561, GNorm = 0.2109, lr_0 = 1.8733e-04
Loss = 3.5258e-05, PNorm = 52.4569, GNorm = 0.1858, lr_0 = 1.8713e-04
Validation rmse logD = 0.557110
Validation R2 logD = 0.773350
Validation rmse logP = 0.451987
Validation R2 logP = 0.940730
Epoch 73
Train function
Loss = 3.3599e-05, PNorm = 52.4596, GNorm = 0.2508, lr_0 = 1.8694e-04
Loss = 3.9179e-05, PNorm = 52.4613, GNorm = 0.2172, lr_0 = 1.8674e-04
Loss = 3.1022e-05, PNorm = 52.4614, GNorm = 0.1430, lr_0 = 1.8655e-04
Loss = 3.7966e-05, PNorm = 52.4633, GNorm = 0.1454, lr_0 = 1.8635e-04
Loss = 5.2155e-05, PNorm = 52.4649, GNorm = 0.2504, lr_0 = 1.8616e-04
Loss = 4.0253e-05, PNorm = 52.4674, GNorm = 0.2426, lr_0 = 1.8597e-04
Loss = 3.9814e-05, PNorm = 52.4709, GNorm = 0.1216, lr_0 = 1.8577e-04
Loss = 4.1186e-05, PNorm = 52.4731, GNorm = 0.2168, lr_0 = 1.8558e-04
Loss = 3.8335e-05, PNorm = 52.4745, GNorm = 0.1666, lr_0 = 1.8538e-04
Loss = 4.5072e-05, PNorm = 52.4755, GNorm = 0.1800, lr_0 = 1.8519e-04
Loss = 3.5886e-05, PNorm = 52.4765, GNorm = 0.1756, lr_0 = 1.8500e-04
Loss = 3.4535e-05, PNorm = 52.4785, GNorm = 0.1808, lr_0 = 1.8480e-04
Loss = 3.4241e-05, PNorm = 52.4800, GNorm = 0.0930, lr_0 = 1.8461e-04
Loss = 3.8260e-05, PNorm = 52.4819, GNorm = 0.1375, lr_0 = 1.8442e-04
Loss = 5.2190e-05, PNorm = 52.4835, GNorm = 0.1372, lr_0 = 1.8423e-04
Loss = 4.4529e-05, PNorm = 52.4848, GNorm = 0.2095, lr_0 = 1.8403e-04
Loss = 3.9766e-05, PNorm = 52.4867, GNorm = 0.0850, lr_0 = 1.8384e-04
Loss = 4.5200e-05, PNorm = 52.4900, GNorm = 0.1691, lr_0 = 1.8365e-04
Loss = 3.1096e-05, PNorm = 52.4918, GNorm = 0.1234, lr_0 = 1.8346e-04
Loss = 3.8650e-05, PNorm = 52.4941, GNorm = 0.1721, lr_0 = 1.8327e-04
Loss = 3.6154e-05, PNorm = 52.4955, GNorm = 0.1912, lr_0 = 1.8308e-04
Loss = 3.5045e-05, PNorm = 52.4968, GNorm = 0.0831, lr_0 = 1.8288e-04
Validation rmse logD = 0.557022
Validation R2 logD = 0.773422
Validation rmse logP = 0.452402
Validation R2 logP = 0.940621
Epoch 74
Train function
Loss = 3.0289e-05, PNorm = 52.4990, GNorm = 0.1675, lr_0 = 1.8267e-04
Loss = 3.1905e-05, PNorm = 52.5005, GNorm = 0.1187, lr_0 = 1.8248e-04
Loss = 4.3863e-05, PNorm = 52.5018, GNorm = 0.2101, lr_0 = 1.8229e-04
Loss = 4.4238e-05, PNorm = 52.5038, GNorm = 0.3063, lr_0 = 1.8210e-04
Loss = 4.3739e-05, PNorm = 52.5040, GNorm = 0.1456, lr_0 = 1.8191e-04
Loss = 4.3627e-05, PNorm = 52.5055, GNorm = 0.1529, lr_0 = 1.8172e-04
Loss = 4.0486e-05, PNorm = 52.5073, GNorm = 0.1685, lr_0 = 1.8153e-04
Loss = 3.4502e-05, PNorm = 52.5088, GNorm = 0.1215, lr_0 = 1.8134e-04
Loss = 3.2835e-05, PNorm = 52.5093, GNorm = 0.1707, lr_0 = 1.8115e-04
Loss = 3.8428e-05, PNorm = 52.5105, GNorm = 0.2151, lr_0 = 1.8097e-04
Loss = 3.3717e-05, PNorm = 52.5126, GNorm = 0.1786, lr_0 = 1.8078e-04
Loss = 3.2394e-05, PNorm = 52.5138, GNorm = 0.1074, lr_0 = 1.8059e-04
Loss = 2.7588e-05, PNorm = 52.5150, GNorm = 0.1005, lr_0 = 1.8040e-04
Loss = 3.0254e-05, PNorm = 52.5172, GNorm = 0.1403, lr_0 = 1.8021e-04
Loss = 4.3316e-05, PNorm = 52.5181, GNorm = 0.1390, lr_0 = 1.8002e-04
Loss = 4.4290e-05, PNorm = 52.5200, GNorm = 0.1622, lr_0 = 1.7984e-04
Loss = 3.6288e-05, PNorm = 52.5234, GNorm = 0.2570, lr_0 = 1.7965e-04
Loss = 3.9284e-05, PNorm = 52.5254, GNorm = 0.1303, lr_0 = 1.7946e-04
Loss = 4.2539e-05, PNorm = 52.5289, GNorm = 0.2021, lr_0 = 1.7927e-04
Loss = 4.4572e-05, PNorm = 52.5319, GNorm = 0.1354, lr_0 = 1.7909e-04
Loss = 4.3711e-05, PNorm = 52.5332, GNorm = 0.1967, lr_0 = 1.7890e-04
Loss = 3.0886e-05, PNorm = 52.5347, GNorm = 0.0674, lr_0 = 1.7871e-04
Loss = 3.9185e-05, PNorm = 52.5350, GNorm = 0.1753, lr_0 = 1.7853e-04
Validation rmse logD = 0.559310
Validation R2 logD = 0.771557
Validation rmse logP = 0.449824
Validation R2 logP = 0.941296
Epoch 75
Train function
Loss = 3.4959e-05, PNorm = 52.5356, GNorm = 0.1755, lr_0 = 1.7834e-04
Loss = 2.6988e-05, PNorm = 52.5374, GNorm = 0.0986, lr_0 = 1.7815e-04
Loss = 2.7169e-05, PNorm = 52.5380, GNorm = 0.0958, lr_0 = 1.7797e-04
Loss = 2.2298e-05, PNorm = 52.5377, GNorm = 0.0978, lr_0 = 1.7778e-04
Loss = 2.1874e-05, PNorm = 52.5396, GNorm = 0.0900, lr_0 = 1.7760e-04
Loss = 2.1383e-05, PNorm = 52.5412, GNorm = 0.0781, lr_0 = 1.7741e-04
Loss = 2.7759e-05, PNorm = 52.5428, GNorm = 0.1506, lr_0 = 1.7723e-04
Loss = 2.7619e-05, PNorm = 52.5443, GNorm = 0.0789, lr_0 = 1.7704e-04
Loss = 2.5565e-05, PNorm = 52.5463, GNorm = 0.1335, lr_0 = 1.7686e-04
Loss = 2.7459e-05, PNorm = 52.5465, GNorm = 0.1168, lr_0 = 1.7667e-04
Loss = 3.5715e-05, PNorm = 52.5460, GNorm = 0.1749, lr_0 = 1.7649e-04
Loss = 3.6848e-05, PNorm = 52.5476, GNorm = 0.1244, lr_0 = 1.7630e-04
Loss = 3.1763e-05, PNorm = 52.5484, GNorm = 0.1192, lr_0 = 1.7612e-04
Loss = 3.5654e-05, PNorm = 52.5507, GNorm = 0.1296, lr_0 = 1.7593e-04
Loss = 3.4993e-05, PNorm = 52.5510, GNorm = 0.1036, lr_0 = 1.7575e-04
Loss = 3.0316e-05, PNorm = 52.5524, GNorm = 0.0806, lr_0 = 1.7557e-04
Loss = 3.5710e-05, PNorm = 52.5533, GNorm = 0.2140, lr_0 = 1.7538e-04
Loss = 2.8278e-05, PNorm = 52.5557, GNorm = 0.0928, lr_0 = 1.7520e-04
Loss = 4.0204e-05, PNorm = 52.5581, GNorm = 0.0848, lr_0 = 1.7502e-04
Loss = 3.8771e-05, PNorm = 52.5593, GNorm = 0.1098, lr_0 = 1.7484e-04
Loss = 4.3432e-05, PNorm = 52.5618, GNorm = 0.1665, lr_0 = 1.7465e-04
Loss = 3.3144e-05, PNorm = 52.5639, GNorm = 0.0923, lr_0 = 1.7447e-04
Validation rmse logD = 0.557636
Validation R2 logD = 0.772922
Validation rmse logP = 0.453123
Validation R2 logP = 0.940431
Epoch 76
Train function
Loss = 3.0956e-05, PNorm = 52.5655, GNorm = 0.1795, lr_0 = 1.7427e-04
Loss = 2.9597e-05, PNorm = 52.5664, GNorm = 0.1522, lr_0 = 1.7409e-04
Loss = 3.3350e-05, PNorm = 52.5679, GNorm = 0.1944, lr_0 = 1.7391e-04
Loss = 2.7451e-05, PNorm = 52.5702, GNorm = 0.1510, lr_0 = 1.7373e-04
Loss = 2.6346e-05, PNorm = 52.5705, GNorm = 0.0844, lr_0 = 1.7354e-04
Loss = 2.9092e-05, PNorm = 52.5723, GNorm = 0.1645, lr_0 = 1.7336e-04
Loss = 3.0705e-05, PNorm = 52.5740, GNorm = 0.1128, lr_0 = 1.7318e-04
Loss = 3.5143e-05, PNorm = 52.5752, GNorm = 0.1050, lr_0 = 1.7300e-04
Loss = 2.5876e-05, PNorm = 52.5763, GNorm = 0.1153, lr_0 = 1.7282e-04
Loss = 2.3931e-05, PNorm = 52.5763, GNorm = 0.1369, lr_0 = 1.7264e-04
Loss = 2.9928e-05, PNorm = 52.5779, GNorm = 0.1539, lr_0 = 1.7246e-04
Loss = 4.4886e-05, PNorm = 52.5798, GNorm = 0.2828, lr_0 = 1.7228e-04
Loss = 3.4620e-05, PNorm = 52.5805, GNorm = 0.1173, lr_0 = 1.7210e-04
Loss = 3.3696e-05, PNorm = 52.5819, GNorm = 0.1426, lr_0 = 1.7192e-04
Loss = 3.2302e-05, PNorm = 52.5828, GNorm = 0.1222, lr_0 = 1.7174e-04
Loss = 3.1946e-05, PNorm = 52.5845, GNorm = 0.1246, lr_0 = 1.7156e-04
Loss = 3.3835e-05, PNorm = 52.5870, GNorm = 0.2297, lr_0 = 1.7138e-04
Loss = 2.6283e-05, PNorm = 52.5888, GNorm = 0.1044, lr_0 = 1.7120e-04
Loss = 3.4270e-05, PNorm = 52.5895, GNorm = 0.1326, lr_0 = 1.7103e-04
Loss = 2.9945e-05, PNorm = 52.5915, GNorm = 0.0957, lr_0 = 1.7085e-04
Loss = 2.9329e-05, PNorm = 52.5927, GNorm = 0.1921, lr_0 = 1.7067e-04
Loss = 3.1531e-05, PNorm = 52.5928, GNorm = 0.1546, lr_0 = 1.7049e-04
Loss = 2.8058e-05, PNorm = 52.5947, GNorm = 0.0901, lr_0 = 1.7031e-04
Validation rmse logD = 0.556433
Validation R2 logD = 0.773900
Validation rmse logP = 0.452974
Validation R2 logP = 0.940471
Epoch 77
Train function
Loss = 3.4499e-05, PNorm = 52.5960, GNorm = 0.2122, lr_0 = 1.7012e-04
Loss = 2.8198e-05, PNorm = 52.5980, GNorm = 0.1434, lr_0 = 1.6994e-04
Loss = 2.3582e-05, PNorm = 52.6006, GNorm = 0.1088, lr_0 = 1.6976e-04
Loss = 2.0106e-05, PNorm = 52.6013, GNorm = 0.0625, lr_0 = 1.6959e-04
Loss = 2.0716e-05, PNorm = 52.6017, GNorm = 0.1282, lr_0 = 1.6941e-04
Loss = 2.4502e-05, PNorm = 52.6027, GNorm = 0.1093, lr_0 = 1.6923e-04
Loss = 2.0030e-05, PNorm = 52.6031, GNorm = 0.0628, lr_0 = 1.6905e-04
Loss = 2.3538e-05, PNorm = 52.6043, GNorm = 0.0754, lr_0 = 1.6888e-04
Loss = 2.3271e-05, PNorm = 52.6049, GNorm = 0.0951, lr_0 = 1.6870e-04
Loss = 2.5039e-05, PNorm = 52.6065, GNorm = 0.0657, lr_0 = 1.6853e-04
Loss = 3.1847e-05, PNorm = 52.6076, GNorm = 0.1858, lr_0 = 1.6835e-04
Loss = 2.6060e-05, PNorm = 52.6082, GNorm = 0.1138, lr_0 = 1.6817e-04
Loss = 2.6197e-05, PNorm = 52.6103, GNorm = 0.0944, lr_0 = 1.6800e-04
Loss = 2.1944e-05, PNorm = 52.6119, GNorm = 0.0941, lr_0 = 1.6782e-04
Loss = 2.1815e-05, PNorm = 52.6139, GNorm = 0.0873, lr_0 = 1.6765e-04
Loss = 3.0156e-05, PNorm = 52.6159, GNorm = 0.1918, lr_0 = 1.6747e-04
Loss = 2.5502e-05, PNorm = 52.6158, GNorm = 0.1048, lr_0 = 1.6730e-04
Loss = 2.9176e-05, PNorm = 52.6169, GNorm = 0.0986, lr_0 = 1.6712e-04
Loss = 3.5874e-05, PNorm = 52.6181, GNorm = 0.0753, lr_0 = 1.6695e-04
Loss = 4.5833e-05, PNorm = 52.6191, GNorm = 0.2817, lr_0 = 1.6678e-04
Loss = 4.3763e-05, PNorm = 52.6202, GNorm = 0.1541, lr_0 = 1.6660e-04
Loss = 4.6043e-05, PNorm = 52.6233, GNorm = 0.2019, lr_0 = 1.6643e-04
Validation rmse logD = 0.555878
Validation R2 logD = 0.774351
Validation rmse logP = 0.450397
Validation R2 logP = 0.941146
Epoch 78
Train function
Loss = 4.7440e-05, PNorm = 52.6249, GNorm = 0.2398, lr_0 = 1.6625e-04
Loss = 3.0888e-05, PNorm = 52.6265, GNorm = 0.1608, lr_0 = 1.6608e-04
Loss = 3.0792e-05, PNorm = 52.6273, GNorm = 0.1260, lr_0 = 1.6591e-04
Loss = 2.9339e-05, PNorm = 52.6295, GNorm = 0.0647, lr_0 = 1.6573e-04
Loss = 3.0073e-05, PNorm = 52.6310, GNorm = 0.1108, lr_0 = 1.6556e-04
Loss = 2.7761e-05, PNorm = 52.6333, GNorm = 0.1077, lr_0 = 1.6539e-04
Loss = 2.7439e-05, PNorm = 52.6353, GNorm = 0.0926, lr_0 = 1.6522e-04
Loss = 2.4983e-05, PNorm = 52.6354, GNorm = 0.1115, lr_0 = 1.6504e-04
Loss = 2.4549e-05, PNorm = 52.6363, GNorm = 0.0805, lr_0 = 1.6487e-04
Loss = 2.3561e-05, PNorm = 52.6374, GNorm = 0.1602, lr_0 = 1.6470e-04
Loss = 2.4896e-05, PNorm = 52.6386, GNorm = 0.1325, lr_0 = 1.6453e-04
Loss = 2.4886e-05, PNorm = 52.6400, GNorm = 0.2197, lr_0 = 1.6435e-04
Loss = 3.2893e-05, PNorm = 52.6415, GNorm = 0.1359, lr_0 = 1.6418e-04
Loss = 2.4757e-05, PNorm = 52.6435, GNorm = 0.1588, lr_0 = 1.6401e-04
Loss = 2.1828e-05, PNorm = 52.6450, GNorm = 0.1090, lr_0 = 1.6384e-04
Loss = 3.9151e-05, PNorm = 52.6460, GNorm = 0.1711, lr_0 = 1.6367e-04
Loss = 3.0896e-05, PNorm = 52.6464, GNorm = 0.1227, lr_0 = 1.6350e-04
Loss = 3.0355e-05, PNorm = 52.6478, GNorm = 0.0937, lr_0 = 1.6333e-04
Loss = 2.7347e-05, PNorm = 52.6486, GNorm = 0.1080, lr_0 = 1.6316e-04
Loss = 3.0434e-05, PNorm = 52.6509, GNorm = 0.0918, lr_0 = 1.6299e-04
Loss = 3.5250e-05, PNorm = 52.6516, GNorm = 0.1857, lr_0 = 1.6282e-04
Loss = 3.3203e-05, PNorm = 52.6535, GNorm = 0.0670, lr_0 = 1.6265e-04
Loss = 2.4935e-05, PNorm = 52.6554, GNorm = 0.1148, lr_0 = 1.6248e-04
Validation rmse logD = 0.557964
Validation R2 logD = 0.772655
Validation rmse logP = 0.451064
Validation R2 logP = 0.940972
Epoch 79
Train function
Loss = 2.2648e-05, PNorm = 52.6584, GNorm = 0.1884, lr_0 = 1.6229e-04
Loss = 2.1759e-05, PNorm = 52.6603, GNorm = 0.1318, lr_0 = 1.6212e-04
Loss = 2.1917e-05, PNorm = 52.6618, GNorm = 0.1018, lr_0 = 1.6195e-04
Loss = 2.4939e-05, PNorm = 52.6635, GNorm = 0.1584, lr_0 = 1.6178e-04
Loss = 2.5025e-05, PNorm = 52.6646, GNorm = 0.1013, lr_0 = 1.6161e-04
Loss = 2.8310e-05, PNorm = 52.6653, GNorm = 0.1169, lr_0 = 1.6145e-04
Loss = 2.9681e-05, PNorm = 52.6663, GNorm = 0.1149, lr_0 = 1.6128e-04
Loss = 2.2073e-05, PNorm = 52.6671, GNorm = 0.1751, lr_0 = 1.6111e-04
Loss = 3.3229e-05, PNorm = 52.6673, GNorm = 0.2315, lr_0 = 1.6094e-04
Loss = 2.4849e-05, PNorm = 52.6682, GNorm = 0.0976, lr_0 = 1.6077e-04
Loss = 2.7188e-05, PNorm = 52.6690, GNorm = 0.1743, lr_0 = 1.6061e-04
Loss = 3.1825e-05, PNorm = 52.6704, GNorm = 0.0991, lr_0 = 1.6044e-04
Loss = 3.4035e-05, PNorm = 52.6712, GNorm = 0.1314, lr_0 = 1.6027e-04
Loss = 2.6684e-05, PNorm = 52.6732, GNorm = 0.0655, lr_0 = 1.6010e-04
Loss = 2.3175e-05, PNorm = 52.6748, GNorm = 0.1469, lr_0 = 1.5994e-04
Loss = 2.2746e-05, PNorm = 52.6757, GNorm = 0.1185, lr_0 = 1.5977e-04
Loss = 3.0253e-05, PNorm = 52.6768, GNorm = 0.0684, lr_0 = 1.5960e-04
Loss = 2.4448e-05, PNorm = 52.6780, GNorm = 0.0797, lr_0 = 1.5944e-04
Loss = 2.3708e-05, PNorm = 52.6791, GNorm = 0.0905, lr_0 = 1.5927e-04
Loss = 2.0854e-05, PNorm = 52.6800, GNorm = 0.1530, lr_0 = 1.5910e-04
Loss = 1.4349e-05, PNorm = 52.6814, GNorm = 0.0889, lr_0 = 1.5894e-04
Loss = 2.3918e-05, PNorm = 52.6819, GNorm = 0.1228, lr_0 = 1.5877e-04
Validation rmse logD = 0.556085
Validation R2 logD = 0.774184
Validation rmse logP = 0.449229
Validation R2 logP = 0.941451
Epoch 80
Train function
Loss = 2.0077e-05, PNorm = 52.6822, GNorm = 0.0817, lr_0 = 1.5861e-04
Loss = 2.2309e-05, PNorm = 52.6832, GNorm = 0.0697, lr_0 = 1.5844e-04
Loss = 1.9268e-05, PNorm = 52.6851, GNorm = 0.0813, lr_0 = 1.5827e-04
Loss = 1.5919e-05, PNorm = 52.6865, GNorm = 0.0879, lr_0 = 1.5811e-04
Loss = 1.8808e-05, PNorm = 52.6868, GNorm = 0.1046, lr_0 = 1.5794e-04
Loss = 2.2544e-05, PNorm = 52.6871, GNorm = 0.1173, lr_0 = 1.5778e-04
Loss = 2.4856e-05, PNorm = 52.6880, GNorm = 0.1870, lr_0 = 1.5761e-04
Loss = 3.0204e-05, PNorm = 52.6885, GNorm = 0.1708, lr_0 = 1.5745e-04
Loss = 2.7951e-05, PNorm = 52.6902, GNorm = 0.0947, lr_0 = 1.5729e-04
Loss = 2.1157e-05, PNorm = 52.6926, GNorm = 0.0977, lr_0 = 1.5712e-04
Loss = 2.2071e-05, PNorm = 52.6942, GNorm = 0.0940, lr_0 = 1.5696e-04
Loss = 1.8474e-05, PNorm = 52.6948, GNorm = 0.0931, lr_0 = 1.5679e-04
Loss = 2.1723e-05, PNorm = 52.6953, GNorm = 0.1064, lr_0 = 1.5663e-04
Loss = 2.0538e-05, PNorm = 52.6955, GNorm = 0.1136, lr_0 = 1.5647e-04
Loss = 2.1590e-05, PNorm = 52.6970, GNorm = 0.1406, lr_0 = 1.5630e-04
Loss = 2.0506e-05, PNorm = 52.6972, GNorm = 0.0939, lr_0 = 1.5614e-04
Loss = 2.3808e-05, PNorm = 52.6963, GNorm = 0.1177, lr_0 = 1.5598e-04
Loss = 2.0092e-05, PNorm = 52.6967, GNorm = 0.0959, lr_0 = 1.5581e-04
Loss = 2.1381e-05, PNorm = 52.6977, GNorm = 0.1032, lr_0 = 1.5565e-04
Loss = 2.2853e-05, PNorm = 52.6979, GNorm = 0.0873, lr_0 = 1.5549e-04
Loss = 1.6686e-05, PNorm = 52.6985, GNorm = 0.0661, lr_0 = 1.5533e-04
Loss = 2.4328e-05, PNorm = 52.6999, GNorm = 0.1124, lr_0 = 1.5516e-04
Loss = 1.9654e-05, PNorm = 52.7011, GNorm = 0.0731, lr_0 = 1.5500e-04
Validation rmse logD = 0.560613
Validation R2 logD = 0.770491
Validation rmse logP = 0.450501
Validation R2 logP = 0.941119
Epoch 81
Train function
Loss = 2.9379e-05, PNorm = 52.7014, GNorm = 0.1102, lr_0 = 1.5483e-04
Loss = 2.9490e-05, PNorm = 52.7045, GNorm = 0.1497, lr_0 = 1.5466e-04
Loss = 3.0775e-05, PNorm = 52.7054, GNorm = 0.1901, lr_0 = 1.5450e-04
Loss = 2.3758e-05, PNorm = 52.7071, GNorm = 0.2505, lr_0 = 1.5434e-04
Loss = 2.3127e-05, PNorm = 52.7090, GNorm = 0.1084, lr_0 = 1.5418e-04
Loss = 2.0202e-05, PNorm = 52.7104, GNorm = 0.1506, lr_0 = 1.5402e-04
Loss = 2.4153e-05, PNorm = 52.7123, GNorm = 0.1231, lr_0 = 1.5386e-04
Loss = 2.2473e-05, PNorm = 52.7146, GNorm = 0.1518, lr_0 = 1.5370e-04
Loss = 2.2283e-05, PNorm = 52.7162, GNorm = 0.1094, lr_0 = 1.5354e-04
Loss = 1.9957e-05, PNorm = 52.7176, GNorm = 0.1125, lr_0 = 1.5338e-04
Loss = 1.7330e-05, PNorm = 52.7190, GNorm = 0.1013, lr_0 = 1.5322e-04
Loss = 2.1251e-05, PNorm = 52.7198, GNorm = 0.0810, lr_0 = 1.5306e-04
Loss = 2.2377e-05, PNorm = 52.7206, GNorm = 0.1274, lr_0 = 1.5290e-04
Loss = 2.5614e-05, PNorm = 52.7209, GNorm = 0.1297, lr_0 = 1.5274e-04
Loss = 2.2463e-05, PNorm = 52.7227, GNorm = 0.0948, lr_0 = 1.5258e-04
Loss = 2.6423e-05, PNorm = 52.7232, GNorm = 0.0894, lr_0 = 1.5242e-04
Loss = 1.8261e-05, PNorm = 52.7243, GNorm = 0.0864, lr_0 = 1.5226e-04
Loss = 2.0833e-05, PNorm = 52.7248, GNorm = 0.0820, lr_0 = 1.5210e-04
Loss = 2.2188e-05, PNorm = 52.7254, GNorm = 0.0994, lr_0 = 1.5194e-04
Loss = 2.1707e-05, PNorm = 52.7282, GNorm = 0.1128, lr_0 = 1.5178e-04
Loss = 1.7700e-05, PNorm = 52.7294, GNorm = 0.0676, lr_0 = 1.5163e-04
Loss = 2.7705e-05, PNorm = 52.7295, GNorm = 0.1890, lr_0 = 1.5147e-04
Validation rmse logD = 0.557014
Validation R2 logD = 0.773428
Validation rmse logP = 0.450287
Validation R2 logP = 0.941175
Epoch 82
Train function
Loss = 2.7675e-05, PNorm = 52.7300, GNorm = 0.1583, lr_0 = 1.5131e-04
Loss = 2.5777e-05, PNorm = 52.7312, GNorm = 0.1887, lr_0 = 1.5115e-04
Loss = 2.5114e-05, PNorm = 52.7307, GNorm = 0.1956, lr_0 = 1.5099e-04
Loss = 3.1045e-05, PNorm = 52.7320, GNorm = 0.0904, lr_0 = 1.5084e-04
Loss = 2.4691e-05, PNorm = 52.7332, GNorm = 0.1007, lr_0 = 1.5068e-04
Loss = 2.0902e-05, PNorm = 52.7347, GNorm = 0.0871, lr_0 = 1.5052e-04
Loss = 2.3707e-05, PNorm = 52.7357, GNorm = 0.0953, lr_0 = 1.5036e-04
Loss = 1.7430e-05, PNorm = 52.7361, GNorm = 0.1078, lr_0 = 1.5021e-04
Loss = 2.0427e-05, PNorm = 52.7383, GNorm = 0.0842, lr_0 = 1.5005e-04
Loss = 2.2376e-05, PNorm = 52.7385, GNorm = 0.1517, lr_0 = 1.4989e-04
Loss = 2.0484e-05, PNorm = 52.7388, GNorm = 0.1050, lr_0 = 1.4974e-04
Loss = 1.9705e-05, PNorm = 52.7398, GNorm = 0.0905, lr_0 = 1.4958e-04
Loss = 2.1632e-05, PNorm = 52.7413, GNorm = 0.0915, lr_0 = 1.4942e-04
Loss = 2.5732e-05, PNorm = 52.7414, GNorm = 0.1987, lr_0 = 1.4927e-04
Loss = 2.6081e-05, PNorm = 52.7426, GNorm = 0.0965, lr_0 = 1.4911e-04
Loss = 2.1416e-05, PNorm = 52.7435, GNorm = 0.0863, lr_0 = 1.4896e-04
Loss = 2.3543e-05, PNorm = 52.7435, GNorm = 0.1035, lr_0 = 1.4880e-04
Loss = 1.8668e-05, PNorm = 52.7435, GNorm = 0.0630, lr_0 = 1.4865e-04
Loss = 2.3063e-05, PNorm = 52.7448, GNorm = 0.1174, lr_0 = 1.4849e-04
Loss = 2.3757e-05, PNorm = 52.7462, GNorm = 0.1796, lr_0 = 1.4834e-04
Loss = 1.9842e-05, PNorm = 52.7477, GNorm = 0.1113, lr_0 = 1.4818e-04
Loss = 2.2120e-05, PNorm = 52.7492, GNorm = 0.1510, lr_0 = 1.4803e-04
Loss = 2.3214e-05, PNorm = 52.7496, GNorm = 0.1857, lr_0 = 1.4787e-04
Validation rmse logD = 0.556679
Validation R2 logD = 0.773700
Validation rmse logP = 0.450214
Validation R2 logP = 0.941194
Epoch 83
Train function
Loss = 2.6351e-05, PNorm = 52.7504, GNorm = 0.1671, lr_0 = 1.4770e-04
Loss = 2.3736e-05, PNorm = 52.7524, GNorm = 0.0942, lr_0 = 1.4755e-04
Loss = 2.6409e-05, PNorm = 52.7534, GNorm = 0.1666, lr_0 = 1.4739e-04
Loss = 2.9360e-05, PNorm = 52.7545, GNorm = 0.1113, lr_0 = 1.4724e-04
Loss = 2.5915e-05, PNorm = 52.7555, GNorm = 0.1185, lr_0 = 1.4709e-04
Loss = 2.5297e-05, PNorm = 52.7571, GNorm = 0.1053, lr_0 = 1.4693e-04
Loss = 2.4912e-05, PNorm = 52.7583, GNorm = 0.0999, lr_0 = 1.4678e-04
Loss = 2.3379e-05, PNorm = 52.7596, GNorm = 0.1162, lr_0 = 1.4663e-04
Loss = 1.7600e-05, PNorm = 52.7610, GNorm = 0.1072, lr_0 = 1.4647e-04
Loss = 1.6762e-05, PNorm = 52.7621, GNorm = 0.1149, lr_0 = 1.4632e-04
Loss = 2.0396e-05, PNorm = 52.7618, GNorm = 0.1057, lr_0 = 1.4617e-04
Loss = 1.9294e-05, PNorm = 52.7619, GNorm = 0.1570, lr_0 = 1.4602e-04
Loss = 1.9698e-05, PNorm = 52.7635, GNorm = 0.1409, lr_0 = 1.4586e-04
Loss = 1.7270e-05, PNorm = 52.7651, GNorm = 0.1370, lr_0 = 1.4571e-04
Loss = 2.8349e-05, PNorm = 52.7665, GNorm = 0.2230, lr_0 = 1.4556e-04
Loss = 1.8372e-05, PNorm = 52.7675, GNorm = 0.1021, lr_0 = 1.4541e-04
Loss = 1.9103e-05, PNorm = 52.7690, GNorm = 0.0632, lr_0 = 1.4526e-04
Loss = 2.0126e-05, PNorm = 52.7703, GNorm = 0.0912, lr_0 = 1.4510e-04
Loss = 1.7748e-05, PNorm = 52.7708, GNorm = 0.0775, lr_0 = 1.4495e-04
Loss = 2.1830e-05, PNorm = 52.7719, GNorm = 0.0856, lr_0 = 1.4480e-04
Loss = 1.4654e-05, PNorm = 52.7729, GNorm = 0.0684, lr_0 = 1.4465e-04
Loss = 1.9677e-05, PNorm = 52.7734, GNorm = 0.1159, lr_0 = 1.4450e-04
Loss = 2.4551e-05, PNorm = 52.7748, GNorm = 0.0849, lr_0 = 1.4435e-04
Validation rmse logD = 0.557996
Validation R2 logD = 0.772629
Validation rmse logP = 0.449257
Validation R2 logP = 0.941444
Epoch 84
Train function
Loss = 1.8973e-05, PNorm = 52.7761, GNorm = 0.1223, lr_0 = 1.4420e-04
Loss = 2.1101e-05, PNorm = 52.7772, GNorm = 0.0754, lr_0 = 1.4405e-04
Loss = 2.9575e-05, PNorm = 52.7781, GNorm = 0.1350, lr_0 = 1.4390e-04
Loss = 2.3714e-05, PNorm = 52.7793, GNorm = 0.1510, lr_0 = 1.4375e-04
Loss = 1.9607e-05, PNorm = 52.7803, GNorm = 0.0968, lr_0 = 1.4360e-04
Loss = 2.3325e-05, PNorm = 52.7815, GNorm = 0.1044, lr_0 = 1.4345e-04
Loss = 2.5846e-05, PNorm = 52.7826, GNorm = 0.2016, lr_0 = 1.4330e-04
Loss = 2.0351e-05, PNorm = 52.7845, GNorm = 0.1146, lr_0 = 1.4315e-04
Loss = 2.0133e-05, PNorm = 52.7853, GNorm = 0.0574, lr_0 = 1.4300e-04
Loss = 2.0979e-05, PNorm = 52.7856, GNorm = 0.1227, lr_0 = 1.4285e-04
Loss = 1.6663e-05, PNorm = 52.7867, GNorm = 0.0823, lr_0 = 1.4270e-04
Loss = 2.1118e-05, PNorm = 52.7871, GNorm = 0.1706, lr_0 = 1.4255e-04
Loss = 2.1280e-05, PNorm = 52.7881, GNorm = 0.1416, lr_0 = 1.4240e-04
Loss = 2.2705e-05, PNorm = 52.7889, GNorm = 0.1418, lr_0 = 1.4225e-04
Loss = 2.4380e-05, PNorm = 52.7897, GNorm = 0.1161, lr_0 = 1.4210e-04
Loss = 2.5013e-05, PNorm = 52.7908, GNorm = 0.1400, lr_0 = 1.4196e-04
Loss = 3.9899e-05, PNorm = 52.7911, GNorm = 0.2278, lr_0 = 1.4181e-04
Loss = 3.3377e-05, PNorm = 52.7919, GNorm = 0.1705, lr_0 = 1.4166e-04
Loss = 2.6997e-05, PNorm = 52.7947, GNorm = 0.1019, lr_0 = 1.4151e-04
Loss = 2.4555e-05, PNorm = 52.7960, GNorm = 0.0918, lr_0 = 1.4136e-04
Loss = 2.6579e-05, PNorm = 52.7983, GNorm = 0.0853, lr_0 = 1.4122e-04
Loss = 2.3630e-05, PNorm = 52.7998, GNorm = 0.1030, lr_0 = 1.4107e-04
Validation rmse logD = 0.558942
Validation R2 logD = 0.771857
Validation rmse logP = 0.451618
Validation R2 logP = 0.940826
Epoch 85
Train function
Loss = 1.7242e-05, PNorm = 52.8001, GNorm = 0.1086, lr_0 = 1.4091e-04
Loss = 1.9307e-05, PNorm = 52.8016, GNorm = 0.1354, lr_0 = 1.4076e-04
Loss = 1.6808e-05, PNorm = 52.8029, GNorm = 0.0805, lr_0 = 1.4061e-04
Loss = 1.9981e-05, PNorm = 52.8038, GNorm = 0.0970, lr_0 = 1.4047e-04
Loss = 2.9148e-05, PNorm = 52.8048, GNorm = 0.1499, lr_0 = 1.4032e-04
Loss = 2.9094e-05, PNorm = 52.8070, GNorm = 0.1990, lr_0 = 1.4017e-04
Loss = 2.9979e-05, PNorm = 52.8095, GNorm = 0.1960, lr_0 = 1.4003e-04
Loss = 2.2583e-05, PNorm = 52.8096, GNorm = 0.1028, lr_0 = 1.3988e-04
Loss = 2.4552e-05, PNorm = 52.8105, GNorm = 0.1424, lr_0 = 1.3974e-04
Loss = 3.0910e-05, PNorm = 52.8113, GNorm = 0.1625, lr_0 = 1.3959e-04
Loss = 2.0741e-05, PNorm = 52.8108, GNorm = 0.1235, lr_0 = 1.3944e-04
Loss = 1.5960e-05, PNorm = 52.8119, GNorm = 0.0999, lr_0 = 1.3930e-04
Loss = 2.4790e-05, PNorm = 52.8126, GNorm = 0.1186, lr_0 = 1.3915e-04
Loss = 1.9192e-05, PNorm = 52.8133, GNorm = 0.1087, lr_0 = 1.3901e-04
Loss = 2.6073e-05, PNorm = 52.8132, GNorm = 0.0755, lr_0 = 1.3886e-04
Loss = 2.1113e-05, PNorm = 52.8145, GNorm = 0.1007, lr_0 = 1.3872e-04
Loss = 2.1291e-05, PNorm = 52.8155, GNorm = 0.1006, lr_0 = 1.3857e-04
Loss = 2.6525e-05, PNorm = 52.8166, GNorm = 0.1794, lr_0 = 1.3843e-04
Loss = 2.5777e-05, PNorm = 52.8186, GNorm = 0.1719, lr_0 = 1.3828e-04
Loss = 2.0901e-05, PNorm = 52.8202, GNorm = 0.0682, lr_0 = 1.3814e-04
Loss = 2.9019e-05, PNorm = 52.8214, GNorm = 0.1569, lr_0 = 1.3800e-04
Loss = 3.3177e-05, PNorm = 52.8218, GNorm = 0.1505, lr_0 = 1.3785e-04
Loss = 2.6929e-05, PNorm = 52.8234, GNorm = 0.1057, lr_0 = 1.3771e-04
Validation rmse logD = 0.556603
Validation R2 logD = 0.773763
Validation rmse logP = 0.449790
Validation R2 logP = 0.941305
Epoch 86
Train function
Loss = 2.1917e-05, PNorm = 52.8247, GNorm = 0.0830, lr_0 = 1.3756e-04
Loss = 2.1334e-05, PNorm = 52.8255, GNorm = 0.1206, lr_0 = 1.3742e-04
Loss = 1.7640e-05, PNorm = 52.8265, GNorm = 0.0718, lr_0 = 1.3728e-04
Loss = 2.0721e-05, PNorm = 52.8274, GNorm = 0.2149, lr_0 = 1.3713e-04
Loss = 2.3940e-05, PNorm = 52.8282, GNorm = 0.1026, lr_0 = 1.3699e-04
Loss = 1.9965e-05, PNorm = 52.8298, GNorm = 0.1202, lr_0 = 1.3685e-04
Loss = 1.9235e-05, PNorm = 52.8308, GNorm = 0.1192, lr_0 = 1.3670e-04
Loss = 1.8823e-05, PNorm = 52.8315, GNorm = 0.1061, lr_0 = 1.3656e-04
Loss = 2.1634e-05, PNorm = 52.8330, GNorm = 0.1410, lr_0 = 1.3642e-04
Loss = 1.6039e-05, PNorm = 52.8343, GNorm = 0.0539, lr_0 = 1.3628e-04
Loss = 1.8863e-05, PNorm = 52.8356, GNorm = 0.0744, lr_0 = 1.3613e-04
Loss = 1.8521e-05, PNorm = 52.8362, GNorm = 0.1116, lr_0 = 1.3599e-04
Loss = 2.0443e-05, PNorm = 52.8368, GNorm = 0.0846, lr_0 = 1.3585e-04
Loss = 1.8404e-05, PNorm = 52.8367, GNorm = 0.0994, lr_0 = 1.3571e-04
Loss = 1.6356e-05, PNorm = 52.8364, GNorm = 0.0965, lr_0 = 1.3557e-04
Loss = 1.7500e-05, PNorm = 52.8372, GNorm = 0.0430, lr_0 = 1.3543e-04
Loss = 2.2104e-05, PNorm = 52.8375, GNorm = 0.0837, lr_0 = 1.3528e-04
Loss = 1.6560e-05, PNorm = 52.8390, GNorm = 0.1291, lr_0 = 1.3514e-04
Loss = 1.6341e-05, PNorm = 52.8410, GNorm = 0.0809, lr_0 = 1.3500e-04
Loss = 2.0551e-05, PNorm = 52.8420, GNorm = 0.0686, lr_0 = 1.3486e-04
Loss = 2.8746e-05, PNorm = 52.8426, GNorm = 0.1263, lr_0 = 1.3472e-04
Loss = 2.5072e-05, PNorm = 52.8422, GNorm = 0.1587, lr_0 = 1.3458e-04
Validation rmse logD = 0.559283
Validation R2 logD = 0.771578
Validation rmse logP = 0.451019
Validation R2 logP = 0.940983
Epoch 87
Train function
Loss = 2.0326e-05, PNorm = 52.8410, GNorm = 0.0959, lr_0 = 1.3443e-04
Loss = 2.2921e-05, PNorm = 52.8408, GNorm = 0.1562, lr_0 = 1.3428e-04
Loss = 1.7184e-05, PNorm = 52.8421, GNorm = 0.1520, lr_0 = 1.3414e-04
Loss = 3.2977e-05, PNorm = 52.8435, GNorm = 0.0927, lr_0 = 1.3400e-04
Loss = 2.3170e-05, PNorm = 52.8442, GNorm = 0.1553, lr_0 = 1.3386e-04
Loss = 2.1636e-05, PNorm = 52.8458, GNorm = 0.1171, lr_0 = 1.3373e-04
Loss = 1.9897e-05, PNorm = 52.8476, GNorm = 0.1109, lr_0 = 1.3359e-04
Loss = 1.6284e-05, PNorm = 52.8491, GNorm = 0.0994, lr_0 = 1.3345e-04
Loss = 1.8260e-05, PNorm = 52.8496, GNorm = 0.1629, lr_0 = 1.3331e-04
Loss = 1.8259e-05, PNorm = 52.8504, GNorm = 0.0799, lr_0 = 1.3317e-04
Loss = 1.3600e-05, PNorm = 52.8515, GNorm = 0.0924, lr_0 = 1.3303e-04
Loss = 1.7337e-05, PNorm = 52.8525, GNorm = 0.0839, lr_0 = 1.3289e-04
Loss = 1.7821e-05, PNorm = 52.8537, GNorm = 0.0645, lr_0 = 1.3275e-04
Loss = 1.4579e-05, PNorm = 52.8549, GNorm = 0.1061, lr_0 = 1.3261e-04
Loss = 1.4889e-05, PNorm = 52.8561, GNorm = 0.0709, lr_0 = 1.3247e-04
Loss = 1.8326e-05, PNorm = 52.8574, GNorm = 0.0722, lr_0 = 1.3234e-04
Loss = 1.9499e-05, PNorm = 52.8577, GNorm = 0.1277, lr_0 = 1.3220e-04
Loss = 1.5507e-05, PNorm = 52.8580, GNorm = 0.0620, lr_0 = 1.3206e-04
Loss = 1.8794e-05, PNorm = 52.8578, GNorm = 0.1066, lr_0 = 1.3192e-04
Loss = 1.1852e-05, PNorm = 52.8577, GNorm = 0.0436, lr_0 = 1.3178e-04
Loss = 1.4149e-05, PNorm = 52.8585, GNorm = 0.0928, lr_0 = 1.3165e-04
Loss = 1.5391e-05, PNorm = 52.8595, GNorm = 0.0869, lr_0 = 1.3151e-04
Loss = 1.8041e-05, PNorm = 52.8596, GNorm = 0.0781, lr_0 = 1.3137e-04
Validation rmse logD = 0.556951
Validation R2 logD = 0.773480
Validation rmse logP = 0.450292
Validation R2 logP = 0.941174
Epoch 88
Train function
Loss = 1.4568e-05, PNorm = 52.8606, GNorm = 0.0916, lr_0 = 1.3124e-04
Loss = 1.4951e-05, PNorm = 52.8612, GNorm = 0.0685, lr_0 = 1.3110e-04
Loss = 1.2998e-05, PNorm = 52.8626, GNorm = 0.0748, lr_0 = 1.3096e-04
Loss = 1.2045e-05, PNorm = 52.8631, GNorm = 0.0608, lr_0 = 1.3082e-04
Loss = 1.5943e-05, PNorm = 52.8639, GNorm = 0.0759, lr_0 = 1.3069e-04
Loss = 1.2987e-05, PNorm = 52.8645, GNorm = 0.0743, lr_0 = 1.3055e-04
Loss = 1.4676e-05, PNorm = 52.8656, GNorm = 0.0755, lr_0 = 1.3042e-04
Loss = 1.5888e-05, PNorm = 52.8665, GNorm = 0.0924, lr_0 = 1.3028e-04
Loss = 1.2334e-05, PNorm = 52.8667, GNorm = 0.0927, lr_0 = 1.3014e-04
Loss = 1.6195e-05, PNorm = 52.8671, GNorm = 0.0922, lr_0 = 1.3001e-04
Loss = 1.4902e-05, PNorm = 52.8673, GNorm = 0.1178, lr_0 = 1.2987e-04
Loss = 2.0266e-05, PNorm = 52.8686, GNorm = 0.1110, lr_0 = 1.2974e-04
Loss = 2.2878e-05, PNorm = 52.8699, GNorm = 0.1288, lr_0 = 1.2960e-04
Loss = 2.4645e-05, PNorm = 52.8713, GNorm = 0.1811, lr_0 = 1.2947e-04
Loss = 2.5719e-05, PNorm = 52.8719, GNorm = 0.0953, lr_0 = 1.2933e-04
Loss = 1.9732e-05, PNorm = 52.8725, GNorm = 0.1955, lr_0 = 1.2920e-04
Loss = 2.4028e-05, PNorm = 52.8732, GNorm = 0.0680, lr_0 = 1.2906e-04
Loss = 2.2277e-05, PNorm = 52.8755, GNorm = 0.1299, lr_0 = 1.2893e-04
Loss = 2.1552e-05, PNorm = 52.8771, GNorm = 0.1332, lr_0 = 1.2879e-04
Loss = 2.0116e-05, PNorm = 52.8774, GNorm = 0.0951, lr_0 = 1.2866e-04
Loss = 2.3047e-05, PNorm = 52.8781, GNorm = 0.0739, lr_0 = 1.2852e-04
Loss = 2.0195e-05, PNorm = 52.8794, GNorm = 0.1163, lr_0 = 1.2839e-04
Validation rmse logD = 0.558665
Validation R2 logD = 0.772083
Validation rmse logP = 0.452350
Validation R2 logP = 0.940635
Epoch 89
Train function
Loss = 1.6051e-05, PNorm = 52.8795, GNorm = 0.0775, lr_0 = 1.2824e-04
Loss = 1.8815e-05, PNorm = 52.8808, GNorm = 0.0918, lr_0 = 1.2811e-04
Loss = 1.9284e-05, PNorm = 52.8821, GNorm = 0.0783, lr_0 = 1.2797e-04
Loss = 1.4885e-05, PNorm = 52.8834, GNorm = 0.0647, lr_0 = 1.2784e-04
Loss = 1.6342e-05, PNorm = 52.8830, GNorm = 0.0556, lr_0 = 1.2771e-04
Loss = 1.7289e-05, PNorm = 52.8839, GNorm = 0.0806, lr_0 = 1.2757e-04
Loss = 1.5774e-05, PNorm = 52.8844, GNorm = 0.0714, lr_0 = 1.2744e-04
Loss = 1.5824e-05, PNorm = 52.8856, GNorm = 0.0808, lr_0 = 1.2731e-04
Loss = 1.7622e-05, PNorm = 52.8871, GNorm = 0.0684, lr_0 = 1.2717e-04
Loss = 1.6282e-05, PNorm = 52.8882, GNorm = 0.0866, lr_0 = 1.2704e-04
Loss = 1.9983e-05, PNorm = 52.8890, GNorm = 0.1477, lr_0 = 1.2691e-04
Loss = 1.7383e-05, PNorm = 52.8887, GNorm = 0.1015, lr_0 = 1.2678e-04
Loss = 1.5058e-05, PNorm = 52.8891, GNorm = 0.0813, lr_0 = 1.2664e-04
Loss = 1.4430e-05, PNorm = 52.8898, GNorm = 0.0721, lr_0 = 1.2651e-04
Loss = 1.2823e-05, PNorm = 52.8907, GNorm = 0.0770, lr_0 = 1.2638e-04
Loss = 1.4710e-05, PNorm = 52.8915, GNorm = 0.0735, lr_0 = 1.2625e-04
Loss = 1.9550e-05, PNorm = 52.8915, GNorm = 0.0826, lr_0 = 1.2612e-04
Loss = 1.6011e-05, PNorm = 52.8920, GNorm = 0.1060, lr_0 = 1.2598e-04
Loss = 1.5193e-05, PNorm = 52.8925, GNorm = 0.1046, lr_0 = 1.2585e-04
Loss = 1.7146e-05, PNorm = 52.8935, GNorm = 0.0765, lr_0 = 1.2572e-04
Loss = 1.3563e-05, PNorm = 52.8933, GNorm = 0.1024, lr_0 = 1.2559e-04
Loss = 1.7658e-05, PNorm = 52.8935, GNorm = 0.1005, lr_0 = 1.2546e-04
Loss = 1.8007e-05, PNorm = 52.8941, GNorm = 0.0713, lr_0 = 1.2533e-04
Validation rmse logD = 0.556604
Validation R2 logD = 0.773762
Validation rmse logP = 0.450263
Validation R2 logP = 0.941181
Epoch 90
Train function
Loss = 1.0231e-05, PNorm = 52.8949, GNorm = 0.0799, lr_0 = 1.2520e-04
Loss = 1.5855e-05, PNorm = 52.8963, GNorm = 0.0458, lr_0 = 1.2507e-04
Loss = 1.2363e-05, PNorm = 52.8974, GNorm = 0.0746, lr_0 = 1.2494e-04
Loss = 1.3904e-05, PNorm = 52.8979, GNorm = 0.0539, lr_0 = 1.2481e-04
Loss = 1.4335e-05, PNorm = 52.8992, GNorm = 0.0970, lr_0 = 1.2468e-04
Loss = 1.2134e-05, PNorm = 52.8996, GNorm = 0.0677, lr_0 = 1.2455e-04
Loss = 1.5544e-05, PNorm = 52.9006, GNorm = 0.1830, lr_0 = 1.2442e-04
Loss = 1.4010e-05, PNorm = 52.9008, GNorm = 0.0716, lr_0 = 1.2429e-04
Loss = 2.2834e-05, PNorm = 52.9011, GNorm = 0.2325, lr_0 = 1.2416e-04
Loss = 1.5657e-05, PNorm = 52.9020, GNorm = 0.0881, lr_0 = 1.2403e-04
Loss = 2.0452e-05, PNorm = 52.9021, GNorm = 0.1036, lr_0 = 1.2390e-04
Loss = 1.4711e-05, PNorm = 52.9026, GNorm = 0.1051, lr_0 = 1.2377e-04
Loss = 1.6540e-05, PNorm = 52.9039, GNorm = 0.0762, lr_0 = 1.2364e-04
Loss = 1.2943e-05, PNorm = 52.9053, GNorm = 0.0999, lr_0 = 1.2351e-04
Loss = 1.8223e-05, PNorm = 52.9062, GNorm = 0.1203, lr_0 = 1.2338e-04
Loss = 1.3817e-05, PNorm = 52.9067, GNorm = 0.0732, lr_0 = 1.2325e-04
Loss = 1.2392e-05, PNorm = 52.9074, GNorm = 0.0619, lr_0 = 1.2312e-04
Loss = 1.2763e-05, PNorm = 52.9082, GNorm = 0.1005, lr_0 = 1.2299e-04
Loss = 1.0636e-05, PNorm = 52.9094, GNorm = 0.0882, lr_0 = 1.2287e-04
Loss = 1.5571e-05, PNorm = 52.9104, GNorm = 0.1213, lr_0 = 1.2274e-04
Loss = 1.5985e-05, PNorm = 52.9112, GNorm = 0.0931, lr_0 = 1.2261e-04
Loss = 1.2522e-05, PNorm = 52.9126, GNorm = 0.1067, lr_0 = 1.2248e-04
Validation rmse logD = 0.556653
Validation R2 logD = 0.773722
Validation rmse logP = 0.451396
Validation R2 logP = 0.940885
Epoch 91
Train function
Loss = 1.2221e-05, PNorm = 52.9131, GNorm = 0.0545, lr_0 = 1.2234e-04
Loss = 1.6895e-05, PNorm = 52.9136, GNorm = 0.0753, lr_0 = 1.2221e-04
Loss = 1.1569e-05, PNorm = 52.9142, GNorm = 0.1135, lr_0 = 1.2209e-04
Loss = 1.5566e-05, PNorm = 52.9146, GNorm = 0.1268, lr_0 = 1.2196e-04
Loss = 1.6769e-05, PNorm = 52.9150, GNorm = 0.1324, lr_0 = 1.2183e-04
Loss = 1.5470e-05, PNorm = 52.9149, GNorm = 0.0616, lr_0 = 1.2170e-04
Loss = 1.4205e-05, PNorm = 52.9156, GNorm = 0.1073, lr_0 = 1.2158e-04
Loss = 1.5827e-05, PNorm = 52.9164, GNorm = 0.0949, lr_0 = 1.2145e-04
Loss = 1.2662e-05, PNorm = 52.9167, GNorm = 0.0857, lr_0 = 1.2132e-04
Loss = 1.9051e-05, PNorm = 52.9176, GNorm = 0.1884, lr_0 = 1.2120e-04
Loss = 1.5410e-05, PNorm = 52.9189, GNorm = 0.1018, lr_0 = 1.2107e-04
Loss = 1.6426e-05, PNorm = 52.9185, GNorm = 0.0830, lr_0 = 1.2094e-04
Loss = 1.7255e-05, PNorm = 52.9194, GNorm = 0.1252, lr_0 = 1.2082e-04
Loss = 1.7796e-05, PNorm = 52.9210, GNorm = 0.0518, lr_0 = 1.2069e-04
Loss = 2.6520e-05, PNorm = 52.9229, GNorm = 0.1507, lr_0 = 1.2057e-04
Loss = 2.0757e-05, PNorm = 52.9249, GNorm = 0.1343, lr_0 = 1.2044e-04
Loss = 1.6136e-05, PNorm = 52.9261, GNorm = 0.0838, lr_0 = 1.2031e-04
Loss = 1.8141e-05, PNorm = 52.9270, GNorm = 0.1457, lr_0 = 1.2019e-04
Loss = 2.0332e-05, PNorm = 52.9277, GNorm = 0.0863, lr_0 = 1.2006e-04
Loss = 1.5965e-05, PNorm = 52.9280, GNorm = 0.0537, lr_0 = 1.1994e-04
Loss = 1.5772e-05, PNorm = 52.9291, GNorm = 0.1158, lr_0 = 1.1981e-04
Loss = 1.6255e-05, PNorm = 52.9314, GNorm = 0.1260, lr_0 = 1.1969e-04
Loss = 1.7089e-05, PNorm = 52.9338, GNorm = 0.1405, lr_0 = 1.1956e-04
Validation rmse logD = 0.558819
Validation R2 logD = 0.771958
Validation rmse logP = 0.450353
Validation R2 logP = 0.941157
Epoch 92
Train function
Loss = 1.6818e-05, PNorm = 52.9340, GNorm = 0.1345, lr_0 = 1.1944e-04
Loss = 1.3773e-05, PNorm = 52.9342, GNorm = 0.0725, lr_0 = 1.1931e-04
Loss = 1.3144e-05, PNorm = 52.9351, GNorm = 0.0658, lr_0 = 1.1919e-04
Loss = 1.9006e-05, PNorm = 52.9367, GNorm = 0.0973, lr_0 = 1.1906e-04
Loss = 1.8375e-05, PNorm = 52.9368, GNorm = 0.1039, lr_0 = 1.1894e-04
Loss = 1.4774e-05, PNorm = 52.9375, GNorm = 0.0642, lr_0 = 1.1882e-04
Loss = 1.0856e-05, PNorm = 52.9387, GNorm = 0.0896, lr_0 = 1.1869e-04
Loss = 1.4401e-05, PNorm = 52.9388, GNorm = 0.0543, lr_0 = 1.1857e-04
Loss = 1.7322e-05, PNorm = 52.9404, GNorm = 0.1148, lr_0 = 1.1844e-04
Loss = 1.3025e-05, PNorm = 52.9411, GNorm = 0.0679, lr_0 = 1.1832e-04
Loss = 1.5345e-05, PNorm = 52.9419, GNorm = 0.0590, lr_0 = 1.1820e-04
Loss = 1.2727e-05, PNorm = 52.9426, GNorm = 0.0795, lr_0 = 1.1807e-04
Loss = 1.2768e-05, PNorm = 52.9424, GNorm = 0.1258, lr_0 = 1.1795e-04
Loss = 1.5505e-05, PNorm = 52.9417, GNorm = 0.1106, lr_0 = 1.1783e-04
Loss = 1.7433e-05, PNorm = 52.9425, GNorm = 0.1055, lr_0 = 1.1770e-04
Loss = 1.0978e-05, PNorm = 52.9432, GNorm = 0.0500, lr_0 = 1.1758e-04
Loss = 1.4547e-05, PNorm = 52.9443, GNorm = 0.0843, lr_0 = 1.1746e-04
Loss = 1.3253e-05, PNorm = 52.9453, GNorm = 0.1302, lr_0 = 1.1734e-04
Loss = 1.3102e-05, PNorm = 52.9457, GNorm = 0.0644, lr_0 = 1.1721e-04
Loss = 1.5088e-05, PNorm = 52.9466, GNorm = 0.0678, lr_0 = 1.1709e-04
Loss = 1.6015e-05, PNorm = 52.9485, GNorm = 0.0694, lr_0 = 1.1697e-04
Loss = 1.3835e-05, PNorm = 52.9489, GNorm = 0.0624, lr_0 = 1.1685e-04
Validation rmse logD = 0.558220
Validation R2 logD = 0.772446
Validation rmse logP = 0.450851
Validation R2 logP = 0.941027
Epoch 93
Train function
Loss = 9.2089e-06, PNorm = 52.9502, GNorm = 0.0687, lr_0 = 1.1671e-04
Loss = 9.6178e-06, PNorm = 52.9504, GNorm = 0.0696, lr_0 = 1.1659e-04
Loss = 1.1708e-05, PNorm = 52.9503, GNorm = 0.1588, lr_0 = 1.1647e-04
Loss = 1.1575e-05, PNorm = 52.9505, GNorm = 0.0723, lr_0 = 1.1635e-04
Loss = 1.1299e-05, PNorm = 52.9511, GNorm = 0.1026, lr_0 = 1.1623e-04
Loss = 1.2495e-05, PNorm = 52.9517, GNorm = 0.0432, lr_0 = 1.1611e-04
Loss = 1.2736e-05, PNorm = 52.9522, GNorm = 0.0743, lr_0 = 1.1598e-04
Loss = 1.2117e-05, PNorm = 52.9525, GNorm = 0.0491, lr_0 = 1.1586e-04
Loss = 1.2112e-05, PNorm = 52.9535, GNorm = 0.0700, lr_0 = 1.1574e-04
Loss = 1.3402e-05, PNorm = 52.9548, GNorm = 0.0607, lr_0 = 1.1562e-04
Loss = 1.2084e-05, PNorm = 52.9553, GNorm = 0.0636, lr_0 = 1.1550e-04
Loss = 1.3950e-05, PNorm = 52.9556, GNorm = 0.0626, lr_0 = 1.1538e-04
Loss = 9.5452e-06, PNorm = 52.9559, GNorm = 0.0869, lr_0 = 1.1526e-04
Loss = 1.3623e-05, PNorm = 52.9567, GNorm = 0.0652, lr_0 = 1.1514e-04
Loss = 1.1608e-05, PNorm = 52.9575, GNorm = 0.0768, lr_0 = 1.1502e-04
Loss = 1.3721e-05, PNorm = 52.9588, GNorm = 0.0664, lr_0 = 1.1490e-04
Loss = 1.3359e-05, PNorm = 52.9595, GNorm = 0.0873, lr_0 = 1.1478e-04
Loss = 1.4054e-05, PNorm = 52.9611, GNorm = 0.0633, lr_0 = 1.1466e-04
Loss = 1.8144e-05, PNorm = 52.9618, GNorm = 0.0636, lr_0 = 1.1454e-04
Loss = 1.2932e-05, PNorm = 52.9621, GNorm = 0.1365, lr_0 = 1.1442e-04
Loss = 1.6418e-05, PNorm = 52.9628, GNorm = 0.1878, lr_0 = 1.1430e-04
Loss = 2.2140e-05, PNorm = 52.9645, GNorm = 0.1037, lr_0 = 1.1418e-04
Loss = 2.0643e-05, PNorm = 52.9652, GNorm = 0.0749, lr_0 = 1.1406e-04
Validation rmse logD = 0.557382
Validation R2 logD = 0.773129
Validation rmse logP = 0.451194
Validation R2 logP = 0.940938
Epoch 94
Train function
Loss = 1.1625e-05, PNorm = 52.9664, GNorm = 0.0916, lr_0 = 1.1394e-04
Loss = 1.3529e-05, PNorm = 52.9669, GNorm = 0.0966, lr_0 = 1.1382e-04
Loss = 1.0674e-05, PNorm = 52.9673, GNorm = 0.0682, lr_0 = 1.1371e-04
Loss = 1.4304e-05, PNorm = 52.9676, GNorm = 0.0974, lr_0 = 1.1359e-04
Loss = 1.5231e-05, PNorm = 52.9683, GNorm = 0.1158, lr_0 = 1.1347e-04
Loss = 1.4055e-05, PNorm = 52.9688, GNorm = 0.0814, lr_0 = 1.1335e-04
Loss = 1.0657e-05, PNorm = 52.9702, GNorm = 0.0623, lr_0 = 1.1323e-04
Loss = 1.2666e-05, PNorm = 52.9710, GNorm = 0.0483, lr_0 = 1.1311e-04
Loss = 1.0687e-05, PNorm = 52.9713, GNorm = 0.1140, lr_0 = 1.1300e-04
Loss = 1.8412e-05, PNorm = 52.9721, GNorm = 0.1410, lr_0 = 1.1288e-04
Loss = 1.2006e-05, PNorm = 52.9732, GNorm = 0.1124, lr_0 = 1.1276e-04
Loss = 1.2663e-05, PNorm = 52.9739, GNorm = 0.0559, lr_0 = 1.1264e-04
Loss = 1.3372e-05, PNorm = 52.9743, GNorm = 0.0521, lr_0 = 1.1252e-04
Loss = 1.0567e-05, PNorm = 52.9743, GNorm = 0.0899, lr_0 = 1.1241e-04
Loss = 1.6658e-05, PNorm = 52.9745, GNorm = 0.0713, lr_0 = 1.1229e-04
Loss = 1.1449e-05, PNorm = 52.9761, GNorm = 0.0692, lr_0 = 1.1217e-04
Loss = 1.1962e-05, PNorm = 52.9767, GNorm = 0.0813, lr_0 = 1.1206e-04
Loss = 1.3788e-05, PNorm = 52.9772, GNorm = 0.0750, lr_0 = 1.1194e-04
Loss = 1.0983e-05, PNorm = 52.9785, GNorm = 0.0908, lr_0 = 1.1182e-04
Loss = 1.6282e-05, PNorm = 52.9782, GNorm = 0.1016, lr_0 = 1.1170e-04
Loss = 1.1590e-05, PNorm = 52.9782, GNorm = 0.0549, lr_0 = 1.1159e-04
Loss = 1.2181e-05, PNorm = 52.9791, GNorm = 0.0586, lr_0 = 1.1147e-04
Loss = 1.2268e-05, PNorm = 52.9790, GNorm = 0.0941, lr_0 = 1.1136e-04
Loss = 3.1086e-05, PNorm = 52.9791, GNorm = 0.0930, lr_0 = 1.1134e-04
Validation rmse logD = 0.556494
Validation R2 logD = 0.773851
Validation rmse logP = 0.449285
Validation R2 logP = 0.941436
Epoch 95
Train function
Loss = 1.1017e-05, PNorm = 52.9796, GNorm = 0.0792, lr_0 = 1.1123e-04
Loss = 1.3158e-05, PNorm = 52.9800, GNorm = 0.0845, lr_0 = 1.1111e-04
Loss = 1.1995e-05, PNorm = 52.9809, GNorm = 0.0684, lr_0 = 1.1100e-04
Loss = 1.2113e-05, PNorm = 52.9818, GNorm = 0.1116, lr_0 = 1.1088e-04
Loss = 1.0591e-05, PNorm = 52.9816, GNorm = 0.0697, lr_0 = 1.1076e-04
Loss = 9.3906e-06, PNorm = 52.9818, GNorm = 0.0541, lr_0 = 1.1065e-04
Loss = 9.8205e-06, PNorm = 52.9823, GNorm = 0.0734, lr_0 = 1.1053e-04
Loss = 7.7537e-06, PNorm = 52.9822, GNorm = 0.0684, lr_0 = 1.1042e-04
Loss = 1.2628e-05, PNorm = 52.9825, GNorm = 0.1161, lr_0 = 1.1030e-04
Loss = 1.4606e-05, PNorm = 52.9825, GNorm = 0.0561, lr_0 = 1.1019e-04
Loss = 1.6756e-05, PNorm = 52.9832, GNorm = 0.0858, lr_0 = 1.1007e-04
Loss = 1.3116e-05, PNorm = 52.9838, GNorm = 0.0678, lr_0 = 1.0996e-04
Loss = 1.0683e-05, PNorm = 52.9845, GNorm = 0.0902, lr_0 = 1.0984e-04
Loss = 1.3300e-05, PNorm = 52.9856, GNorm = 0.0894, lr_0 = 1.0973e-04
Loss = 1.6180e-05, PNorm = 52.9863, GNorm = 0.0859, lr_0 = 1.0961e-04
Loss = 1.0391e-05, PNorm = 52.9870, GNorm = 0.0554, lr_0 = 1.0950e-04
Loss = 9.3281e-06, PNorm = 52.9878, GNorm = 0.0351, lr_0 = 1.0938e-04
Loss = 1.2824e-05, PNorm = 52.9881, GNorm = 0.0850, lr_0 = 1.0927e-04
Loss = 1.3272e-05, PNorm = 52.9887, GNorm = 0.1465, lr_0 = 1.0916e-04
Loss = 1.2722e-05, PNorm = 52.9892, GNorm = 0.0648, lr_0 = 1.0904e-04
Loss = 1.3908e-05, PNorm = 52.9900, GNorm = 0.0677, lr_0 = 1.0893e-04
Loss = 1.1680e-05, PNorm = 52.9912, GNorm = 0.0667, lr_0 = 1.0882e-04
Validation rmse logD = 0.558323
Validation R2 logD = 0.772363
Validation rmse logP = 0.451248
Validation R2 logP = 0.940924
Epoch 96
Train function
Loss = 9.6767e-06, PNorm = 52.9926, GNorm = 0.0512, lr_0 = 1.0870e-04
Loss = 1.2568e-05, PNorm = 52.9930, GNorm = 0.1188, lr_0 = 1.0859e-04
Loss = 1.2226e-05, PNorm = 52.9928, GNorm = 0.0943, lr_0 = 1.0847e-04
Loss = 1.3030e-05, PNorm = 52.9936, GNorm = 0.1094, lr_0 = 1.0836e-04
Loss = 1.2078e-05, PNorm = 52.9945, GNorm = 0.1133, lr_0 = 1.0825e-04
Loss = 9.2041e-06, PNorm = 52.9944, GNorm = 0.0741, lr_0 = 1.0814e-04
Loss = 1.0449e-05, PNorm = 52.9949, GNorm = 0.0778, lr_0 = 1.0802e-04
Loss = 1.1665e-05, PNorm = 52.9957, GNorm = 0.1020, lr_0 = 1.0791e-04
Loss = 1.3167e-05, PNorm = 52.9961, GNorm = 0.0739, lr_0 = 1.0780e-04
Loss = 1.2984e-05, PNorm = 52.9966, GNorm = 0.1197, lr_0 = 1.0768e-04
Loss = 1.2361e-05, PNorm = 52.9970, GNorm = 0.1349, lr_0 = 1.0757e-04
Loss = 1.0961e-05, PNorm = 52.9976, GNorm = 0.0522, lr_0 = 1.0746e-04
Loss = 1.0920e-05, PNorm = 52.9983, GNorm = 0.0569, lr_0 = 1.0735e-04
Loss = 1.2436e-05, PNorm = 52.9987, GNorm = 0.0931, lr_0 = 1.0724e-04
Loss = 1.5920e-05, PNorm = 52.9996, GNorm = 0.0926, lr_0 = 1.0712e-04
Loss = 1.5789e-05, PNorm = 53.0012, GNorm = 0.0707, lr_0 = 1.0701e-04
Loss = 1.2993e-05, PNorm = 53.0019, GNorm = 0.0972, lr_0 = 1.0690e-04
Loss = 1.3554e-05, PNorm = 53.0016, GNorm = 0.1310, lr_0 = 1.0679e-04
Loss = 1.4182e-05, PNorm = 53.0024, GNorm = 0.0602, lr_0 = 1.0668e-04
Loss = 1.4354e-05, PNorm = 53.0029, GNorm = 0.0842, lr_0 = 1.0657e-04
Loss = 1.3144e-05, PNorm = 53.0033, GNorm = 0.0862, lr_0 = 1.0645e-04
Loss = 1.1971e-05, PNorm = 53.0053, GNorm = 0.0574, lr_0 = 1.0634e-04
Loss = 1.1495e-05, PNorm = 53.0063, GNorm = 0.0717, lr_0 = 1.0623e-04
Validation rmse logD = 0.557877
Validation R2 logD = 0.772726
Validation rmse logP = 0.449264
Validation R2 logP = 0.941442
Epoch 97
Train function
Loss = 1.4295e-05, PNorm = 53.0063, GNorm = 0.0754, lr_0 = 1.0611e-04
Loss = 1.3459e-05, PNorm = 53.0067, GNorm = 0.0709, lr_0 = 1.0600e-04
Loss = 1.3606e-05, PNorm = 53.0079, GNorm = 0.1098, lr_0 = 1.0589e-04
Loss = 1.1618e-05, PNorm = 53.0085, GNorm = 0.0458, lr_0 = 1.0578e-04
Loss = 1.2016e-05, PNorm = 53.0092, GNorm = 0.0533, lr_0 = 1.0567e-04
Loss = 1.1253e-05, PNorm = 53.0095, GNorm = 0.0859, lr_0 = 1.0556e-04
Loss = 9.7364e-06, PNorm = 53.0096, GNorm = 0.0698, lr_0 = 1.0545e-04
Loss = 1.3464e-05, PNorm = 53.0106, GNorm = 0.0925, lr_0 = 1.0534e-04
Loss = 1.3461e-05, PNorm = 53.0124, GNorm = 0.0583, lr_0 = 1.0523e-04
Loss = 1.5873e-05, PNorm = 53.0134, GNorm = 0.1386, lr_0 = 1.0512e-04
Loss = 1.4150e-05, PNorm = 53.0134, GNorm = 0.0902, lr_0 = 1.0501e-04
Loss = 9.1340e-06, PNorm = 53.0137, GNorm = 0.0577, lr_0 = 1.0490e-04
Loss = 9.4346e-06, PNorm = 53.0139, GNorm = 0.0551, lr_0 = 1.0479e-04
Loss = 1.1070e-05, PNorm = 53.0135, GNorm = 0.1007, lr_0 = 1.0468e-04
Loss = 1.3662e-05, PNorm = 53.0141, GNorm = 0.0543, lr_0 = 1.0457e-04
Loss = 9.5910e-06, PNorm = 53.0145, GNorm = 0.0951, lr_0 = 1.0446e-04
Loss = 1.3179e-05, PNorm = 53.0149, GNorm = 0.1644, lr_0 = 1.0435e-04
Loss = 1.3026e-05, PNorm = 53.0142, GNorm = 0.0748, lr_0 = 1.0424e-04
Loss = 1.0983e-05, PNorm = 53.0141, GNorm = 0.0960, lr_0 = 1.0413e-04
Loss = 1.3154e-05, PNorm = 53.0149, GNorm = 0.0501, lr_0 = 1.0403e-04
Loss = 1.5750e-05, PNorm = 53.0148, GNorm = 0.0542, lr_0 = 1.0392e-04
Loss = 1.1937e-05, PNorm = 53.0157, GNorm = 0.0619, lr_0 = 1.0381e-04
Validation rmse logD = 0.558261
Validation R2 logD = 0.772413
Validation rmse logP = 0.450716
Validation R2 logP = 0.941063
Epoch 98
Train function
Loss = 1.7325e-05, PNorm = 53.0165, GNorm = 0.0700, lr_0 = 1.0370e-04
Loss = 1.3965e-05, PNorm = 53.0180, GNorm = 0.1029, lr_0 = 1.0359e-04
Loss = 1.3524e-05, PNorm = 53.0185, GNorm = 0.0689, lr_0 = 1.0348e-04
Loss = 1.3218e-05, PNorm = 53.0192, GNorm = 0.0489, lr_0 = 1.0338e-04
Loss = 1.0932e-05, PNorm = 53.0196, GNorm = 0.0668, lr_0 = 1.0327e-04
Loss = 1.0236e-05, PNorm = 53.0202, GNorm = 0.0625, lr_0 = 1.0316e-04
Loss = 8.9404e-06, PNorm = 53.0207, GNorm = 0.1298, lr_0 = 1.0305e-04
Loss = 1.0774e-05, PNorm = 53.0212, GNorm = 0.0813, lr_0 = 1.0295e-04
Loss = 8.9600e-06, PNorm = 53.0213, GNorm = 0.0578, lr_0 = 1.0284e-04
Loss = 8.3640e-06, PNorm = 53.0217, GNorm = 0.0639, lr_0 = 1.0273e-04
Loss = 9.1469e-06, PNorm = 53.0219, GNorm = 0.0554, lr_0 = 1.0262e-04
Loss = 1.0261e-05, PNorm = 53.0228, GNorm = 0.0440, lr_0 = 1.0252e-04
Loss = 9.2293e-06, PNorm = 53.0238, GNorm = 0.0366, lr_0 = 1.0241e-04
Loss = 1.1329e-05, PNorm = 53.0239, GNorm = 0.0784, lr_0 = 1.0230e-04
Loss = 9.2480e-06, PNorm = 53.0246, GNorm = 0.1134, lr_0 = 1.0220e-04
Loss = 1.4246e-05, PNorm = 53.0250, GNorm = 0.0725, lr_0 = 1.0209e-04
Loss = 1.4684e-05, PNorm = 53.0255, GNorm = 0.0579, lr_0 = 1.0198e-04
Loss = 1.4555e-05, PNorm = 53.0255, GNorm = 0.0859, lr_0 = 1.0188e-04
Loss = 1.3500e-05, PNorm = 53.0261, GNorm = 0.0922, lr_0 = 1.0177e-04
Loss = 1.2955e-05, PNorm = 53.0273, GNorm = 0.0848, lr_0 = 1.0166e-04
Loss = 1.1932e-05, PNorm = 53.0286, GNorm = 0.1088, lr_0 = 1.0156e-04
Loss = 1.0265e-05, PNorm = 53.0297, GNorm = 0.0616, lr_0 = 1.0145e-04
Loss = 1.3486e-05, PNorm = 53.0307, GNorm = 0.0708, lr_0 = 1.0135e-04
Validation rmse logD = 0.557718
Validation R2 logD = 0.772855
Validation rmse logP = 0.449693
Validation R2 logP = 0.941330
Epoch 99
Train function
Loss = 1.0563e-05, PNorm = 53.0309, GNorm = 0.0961, lr_0 = 1.0123e-04
Loss = 8.5788e-06, PNorm = 53.0312, GNorm = 0.0785, lr_0 = 1.0112e-04
Loss = 1.1563e-05, PNorm = 53.0317, GNorm = 0.1273, lr_0 = 1.0102e-04
Loss = 1.3028e-05, PNorm = 53.0321, GNorm = 0.0916, lr_0 = 1.0091e-04
Loss = 1.4557e-05, PNorm = 53.0332, GNorm = 0.0885, lr_0 = 1.0081e-04
Loss = 1.2850e-05, PNorm = 53.0344, GNorm = 0.1093, lr_0 = 1.0070e-04
Loss = 9.7658e-06, PNorm = 53.0347, GNorm = 0.0404, lr_0 = 1.0060e-04
Loss = 1.1718e-05, PNorm = 53.0349, GNorm = 0.0552, lr_0 = 1.0049e-04
Loss = 1.1744e-05, PNorm = 53.0357, GNorm = 0.0728, lr_0 = 1.0039e-04
Loss = 9.9294e-06, PNorm = 53.0363, GNorm = 0.0659, lr_0 = 1.0028e-04
Loss = 1.0138e-05, PNorm = 53.0366, GNorm = 0.0642, lr_0 = 1.0018e-04
Loss = 1.1401e-05, PNorm = 53.0374, GNorm = 0.0747, lr_0 = 1.0007e-04
Loss = 1.3370e-05, PNorm = 53.0379, GNorm = 0.1539, lr_0 = 1.0000e-04
Loss = 1.1328e-05, PNorm = 53.0391, GNorm = 0.0910, lr_0 = 1.0000e-04
Loss = 1.1238e-05, PNorm = 53.0398, GNorm = 0.0725, lr_0 = 1.0000e-04
Loss = 1.0115e-05, PNorm = 53.0403, GNorm = 0.0741, lr_0 = 1.0000e-04
Loss = 9.2318e-06, PNorm = 53.0410, GNorm = 0.0645, lr_0 = 1.0000e-04
Loss = 1.3718e-05, PNorm = 53.0410, GNorm = 0.1299, lr_0 = 1.0000e-04
Loss = 1.5931e-05, PNorm = 53.0410, GNorm = 0.1217, lr_0 = 1.0000e-04
Loss = 1.7354e-05, PNorm = 53.0405, GNorm = 0.0700, lr_0 = 1.0000e-04
Loss = 2.1289e-05, PNorm = 53.0417, GNorm = 0.1126, lr_0 = 1.0000e-04
Loss = 1.9661e-05, PNorm = 53.0428, GNorm = 0.1331, lr_0 = 1.0000e-04
Validation rmse logD = 0.558450
Validation R2 logD = 0.772258
Validation rmse logP = 0.451761
Validation R2 logP = 0.940789
Model 0 best validation rmse = 0.500201 on epoch 52
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logD = 0.557129
Model 0 test R2 logD = 0.803136
Model 0 test rmse logP = 0.436604
Model 0 test R2 logP = 0.945248
Ensemble test rmse  logD= 0.557129
Ensemble test R2  logD= 0.803136
Ensemble test rmse  logP= 0.436604
Ensemble test R2  logP= 0.945248
Fold 2
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_best'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 199,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_2',
 'save_smiles_splits': False,
 'seed': 2,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD', 'logP'],
 'task_names': ['logD', 'logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 11274,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 2
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=499, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 414,902
Moving model to cuda
Epoch 0
Train function
Loss = 2.3879e-02, PNorm = 35.0903, GNorm = 4.4101, lr_0 = 1.2200e-04
Loss = 1.6667e-02, PNorm = 35.0923, GNorm = 3.6966, lr_0 = 1.4200e-04
Loss = 1.4629e-02, PNorm = 35.0953, GNorm = 2.3955, lr_0 = 1.6200e-04
Loss = 1.4800e-02, PNorm = 35.1003, GNorm = 1.9316, lr_0 = 1.8200e-04
Loss = 1.1833e-02, PNorm = 35.1053, GNorm = 8.7646, lr_0 = 2.0200e-04
Loss = 1.0397e-02, PNorm = 35.1114, GNorm = 2.2421, lr_0 = 2.2200e-04
Loss = 1.0352e-02, PNorm = 35.1182, GNorm = 3.8206, lr_0 = 2.4200e-04
Loss = 1.1112e-02, PNorm = 35.1233, GNorm = 1.2861, lr_0 = 2.6200e-04
Loss = 9.8195e-03, PNorm = 35.1294, GNorm = 3.4104, lr_0 = 2.8200e-04
Loss = 7.9210e-03, PNorm = 35.1373, GNorm = 3.3847, lr_0 = 3.0200e-04
Loss = 6.8639e-03, PNorm = 35.1442, GNorm = 6.5577, lr_0 = 3.2200e-04
Loss = 8.0577e-03, PNorm = 35.1497, GNorm = 3.6707, lr_0 = 3.4200e-04
Loss = 7.0708e-03, PNorm = 35.1587, GNorm = 1.6998, lr_0 = 3.6200e-04
Loss = 7.0683e-03, PNorm = 35.1733, GNorm = 2.2662, lr_0 = 3.8200e-04
Loss = 6.0326e-03, PNorm = 35.1850, GNorm = 4.4200, lr_0 = 4.0200e-04
Loss = 6.2068e-03, PNorm = 35.1923, GNorm = 4.3927, lr_0 = 4.2200e-04
Loss = 8.3222e-03, PNorm = 35.2066, GNorm = 3.5868, lr_0 = 4.4200e-04
Loss = 7.4077e-03, PNorm = 35.2182, GNorm = 5.6993, lr_0 = 4.6200e-04
Loss = 6.8256e-03, PNorm = 35.2317, GNorm = 2.6104, lr_0 = 4.8200e-04
Loss = 7.5028e-03, PNorm = 35.2435, GNorm = 3.1208, lr_0 = 5.0200e-04
Loss = 6.1654e-03, PNorm = 35.2585, GNorm = 2.7140, lr_0 = 5.2200e-04
Loss = 6.4384e-03, PNorm = 35.2770, GNorm = 5.9230, lr_0 = 5.4200e-04
Validation rmse logD = 0.927155
Validation R2 logD = 0.391596
Validation rmse logP = 0.836965
Validation R2 logP = 0.794953
Epoch 1
Train function
Loss = 5.3160e-03, PNorm = 35.3001, GNorm = 2.1271, lr_0 = 5.6400e-04
Loss = 5.9031e-03, PNorm = 35.3192, GNorm = 1.6688, lr_0 = 5.8400e-04
Loss = 5.9619e-03, PNorm = 35.3377, GNorm = 3.1069, lr_0 = 6.0400e-04
Loss = 6.4127e-03, PNorm = 35.3566, GNorm = 3.1290, lr_0 = 6.2400e-04
Loss = 5.2343e-03, PNorm = 35.3780, GNorm = 2.1227, lr_0 = 6.4400e-04
Loss = 5.0299e-03, PNorm = 35.4054, GNorm = 1.5147, lr_0 = 6.6400e-04
Loss = 6.2735e-03, PNorm = 35.4233, GNorm = 1.8770, lr_0 = 6.8400e-04
Loss = 5.6712e-03, PNorm = 35.4437, GNorm = 4.5450, lr_0 = 7.0400e-04
Loss = 5.0669e-03, PNorm = 35.4634, GNorm = 2.8027, lr_0 = 7.2400e-04
Loss = 5.6453e-03, PNorm = 35.4895, GNorm = 1.2023, lr_0 = 7.4400e-04
Loss = 4.7896e-03, PNorm = 35.5216, GNorm = 4.1499, lr_0 = 7.6400e-04
Loss = 5.2239e-03, PNorm = 35.5448, GNorm = 3.6244, lr_0 = 7.8400e-04
Loss = 6.3146e-03, PNorm = 35.5646, GNorm = 3.0662, lr_0 = 8.0400e-04
Loss = 6.1978e-03, PNorm = 35.5928, GNorm = 2.1447, lr_0 = 8.2400e-04
Loss = 7.4764e-03, PNorm = 35.6289, GNorm = 2.3853, lr_0 = 8.4400e-04
Loss = 5.5239e-03, PNorm = 35.6661, GNorm = 1.4845, lr_0 = 8.6400e-04
Loss = 6.5994e-03, PNorm = 35.7014, GNorm = 4.0729, lr_0 = 8.8400e-04
Loss = 5.4112e-03, PNorm = 35.7321, GNorm = 1.7133, lr_0 = 9.0400e-04
Loss = 4.8558e-03, PNorm = 35.7696, GNorm = 3.1421, lr_0 = 9.2400e-04
Loss = 5.2218e-03, PNorm = 35.8029, GNorm = 3.2692, lr_0 = 9.4400e-04
Loss = 5.2203e-03, PNorm = 35.8359, GNorm = 1.2810, lr_0 = 9.6400e-04
Loss = 4.3010e-03, PNorm = 35.8712, GNorm = 4.1999, lr_0 = 9.8400e-04
Loss = 5.4205e-03, PNorm = 35.9089, GNorm = 3.3792, lr_0 = 9.9979e-04
Loss = 9.4950e-03, PNorm = 35.9123, GNorm = 3.6174, lr_0 = 9.9969e-04
Validation rmse logD = 0.881355
Validation R2 logD = 0.450221
Validation rmse logP = 0.704349
Validation R2 logP = 0.854784
Epoch 2
Train function
Loss = 4.7367e-03, PNorm = 35.9529, GNorm = 1.7172, lr_0 = 9.9864e-04
Loss = 4.5544e-03, PNorm = 36.0003, GNorm = 1.9294, lr_0 = 9.9760e-04
Loss = 4.3353e-03, PNorm = 36.0403, GNorm = 1.3293, lr_0 = 9.9656e-04
Loss = 4.6844e-03, PNorm = 36.0747, GNorm = 2.2171, lr_0 = 9.9552e-04
Loss = 4.7152e-03, PNorm = 36.1038, GNorm = 1.2615, lr_0 = 9.9448e-04
Loss = 3.8988e-03, PNorm = 36.1369, GNorm = 0.9358, lr_0 = 9.9344e-04
Loss = 4.5470e-03, PNorm = 36.1669, GNorm = 2.5165, lr_0 = 9.9241e-04
Loss = 4.1600e-03, PNorm = 36.1970, GNorm = 3.1968, lr_0 = 9.9137e-04
Loss = 4.0166e-03, PNorm = 36.2210, GNorm = 2.6041, lr_0 = 9.9034e-04
Loss = 5.3755e-03, PNorm = 36.2436, GNorm = 1.6924, lr_0 = 9.8930e-04
Loss = 4.2354e-03, PNorm = 36.2690, GNorm = 0.7839, lr_0 = 9.8827e-04
Loss = 4.2039e-03, PNorm = 36.2941, GNorm = 0.8326, lr_0 = 9.8724e-04
Loss = 4.1943e-03, PNorm = 36.3291, GNorm = 1.9062, lr_0 = 9.8621e-04
Loss = 3.5654e-03, PNorm = 36.3564, GNorm = 2.8654, lr_0 = 9.8518e-04
Loss = 4.1871e-03, PNorm = 36.3815, GNorm = 1.0334, lr_0 = 9.8415e-04
Loss = 3.4451e-03, PNorm = 36.4054, GNorm = 1.0490, lr_0 = 9.8312e-04
Loss = 4.0564e-03, PNorm = 36.4404, GNorm = 3.1087, lr_0 = 9.8210e-04
Loss = 3.6621e-03, PNorm = 36.4581, GNorm = 1.1687, lr_0 = 9.8107e-04
Loss = 3.2324e-03, PNorm = 36.4820, GNorm = 0.7627, lr_0 = 9.8005e-04
Loss = 3.9474e-03, PNorm = 36.5040, GNorm = 2.4081, lr_0 = 9.7902e-04
Loss = 3.8311e-03, PNorm = 36.5203, GNorm = 3.2274, lr_0 = 9.7800e-04
Loss = 4.3542e-03, PNorm = 36.5613, GNorm = 1.4246, lr_0 = 9.7698e-04
Validation rmse logD = 0.794438
Validation R2 logD = 0.553309
Validation rmse logP = 0.744424
Validation R2 logP = 0.837789
Epoch 3
Train function
Loss = 3.5066e-03, PNorm = 36.6010, GNorm = 1.2726, lr_0 = 9.7596e-04
Loss = 4.1978e-03, PNorm = 36.6390, GNorm = 1.8778, lr_0 = 9.7494e-04
Loss = 4.5892e-03, PNorm = 36.6829, GNorm = 2.1282, lr_0 = 9.7393e-04
Loss = 3.4088e-03, PNorm = 36.7176, GNorm = 0.8910, lr_0 = 9.7291e-04
Loss = 3.4329e-03, PNorm = 36.7484, GNorm = 1.1863, lr_0 = 9.7189e-04
Loss = 2.8830e-03, PNorm = 36.7840, GNorm = 1.1138, lr_0 = 9.7088e-04
Loss = 3.2610e-03, PNorm = 36.8084, GNorm = 1.0745, lr_0 = 9.6987e-04
Loss = 2.9006e-03, PNorm = 36.8324, GNorm = 2.7778, lr_0 = 9.6885e-04
Loss = 3.5435e-03, PNorm = 36.8495, GNorm = 3.6240, lr_0 = 9.6784e-04
Loss = 2.6985e-03, PNorm = 36.8893, GNorm = 1.3780, lr_0 = 9.6683e-04
Loss = 2.9717e-03, PNorm = 36.9208, GNorm = 1.2273, lr_0 = 9.6582e-04
Loss = 3.3066e-03, PNorm = 36.9572, GNorm = 1.2406, lr_0 = 9.6482e-04
Loss = 2.9488e-03, PNorm = 36.9851, GNorm = 1.2733, lr_0 = 9.6381e-04
Loss = 3.3933e-03, PNorm = 37.0205, GNorm = 2.2817, lr_0 = 9.6280e-04
Loss = 4.2333e-03, PNorm = 37.0479, GNorm = 0.8071, lr_0 = 9.6180e-04
Loss = 3.5189e-03, PNorm = 37.0778, GNorm = 0.8184, lr_0 = 9.6079e-04
Loss = 4.1592e-03, PNorm = 37.1074, GNorm = 1.8095, lr_0 = 9.5979e-04
Loss = 3.3106e-03, PNorm = 37.1311, GNorm = 1.6292, lr_0 = 9.5879e-04
Loss = 3.4117e-03, PNorm = 37.1629, GNorm = 1.0463, lr_0 = 9.5779e-04
Loss = 2.9620e-03, PNorm = 37.1903, GNorm = 1.1284, lr_0 = 9.5679e-04
Loss = 3.3216e-03, PNorm = 37.2148, GNorm = 1.2571, lr_0 = 9.5579e-04
Loss = 3.1909e-03, PNorm = 37.2413, GNorm = 0.9817, lr_0 = 9.5479e-04
Loss = 2.9579e-03, PNorm = 37.2755, GNorm = 1.4638, lr_0 = 9.5380e-04
Validation rmse logD = 0.759724
Validation R2 logD = 0.591494
Validation rmse logP = 0.604129
Validation R2 logP = 0.893169
Epoch 4
Train function
Loss = 2.8229e-03, PNorm = 37.3073, GNorm = 1.1541, lr_0 = 9.5270e-04
Loss = 3.3452e-03, PNorm = 37.3322, GNorm = 2.2830, lr_0 = 9.5171e-04
Loss = 2.5416e-03, PNorm = 37.3572, GNorm = 0.9736, lr_0 = 9.5071e-04
Loss = 2.5831e-03, PNorm = 37.3806, GNorm = 0.8815, lr_0 = 9.4972e-04
Loss = 2.6566e-03, PNorm = 37.3996, GNorm = 1.0987, lr_0 = 9.4873e-04
Loss = 2.9618e-03, PNorm = 37.4256, GNorm = 1.5719, lr_0 = 9.4774e-04
Loss = 2.5858e-03, PNorm = 37.4541, GNorm = 0.8436, lr_0 = 9.4675e-04
Loss = 2.9443e-03, PNorm = 37.4860, GNorm = 0.7278, lr_0 = 9.4576e-04
Loss = 2.3885e-03, PNorm = 37.5211, GNorm = 1.0922, lr_0 = 9.4478e-04
Loss = 2.6892e-03, PNorm = 37.5441, GNorm = 1.3766, lr_0 = 9.4379e-04
Loss = 3.0618e-03, PNorm = 37.5614, GNorm = 0.8065, lr_0 = 9.4280e-04
Loss = 2.5965e-03, PNorm = 37.5922, GNorm = 0.8356, lr_0 = 9.4182e-04
Loss = 3.2523e-03, PNorm = 37.6282, GNorm = 0.6839, lr_0 = 9.4084e-04
Loss = 3.2208e-03, PNorm = 37.6652, GNorm = 1.8592, lr_0 = 9.3986e-04
Loss = 3.0643e-03, PNorm = 37.6928, GNorm = 0.9239, lr_0 = 9.3887e-04
Loss = 2.5421e-03, PNorm = 37.7136, GNorm = 1.2362, lr_0 = 9.3789e-04
Loss = 2.9787e-03, PNorm = 37.7416, GNorm = 1.2181, lr_0 = 9.3692e-04
Loss = 3.1266e-03, PNorm = 37.7785, GNorm = 1.2812, lr_0 = 9.3594e-04
Loss = 2.8543e-03, PNorm = 37.8145, GNorm = 0.5860, lr_0 = 9.3496e-04
Loss = 3.2008e-03, PNorm = 37.8511, GNorm = 1.3444, lr_0 = 9.3399e-04
Loss = 2.6300e-03, PNorm = 37.8802, GNorm = 1.3026, lr_0 = 9.3301e-04
Loss = 2.9628e-03, PNorm = 37.8987, GNorm = 2.5003, lr_0 = 9.3204e-04
Validation rmse logD = 0.717050
Validation R2 logD = 0.636097
Validation rmse logP = 0.581345
Validation R2 logP = 0.901075
Epoch 5
Train function
Loss = 3.6815e-03, PNorm = 37.9223, GNorm = 1.8085, lr_0 = 9.3106e-04
Loss = 1.8365e-03, PNorm = 37.9514, GNorm = 0.6028, lr_0 = 9.3009e-04
Loss = 2.7102e-03, PNorm = 37.9752, GNorm = 1.1838, lr_0 = 9.2912e-04
Loss = 2.6165e-03, PNorm = 38.0109, GNorm = 1.2622, lr_0 = 9.2815e-04
Loss = 2.5331e-03, PNorm = 38.0322, GNorm = 2.5507, lr_0 = 9.2718e-04
Loss = 2.7062e-03, PNorm = 38.0648, GNorm = 1.5384, lr_0 = 9.2622e-04
Loss = 2.6238e-03, PNorm = 38.1002, GNorm = 1.1750, lr_0 = 9.2525e-04
Loss = 2.6462e-03, PNorm = 38.1308, GNorm = 0.7314, lr_0 = 9.2428e-04
Loss = 2.6720e-03, PNorm = 38.1626, GNorm = 2.3293, lr_0 = 9.2332e-04
Loss = 3.1027e-03, PNorm = 38.1905, GNorm = 2.3505, lr_0 = 9.2235e-04
Loss = 2.4358e-03, PNorm = 38.2274, GNorm = 0.6850, lr_0 = 9.2139e-04
Loss = 3.2269e-03, PNorm = 38.2597, GNorm = 1.9141, lr_0 = 9.2043e-04
Loss = 3.2670e-03, PNorm = 38.2924, GNorm = 1.2112, lr_0 = 9.1947e-04
Loss = 2.2148e-03, PNorm = 38.3203, GNorm = 0.8709, lr_0 = 9.1851e-04
Loss = 2.7311e-03, PNorm = 38.3476, GNorm = 2.6282, lr_0 = 9.1755e-04
Loss = 2.2724e-03, PNorm = 38.3745, GNorm = 1.2023, lr_0 = 9.1659e-04
Loss = 2.7068e-03, PNorm = 38.3990, GNorm = 1.3245, lr_0 = 9.1564e-04
Loss = 2.3580e-03, PNorm = 38.4277, GNorm = 0.9370, lr_0 = 9.1468e-04
Loss = 2.1793e-03, PNorm = 38.4582, GNorm = 0.7941, lr_0 = 9.1373e-04
Loss = 2.1885e-03, PNorm = 38.4762, GNorm = 0.6464, lr_0 = 9.1277e-04
Loss = 1.9888e-03, PNorm = 38.4968, GNorm = 1.0933, lr_0 = 9.1182e-04
Loss = 2.3055e-03, PNorm = 38.5212, GNorm = 0.9154, lr_0 = 9.1087e-04
Loss = 2.4864e-03, PNorm = 38.5444, GNorm = 1.0330, lr_0 = 9.0992e-04
Validation rmse logD = 0.759388
Validation R2 logD = 0.591855
Validation rmse logP = 0.545641
Validation R2 logP = 0.912853
Epoch 6
Train function
Loss = 2.5690e-03, PNorm = 38.5682, GNorm = 1.2776, lr_0 = 9.0887e-04
Loss = 2.0321e-03, PNorm = 38.6058, GNorm = 1.8571, lr_0 = 9.0792e-04
Loss = 2.2752e-03, PNorm = 38.6250, GNorm = 1.0903, lr_0 = 9.0698e-04
Loss = 2.3141e-03, PNorm = 38.6564, GNorm = 1.8931, lr_0 = 9.0603e-04
Loss = 2.1681e-03, PNorm = 38.6820, GNorm = 0.7806, lr_0 = 9.0508e-04
Loss = 1.8016e-03, PNorm = 38.7119, GNorm = 1.0187, lr_0 = 9.0414e-04
Loss = 1.8374e-03, PNorm = 38.7290, GNorm = 0.7912, lr_0 = 9.0320e-04
Loss = 2.0744e-03, PNorm = 38.7582, GNorm = 0.8954, lr_0 = 9.0225e-04
Loss = 2.4711e-03, PNorm = 38.7838, GNorm = 1.4302, lr_0 = 9.0131e-04
Loss = 1.9811e-03, PNorm = 38.8140, GNorm = 1.1454, lr_0 = 9.0037e-04
Loss = 2.0151e-03, PNorm = 38.8368, GNorm = 0.9553, lr_0 = 8.9943e-04
Loss = 2.5578e-03, PNorm = 38.8662, GNorm = 1.0341, lr_0 = 8.9849e-04
Loss = 3.0596e-03, PNorm = 38.8903, GNorm = 0.8881, lr_0 = 8.9756e-04
Loss = 2.3004e-03, PNorm = 38.9262, GNorm = 1.4186, lr_0 = 8.9662e-04
Loss = 2.1331e-03, PNorm = 38.9535, GNorm = 0.8187, lr_0 = 8.9568e-04
Loss = 2.7629e-03, PNorm = 38.9741, GNorm = 1.0113, lr_0 = 8.9475e-04
Loss = 2.4811e-03, PNorm = 38.9967, GNorm = 1.1476, lr_0 = 8.9381e-04
Loss = 2.5528e-03, PNorm = 39.0251, GNorm = 1.1348, lr_0 = 8.9288e-04
Loss = 1.9012e-03, PNorm = 39.0514, GNorm = 1.2497, lr_0 = 8.9195e-04
Loss = 1.8908e-03, PNorm = 39.0709, GNorm = 0.9021, lr_0 = 8.9102e-04
Loss = 2.5218e-03, PNorm = 39.1010, GNorm = 1.2118, lr_0 = 8.9009e-04
Loss = 2.5960e-03, PNorm = 39.1306, GNorm = 0.8193, lr_0 = 8.8916e-04
Validation rmse logD = 0.601118
Validation R2 logD = 0.744256
Validation rmse logP = 0.553433
Validation R2 logP = 0.910346
Epoch 7
Train function
Loss = 1.4934e-03, PNorm = 39.1607, GNorm = 1.3032, lr_0 = 8.8823e-04
Loss = 2.1709e-03, PNorm = 39.1846, GNorm = 2.1731, lr_0 = 8.8730e-04
Loss = 1.9245e-03, PNorm = 39.2190, GNorm = 0.8636, lr_0 = 8.8638e-04
Loss = 1.5360e-03, PNorm = 39.2408, GNorm = 0.8550, lr_0 = 8.8545e-04
Loss = 2.1202e-03, PNorm = 39.2629, GNorm = 0.5973, lr_0 = 8.8453e-04
Loss = 2.0673e-03, PNorm = 39.2900, GNorm = 0.8381, lr_0 = 8.8361e-04
Loss = 2.4028e-03, PNorm = 39.3152, GNorm = 1.7297, lr_0 = 8.8268e-04
Loss = 1.9699e-03, PNorm = 39.3426, GNorm = 1.7037, lr_0 = 8.8176e-04
Loss = 2.0108e-03, PNorm = 39.3641, GNorm = 1.1477, lr_0 = 8.8084e-04
Loss = 1.9424e-03, PNorm = 39.3907, GNorm = 1.1220, lr_0 = 8.7992e-04
Loss = 2.1446e-03, PNorm = 39.4189, GNorm = 0.9038, lr_0 = 8.7900e-04
Loss = 1.7693e-03, PNorm = 39.4502, GNorm = 0.5927, lr_0 = 8.7809e-04
Loss = 2.0589e-03, PNorm = 39.4729, GNorm = 2.3566, lr_0 = 8.7717e-04
Loss = 2.3504e-03, PNorm = 39.5029, GNorm = 0.8162, lr_0 = 8.7625e-04
Loss = 2.4571e-03, PNorm = 39.5327, GNorm = 0.8871, lr_0 = 8.7534e-04
Loss = 2.2359e-03, PNorm = 39.5598, GNorm = 1.0900, lr_0 = 8.7443e-04
Loss = 2.0215e-03, PNorm = 39.5845, GNorm = 1.1866, lr_0 = 8.7351e-04
Loss = 2.2259e-03, PNorm = 39.6075, GNorm = 2.5164, lr_0 = 8.7260e-04
Loss = 2.1649e-03, PNorm = 39.6383, GNorm = 0.6695, lr_0 = 8.7169e-04
Loss = 2.5114e-03, PNorm = 39.6644, GNorm = 1.2291, lr_0 = 8.7078e-04
Loss = 2.3633e-03, PNorm = 39.6920, GNorm = 0.8476, lr_0 = 8.6987e-04
Loss = 1.6403e-03, PNorm = 39.7186, GNorm = 0.5094, lr_0 = 8.6896e-04
Loss = 2.4926e-03, PNorm = 39.7376, GNorm = 0.5067, lr_0 = 8.6806e-04
Validation rmse logD = 0.614944
Validation R2 logD = 0.732356
Validation rmse logP = 0.562278
Validation R2 logP = 0.907458
Epoch 8
Train function
Loss = 1.6956e-03, PNorm = 39.7649, GNorm = 1.2414, lr_0 = 8.6706e-04
Loss = 1.7030e-03, PNorm = 39.7926, GNorm = 1.4046, lr_0 = 8.6616e-04
Loss = 1.9626e-03, PNorm = 39.8167, GNorm = 0.8807, lr_0 = 8.6525e-04
Loss = 1.6828e-03, PNorm = 39.8485, GNorm = 0.8078, lr_0 = 8.6435e-04
Loss = 1.9701e-03, PNorm = 39.8789, GNorm = 1.8746, lr_0 = 8.6345e-04
Loss = 1.8034e-03, PNorm = 39.8967, GNorm = 1.0863, lr_0 = 8.6255e-04
Loss = 1.6338e-03, PNorm = 39.9261, GNorm = 1.2410, lr_0 = 8.6165e-04
Loss = 2.1137e-03, PNorm = 39.9507, GNorm = 2.5033, lr_0 = 8.6075e-04
Loss = 1.9528e-03, PNorm = 39.9792, GNorm = 0.7171, lr_0 = 8.5985e-04
Loss = 1.5931e-03, PNorm = 40.0094, GNorm = 0.7610, lr_0 = 8.5895e-04
Loss = 1.5789e-03, PNorm = 40.0324, GNorm = 0.5435, lr_0 = 8.5805e-04
Loss = 1.9671e-03, PNorm = 40.0513, GNorm = 0.7680, lr_0 = 8.5716e-04
Loss = 1.8192e-03, PNorm = 40.0727, GNorm = 1.5985, lr_0 = 8.5626e-04
Loss = 1.6760e-03, PNorm = 40.0917, GNorm = 0.4750, lr_0 = 8.5537e-04
Loss = 2.1600e-03, PNorm = 40.1258, GNorm = 1.1586, lr_0 = 8.5448e-04
Loss = 2.0157e-03, PNorm = 40.1465, GNorm = 1.1593, lr_0 = 8.5359e-04
Loss = 2.1676e-03, PNorm = 40.1777, GNorm = 1.2982, lr_0 = 8.5269e-04
Loss = 1.8139e-03, PNorm = 40.2039, GNorm = 0.7843, lr_0 = 8.5180e-04
Loss = 1.6492e-03, PNorm = 40.2244, GNorm = 0.7812, lr_0 = 8.5092e-04
Loss = 1.3382e-03, PNorm = 40.2478, GNorm = 0.6166, lr_0 = 8.5003e-04
Loss = 2.1756e-03, PNorm = 40.2669, GNorm = 0.8423, lr_0 = 8.4914e-04
Loss = 1.7710e-03, PNorm = 40.2830, GNorm = 1.2676, lr_0 = 8.4825e-04
Validation rmse logD = 0.608079
Validation R2 logD = 0.738298
Validation rmse logP = 0.525136
Validation R2 logP = 0.919280
Epoch 9
Train function
Loss = 1.1123e-03, PNorm = 40.3157, GNorm = 0.9514, lr_0 = 8.4737e-04
Loss = 1.8470e-03, PNorm = 40.3408, GNorm = 0.8906, lr_0 = 8.4648e-04
Loss = 1.4883e-03, PNorm = 40.3549, GNorm = 0.8632, lr_0 = 8.4560e-04
Loss = 1.6192e-03, PNorm = 40.3878, GNorm = 1.1875, lr_0 = 8.4472e-04
Loss = 1.2768e-03, PNorm = 40.4087, GNorm = 0.5355, lr_0 = 8.4384e-04
Loss = 1.3407e-03, PNorm = 40.4253, GNorm = 0.5964, lr_0 = 8.4296e-04
Loss = 1.4796e-03, PNorm = 40.4423, GNorm = 0.6960, lr_0 = 8.4208e-04
Loss = 1.4931e-03, PNorm = 40.4564, GNorm = 0.5915, lr_0 = 8.4120e-04
Loss = 1.8216e-03, PNorm = 40.4830, GNorm = 0.6834, lr_0 = 8.4032e-04
Loss = 1.7197e-03, PNorm = 40.4924, GNorm = 1.9061, lr_0 = 8.3944e-04
Loss = 2.1855e-03, PNorm = 40.5159, GNorm = 2.3904, lr_0 = 8.3857e-04
Loss = 2.1850e-03, PNorm = 40.5521, GNorm = 2.1151, lr_0 = 8.3769e-04
Loss = 1.9729e-03, PNorm = 40.5939, GNorm = 1.6339, lr_0 = 8.3682e-04
Loss = 1.6155e-03, PNorm = 40.6182, GNorm = 1.0357, lr_0 = 8.3594e-04
Loss = 1.5428e-03, PNorm = 40.6469, GNorm = 1.3349, lr_0 = 8.3507e-04
Loss = 1.6190e-03, PNorm = 40.6789, GNorm = 1.5867, lr_0 = 8.3420e-04
Loss = 1.8169e-03, PNorm = 40.7009, GNorm = 0.8507, lr_0 = 8.3333e-04
Loss = 1.3717e-03, PNorm = 40.7179, GNorm = 0.6269, lr_0 = 8.3246e-04
Loss = 1.3686e-03, PNorm = 40.7395, GNorm = 1.0633, lr_0 = 8.3159e-04
Loss = 1.8135e-03, PNorm = 40.7638, GNorm = 1.0873, lr_0 = 8.3072e-04
Loss = 1.7110e-03, PNorm = 40.7855, GNorm = 0.6912, lr_0 = 8.2986e-04
Loss = 1.5513e-03, PNorm = 40.8045, GNorm = 1.3509, lr_0 = 8.2899e-04
Loss = 1.5545e-03, PNorm = 40.8264, GNorm = 1.0185, lr_0 = 8.2812e-04
Validation rmse logD = 0.581878
Validation R2 logD = 0.760365
Validation rmse logP = 0.495723
Validation R2 logP = 0.928069
Epoch 10
Train function
Loss = 1.2907e-03, PNorm = 40.8437, GNorm = 0.8192, lr_0 = 8.2717e-04
Loss = 1.4469e-03, PNorm = 40.8609, GNorm = 0.4728, lr_0 = 8.2631e-04
Loss = 1.3798e-03, PNorm = 40.8816, GNorm = 0.8532, lr_0 = 8.2545e-04
Loss = 1.6191e-03, PNorm = 40.9075, GNorm = 1.0497, lr_0 = 8.2459e-04
Loss = 1.2790e-03, PNorm = 40.9287, GNorm = 1.0471, lr_0 = 8.2373e-04
Loss = 1.5654e-03, PNorm = 40.9553, GNorm = 0.6991, lr_0 = 8.2287e-04
Loss = 1.3229e-03, PNorm = 40.9820, GNorm = 0.9534, lr_0 = 8.2201e-04
Loss = 1.7550e-03, PNorm = 40.9996, GNorm = 1.7680, lr_0 = 8.2115e-04
Loss = 1.9836e-03, PNorm = 41.0255, GNorm = 1.3942, lr_0 = 8.2029e-04
Loss = 1.6601e-03, PNorm = 41.0444, GNorm = 1.4437, lr_0 = 8.1944e-04
Loss = 1.3930e-03, PNorm = 41.0742, GNorm = 0.9183, lr_0 = 8.1858e-04
Loss = 1.3369e-03, PNorm = 41.1020, GNorm = 0.8406, lr_0 = 8.1773e-04
Loss = 1.3685e-03, PNorm = 41.1185, GNorm = 0.7846, lr_0 = 8.1687e-04
Loss = 1.5159e-03, PNorm = 41.1369, GNorm = 0.4149, lr_0 = 8.1602e-04
Loss = 1.4853e-03, PNorm = 41.1650, GNorm = 1.0318, lr_0 = 8.1517e-04
Loss = 1.5733e-03, PNorm = 41.1846, GNorm = 1.2112, lr_0 = 8.1432e-04
Loss = 1.4670e-03, PNorm = 41.2082, GNorm = 1.0623, lr_0 = 8.1347e-04
Loss = 1.1429e-03, PNorm = 41.2246, GNorm = 0.5270, lr_0 = 8.1262e-04
Loss = 1.5030e-03, PNorm = 41.2499, GNorm = 0.8464, lr_0 = 8.1177e-04
Loss = 1.5573e-03, PNorm = 41.2924, GNorm = 0.6574, lr_0 = 8.1092e-04
Loss = 1.8727e-03, PNorm = 41.3228, GNorm = 1.3773, lr_0 = 8.1008e-04
Loss = 1.4797e-03, PNorm = 41.3454, GNorm = 1.1241, lr_0 = 8.0923e-04
Loss = 1.7244e-03, PNorm = 41.3601, GNorm = 1.5339, lr_0 = 8.0839e-04
Validation rmse logD = 0.583993
Validation R2 logD = 0.758619
Validation rmse logP = 0.516541
Validation R2 logP = 0.921900
Epoch 11
Train function
Loss = 1.4651e-03, PNorm = 41.3797, GNorm = 0.7090, lr_0 = 8.0754e-04
Loss = 1.3339e-03, PNorm = 41.4064, GNorm = 0.4958, lr_0 = 8.0670e-04
Loss = 1.3447e-03, PNorm = 41.4253, GNorm = 1.1316, lr_0 = 8.0586e-04
Loss = 1.0716e-03, PNorm = 41.4444, GNorm = 0.9124, lr_0 = 8.0502e-04
Loss = 1.4523e-03, PNorm = 41.4691, GNorm = 1.0082, lr_0 = 8.0418e-04
Loss = 1.6090e-03, PNorm = 41.4787, GNorm = 1.3781, lr_0 = 8.0334e-04
Loss = 1.5737e-03, PNorm = 41.4989, GNorm = 0.7873, lr_0 = 8.0250e-04
Loss = 1.3480e-03, PNorm = 41.5170, GNorm = 0.8835, lr_0 = 8.0166e-04
Loss = 1.2881e-03, PNorm = 41.5386, GNorm = 0.8588, lr_0 = 8.0082e-04
Loss = 1.2819e-03, PNorm = 41.5624, GNorm = 1.0211, lr_0 = 7.9999e-04
Loss = 1.1983e-03, PNorm = 41.5751, GNorm = 0.6306, lr_0 = 7.9915e-04
Loss = 1.4922e-03, PNorm = 41.5871, GNorm = 0.6967, lr_0 = 7.9832e-04
Loss = 1.2543e-03, PNorm = 41.6107, GNorm = 0.8714, lr_0 = 7.9749e-04
Loss = 1.3628e-03, PNorm = 41.6335, GNorm = 0.6837, lr_0 = 7.9665e-04
Loss = 1.1933e-03, PNorm = 41.6543, GNorm = 0.7667, lr_0 = 7.9582e-04
Loss = 1.5003e-03, PNorm = 41.6715, GNorm = 1.2027, lr_0 = 7.9499e-04
Loss = 1.3054e-03, PNorm = 41.6969, GNorm = 0.9902, lr_0 = 7.9416e-04
Loss = 1.2616e-03, PNorm = 41.7164, GNorm = 0.5682, lr_0 = 7.9333e-04
Loss = 1.2207e-03, PNorm = 41.7398, GNorm = 0.5752, lr_0 = 7.9251e-04
Loss = 1.5396e-03, PNorm = 41.7647, GNorm = 0.6148, lr_0 = 7.9168e-04
Loss = 1.6478e-03, PNorm = 41.7867, GNorm = 0.5660, lr_0 = 7.9085e-04
Loss = 1.5294e-03, PNorm = 41.8042, GNorm = 0.9533, lr_0 = 7.9003e-04
Validation rmse logD = 0.565180
Validation R2 logD = 0.773921
Validation rmse logP = 0.509677
Validation R2 logP = 0.923962
Epoch 12
Train function
Loss = 1.1824e-03, PNorm = 41.8320, GNorm = 0.8369, lr_0 = 7.8912e-04
Loss = 1.1733e-03, PNorm = 41.8402, GNorm = 0.8536, lr_0 = 7.8830e-04
Loss = 1.1336e-03, PNorm = 41.8586, GNorm = 0.9964, lr_0 = 7.8747e-04
Loss = 1.3342e-03, PNorm = 41.8784, GNorm = 0.5684, lr_0 = 7.8665e-04
Loss = 1.3943e-03, PNorm = 41.8998, GNorm = 1.5754, lr_0 = 7.8583e-04
Loss = 1.2270e-03, PNorm = 41.9258, GNorm = 0.9937, lr_0 = 7.8501e-04
Loss = 1.0205e-03, PNorm = 41.9532, GNorm = 0.5440, lr_0 = 7.8419e-04
Loss = 1.3731e-03, PNorm = 41.9745, GNorm = 1.9064, lr_0 = 7.8337e-04
Loss = 1.2601e-03, PNorm = 41.9824, GNorm = 0.7432, lr_0 = 7.8255e-04
Loss = 1.6056e-03, PNorm = 42.0194, GNorm = 1.4713, lr_0 = 7.8174e-04
Loss = 1.6023e-03, PNorm = 42.0476, GNorm = 0.6604, lr_0 = 7.8092e-04
Loss = 1.4105e-03, PNorm = 42.0674, GNorm = 0.5845, lr_0 = 7.8011e-04
Loss = 1.5399e-03, PNorm = 42.0936, GNorm = 0.5385, lr_0 = 7.7929e-04
Loss = 1.2718e-03, PNorm = 42.1138, GNorm = 0.9468, lr_0 = 7.7848e-04
Loss = 1.3929e-03, PNorm = 42.1404, GNorm = 0.6061, lr_0 = 7.7767e-04
Loss = 1.0988e-03, PNorm = 42.1639, GNorm = 0.5798, lr_0 = 7.7686e-04
Loss = 1.2416e-03, PNorm = 42.1927, GNorm = 0.8280, lr_0 = 7.7604e-04
Loss = 1.0772e-03, PNorm = 42.2128, GNorm = 1.3393, lr_0 = 7.7523e-04
Loss = 1.0450e-03, PNorm = 42.2196, GNorm = 0.6581, lr_0 = 7.7443e-04
Loss = 1.3234e-03, PNorm = 42.2363, GNorm = 0.6349, lr_0 = 7.7362e-04
Loss = 1.0495e-03, PNorm = 42.2513, GNorm = 0.8731, lr_0 = 7.7281e-04
Loss = 1.3683e-03, PNorm = 42.2636, GNorm = 1.0511, lr_0 = 7.7200e-04
Loss = 1.3977e-03, PNorm = 42.2820, GNorm = 0.9112, lr_0 = 7.7120e-04
Validation rmse logD = 0.567162
Validation R2 logD = 0.772332
Validation rmse logP = 0.502231
Validation R2 logP = 0.926168
Epoch 13
Train function
Loss = 9.6724e-04, PNorm = 42.3018, GNorm = 0.5874, lr_0 = 7.7039e-04
Loss = 1.0196e-03, PNorm = 42.3151, GNorm = 1.0158, lr_0 = 7.6959e-04
Loss = 9.2591e-04, PNorm = 42.3355, GNorm = 0.8042, lr_0 = 7.6879e-04
Loss = 1.0040e-03, PNorm = 42.3461, GNorm = 0.6708, lr_0 = 7.6798e-04
Loss = 1.1463e-03, PNorm = 42.3617, GNorm = 0.6966, lr_0 = 7.6718e-04
Loss = 1.1230e-03, PNorm = 42.3836, GNorm = 0.5093, lr_0 = 7.6638e-04
Loss = 1.1598e-03, PNorm = 42.4037, GNorm = 0.9809, lr_0 = 7.6558e-04
Loss = 9.8790e-04, PNorm = 42.4249, GNorm = 0.4886, lr_0 = 7.6478e-04
Loss = 1.4379e-03, PNorm = 42.4549, GNorm = 1.0002, lr_0 = 7.6398e-04
Loss = 1.1282e-03, PNorm = 42.4791, GNorm = 0.6959, lr_0 = 7.6319e-04
Loss = 9.4029e-04, PNorm = 42.5034, GNorm = 0.5612, lr_0 = 7.6239e-04
Loss = 1.2190e-03, PNorm = 42.5190, GNorm = 0.6781, lr_0 = 7.6159e-04
Loss = 1.1859e-03, PNorm = 42.5325, GNorm = 0.8422, lr_0 = 7.6080e-04
Loss = 1.0889e-03, PNorm = 42.5488, GNorm = 0.6730, lr_0 = 7.6000e-04
Loss = 1.3353e-03, PNorm = 42.5620, GNorm = 0.7408, lr_0 = 7.5921e-04
Loss = 1.4981e-03, PNorm = 42.5831, GNorm = 1.0596, lr_0 = 7.5842e-04
Loss = 9.8198e-04, PNorm = 42.6000, GNorm = 0.7723, lr_0 = 7.5763e-04
Loss = 1.3257e-03, PNorm = 42.6169, GNorm = 0.6447, lr_0 = 7.5684e-04
Loss = 1.2008e-03, PNorm = 42.6307, GNorm = 1.0905, lr_0 = 7.5605e-04
Loss = 1.3464e-03, PNorm = 42.6449, GNorm = 0.8255, lr_0 = 7.5526e-04
Loss = 1.2416e-03, PNorm = 42.6624, GNorm = 0.6405, lr_0 = 7.5447e-04
Loss = 1.3152e-03, PNorm = 42.6798, GNorm = 0.5254, lr_0 = 7.5368e-04
Validation rmse logD = 0.561167
Validation R2 logD = 0.777120
Validation rmse logP = 0.497311
Validation R2 logP = 0.927607
Epoch 14
Train function
Loss = 9.1164e-04, PNorm = 42.6984, GNorm = 0.6274, lr_0 = 7.5282e-04
Loss = 9.6679e-04, PNorm = 42.7120, GNorm = 0.8462, lr_0 = 7.5203e-04
Loss = 9.4588e-04, PNorm = 42.7293, GNorm = 0.3880, lr_0 = 7.5125e-04
Loss = 8.0740e-04, PNorm = 42.7456, GNorm = 0.5782, lr_0 = 7.5046e-04
Loss = 1.0005e-03, PNorm = 42.7669, GNorm = 0.4797, lr_0 = 7.4968e-04
Loss = 7.2756e-04, PNorm = 42.7792, GNorm = 0.5070, lr_0 = 7.4890e-04
Loss = 1.0270e-03, PNorm = 42.7976, GNorm = 0.6055, lr_0 = 7.4811e-04
Loss = 9.5995e-04, PNorm = 42.8132, GNorm = 0.5709, lr_0 = 7.4733e-04
Loss = 1.3045e-03, PNorm = 42.8295, GNorm = 0.9694, lr_0 = 7.4655e-04
Loss = 1.0724e-03, PNorm = 42.8481, GNorm = 0.4509, lr_0 = 7.4577e-04
Loss = 9.1274e-04, PNorm = 42.8631, GNorm = 0.7534, lr_0 = 7.4500e-04
Loss = 9.9926e-04, PNorm = 42.8759, GNorm = 0.4596, lr_0 = 7.4422e-04
Loss = 1.0726e-03, PNorm = 42.8949, GNorm = 0.6876, lr_0 = 7.4344e-04
Loss = 1.4465e-03, PNorm = 42.9143, GNorm = 0.9454, lr_0 = 7.4267e-04
Loss = 1.1061e-03, PNorm = 42.9348, GNorm = 0.7187, lr_0 = 7.4189e-04
Loss = 1.1412e-03, PNorm = 42.9613, GNorm = 0.6792, lr_0 = 7.4112e-04
Loss = 1.2484e-03, PNorm = 42.9783, GNorm = 1.4055, lr_0 = 7.4034e-04
Loss = 1.3419e-03, PNorm = 42.9959, GNorm = 0.8369, lr_0 = 7.3957e-04
Loss = 1.0625e-03, PNorm = 43.0192, GNorm = 0.4838, lr_0 = 7.3880e-04
Loss = 1.1179e-03, PNorm = 43.0431, GNorm = 0.8503, lr_0 = 7.3803e-04
Loss = 1.0594e-03, PNorm = 43.0592, GNorm = 0.5232, lr_0 = 7.3726e-04
Loss = 8.7746e-04, PNorm = 43.0844, GNorm = 0.6486, lr_0 = 7.3649e-04
Loss = 1.2312e-03, PNorm = 43.1140, GNorm = 2.0586, lr_0 = 7.3572e-04
Validation rmse logD = 0.585722
Validation R2 logD = 0.757188
Validation rmse logP = 0.503329
Validation R2 logP = 0.925845
Epoch 15
Train function
Loss = 7.8539e-04, PNorm = 43.1299, GNorm = 0.8537, lr_0 = 7.3495e-04
Loss = 8.5555e-04, PNorm = 43.1499, GNorm = 0.6183, lr_0 = 7.3418e-04
Loss = 9.8569e-04, PNorm = 43.1719, GNorm = 0.9210, lr_0 = 7.3342e-04
Loss = 8.7141e-04, PNorm = 43.1947, GNorm = 0.5249, lr_0 = 7.3265e-04
Loss = 9.2377e-04, PNorm = 43.2112, GNorm = 0.5079, lr_0 = 7.3189e-04
Loss = 8.3511e-04, PNorm = 43.2204, GNorm = 0.6651, lr_0 = 7.3112e-04
Loss = 7.6130e-04, PNorm = 43.2364, GNorm = 0.3791, lr_0 = 7.3036e-04
Loss = 8.5920e-04, PNorm = 43.2505, GNorm = 0.7768, lr_0 = 7.2960e-04
Loss = 9.5007e-04, PNorm = 43.2691, GNorm = 0.7517, lr_0 = 7.2884e-04
Loss = 1.1305e-03, PNorm = 43.2827, GNorm = 1.5165, lr_0 = 7.2808e-04
Loss = 1.2519e-03, PNorm = 43.3050, GNorm = 0.8614, lr_0 = 7.2732e-04
Loss = 9.6231e-04, PNorm = 43.3326, GNorm = 0.5883, lr_0 = 7.2656e-04
Loss = 1.0374e-03, PNorm = 43.3443, GNorm = 1.1558, lr_0 = 7.2580e-04
Loss = 9.4493e-04, PNorm = 43.3600, GNorm = 0.6056, lr_0 = 7.2504e-04
Loss = 9.4695e-04, PNorm = 43.3768, GNorm = 0.5469, lr_0 = 7.2428e-04
Loss = 8.6828e-04, PNorm = 43.3927, GNorm = 0.9584, lr_0 = 7.2353e-04
Loss = 1.0951e-03, PNorm = 43.4104, GNorm = 1.0042, lr_0 = 7.2277e-04
Loss = 9.8519e-04, PNorm = 43.4235, GNorm = 0.7751, lr_0 = 7.2202e-04
Loss = 1.2332e-03, PNorm = 43.4448, GNorm = 1.5665, lr_0 = 7.2127e-04
Loss = 1.4953e-03, PNorm = 43.4639, GNorm = 1.6124, lr_0 = 7.2051e-04
Loss = 1.3700e-03, PNorm = 43.4906, GNorm = 0.9243, lr_0 = 7.1976e-04
Loss = 9.5351e-04, PNorm = 43.5145, GNorm = 0.4403, lr_0 = 7.1901e-04
Validation rmse logD = 0.570421
Validation R2 logD = 0.769708
Validation rmse logP = 0.506518
Validation R2 logP = 0.924902
Epoch 16
Train function
Loss = 8.4521e-04, PNorm = 43.5338, GNorm = 1.0774, lr_0 = 7.1818e-04
Loss = 1.0977e-03, PNorm = 43.5563, GNorm = 0.9431, lr_0 = 7.1743e-04
Loss = 9.2630e-04, PNorm = 43.5743, GNorm = 0.4177, lr_0 = 7.1669e-04
Loss = 9.0476e-04, PNorm = 43.5971, GNorm = 0.9599, lr_0 = 7.1594e-04
Loss = 8.4671e-04, PNorm = 43.6166, GNorm = 0.4701, lr_0 = 7.1519e-04
Loss = 8.2451e-04, PNorm = 43.6336, GNorm = 0.6765, lr_0 = 7.1444e-04
Loss = 8.9694e-04, PNorm = 43.6544, GNorm = 0.5559, lr_0 = 7.1370e-04
Loss = 9.4052e-04, PNorm = 43.6645, GNorm = 0.8217, lr_0 = 7.1295e-04
Loss = 8.8590e-04, PNorm = 43.6799, GNorm = 0.6686, lr_0 = 7.1221e-04
Loss = 8.9988e-04, PNorm = 43.6928, GNorm = 0.7270, lr_0 = 7.1147e-04
Loss = 8.8503e-04, PNorm = 43.7113, GNorm = 0.6127, lr_0 = 7.1072e-04
Loss = 1.1891e-03, PNorm = 43.7317, GNorm = 0.9758, lr_0 = 7.0998e-04
Loss = 8.5850e-04, PNorm = 43.7516, GNorm = 0.5187, lr_0 = 7.0924e-04
Loss = 9.2604e-04, PNorm = 43.7667, GNorm = 0.8924, lr_0 = 7.0850e-04
Loss = 8.4765e-04, PNorm = 43.7745, GNorm = 0.6306, lr_0 = 7.0776e-04
Loss = 1.2128e-03, PNorm = 43.7959, GNorm = 0.9752, lr_0 = 7.0702e-04
Loss = 9.8533e-04, PNorm = 43.8143, GNorm = 0.8385, lr_0 = 7.0628e-04
Loss = 8.1187e-04, PNorm = 43.8349, GNorm = 1.0082, lr_0 = 7.0555e-04
Loss = 1.0419e-03, PNorm = 43.8515, GNorm = 0.9182, lr_0 = 7.0481e-04
Loss = 9.4393e-04, PNorm = 43.8728, GNorm = 0.6535, lr_0 = 7.0408e-04
Loss = 8.6783e-04, PNorm = 43.8956, GNorm = 0.7501, lr_0 = 7.0334e-04
Loss = 9.4095e-04, PNorm = 43.9212, GNorm = 0.9277, lr_0 = 7.0261e-04
Loss = 1.0225e-03, PNorm = 43.9434, GNorm = 0.6297, lr_0 = 7.0187e-04
Validation rmse logD = 0.592078
Validation R2 logD = 0.751890
Validation rmse logP = 0.487263
Validation R2 logP = 0.930503
Epoch 17
Train function
Loss = 8.6363e-04, PNorm = 43.9583, GNorm = 0.6530, lr_0 = 7.0114e-04
Loss = 7.7057e-04, PNorm = 43.9665, GNorm = 1.3278, lr_0 = 7.0041e-04
Loss = 1.0450e-03, PNorm = 43.9780, GNorm = 0.7901, lr_0 = 6.9968e-04
Loss = 8.5889e-04, PNorm = 43.9884, GNorm = 0.6495, lr_0 = 6.9895e-04
Loss = 9.2351e-04, PNorm = 44.0027, GNorm = 0.3022, lr_0 = 6.9822e-04
Loss = 8.2998e-04, PNorm = 44.0131, GNorm = 0.5594, lr_0 = 6.9749e-04
Loss = 8.2041e-04, PNorm = 44.0220, GNorm = 0.8245, lr_0 = 6.9676e-04
Loss = 8.2756e-04, PNorm = 44.0401, GNorm = 0.5149, lr_0 = 6.9603e-04
Loss = 8.0093e-04, PNorm = 44.0563, GNorm = 0.3544, lr_0 = 6.9531e-04
Loss = 7.3599e-04, PNorm = 44.0700, GNorm = 0.5799, lr_0 = 6.9458e-04
Loss = 7.2161e-04, PNorm = 44.0858, GNorm = 0.5310, lr_0 = 6.9386e-04
Loss = 6.3804e-04, PNorm = 44.0990, GNorm = 0.5999, lr_0 = 6.9313e-04
Loss = 9.8364e-04, PNorm = 44.1185, GNorm = 0.3981, lr_0 = 6.9241e-04
Loss = 9.7502e-04, PNorm = 44.1304, GNorm = 0.5699, lr_0 = 6.9169e-04
Loss = 9.2156e-04, PNorm = 44.1487, GNorm = 0.4922, lr_0 = 6.9096e-04
Loss = 8.5304e-04, PNorm = 44.1735, GNorm = 0.7599, lr_0 = 6.9024e-04
Loss = 8.9465e-04, PNorm = 44.1910, GNorm = 0.4276, lr_0 = 6.8952e-04
Loss = 9.4960e-04, PNorm = 44.2097, GNorm = 1.5681, lr_0 = 6.8880e-04
Loss = 8.5560e-04, PNorm = 44.2273, GNorm = 0.5459, lr_0 = 6.8808e-04
Loss = 7.0021e-04, PNorm = 44.2475, GNorm = 0.6234, lr_0 = 6.8737e-04
Loss = 9.6337e-04, PNorm = 44.2584, GNorm = 0.7333, lr_0 = 6.8665e-04
Loss = 7.5746e-04, PNorm = 44.2698, GNorm = 0.4536, lr_0 = 6.8593e-04
Validation rmse logD = 0.561119
Validation R2 logD = 0.777158
Validation rmse logP = 0.486344
Validation R2 logP = 0.930765
Epoch 18
Train function
Loss = 9.9317e-04, PNorm = 44.2888, GNorm = 0.8769, lr_0 = 6.8514e-04
Loss = 6.9174e-04, PNorm = 44.3007, GNorm = 0.5444, lr_0 = 6.8443e-04
Loss = 6.1159e-04, PNorm = 44.3190, GNorm = 0.4364, lr_0 = 6.8372e-04
Loss = 5.6764e-04, PNorm = 44.3311, GNorm = 0.4717, lr_0 = 6.8300e-04
Loss = 6.1695e-04, PNorm = 44.3418, GNorm = 0.4033, lr_0 = 6.8229e-04
Loss = 5.9976e-04, PNorm = 44.3529, GNorm = 0.3257, lr_0 = 6.8158e-04
Loss = 8.5299e-04, PNorm = 44.3698, GNorm = 0.8248, lr_0 = 6.8087e-04
Loss = 6.4323e-04, PNorm = 44.3838, GNorm = 0.4720, lr_0 = 6.8015e-04
Loss = 6.9441e-04, PNorm = 44.3914, GNorm = 0.6453, lr_0 = 6.7944e-04
Loss = 7.9056e-04, PNorm = 44.4041, GNorm = 1.2712, lr_0 = 6.7874e-04
Loss = 9.4464e-04, PNorm = 44.4233, GNorm = 1.1114, lr_0 = 6.7803e-04
Loss = 1.0562e-03, PNorm = 44.4409, GNorm = 0.4574, lr_0 = 6.7732e-04
Loss = 9.7002e-04, PNorm = 44.4593, GNorm = 1.0457, lr_0 = 6.7661e-04
Loss = 6.5290e-04, PNorm = 44.4803, GNorm = 0.4513, lr_0 = 6.7591e-04
Loss = 8.1283e-04, PNorm = 44.5032, GNorm = 0.5202, lr_0 = 6.7520e-04
Loss = 7.4966e-04, PNorm = 44.5143, GNorm = 0.9444, lr_0 = 6.7450e-04
Loss = 9.0834e-04, PNorm = 44.5254, GNorm = 0.5102, lr_0 = 6.7379e-04
Loss = 8.2088e-04, PNorm = 44.5445, GNorm = 0.4219, lr_0 = 6.7309e-04
Loss = 7.6289e-04, PNorm = 44.5666, GNorm = 0.9474, lr_0 = 6.7239e-04
Loss = 1.0146e-03, PNorm = 44.5820, GNorm = 1.5616, lr_0 = 6.7168e-04
Loss = 9.9892e-04, PNorm = 44.5973, GNorm = 1.3847, lr_0 = 6.7098e-04
Loss = 1.2081e-03, PNorm = 44.6251, GNorm = 1.0047, lr_0 = 6.7028e-04
Loss = 1.2838e-03, PNorm = 44.6563, GNorm = 1.6178, lr_0 = 6.6958e-04
Validation rmse logD = 0.645245
Validation R2 logD = 0.705330
Validation rmse logP = 0.486729
Validation R2 logP = 0.930655
Epoch 19
Train function
Loss = 8.4314e-04, PNorm = 44.6806, GNorm = 0.7630, lr_0 = 6.6889e-04
Loss = 7.4192e-04, PNorm = 44.6982, GNorm = 0.5384, lr_0 = 6.6819e-04
Loss = 7.0086e-04, PNorm = 44.7118, GNorm = 0.3700, lr_0 = 6.6749e-04
Loss = 7.3957e-04, PNorm = 44.7276, GNorm = 0.7274, lr_0 = 6.6679e-04
Loss = 6.9326e-04, PNorm = 44.7496, GNorm = 0.9928, lr_0 = 6.6610e-04
Loss = 6.9557e-04, PNorm = 44.7631, GNorm = 0.8641, lr_0 = 6.6540e-04
Loss = 6.6021e-04, PNorm = 44.7756, GNorm = 0.7371, lr_0 = 6.6471e-04
Loss = 6.9874e-04, PNorm = 44.7850, GNorm = 0.4647, lr_0 = 6.6401e-04
Loss = 7.7137e-04, PNorm = 44.7994, GNorm = 0.5248, lr_0 = 6.6332e-04
Loss = 8.3540e-04, PNorm = 44.8152, GNorm = 0.6816, lr_0 = 6.6263e-04
Loss = 6.9073e-04, PNorm = 44.8283, GNorm = 0.4282, lr_0 = 6.6194e-04
Loss = 8.7787e-04, PNorm = 44.8416, GNorm = 0.4881, lr_0 = 6.6125e-04
Loss = 7.3047e-04, PNorm = 44.8587, GNorm = 0.8244, lr_0 = 6.6056e-04
Loss = 6.7472e-04, PNorm = 44.8767, GNorm = 0.6317, lr_0 = 6.5987e-04
Loss = 6.8344e-04, PNorm = 44.8904, GNorm = 0.3878, lr_0 = 6.5918e-04
Loss = 7.5860e-04, PNorm = 44.9040, GNorm = 0.8213, lr_0 = 6.5849e-04
Loss = 7.0017e-04, PNorm = 44.9175, GNorm = 0.6037, lr_0 = 6.5780e-04
Loss = 9.4107e-04, PNorm = 44.9371, GNorm = 0.8881, lr_0 = 6.5712e-04
Loss = 1.0508e-03, PNorm = 44.9565, GNorm = 0.5250, lr_0 = 6.5643e-04
Loss = 8.0112e-04, PNorm = 44.9761, GNorm = 0.8514, lr_0 = 6.5574e-04
Loss = 6.7555e-04, PNorm = 44.9856, GNorm = 0.6226, lr_0 = 6.5506e-04
Loss = 6.8370e-04, PNorm = 45.0021, GNorm = 0.7125, lr_0 = 6.5438e-04
Validation rmse logD = 0.599739
Validation R2 logD = 0.745428
Validation rmse logP = 0.502383
Validation R2 logP = 0.926123
Epoch 20
Train function
Loss = 6.4196e-04, PNorm = 45.0159, GNorm = 0.9992, lr_0 = 6.5363e-04
Loss = 7.0371e-04, PNorm = 45.0352, GNorm = 0.7016, lr_0 = 6.5294e-04
Loss = 5.7775e-04, PNorm = 45.0517, GNorm = 0.4172, lr_0 = 6.5226e-04
Loss = 6.2355e-04, PNorm = 45.0684, GNorm = 0.9925, lr_0 = 6.5158e-04
Loss = 6.0423e-04, PNorm = 45.0824, GNorm = 0.6975, lr_0 = 6.5090e-04
Loss = 6.9498e-04, PNorm = 45.0972, GNorm = 1.6117, lr_0 = 6.5022e-04
Loss = 6.8414e-04, PNorm = 45.1180, GNorm = 0.4165, lr_0 = 6.4954e-04
Loss = 5.2733e-04, PNorm = 45.1257, GNorm = 0.6952, lr_0 = 6.4886e-04
Loss = 5.7684e-04, PNorm = 45.1320, GNorm = 0.5963, lr_0 = 6.4819e-04
Loss = 7.0294e-04, PNorm = 45.1460, GNorm = 0.7982, lr_0 = 6.4751e-04
Loss = 7.3276e-04, PNorm = 45.1615, GNorm = 0.5531, lr_0 = 6.4684e-04
Loss = 5.5144e-04, PNorm = 45.1769, GNorm = 0.8860, lr_0 = 6.4616e-04
Loss = 5.3974e-04, PNorm = 45.1855, GNorm = 0.4686, lr_0 = 6.4549e-04
Loss = 6.0226e-04, PNorm = 45.1973, GNorm = 0.5131, lr_0 = 6.4481e-04
Loss = 6.9719e-04, PNorm = 45.2150, GNorm = 0.8290, lr_0 = 6.4414e-04
Loss = 6.6540e-04, PNorm = 45.2275, GNorm = 0.5581, lr_0 = 6.4347e-04
Loss = 7.6609e-04, PNorm = 45.2398, GNorm = 0.8734, lr_0 = 6.4280e-04
Loss = 8.8529e-04, PNorm = 45.2581, GNorm = 0.6126, lr_0 = 6.4212e-04
Loss = 8.0031e-04, PNorm = 45.2767, GNorm = 0.7292, lr_0 = 6.4145e-04
Loss = 6.4117e-04, PNorm = 45.3011, GNorm = 0.7355, lr_0 = 6.4078e-04
Loss = 7.4434e-04, PNorm = 45.3114, GNorm = 0.7278, lr_0 = 6.4012e-04
Loss = 7.7790e-04, PNorm = 45.3280, GNorm = 0.7029, lr_0 = 6.3945e-04
Loss = 7.7106e-04, PNorm = 45.3436, GNorm = 0.5532, lr_0 = 6.3878e-04
Validation rmse logD = 0.550138
Validation R2 logD = 0.785795
Validation rmse logP = 0.482024
Validation R2 logP = 0.931990
Epoch 21
Train function
Loss = 5.5478e-04, PNorm = 45.3595, GNorm = 0.8589, lr_0 = 6.3811e-04
Loss = 5.8486e-04, PNorm = 45.3732, GNorm = 0.2474, lr_0 = 6.3745e-04
Loss = 5.8411e-04, PNorm = 45.3873, GNorm = 0.4817, lr_0 = 6.3678e-04
Loss = 6.3423e-04, PNorm = 45.3962, GNorm = 0.8495, lr_0 = 6.3612e-04
Loss = 5.5212e-04, PNorm = 45.4148, GNorm = 0.4185, lr_0 = 6.3545e-04
Loss = 5.3588e-04, PNorm = 45.4240, GNorm = 0.2751, lr_0 = 6.3479e-04
Loss = 5.6982e-04, PNorm = 45.4399, GNorm = 0.3809, lr_0 = 6.3413e-04
Loss = 5.4723e-04, PNorm = 45.4600, GNorm = 0.5038, lr_0 = 6.3347e-04
Loss = 6.6342e-04, PNorm = 45.4734, GNorm = 0.7006, lr_0 = 6.3280e-04
Loss = 7.1882e-04, PNorm = 45.4868, GNorm = 0.8925, lr_0 = 6.3214e-04
Loss = 6.2202e-04, PNorm = 45.5059, GNorm = 1.1011, lr_0 = 6.3148e-04
Loss = 6.2222e-04, PNorm = 45.5199, GNorm = 0.6687, lr_0 = 6.3083e-04
Loss = 6.5465e-04, PNorm = 45.5317, GNorm = 0.4190, lr_0 = 6.3017e-04
Loss = 6.0441e-04, PNorm = 45.5442, GNorm = 0.6785, lr_0 = 6.2951e-04
Loss = 7.5908e-04, PNorm = 45.5583, GNorm = 1.1162, lr_0 = 6.2885e-04
Loss = 6.0657e-04, PNorm = 45.5719, GNorm = 1.1116, lr_0 = 6.2820e-04
Loss = 7.7806e-04, PNorm = 45.5839, GNorm = 0.4537, lr_0 = 6.2754e-04
Loss = 8.0277e-04, PNorm = 45.6015, GNorm = 0.4345, lr_0 = 6.2689e-04
Loss = 5.9503e-04, PNorm = 45.6174, GNorm = 0.7759, lr_0 = 6.2623e-04
Loss = 8.3227e-04, PNorm = 45.6319, GNorm = 0.7314, lr_0 = 6.2558e-04
Loss = 8.0996e-04, PNorm = 45.6441, GNorm = 0.5919, lr_0 = 6.2492e-04
Loss = 6.7023e-04, PNorm = 45.6632, GNorm = 0.3331, lr_0 = 6.2427e-04
Loss = 7.0489e-04, PNorm = 45.6834, GNorm = 0.5280, lr_0 = 6.2362e-04
Loss = 1.3792e-03, PNorm = 45.6855, GNorm = 0.7264, lr_0 = 6.2356e-04
Validation rmse logD = 0.548701
Validation R2 logD = 0.786912
Validation rmse logP = 0.510474
Validation R2 logP = 0.923724
Epoch 22
Train function
Loss = 5.9483e-04, PNorm = 45.6970, GNorm = 0.6386, lr_0 = 6.2290e-04
Loss = 5.8660e-04, PNorm = 45.7115, GNorm = 1.0423, lr_0 = 6.2225e-04
Loss = 7.7398e-04, PNorm = 45.7266, GNorm = 0.5881, lr_0 = 6.2161e-04
Loss = 7.5311e-04, PNorm = 45.7382, GNorm = 1.0865, lr_0 = 6.2096e-04
Loss = 5.7361e-04, PNorm = 45.7578, GNorm = 0.8382, lr_0 = 6.2031e-04
Loss = 6.9409e-04, PNorm = 45.7749, GNorm = 0.8582, lr_0 = 6.1966e-04
Loss = 6.1714e-04, PNorm = 45.7902, GNorm = 0.7486, lr_0 = 6.1901e-04
Loss = 5.9274e-04, PNorm = 45.8036, GNorm = 0.5294, lr_0 = 6.1837e-04
Loss = 4.4760e-04, PNorm = 45.8103, GNorm = 0.3752, lr_0 = 6.1772e-04
Loss = 5.5956e-04, PNorm = 45.8229, GNorm = 0.3216, lr_0 = 6.1708e-04
Loss = 4.2626e-04, PNorm = 45.8317, GNorm = 0.7497, lr_0 = 6.1643e-04
Loss = 5.8453e-04, PNorm = 45.8439, GNorm = 0.6663, lr_0 = 6.1579e-04
Loss = 6.3731e-04, PNorm = 45.8547, GNorm = 0.9027, lr_0 = 6.1515e-04
Loss = 5.4223e-04, PNorm = 45.8686, GNorm = 0.3499, lr_0 = 6.1451e-04
Loss = 5.6768e-04, PNorm = 45.8845, GNorm = 0.5102, lr_0 = 6.1386e-04
Loss = 5.7997e-04, PNorm = 45.8978, GNorm = 0.3764, lr_0 = 6.1322e-04
Loss = 6.3778e-04, PNorm = 45.9093, GNorm = 0.8390, lr_0 = 6.1258e-04
Loss = 6.9578e-04, PNorm = 45.9157, GNorm = 0.7906, lr_0 = 6.1194e-04
Loss = 6.1899e-04, PNorm = 45.9340, GNorm = 1.0641, lr_0 = 6.1131e-04
Loss = 6.0985e-04, PNorm = 45.9432, GNorm = 0.6983, lr_0 = 6.1067e-04
Loss = 6.0423e-04, PNorm = 45.9575, GNorm = 0.3767, lr_0 = 6.1003e-04
Loss = 5.8800e-04, PNorm = 45.9712, GNorm = 0.5264, lr_0 = 6.0939e-04
Validation rmse logD = 0.551139
Validation R2 logD = 0.785014
Validation rmse logP = 0.483764
Validation R2 logP = 0.931497
Epoch 23
Train function
Loss = 7.0914e-04, PNorm = 45.9862, GNorm = 0.8606, lr_0 = 6.0876e-04
Loss = 4.5580e-04, PNorm = 46.0038, GNorm = 0.5283, lr_0 = 6.0812e-04
Loss = 4.7578e-04, PNorm = 46.0169, GNorm = 0.6043, lr_0 = 6.0749e-04
Loss = 4.7740e-04, PNorm = 46.0296, GNorm = 0.6870, lr_0 = 6.0685e-04
Loss = 5.3185e-04, PNorm = 46.0450, GNorm = 0.2933, lr_0 = 6.0622e-04
Loss = 4.8327e-04, PNorm = 46.0527, GNorm = 0.5749, lr_0 = 6.0559e-04
Loss = 4.7416e-04, PNorm = 46.0642, GNorm = 0.3694, lr_0 = 6.0496e-04
Loss = 6.1996e-04, PNorm = 46.0792, GNorm = 0.6207, lr_0 = 6.0432e-04
Loss = 6.3552e-04, PNorm = 46.0927, GNorm = 0.8761, lr_0 = 6.0369e-04
Loss = 6.1009e-04, PNorm = 46.1065, GNorm = 0.5920, lr_0 = 6.0306e-04
Loss = 5.3782e-04, PNorm = 46.1148, GNorm = 0.3948, lr_0 = 6.0243e-04
Loss = 6.7454e-04, PNorm = 46.1280, GNorm = 1.2013, lr_0 = 6.0180e-04
Loss = 6.4120e-04, PNorm = 46.1372, GNorm = 0.6046, lr_0 = 6.0118e-04
Loss = 8.0941e-04, PNorm = 46.1545, GNorm = 0.4293, lr_0 = 6.0055e-04
Loss = 7.1742e-04, PNorm = 46.1645, GNorm = 0.5815, lr_0 = 5.9992e-04
Loss = 6.7850e-04, PNorm = 46.1835, GNorm = 0.4823, lr_0 = 5.9930e-04
Loss = 6.2436e-04, PNorm = 46.1987, GNorm = 0.4624, lr_0 = 5.9867e-04
Loss = 5.2896e-04, PNorm = 46.2161, GNorm = 0.4794, lr_0 = 5.9805e-04
Loss = 4.7256e-04, PNorm = 46.2281, GNorm = 0.5559, lr_0 = 5.9742e-04
Loss = 4.7903e-04, PNorm = 46.2444, GNorm = 0.4837, lr_0 = 5.9680e-04
Loss = 5.4896e-04, PNorm = 46.2524, GNorm = 0.4516, lr_0 = 5.9618e-04
Loss = 4.9851e-04, PNorm = 46.2617, GNorm = 0.3211, lr_0 = 5.9555e-04
Loss = 4.3774e-04, PNorm = 46.2717, GNorm = 0.3162, lr_0 = 5.9493e-04
Validation rmse logD = 0.584310
Validation R2 logD = 0.758357
Validation rmse logP = 0.475311
Validation R2 logP = 0.933871
Epoch 24
Train function
Loss = 3.6806e-04, PNorm = 46.2823, GNorm = 0.2772, lr_0 = 5.9425e-04
Loss = 5.4546e-04, PNorm = 46.2950, GNorm = 0.3976, lr_0 = 5.9363e-04
Loss = 4.5892e-04, PNorm = 46.3088, GNorm = 0.7615, lr_0 = 5.9301e-04
Loss = 4.3412e-04, PNorm = 46.3247, GNorm = 0.4692, lr_0 = 5.9239e-04
Loss = 3.8420e-04, PNorm = 46.3377, GNorm = 0.3536, lr_0 = 5.9177e-04
Loss = 4.3952e-04, PNorm = 46.3450, GNorm = 0.3027, lr_0 = 5.9115e-04
Loss = 4.5172e-04, PNorm = 46.3549, GNorm = 0.4378, lr_0 = 5.9054e-04
Loss = 4.4730e-04, PNorm = 46.3624, GNorm = 0.5630, lr_0 = 5.8992e-04
Loss = 5.1918e-04, PNorm = 46.3719, GNorm = 0.6759, lr_0 = 5.8931e-04
Loss = 6.4779e-04, PNorm = 46.3809, GNorm = 0.8235, lr_0 = 5.8869e-04
Loss = 4.9420e-04, PNorm = 46.4001, GNorm = 0.4409, lr_0 = 5.8808e-04
Loss = 5.5817e-04, PNorm = 46.4203, GNorm = 0.7211, lr_0 = 5.8746e-04
Loss = 4.7130e-04, PNorm = 46.4274, GNorm = 0.5771, lr_0 = 5.8685e-04
Loss = 4.7383e-04, PNorm = 46.4364, GNorm = 0.6106, lr_0 = 5.8624e-04
Loss = 5.2573e-04, PNorm = 46.4428, GNorm = 0.3555, lr_0 = 5.8562e-04
Loss = 4.4998e-04, PNorm = 46.4491, GNorm = 0.2631, lr_0 = 5.8501e-04
Loss = 5.0541e-04, PNorm = 46.4540, GNorm = 0.4031, lr_0 = 5.8440e-04
Loss = 4.2186e-04, PNorm = 46.4626, GNorm = 0.5843, lr_0 = 5.8379e-04
Loss = 5.0508e-04, PNorm = 46.4746, GNorm = 0.3066, lr_0 = 5.8318e-04
Loss = 5.3652e-04, PNorm = 46.4887, GNorm = 0.4272, lr_0 = 5.8257e-04
Loss = 5.0508e-04, PNorm = 46.4968, GNorm = 0.7298, lr_0 = 5.8197e-04
Loss = 5.7966e-04, PNorm = 46.5088, GNorm = 0.6187, lr_0 = 5.8136e-04
Validation rmse logD = 0.535552
Validation R2 logD = 0.797002
Validation rmse logP = 0.475901
Validation R2 logP = 0.933706
Epoch 25
Train function
Loss = 3.0756e-04, PNorm = 46.5165, GNorm = 0.4643, lr_0 = 5.8075e-04
Loss = 4.2570e-04, PNorm = 46.5279, GNorm = 0.4500, lr_0 = 5.8015e-04
Loss = 4.6800e-04, PNorm = 46.5362, GNorm = 0.9995, lr_0 = 5.7954e-04
Loss = 4.3836e-04, PNorm = 46.5506, GNorm = 0.5143, lr_0 = 5.7894e-04
Loss = 4.6608e-04, PNorm = 46.5617, GNorm = 0.5885, lr_0 = 5.7833e-04
Loss = 5.1320e-04, PNorm = 46.5757, GNorm = 0.5199, lr_0 = 5.7773e-04
Loss = 5.2192e-04, PNorm = 46.5850, GNorm = 0.3947, lr_0 = 5.7712e-04
Loss = 4.4453e-04, PNorm = 46.5960, GNorm = 0.5639, lr_0 = 5.7652e-04
Loss = 3.9823e-04, PNorm = 46.6057, GNorm = 0.3989, lr_0 = 5.7592e-04
Loss = 5.4356e-04, PNorm = 46.6155, GNorm = 0.8569, lr_0 = 5.7532e-04
Loss = 4.3995e-04, PNorm = 46.6278, GNorm = 0.5379, lr_0 = 5.7472e-04
Loss = 4.7367e-04, PNorm = 46.6342, GNorm = 0.7314, lr_0 = 5.7412e-04
Loss = 4.5074e-04, PNorm = 46.6455, GNorm = 0.3887, lr_0 = 5.7352e-04
Loss = 4.9830e-04, PNorm = 46.6653, GNorm = 0.5114, lr_0 = 5.7292e-04
Loss = 4.8985e-04, PNorm = 46.6803, GNorm = 0.5288, lr_0 = 5.7232e-04
Loss = 3.9448e-04, PNorm = 46.6929, GNorm = 0.2586, lr_0 = 5.7173e-04
Loss = 5.2529e-04, PNorm = 46.7057, GNorm = 0.6066, lr_0 = 5.7113e-04
Loss = 5.3905e-04, PNorm = 46.7158, GNorm = 0.4368, lr_0 = 5.7053e-04
Loss = 5.0194e-04, PNorm = 46.7307, GNorm = 0.3785, lr_0 = 5.6994e-04
Loss = 4.8652e-04, PNorm = 46.7433, GNorm = 0.8308, lr_0 = 5.6934e-04
Loss = 4.9131e-04, PNorm = 46.7543, GNorm = 0.3842, lr_0 = 5.6875e-04
Loss = 4.2029e-04, PNorm = 46.7675, GNorm = 0.3453, lr_0 = 5.6816e-04
Loss = 4.3772e-04, PNorm = 46.7780, GNorm = 0.3656, lr_0 = 5.6756e-04
Validation rmse logD = 0.538754
Validation R2 logD = 0.794569
Validation rmse logP = 0.469823
Validation R2 logP = 0.935389
Epoch 26
Train function
Loss = 3.0573e-04, PNorm = 46.7889, GNorm = 0.5228, lr_0 = 5.6691e-04
Loss = 3.9927e-04, PNorm = 46.7977, GNorm = 0.7482, lr_0 = 5.6632e-04
Loss = 4.0939e-04, PNorm = 46.8087, GNorm = 0.4223, lr_0 = 5.6573e-04
Loss = 4.4883e-04, PNorm = 46.8228, GNorm = 0.5539, lr_0 = 5.6514e-04
Loss = 3.7594e-04, PNorm = 46.8344, GNorm = 0.4620, lr_0 = 5.6455e-04
Loss = 3.7730e-04, PNorm = 46.8423, GNorm = 0.3345, lr_0 = 5.6396e-04
Loss = 3.9689e-04, PNorm = 46.8504, GNorm = 0.3789, lr_0 = 5.6337e-04
Loss = 3.6650e-04, PNorm = 46.8576, GNorm = 0.2910, lr_0 = 5.6278e-04
Loss = 4.5215e-04, PNorm = 46.8713, GNorm = 0.8471, lr_0 = 5.6219e-04
Loss = 4.2469e-04, PNorm = 46.8848, GNorm = 0.6607, lr_0 = 5.6161e-04
Loss = 4.2018e-04, PNorm = 46.8888, GNorm = 0.4375, lr_0 = 5.6102e-04
Loss = 4.6525e-04, PNorm = 46.8995, GNorm = 0.6763, lr_0 = 5.6044e-04
Loss = 4.0798e-04, PNorm = 46.9140, GNorm = 0.5336, lr_0 = 5.5985e-04
Loss = 4.2308e-04, PNorm = 46.9262, GNorm = 0.4360, lr_0 = 5.5927e-04
Loss = 4.3612e-04, PNorm = 46.9348, GNorm = 0.3034, lr_0 = 5.5868e-04
Loss = 4.8011e-04, PNorm = 46.9470, GNorm = 0.5396, lr_0 = 5.5810e-04
Loss = 4.1510e-04, PNorm = 46.9615, GNorm = 0.4385, lr_0 = 5.5752e-04
Loss = 4.2662e-04, PNorm = 46.9681, GNorm = 0.2965, lr_0 = 5.5694e-04
Loss = 4.9442e-04, PNorm = 46.9846, GNorm = 0.2630, lr_0 = 5.5635e-04
Loss = 5.1371e-04, PNorm = 46.9958, GNorm = 0.3980, lr_0 = 5.5577e-04
Loss = 4.2005e-04, PNorm = 47.0040, GNorm = 0.3366, lr_0 = 5.5519e-04
Loss = 5.1911e-04, PNorm = 47.0125, GNorm = 0.2502, lr_0 = 5.5461e-04
Validation rmse logD = 0.535422
Validation R2 logD = 0.797102
Validation rmse logP = 0.474322
Validation R2 logP = 0.934146
Epoch 27
Train function
Loss = 3.1492e-04, PNorm = 47.0228, GNorm = 0.3297, lr_0 = 5.5398e-04
Loss = 3.7628e-04, PNorm = 47.0338, GNorm = 0.3680, lr_0 = 5.5340e-04
Loss = 5.6368e-04, PNorm = 47.0494, GNorm = 0.8625, lr_0 = 5.5282e-04
Loss = 4.4506e-04, PNorm = 47.0645, GNorm = 0.5644, lr_0 = 5.5224e-04
Loss = 3.5141e-04, PNorm = 47.0723, GNorm = 0.5961, lr_0 = 5.5167e-04
Loss = 4.3118e-04, PNorm = 47.0850, GNorm = 0.2781, lr_0 = 5.5109e-04
Loss = 4.1931e-04, PNorm = 47.0951, GNorm = 0.6650, lr_0 = 5.5052e-04
Loss = 3.7966e-04, PNorm = 47.1083, GNorm = 0.5631, lr_0 = 5.4994e-04
Loss = 3.8315e-04, PNorm = 47.1197, GNorm = 0.6328, lr_0 = 5.4937e-04
Loss = 3.9188e-04, PNorm = 47.1295, GNorm = 0.2797, lr_0 = 5.4880e-04
Loss = 4.9912e-04, PNorm = 47.1403, GNorm = 0.6133, lr_0 = 5.4822e-04
Loss = 4.8089e-04, PNorm = 47.1466, GNorm = 0.3705, lr_0 = 5.4765e-04
Loss = 4.6644e-04, PNorm = 47.1615, GNorm = 0.5797, lr_0 = 5.4708e-04
Loss = 4.5214e-04, PNorm = 47.1703, GNorm = 0.5008, lr_0 = 5.4651e-04
Loss = 4.2913e-04, PNorm = 47.1827, GNorm = 0.5214, lr_0 = 5.4594e-04
Loss = 3.6462e-04, PNorm = 47.1912, GNorm = 0.5960, lr_0 = 5.4537e-04
Loss = 4.6005e-04, PNorm = 47.2019, GNorm = 0.3598, lr_0 = 5.4480e-04
Loss = 5.0511e-04, PNorm = 47.2169, GNorm = 0.8055, lr_0 = 5.4423e-04
Loss = 5.1967e-04, PNorm = 47.2324, GNorm = 0.7533, lr_0 = 5.4366e-04
Loss = 4.3816e-04, PNorm = 47.2449, GNorm = 0.5854, lr_0 = 5.4309e-04
Loss = 5.5352e-04, PNorm = 47.2610, GNorm = 1.0719, lr_0 = 5.4253e-04
Loss = 5.5430e-04, PNorm = 47.2731, GNorm = 1.0473, lr_0 = 5.4196e-04
Loss = 4.5562e-04, PNorm = 47.2852, GNorm = 0.4580, lr_0 = 5.4140e-04
Validation rmse logD = 0.548839
Validation R2 logD = 0.786805
Validation rmse logP = 0.470147
Validation R2 logP = 0.935300
Epoch 28
Train function
Loss = 3.7383e-04, PNorm = 47.2993, GNorm = 0.5443, lr_0 = 5.4083e-04
Loss = 4.1580e-04, PNorm = 47.3117, GNorm = 0.2774, lr_0 = 5.4027e-04
Loss = 4.1627e-04, PNorm = 47.3224, GNorm = 0.8352, lr_0 = 5.3970e-04
Loss = 3.5474e-04, PNorm = 47.3317, GNorm = 0.2488, lr_0 = 5.3914e-04
Loss = 4.0217e-04, PNorm = 47.3437, GNorm = 0.4070, lr_0 = 5.3858e-04
Loss = 5.4494e-04, PNorm = 47.3587, GNorm = 0.4445, lr_0 = 5.3801e-04
Loss = 3.9729e-04, PNorm = 47.3725, GNorm = 0.4976, lr_0 = 5.3745e-04
Loss = 3.2114e-04, PNorm = 47.3765, GNorm = 0.4770, lr_0 = 5.3689e-04
Loss = 4.0885e-04, PNorm = 47.3892, GNorm = 0.5392, lr_0 = 5.3633e-04
Loss = 4.8261e-04, PNorm = 47.4034, GNorm = 0.6095, lr_0 = 5.3577e-04
Loss = 3.9957e-04, PNorm = 47.4132, GNorm = 0.4423, lr_0 = 5.3521e-04
Loss = 3.7003e-04, PNorm = 47.4277, GNorm = 0.4453, lr_0 = 5.3465e-04
Loss = 3.6657e-04, PNorm = 47.4382, GNorm = 0.5007, lr_0 = 5.3410e-04
Loss = 3.9540e-04, PNorm = 47.4464, GNorm = 0.3091, lr_0 = 5.3354e-04
Loss = 3.3170e-04, PNorm = 47.4558, GNorm = 0.4329, lr_0 = 5.3298e-04
Loss = 3.3472e-04, PNorm = 47.4651, GNorm = 0.6067, lr_0 = 5.3243e-04
Loss = 4.7839e-04, PNorm = 47.4763, GNorm = 0.8562, lr_0 = 5.3187e-04
Loss = 4.0203e-04, PNorm = 47.4877, GNorm = 0.3749, lr_0 = 5.3131e-04
Loss = 4.2774e-04, PNorm = 47.4998, GNorm = 0.6938, lr_0 = 5.3076e-04
Loss = 4.2944e-04, PNorm = 47.5119, GNorm = 0.5786, lr_0 = 5.3021e-04
Loss = 3.8008e-04, PNorm = 47.5223, GNorm = 0.5637, lr_0 = 5.2965e-04
Loss = 4.2608e-04, PNorm = 47.5334, GNorm = 0.5096, lr_0 = 5.2910e-04
Validation rmse logD = 0.548298
Validation R2 logD = 0.787225
Validation rmse logP = 0.468660
Validation R2 logP = 0.935708
Epoch 29
Train function
Loss = 2.2233e-04, PNorm = 47.5485, GNorm = 0.2900, lr_0 = 5.2849e-04
Loss = 3.6279e-04, PNorm = 47.5600, GNorm = 0.7951, lr_0 = 5.2794e-04
Loss = 4.2214e-04, PNorm = 47.5721, GNorm = 0.4621, lr_0 = 5.2739e-04
Loss = 4.5539e-04, PNorm = 47.5800, GNorm = 0.3429, lr_0 = 5.2684e-04
Loss = 3.8909e-04, PNorm = 47.5941, GNorm = 0.4591, lr_0 = 5.2629e-04
Loss = 4.4084e-04, PNorm = 47.6058, GNorm = 0.4394, lr_0 = 5.2574e-04
Loss = 4.6166e-04, PNorm = 47.6159, GNorm = 0.4832, lr_0 = 5.2519e-04
Loss = 3.6901e-04, PNorm = 47.6250, GNorm = 0.6134, lr_0 = 5.2464e-04
Loss = 4.1809e-04, PNorm = 47.6329, GNorm = 0.3462, lr_0 = 5.2410e-04
Loss = 4.7322e-04, PNorm = 47.6486, GNorm = 0.4123, lr_0 = 5.2355e-04
Loss = 3.3089e-04, PNorm = 47.6607, GNorm = 0.5237, lr_0 = 5.2300e-04
Loss = 3.4299e-04, PNorm = 47.6650, GNorm = 0.7275, lr_0 = 5.2246e-04
Loss = 3.3323e-04, PNorm = 47.6736, GNorm = 0.4396, lr_0 = 5.2191e-04
Loss = 3.5388e-04, PNorm = 47.6869, GNorm = 0.4256, lr_0 = 5.2137e-04
Loss = 4.1690e-04, PNorm = 47.6969, GNorm = 0.6149, lr_0 = 5.2082e-04
Loss = 3.5988e-04, PNorm = 47.7085, GNorm = 0.4545, lr_0 = 5.2028e-04
Loss = 3.5241e-04, PNorm = 47.7151, GNorm = 0.7280, lr_0 = 5.1974e-04
Loss = 4.3000e-04, PNorm = 47.7260, GNorm = 0.5309, lr_0 = 5.1919e-04
Loss = 5.9796e-04, PNorm = 47.7418, GNorm = 1.2226, lr_0 = 5.1865e-04
Loss = 4.8924e-04, PNorm = 47.7540, GNorm = 0.4170, lr_0 = 5.1811e-04
Loss = 3.9003e-04, PNorm = 47.7640, GNorm = 0.2980, lr_0 = 5.1757e-04
Loss = 4.0632e-04, PNorm = 47.7773, GNorm = 0.7374, lr_0 = 5.1703e-04
Loss = 4.3915e-04, PNorm = 47.7889, GNorm = 0.3285, lr_0 = 5.1649e-04
Validation rmse logD = 0.563741
Validation R2 logD = 0.775071
Validation rmse logP = 0.486108
Validation R2 logP = 0.930832
Epoch 30
Train function
Loss = 3.8980e-04, PNorm = 47.8024, GNorm = 0.7274, lr_0 = 5.1595e-04
Loss = 4.1333e-04, PNorm = 47.8131, GNorm = 0.5222, lr_0 = 5.1541e-04
Loss = 4.8226e-04, PNorm = 47.8231, GNorm = 0.5890, lr_0 = 5.1487e-04
Loss = 3.8026e-04, PNorm = 47.8368, GNorm = 0.5296, lr_0 = 5.1434e-04
Loss = 3.7524e-04, PNorm = 47.8458, GNorm = 0.5766, lr_0 = 5.1380e-04
Loss = 3.2910e-04, PNorm = 47.8552, GNorm = 0.2815, lr_0 = 5.1326e-04
Loss = 3.4399e-04, PNorm = 47.8665, GNorm = 0.4972, lr_0 = 5.1273e-04
Loss = 3.5155e-04, PNorm = 47.8769, GNorm = 0.2035, lr_0 = 5.1219e-04
Loss = 2.8892e-04, PNorm = 47.8844, GNorm = 0.4586, lr_0 = 5.1166e-04
Loss = 4.1790e-04, PNorm = 47.8941, GNorm = 0.2821, lr_0 = 5.1112e-04
Loss = 3.1416e-04, PNorm = 47.8992, GNorm = 0.4327, lr_0 = 5.1059e-04
Loss = 3.8248e-04, PNorm = 47.9063, GNorm = 0.4567, lr_0 = 5.1006e-04
Loss = 2.6570e-04, PNorm = 47.9175, GNorm = 0.3066, lr_0 = 5.0953e-04
Loss = 3.2450e-04, PNorm = 47.9292, GNorm = 0.4826, lr_0 = 5.0899e-04
Loss = 3.7064e-04, PNorm = 47.9315, GNorm = 0.6470, lr_0 = 5.0846e-04
Loss = 3.5102e-04, PNorm = 47.9398, GNorm = 0.3645, lr_0 = 5.0793e-04
Loss = 3.4075e-04, PNorm = 47.9528, GNorm = 0.3637, lr_0 = 5.0740e-04
Loss = 3.6717e-04, PNorm = 47.9635, GNorm = 0.3646, lr_0 = 5.0687e-04
Loss = 3.8671e-04, PNorm = 47.9740, GNorm = 0.4771, lr_0 = 5.0634e-04
Loss = 3.5696e-04, PNorm = 47.9847, GNorm = 0.6491, lr_0 = 5.0581e-04
Loss = 3.3766e-04, PNorm = 47.9927, GNorm = 0.3145, lr_0 = 5.0529e-04
Loss = 3.4418e-04, PNorm = 48.0032, GNorm = 0.5927, lr_0 = 5.0476e-04
Validation rmse logD = 0.565165
Validation R2 logD = 0.773933
Validation rmse logP = 0.475073
Validation R2 logP = 0.933937
Epoch 31
Train function
Loss = 4.8307e-04, PNorm = 48.0182, GNorm = 0.7869, lr_0 = 5.0418e-04
Loss = 3.5760e-04, PNorm = 48.0258, GNorm = 0.5995, lr_0 = 5.0365e-04
Loss = 3.8265e-04, PNorm = 48.0312, GNorm = 0.3441, lr_0 = 5.0313e-04
Loss = 2.5775e-04, PNorm = 48.0412, GNorm = 0.3484, lr_0 = 5.0260e-04
Loss = 2.7823e-04, PNorm = 48.0515, GNorm = 0.3575, lr_0 = 5.0208e-04
Loss = 2.5805e-04, PNorm = 48.0578, GNorm = 0.3093, lr_0 = 5.0155e-04
Loss = 2.3638e-04, PNorm = 48.0593, GNorm = 0.5253, lr_0 = 5.0103e-04
Loss = 3.9049e-04, PNorm = 48.0656, GNorm = 0.3411, lr_0 = 5.0051e-04
Loss = 2.9397e-04, PNorm = 48.0709, GNorm = 0.5666, lr_0 = 4.9998e-04
Loss = 3.2806e-04, PNorm = 48.0812, GNorm = 0.3168, lr_0 = 4.9946e-04
Loss = 3.1894e-04, PNorm = 48.0908, GNorm = 0.2793, lr_0 = 4.9894e-04
Loss = 3.1530e-04, PNorm = 48.1000, GNorm = 0.3942, lr_0 = 4.9842e-04
Loss = 3.9584e-04, PNorm = 48.1138, GNorm = 0.7606, lr_0 = 4.9790e-04
Loss = 4.2842e-04, PNorm = 48.1223, GNorm = 0.7284, lr_0 = 4.9738e-04
Loss = 3.2797e-04, PNorm = 48.1308, GNorm = 0.6685, lr_0 = 4.9686e-04
Loss = 3.8430e-04, PNorm = 48.1424, GNorm = 0.3741, lr_0 = 4.9634e-04
Loss = 3.2300e-04, PNorm = 48.1505, GNorm = 0.4348, lr_0 = 4.9583e-04
Loss = 3.2901e-04, PNorm = 48.1590, GNorm = 0.2804, lr_0 = 4.9531e-04
Loss = 4.1748e-04, PNorm = 48.1685, GNorm = 0.4264, lr_0 = 4.9479e-04
Loss = 3.6609e-04, PNorm = 48.1844, GNorm = 0.3609, lr_0 = 4.9427e-04
Loss = 3.5426e-04, PNorm = 48.1919, GNorm = 0.2489, lr_0 = 4.9376e-04
Loss = 2.7675e-04, PNorm = 48.1989, GNorm = 0.2808, lr_0 = 4.9324e-04
Loss = 2.9069e-04, PNorm = 48.2057, GNorm = 0.5013, lr_0 = 4.9273e-04
Validation rmse logD = 0.560861
Validation R2 logD = 0.777363
Validation rmse logP = 0.467855
Validation R2 logP = 0.935929
Epoch 32
Train function
Loss = 2.8012e-04, PNorm = 48.2166, GNorm = 0.6099, lr_0 = 4.9221e-04
Loss = 2.7768e-04, PNorm = 48.2224, GNorm = 0.4697, lr_0 = 4.9170e-04
Loss = 2.8681e-04, PNorm = 48.2317, GNorm = 0.3190, lr_0 = 4.9119e-04
Loss = 3.6567e-04, PNorm = 48.2409, GNorm = 0.3351, lr_0 = 4.9067e-04
Loss = 2.9397e-04, PNorm = 48.2503, GNorm = 0.5409, lr_0 = 4.9016e-04
Loss = 3.1360e-04, PNorm = 48.2608, GNorm = 0.6764, lr_0 = 4.8965e-04
Loss = 3.1131e-04, PNorm = 48.2703, GNorm = 0.4740, lr_0 = 4.8914e-04
Loss = 3.2055e-04, PNorm = 48.2781, GNorm = 0.3635, lr_0 = 4.8863e-04
Loss = 2.3544e-04, PNorm = 48.2820, GNorm = 0.4573, lr_0 = 4.8812e-04
Loss = 2.5193e-04, PNorm = 48.2852, GNorm = 0.3873, lr_0 = 4.8761e-04
Loss = 2.8155e-04, PNorm = 48.2915, GNorm = 0.4092, lr_0 = 4.8710e-04
Loss = 2.7301e-04, PNorm = 48.2999, GNorm = 0.3219, lr_0 = 4.8659e-04
Loss = 2.7037e-04, PNorm = 48.3064, GNorm = 0.2617, lr_0 = 4.8608e-04
Loss = 2.7242e-04, PNorm = 48.3151, GNorm = 0.5795, lr_0 = 4.8558e-04
Loss = 3.6606e-04, PNorm = 48.3238, GNorm = 0.3936, lr_0 = 4.8507e-04
Loss = 3.3021e-04, PNorm = 48.3354, GNorm = 0.5379, lr_0 = 4.8456e-04
Loss = 3.7938e-04, PNorm = 48.3457, GNorm = 0.2476, lr_0 = 4.8406e-04
Loss = 3.0744e-04, PNorm = 48.3535, GNorm = 0.5085, lr_0 = 4.8355e-04
Loss = 3.0137e-04, PNorm = 48.3606, GNorm = 0.5481, lr_0 = 4.8305e-04
Loss = 3.0218e-04, PNorm = 48.3715, GNorm = 0.5510, lr_0 = 4.8254e-04
Loss = 2.9094e-04, PNorm = 48.3795, GNorm = 0.2335, lr_0 = 4.8204e-04
Loss = 3.6965e-04, PNorm = 48.3920, GNorm = 0.2433, lr_0 = 4.8154e-04
Loss = 3.2924e-04, PNorm = 48.4029, GNorm = 0.3717, lr_0 = 4.8104e-04
Loss = 5.5841e-04, PNorm = 48.4040, GNorm = 0.4007, lr_0 = 4.8098e-04
Validation rmse logD = 0.531068
Validation R2 logD = 0.800388
Validation rmse logP = 0.466062
Validation R2 logP = 0.936419
Epoch 33
Train function
Loss = 2.4226e-04, PNorm = 48.4152, GNorm = 0.4953, lr_0 = 4.8048e-04
Loss = 2.0249e-04, PNorm = 48.4224, GNorm = 0.2387, lr_0 = 4.7998e-04
Loss = 2.5236e-04, PNorm = 48.4260, GNorm = 0.4738, lr_0 = 4.7948e-04
Loss = 2.4317e-04, PNorm = 48.4306, GNorm = 0.3551, lr_0 = 4.7898e-04
Loss = 2.2612e-04, PNorm = 48.4361, GNorm = 0.3067, lr_0 = 4.7848e-04
Loss = 2.7768e-04, PNorm = 48.4437, GNorm = 0.4487, lr_0 = 4.7798e-04
Loss = 2.1796e-04, PNorm = 48.4472, GNorm = 0.3407, lr_0 = 4.7748e-04
Loss = 2.8478e-04, PNorm = 48.4564, GNorm = 0.2916, lr_0 = 4.7698e-04
Loss = 3.3242e-04, PNorm = 48.4668, GNorm = 0.4009, lr_0 = 4.7649e-04
Loss = 2.4706e-04, PNorm = 48.4772, GNorm = 0.4072, lr_0 = 4.7599e-04
Loss = 2.2477e-04, PNorm = 48.4822, GNorm = 0.5106, lr_0 = 4.7549e-04
Loss = 2.8233e-04, PNorm = 48.4912, GNorm = 0.3708, lr_0 = 4.7500e-04
Loss = 2.8237e-04, PNorm = 48.4993, GNorm = 0.3172, lr_0 = 4.7450e-04
Loss = 2.6548e-04, PNorm = 48.5078, GNorm = 0.3760, lr_0 = 4.7400e-04
Loss = 3.0493e-04, PNorm = 48.5187, GNorm = 0.2531, lr_0 = 4.7351e-04
Loss = 2.7583e-04, PNorm = 48.5291, GNorm = 0.3586, lr_0 = 4.7302e-04
Loss = 2.6472e-04, PNorm = 48.5386, GNorm = 0.2914, lr_0 = 4.7252e-04
Loss = 2.7756e-04, PNorm = 48.5476, GNorm = 0.3478, lr_0 = 4.7203e-04
Loss = 2.7544e-04, PNorm = 48.5562, GNorm = 0.3501, lr_0 = 4.7154e-04
Loss = 4.5844e-04, PNorm = 48.5612, GNorm = 0.7127, lr_0 = 4.7104e-04
Loss = 3.3598e-04, PNorm = 48.5745, GNorm = 0.4672, lr_0 = 4.7055e-04
Loss = 3.5138e-04, PNorm = 48.5849, GNorm = 0.2557, lr_0 = 4.7006e-04
Validation rmse logD = 0.544130
Validation R2 logD = 0.790448
Validation rmse logP = 0.474192
Validation R2 logP = 0.934182
Epoch 34
Train function
Loss = 2.0928e-04, PNorm = 48.5964, GNorm = 0.4023, lr_0 = 4.6957e-04
Loss = 2.4424e-04, PNorm = 48.6025, GNorm = 0.3340, lr_0 = 4.6908e-04
Loss = 2.2800e-04, PNorm = 48.6042, GNorm = 0.3414, lr_0 = 4.6859e-04
Loss = 2.3032e-04, PNorm = 48.6123, GNorm = 0.3599, lr_0 = 4.6810e-04
Loss = 2.2540e-04, PNorm = 48.6188, GNorm = 0.2186, lr_0 = 4.6761e-04
Loss = 2.5343e-04, PNorm = 48.6247, GNorm = 0.4261, lr_0 = 4.6712e-04
Loss = 3.2223e-04, PNorm = 48.6338, GNorm = 0.4282, lr_0 = 4.6664e-04
Loss = 2.4646e-04, PNorm = 48.6442, GNorm = 0.5422, lr_0 = 4.6615e-04
Loss = 2.5873e-04, PNorm = 48.6517, GNorm = 0.3386, lr_0 = 4.6566e-04
Loss = 2.9104e-04, PNorm = 48.6589, GNorm = 0.6288, lr_0 = 4.6518e-04
Loss = 3.7358e-04, PNorm = 48.6654, GNorm = 1.1925, lr_0 = 4.6469e-04
Loss = 3.5690e-04, PNorm = 48.6759, GNorm = 0.4507, lr_0 = 4.6421e-04
Loss = 3.6978e-04, PNorm = 48.6885, GNorm = 0.4729, lr_0 = 4.6372e-04
Loss = 3.8064e-04, PNorm = 48.7019, GNorm = 0.6340, lr_0 = 4.6324e-04
Loss = 3.0859e-04, PNorm = 48.7127, GNorm = 0.4602, lr_0 = 4.6276e-04
Loss = 2.5473e-04, PNorm = 48.7217, GNorm = 0.3845, lr_0 = 4.6227e-04
Loss = 2.3735e-04, PNorm = 48.7269, GNorm = 0.3926, lr_0 = 4.6179e-04
Loss = 2.7824e-04, PNorm = 48.7345, GNorm = 0.4758, lr_0 = 4.6131e-04
Loss = 3.5559e-04, PNorm = 48.7442, GNorm = 0.5071, lr_0 = 4.6083e-04
Loss = 3.0315e-04, PNorm = 48.7544, GNorm = 0.3610, lr_0 = 4.6035e-04
Loss = 2.9997e-04, PNorm = 48.7660, GNorm = 0.2638, lr_0 = 4.5987e-04
Loss = 2.6455e-04, PNorm = 48.7750, GNorm = 0.3477, lr_0 = 4.5939e-04
Loss = 3.3491e-04, PNorm = 48.7860, GNorm = 0.4610, lr_0 = 4.5891e-04
Validation rmse logD = 0.547644
Validation R2 logD = 0.787733
Validation rmse logP = 0.467346
Validation R2 logP = 0.936068
Epoch 35
Train function
Loss = 2.6882e-04, PNorm = 48.7947, GNorm = 0.2838, lr_0 = 4.5838e-04
Loss = 3.0189e-04, PNorm = 48.8022, GNorm = 0.3783, lr_0 = 4.5790e-04
Loss = 2.7725e-04, PNorm = 48.8102, GNorm = 0.8022, lr_0 = 4.5742e-04
Loss = 3.2817e-04, PNorm = 48.8185, GNorm = 0.4041, lr_0 = 4.5695e-04
Loss = 2.2592e-04, PNorm = 48.8240, GNorm = 0.2759, lr_0 = 4.5647e-04
Loss = 2.6588e-04, PNorm = 48.8305, GNorm = 0.4320, lr_0 = 4.5599e-04
Loss = 2.3929e-04, PNorm = 48.8363, GNorm = 0.3027, lr_0 = 4.5552e-04
Loss = 2.6851e-04, PNorm = 48.8401, GNorm = 0.8157, lr_0 = 4.5504e-04
Loss = 2.6196e-04, PNorm = 48.8497, GNorm = 0.3279, lr_0 = 4.5457e-04
Loss = 2.5601e-04, PNorm = 48.8602, GNorm = 0.2451, lr_0 = 4.5409e-04
Loss = 2.6111e-04, PNorm = 48.8707, GNorm = 0.2680, lr_0 = 4.5362e-04
Loss = 2.4608e-04, PNorm = 48.8757, GNorm = 0.2134, lr_0 = 4.5314e-04
Loss = 2.5434e-04, PNorm = 48.8847, GNorm = 0.3873, lr_0 = 4.5267e-04
Loss = 2.4014e-04, PNorm = 48.8925, GNorm = 0.4717, lr_0 = 4.5220e-04
Loss = 3.1026e-04, PNorm = 48.9037, GNorm = 0.3768, lr_0 = 4.5173e-04
Loss = 2.7063e-04, PNorm = 48.9121, GNorm = 0.3241, lr_0 = 4.5125e-04
Loss = 2.5017e-04, PNorm = 48.9185, GNorm = 0.3800, lr_0 = 4.5078e-04
Loss = 2.2378e-04, PNorm = 48.9263, GNorm = 0.3411, lr_0 = 4.5031e-04
Loss = 2.7561e-04, PNorm = 48.9340, GNorm = 0.4350, lr_0 = 4.4984e-04
Loss = 3.3741e-04, PNorm = 48.9436, GNorm = 0.4611, lr_0 = 4.4937e-04
Loss = 2.9235e-04, PNorm = 48.9533, GNorm = 0.2551, lr_0 = 4.4890e-04
Loss = 3.9121e-04, PNorm = 48.9621, GNorm = 0.3606, lr_0 = 4.4844e-04
Validation rmse logD = 0.537712
Validation R2 logD = 0.795362
Validation rmse logP = 0.471399
Validation R2 logP = 0.934955
Epoch 36
Train function
Loss = 2.5250e-04, PNorm = 48.9701, GNorm = 0.3429, lr_0 = 4.4797e-04
Loss = 2.8797e-04, PNorm = 48.9813, GNorm = 0.2814, lr_0 = 4.4750e-04
Loss = 2.3218e-04, PNorm = 48.9897, GNorm = 0.2463, lr_0 = 4.4703e-04
Loss = 2.2469e-04, PNorm = 48.9978, GNorm = 0.2221, lr_0 = 4.4657e-04
Loss = 2.7501e-04, PNorm = 49.0031, GNorm = 0.4940, lr_0 = 4.4610e-04
Loss = 2.0044e-04, PNorm = 49.0112, GNorm = 0.2675, lr_0 = 4.4564e-04
Loss = 2.2507e-04, PNorm = 49.0183, GNorm = 0.4839, lr_0 = 4.4517e-04
Loss = 1.9721e-04, PNorm = 49.0239, GNorm = 0.3047, lr_0 = 4.4471e-04
Loss = 2.2577e-04, PNorm = 49.0314, GNorm = 0.4382, lr_0 = 4.4424e-04
Loss = 1.9462e-04, PNorm = 49.0356, GNorm = 0.2866, lr_0 = 4.4378e-04
Loss = 2.6149e-04, PNorm = 49.0393, GNorm = 0.3424, lr_0 = 4.4331e-04
Loss = 2.4325e-04, PNorm = 49.0476, GNorm = 0.2036, lr_0 = 4.4285e-04
Loss = 2.6631e-04, PNorm = 49.0530, GNorm = 0.3335, lr_0 = 4.4239e-04
Loss = 2.4259e-04, PNorm = 49.0601, GNorm = 0.6294, lr_0 = 4.4193e-04
Loss = 2.7911e-04, PNorm = 49.0670, GNorm = 0.2128, lr_0 = 4.4147e-04
Loss = 2.7295e-04, PNorm = 49.0773, GNorm = 0.1662, lr_0 = 4.4101e-04
Loss = 2.2520e-04, PNorm = 49.0892, GNorm = 0.2432, lr_0 = 4.4055e-04
Loss = 2.4115e-04, PNorm = 49.0975, GNorm = 0.6045, lr_0 = 4.4009e-04
Loss = 3.3082e-04, PNorm = 49.1050, GNorm = 0.3593, lr_0 = 4.3963e-04
Loss = 2.1526e-04, PNorm = 49.1125, GNorm = 0.2406, lr_0 = 4.3917e-04
Loss = 3.1159e-04, PNorm = 49.1213, GNorm = 0.2191, lr_0 = 4.3871e-04
Loss = 2.1526e-04, PNorm = 49.1304, GNorm = 0.2746, lr_0 = 4.3825e-04
Loss = 2.5326e-04, PNorm = 49.1380, GNorm = 0.3493, lr_0 = 4.3779e-04
Validation rmse logD = 0.537417
Validation R2 logD = 0.795587
Validation rmse logP = 0.468651
Validation R2 logP = 0.935711
Epoch 37
Train function
Loss = 2.1775e-04, PNorm = 49.1448, GNorm = 0.3375, lr_0 = 4.3729e-04
Loss = 2.5043e-04, PNorm = 49.1520, GNorm = 0.3813, lr_0 = 4.3684e-04
Loss = 2.3219e-04, PNorm = 49.1588, GNorm = 0.5665, lr_0 = 4.3638e-04
Loss = 2.4044e-04, PNorm = 49.1634, GNorm = 0.3462, lr_0 = 4.3592e-04
Loss = 1.9436e-04, PNorm = 49.1689, GNorm = 0.3929, lr_0 = 4.3547e-04
Loss = 2.2889e-04, PNorm = 49.1785, GNorm = 0.4218, lr_0 = 4.3501e-04
Loss = 2.9996e-04, PNorm = 49.1860, GNorm = 0.3847, lr_0 = 4.3456e-04
Loss = 2.2987e-04, PNorm = 49.1976, GNorm = 0.3185, lr_0 = 4.3411e-04
Loss = 3.6202e-04, PNorm = 49.2025, GNorm = 0.5499, lr_0 = 4.3365e-04
Loss = 3.8179e-04, PNorm = 49.2169, GNorm = 0.4561, lr_0 = 4.3320e-04
Loss = 3.5143e-04, PNorm = 49.2292, GNorm = 0.4819, lr_0 = 4.3275e-04
Loss = 3.3279e-04, PNorm = 49.2402, GNorm = 0.4053, lr_0 = 4.3230e-04
Loss = 3.0953e-04, PNorm = 49.2528, GNorm = 0.5831, lr_0 = 4.3185e-04
Loss = 2.0877e-04, PNorm = 49.2655, GNorm = 0.3161, lr_0 = 4.3140e-04
Loss = 2.6300e-04, PNorm = 49.2737, GNorm = 0.6198, lr_0 = 4.3094e-04
Loss = 2.2873e-04, PNorm = 49.2824, GNorm = 0.3927, lr_0 = 4.3050e-04
Loss = 2.4172e-04, PNorm = 49.2886, GNorm = 0.3387, lr_0 = 4.3005e-04
Loss = 2.2168e-04, PNorm = 49.2946, GNorm = 0.2335, lr_0 = 4.2960e-04
Loss = 2.6300e-04, PNorm = 49.3002, GNorm = 0.5983, lr_0 = 4.2915e-04
Loss = 2.1447e-04, PNorm = 49.3084, GNorm = 0.2512, lr_0 = 4.2870e-04
Loss = 2.1723e-04, PNorm = 49.3155, GNorm = 0.4014, lr_0 = 4.2825e-04
Loss = 2.3822e-04, PNorm = 49.3189, GNorm = 0.2317, lr_0 = 4.2781e-04
Validation rmse logD = 0.538259
Validation R2 logD = 0.794945
Validation rmse logP = 0.459063
Validation R2 logP = 0.938314
Epoch 38
Train function
Loss = 2.6583e-04, PNorm = 49.3255, GNorm = 0.2804, lr_0 = 4.2736e-04
Loss = 1.8911e-04, PNorm = 49.3289, GNorm = 0.3503, lr_0 = 4.2691e-04
Loss = 1.9793e-04, PNorm = 49.3340, GNorm = 0.1996, lr_0 = 4.2647e-04
Loss = 1.7149e-04, PNorm = 49.3401, GNorm = 0.3044, lr_0 = 4.2602e-04
Loss = 2.2630e-04, PNorm = 49.3463, GNorm = 0.4618, lr_0 = 4.2558e-04
Loss = 2.0946e-04, PNorm = 49.3565, GNorm = 0.2561, lr_0 = 4.2513e-04
Loss = 2.5571e-04, PNorm = 49.3633, GNorm = 0.4702, lr_0 = 4.2469e-04
Loss = 2.1822e-04, PNorm = 49.3703, GNorm = 0.4077, lr_0 = 4.2425e-04
Loss = 2.0287e-04, PNorm = 49.3788, GNorm = 0.6242, lr_0 = 4.2380e-04
Loss = 2.6931e-04, PNorm = 49.3840, GNorm = 0.3176, lr_0 = 4.2336e-04
Loss = 2.1707e-04, PNorm = 49.3912, GNorm = 0.3461, lr_0 = 4.2292e-04
Loss = 2.1953e-04, PNorm = 49.3964, GNorm = 0.3358, lr_0 = 4.2248e-04
Loss = 2.2349e-04, PNorm = 49.4068, GNorm = 0.2492, lr_0 = 4.2204e-04
Loss = 2.5207e-04, PNorm = 49.4138, GNorm = 0.3688, lr_0 = 4.2160e-04
Loss = 2.1749e-04, PNorm = 49.4213, GNorm = 0.4196, lr_0 = 4.2116e-04
Loss = 2.4260e-04, PNorm = 49.4263, GNorm = 0.4211, lr_0 = 4.2072e-04
Loss = 1.9399e-04, PNorm = 49.4306, GNorm = 0.4642, lr_0 = 4.2028e-04
Loss = 2.6141e-04, PNorm = 49.4363, GNorm = 0.5834, lr_0 = 4.1984e-04
Loss = 2.0119e-04, PNorm = 49.4416, GNorm = 0.1770, lr_0 = 4.1940e-04
Loss = 2.5038e-04, PNorm = 49.4468, GNorm = 0.5312, lr_0 = 4.1896e-04
Loss = 2.2133e-04, PNorm = 49.4538, GNorm = 0.4481, lr_0 = 4.1853e-04
Loss = 2.1404e-04, PNorm = 49.4611, GNorm = 0.5063, lr_0 = 4.1809e-04
Loss = 2.2112e-04, PNorm = 49.4617, GNorm = 0.4501, lr_0 = 4.1765e-04
Validation rmse logD = 0.561368
Validation R2 logD = 0.776960
Validation rmse logP = 0.482029
Validation R2 logP = 0.931988
Epoch 39
Train function
Loss = 2.7457e-04, PNorm = 49.4683, GNorm = 0.8201, lr_0 = 4.1717e-04
Loss = 2.7674e-04, PNorm = 49.4775, GNorm = 0.3112, lr_0 = 4.1674e-04
Loss = 2.9728e-04, PNorm = 49.4855, GNorm = 0.6328, lr_0 = 4.1630e-04
Loss = 2.8211e-04, PNorm = 49.4947, GNorm = 0.1850, lr_0 = 4.1587e-04
Loss = 2.8642e-04, PNorm = 49.5086, GNorm = 0.4347, lr_0 = 4.1544e-04
Loss = 2.1391e-04, PNorm = 49.5173, GNorm = 0.2301, lr_0 = 4.1500e-04
Loss = 1.6601e-04, PNorm = 49.5239, GNorm = 0.1795, lr_0 = 4.1457e-04
Loss = 1.6706e-04, PNorm = 49.5323, GNorm = 0.2845, lr_0 = 4.1414e-04
Loss = 1.8278e-04, PNorm = 49.5363, GNorm = 0.5471, lr_0 = 4.1370e-04
Loss = 2.2875e-04, PNorm = 49.5436, GNorm = 0.2531, lr_0 = 4.1327e-04
Loss = 2.0608e-04, PNorm = 49.5495, GNorm = 0.4683, lr_0 = 4.1284e-04
Loss = 1.7940e-04, PNorm = 49.5530, GNorm = 0.2366, lr_0 = 4.1241e-04
Loss = 1.8393e-04, PNorm = 49.5595, GNorm = 0.3659, lr_0 = 4.1198e-04
Loss = 1.7311e-04, PNorm = 49.5687, GNorm = 0.2245, lr_0 = 4.1155e-04
Loss = 1.8319e-04, PNorm = 49.5740, GNorm = 0.4685, lr_0 = 4.1112e-04
Loss = 1.8282e-04, PNorm = 49.5793, GNorm = 0.2646, lr_0 = 4.1069e-04
Loss = 2.0647e-04, PNorm = 49.5846, GNorm = 0.3373, lr_0 = 4.1026e-04
Loss = 2.7248e-04, PNorm = 49.5924, GNorm = 0.4077, lr_0 = 4.0983e-04
Loss = 2.0210e-04, PNorm = 49.6053, GNorm = 0.3027, lr_0 = 4.0941e-04
Loss = 1.9054e-04, PNorm = 49.6155, GNorm = 0.1920, lr_0 = 4.0898e-04
Loss = 2.1602e-04, PNorm = 49.6211, GNorm = 0.3024, lr_0 = 4.0855e-04
Loss = 1.9820e-04, PNorm = 49.6259, GNorm = 0.4393, lr_0 = 4.0813e-04
Validation rmse logD = 0.541854
Validation R2 logD = 0.792197
Validation rmse logP = 0.472405
Validation R2 logP = 0.934677
Epoch 40
Train function
Loss = 2.2017e-04, PNorm = 49.6307, GNorm = 0.5050, lr_0 = 4.0770e-04
Loss = 2.3405e-04, PNorm = 49.6355, GNorm = 0.5947, lr_0 = 4.0727e-04
Loss = 2.5147e-04, PNorm = 49.6447, GNorm = 0.4473, lr_0 = 4.0685e-04
Loss = 2.3600e-04, PNorm = 49.6523, GNorm = 0.7187, lr_0 = 4.0642e-04
Loss = 1.8531e-04, PNorm = 49.6560, GNorm = 0.3966, lr_0 = 4.0600e-04
Loss = 2.0423e-04, PNorm = 49.6624, GNorm = 0.3470, lr_0 = 4.0558e-04
Loss = 1.9815e-04, PNorm = 49.6667, GNorm = 0.3200, lr_0 = 4.0515e-04
Loss = 1.9574e-04, PNorm = 49.6719, GNorm = 0.3674, lr_0 = 4.0473e-04
Loss = 1.5985e-04, PNorm = 49.6787, GNorm = 0.1972, lr_0 = 4.0431e-04
Loss = 1.8139e-04, PNorm = 49.6830, GNorm = 0.3571, lr_0 = 4.0389e-04
Loss = 2.1067e-04, PNorm = 49.6873, GNorm = 0.2896, lr_0 = 4.0346e-04
Loss = 1.9532e-04, PNorm = 49.6943, GNorm = 0.3331, lr_0 = 4.0304e-04
Loss = 1.8041e-04, PNorm = 49.6991, GNorm = 0.3129, lr_0 = 4.0262e-04
Loss = 2.5066e-04, PNorm = 49.7048, GNorm = 0.3968, lr_0 = 4.0220e-04
Loss = 2.2958e-04, PNorm = 49.7097, GNorm = 0.2072, lr_0 = 4.0178e-04
Loss = 1.7498e-04, PNorm = 49.7128, GNorm = 0.5598, lr_0 = 4.0136e-04
Loss = 1.9749e-04, PNorm = 49.7202, GNorm = 0.2590, lr_0 = 4.0094e-04
Loss = 1.8431e-04, PNorm = 49.7289, GNorm = 0.2812, lr_0 = 4.0053e-04
Loss = 2.1778e-04, PNorm = 49.7341, GNorm = 0.6248, lr_0 = 4.0011e-04
Loss = 2.2960e-04, PNorm = 49.7424, GNorm = 0.2780, lr_0 = 3.9969e-04
Loss = 2.3677e-04, PNorm = 49.7508, GNorm = 0.4819, lr_0 = 3.9927e-04
Loss = 2.7680e-04, PNorm = 49.7572, GNorm = 0.7831, lr_0 = 3.9886e-04
Loss = 2.3727e-04, PNorm = 49.7626, GNorm = 0.3927, lr_0 = 3.9844e-04
Validation rmse logD = 0.545527
Validation R2 logD = 0.789371
Validation rmse logP = 0.474608
Validation R2 logP = 0.934066
Epoch 41
Train function
Loss = 2.1914e-04, PNorm = 49.7709, GNorm = 0.2504, lr_0 = 3.9798e-04
Loss = 2.2897e-04, PNorm = 49.7773, GNorm = 0.5681, lr_0 = 3.9757e-04
Loss = 2.3189e-04, PNorm = 49.7827, GNorm = 0.3895, lr_0 = 3.9715e-04
Loss = 2.0532e-04, PNorm = 49.7895, GNorm = 0.2360, lr_0 = 3.9674e-04
Loss = 1.9823e-04, PNorm = 49.7993, GNorm = 0.5112, lr_0 = 3.9632e-04
Loss = 2.0482e-04, PNorm = 49.8068, GNorm = 0.3354, lr_0 = 3.9591e-04
Loss = 1.8552e-04, PNorm = 49.8136, GNorm = 0.2542, lr_0 = 3.9550e-04
Loss = 1.9071e-04, PNorm = 49.8216, GNorm = 0.4495, lr_0 = 3.9508e-04
Loss = 1.8289e-04, PNorm = 49.8269, GNorm = 0.2869, lr_0 = 3.9467e-04
Loss = 1.7895e-04, PNorm = 49.8311, GNorm = 0.2897, lr_0 = 3.9426e-04
Loss = 1.7525e-04, PNorm = 49.8320, GNorm = 0.2785, lr_0 = 3.9385e-04
Loss = 1.7000e-04, PNorm = 49.8387, GNorm = 0.1930, lr_0 = 3.9344e-04
Loss = 2.1805e-04, PNorm = 49.8471, GNorm = 0.4046, lr_0 = 3.9303e-04
Loss = 1.9021e-04, PNorm = 49.8544, GNorm = 0.2164, lr_0 = 3.9262e-04
Loss = 1.8148e-04, PNorm = 49.8569, GNorm = 0.3720, lr_0 = 3.9221e-04
Loss = 2.1333e-04, PNorm = 49.8659, GNorm = 0.1666, lr_0 = 3.9180e-04
Loss = 1.8711e-04, PNorm = 49.8703, GNorm = 0.4727, lr_0 = 3.9139e-04
Loss = 1.6077e-04, PNorm = 49.8761, GNorm = 0.3040, lr_0 = 3.9098e-04
Loss = 1.5609e-04, PNorm = 49.8840, GNorm = 0.2801, lr_0 = 3.9057e-04
Loss = 1.6220e-04, PNorm = 49.8897, GNorm = 0.3253, lr_0 = 3.9016e-04
Loss = 1.4531e-04, PNorm = 49.8940, GNorm = 0.3187, lr_0 = 3.8976e-04
Loss = 1.9571e-04, PNorm = 49.9001, GNorm = 0.5372, lr_0 = 3.8935e-04
Loss = 1.9767e-04, PNorm = 49.9077, GNorm = 0.2862, lr_0 = 3.8894e-04
Validation rmse logD = 0.540934
Validation R2 logD = 0.792902
Validation rmse logP = 0.463777
Validation R2 logP = 0.937041
Epoch 42
Train function
Loss = 1.3954e-04, PNorm = 49.9148, GNorm = 0.2550, lr_0 = 3.8854e-04
Loss = 2.0577e-04, PNorm = 49.9154, GNorm = 0.6985, lr_0 = 3.8813e-04
Loss = 1.5041e-04, PNorm = 49.9195, GNorm = 0.3027, lr_0 = 3.8773e-04
Loss = 1.5500e-04, PNorm = 49.9243, GNorm = 0.3846, lr_0 = 3.8732e-04
Loss = 1.3021e-04, PNorm = 49.9296, GNorm = 0.2978, lr_0 = 3.8692e-04
Loss = 1.6741e-04, PNorm = 49.9343, GNorm = 0.2367, lr_0 = 3.8651e-04
Loss = 1.4444e-04, PNorm = 49.9386, GNorm = 0.2823, lr_0 = 3.8611e-04
Loss = 1.3491e-04, PNorm = 49.9440, GNorm = 0.2632, lr_0 = 3.8571e-04
Loss = 1.2640e-04, PNorm = 49.9480, GNorm = 0.2163, lr_0 = 3.8531e-04
Loss = 1.4430e-04, PNorm = 49.9536, GNorm = 0.2233, lr_0 = 3.8490e-04
Loss = 1.2072e-04, PNorm = 49.9583, GNorm = 0.2257, lr_0 = 3.8450e-04
Loss = 1.6200e-04, PNorm = 49.9629, GNorm = 0.2812, lr_0 = 3.8410e-04
Loss = 1.4499e-04, PNorm = 49.9681, GNorm = 0.2694, lr_0 = 3.8370e-04
Loss = 1.7444e-04, PNorm = 49.9720, GNorm = 0.2122, lr_0 = 3.8330e-04
Loss = 1.5375e-04, PNorm = 49.9778, GNorm = 0.2066, lr_0 = 3.8290e-04
Loss = 2.0738e-04, PNorm = 49.9848, GNorm = 0.3980, lr_0 = 3.8250e-04
Loss = 1.9812e-04, PNorm = 49.9891, GNorm = 0.7320, lr_0 = 3.8210e-04
Loss = 1.8910e-04, PNorm = 49.9965, GNorm = 0.3044, lr_0 = 3.8170e-04
Loss = 1.9260e-04, PNorm = 50.0040, GNorm = 0.3292, lr_0 = 3.8130e-04
Loss = 1.7967e-04, PNorm = 50.0105, GNorm = 0.3256, lr_0 = 3.8090e-04
Loss = 1.6720e-04, PNorm = 50.0180, GNorm = 0.2120, lr_0 = 3.8051e-04
Loss = 1.5129e-04, PNorm = 50.0234, GNorm = 0.2116, lr_0 = 3.8011e-04
Validation rmse logD = 0.532639
Validation R2 logD = 0.799205
Validation rmse logP = 0.464949
Validation R2 logP = 0.936723
Epoch 43
Train function
Loss = 1.3346e-04, PNorm = 50.0309, GNorm = 0.3914, lr_0 = 3.7967e-04
Loss = 1.2427e-04, PNorm = 50.0370, GNorm = 0.2328, lr_0 = 3.7928e-04
Loss = 1.5905e-04, PNorm = 50.0383, GNorm = 0.2084, lr_0 = 3.7888e-04
Loss = 1.5176e-04, PNorm = 50.0422, GNorm = 0.3678, lr_0 = 3.7849e-04
Loss = 1.3389e-04, PNorm = 50.0473, GNorm = 0.1494, lr_0 = 3.7809e-04
Loss = 1.9511e-04, PNorm = 50.0558, GNorm = 0.1482, lr_0 = 3.7770e-04
Loss = 1.9219e-04, PNorm = 50.0632, GNorm = 0.2303, lr_0 = 3.7730e-04
Loss = 1.4289e-04, PNorm = 50.0695, GNorm = 0.2709, lr_0 = 3.7691e-04
Loss = 1.7095e-04, PNorm = 50.0750, GNorm = 0.4685, lr_0 = 3.7652e-04
Loss = 1.7481e-04, PNorm = 50.0816, GNorm = 0.4241, lr_0 = 3.7612e-04
Loss = 1.3890e-04, PNorm = 50.0870, GNorm = 0.1838, lr_0 = 3.7573e-04
Loss = 1.6615e-04, PNorm = 50.0897, GNorm = 0.2972, lr_0 = 3.7534e-04
Loss = 1.5452e-04, PNorm = 50.0931, GNorm = 0.2846, lr_0 = 3.7495e-04
Loss = 2.1260e-04, PNorm = 50.1007, GNorm = 0.6219, lr_0 = 3.7455e-04
Loss = 1.9637e-04, PNorm = 50.1075, GNorm = 0.2641, lr_0 = 3.7416e-04
Loss = 1.7616e-04, PNorm = 50.1156, GNorm = 0.2852, lr_0 = 3.7377e-04
Loss = 1.9773e-04, PNorm = 50.1256, GNorm = 0.2192, lr_0 = 3.7338e-04
Loss = 2.2181e-04, PNorm = 50.1319, GNorm = 0.2904, lr_0 = 3.7299e-04
Loss = 1.7592e-04, PNorm = 50.1368, GNorm = 0.4385, lr_0 = 3.7260e-04
Loss = 1.7326e-04, PNorm = 50.1412, GNorm = 0.2643, lr_0 = 3.7221e-04
Loss = 1.7882e-04, PNorm = 50.1467, GNorm = 0.3804, lr_0 = 3.7183e-04
Loss = 1.7297e-04, PNorm = 50.1541, GNorm = 0.1907, lr_0 = 3.7144e-04
Loss = 1.7511e-04, PNorm = 50.1595, GNorm = 0.2294, lr_0 = 3.7105e-04
Validation rmse logD = 0.542223
Validation R2 logD = 0.791914
Validation rmse logP = 0.468904
Validation R2 logP = 0.935641
Epoch 44
Train function
Loss = 1.6249e-04, PNorm = 50.1656, GNorm = 0.4750, lr_0 = 3.7066e-04
Loss = 1.7452e-04, PNorm = 50.1706, GNorm = 0.1839, lr_0 = 3.7028e-04
Loss = 1.3154e-04, PNorm = 50.1777, GNorm = 0.2897, lr_0 = 3.6989e-04
Loss = 1.3519e-04, PNorm = 50.1832, GNorm = 0.2861, lr_0 = 3.6950e-04
Loss = 1.3942e-04, PNorm = 50.1889, GNorm = 0.4911, lr_0 = 3.6912e-04
Loss = 1.9009e-04, PNorm = 50.1932, GNorm = 0.3279, lr_0 = 3.6873e-04
Loss = 2.2207e-04, PNorm = 50.1977, GNorm = 0.4078, lr_0 = 3.6835e-04
Loss = 1.6609e-04, PNorm = 50.2003, GNorm = 0.3223, lr_0 = 3.6796e-04
Loss = 1.3531e-04, PNorm = 50.2051, GNorm = 0.3588, lr_0 = 3.6758e-04
Loss = 1.7608e-04, PNorm = 50.2133, GNorm = 0.4523, lr_0 = 3.6720e-04
Loss = 1.6113e-04, PNorm = 50.2151, GNorm = 0.3497, lr_0 = 3.6681e-04
Loss = 2.0502e-04, PNorm = 50.2206, GNorm = 0.3730, lr_0 = 3.6643e-04
Loss = 1.5648e-04, PNorm = 50.2270, GNorm = 0.3255, lr_0 = 3.6605e-04
Loss = 1.8235e-04, PNorm = 50.2320, GNorm = 0.2821, lr_0 = 3.6567e-04
Loss = 1.5970e-04, PNorm = 50.2379, GNorm = 0.3525, lr_0 = 3.6528e-04
Loss = 1.6835e-04, PNorm = 50.2443, GNorm = 0.3702, lr_0 = 3.6490e-04
Loss = 1.4411e-04, PNorm = 50.2518, GNorm = 0.2257, lr_0 = 3.6452e-04
Loss = 1.5484e-04, PNorm = 50.2574, GNorm = 0.3575, lr_0 = 3.6414e-04
Loss = 2.1314e-04, PNorm = 50.2609, GNorm = 0.6943, lr_0 = 3.6376e-04
Loss = 1.9015e-04, PNorm = 50.2641, GNorm = 0.2459, lr_0 = 3.6338e-04
Loss = 1.6379e-04, PNorm = 50.2704, GNorm = 0.2625, lr_0 = 3.6300e-04
Loss = 1.5782e-04, PNorm = 50.2775, GNorm = 0.2186, lr_0 = 3.6262e-04
Validation rmse logD = 0.542770
Validation R2 logD = 0.791494
Validation rmse logP = 0.469569
Validation R2 logP = 0.935459
Epoch 45
Train function
Loss = 1.4327e-04, PNorm = 50.2860, GNorm = 0.2907, lr_0 = 3.6221e-04
Loss = 1.3733e-04, PNorm = 50.2892, GNorm = 0.1683, lr_0 = 3.6183e-04
Loss = 1.4498e-04, PNorm = 50.2920, GNorm = 0.4438, lr_0 = 3.6145e-04
Loss = 1.3161e-04, PNorm = 50.2974, GNorm = 0.2591, lr_0 = 3.6107e-04
Loss = 1.3028e-04, PNorm = 50.3022, GNorm = 0.3784, lr_0 = 3.6070e-04
Loss = 1.4800e-04, PNorm = 50.3071, GNorm = 0.1697, lr_0 = 3.6032e-04
Loss = 1.4054e-04, PNorm = 50.3101, GNorm = 0.3107, lr_0 = 3.5994e-04
Loss = 2.0135e-04, PNorm = 50.3149, GNorm = 0.4139, lr_0 = 3.5957e-04
Loss = 1.4490e-04, PNorm = 50.3206, GNorm = 0.2562, lr_0 = 3.5919e-04
Loss = 1.3337e-04, PNorm = 50.3271, GNorm = 0.1716, lr_0 = 3.5882e-04
Loss = 1.1672e-04, PNorm = 50.3314, GNorm = 0.2405, lr_0 = 3.5844e-04
Loss = 1.2900e-04, PNorm = 50.3346, GNorm = 0.2928, lr_0 = 3.5807e-04
Loss = 1.9080e-04, PNorm = 50.3409, GNorm = 0.4840, lr_0 = 3.5770e-04
Loss = 1.3421e-04, PNorm = 50.3451, GNorm = 0.4081, lr_0 = 3.5732e-04
Loss = 1.1311e-04, PNorm = 50.3487, GNorm = 0.1782, lr_0 = 3.5695e-04
Loss = 1.3267e-04, PNorm = 50.3509, GNorm = 0.2296, lr_0 = 3.5658e-04
Loss = 1.2054e-04, PNorm = 50.3536, GNorm = 0.4306, lr_0 = 3.5621e-04
Loss = 1.8166e-04, PNorm = 50.3561, GNorm = 0.4579, lr_0 = 3.5583e-04
Loss = 1.6389e-04, PNorm = 50.3649, GNorm = 0.3335, lr_0 = 3.5546e-04
Loss = 1.6050e-04, PNorm = 50.3723, GNorm = 0.2541, lr_0 = 3.5509e-04
Loss = 1.6407e-04, PNorm = 50.3783, GNorm = 0.2530, lr_0 = 3.5472e-04
Loss = 1.3030e-04, PNorm = 50.3853, GNorm = 0.1966, lr_0 = 3.5435e-04
Loss = 1.4792e-04, PNorm = 50.3894, GNorm = 0.2373, lr_0 = 3.5398e-04
Validation rmse logD = 0.545217
Validation R2 logD = 0.789610
Validation rmse logP = 0.473073
Validation R2 logP = 0.934492
Epoch 46
Train function
Loss = 1.1822e-04, PNorm = 50.3922, GNorm = 0.2737, lr_0 = 3.5361e-04
Loss = 1.2146e-04, PNorm = 50.3948, GNorm = 0.3180, lr_0 = 3.5324e-04
Loss = 1.3466e-04, PNorm = 50.4006, GNorm = 0.2522, lr_0 = 3.5287e-04
Loss = 1.3737e-04, PNorm = 50.4054, GNorm = 0.4057, lr_0 = 3.5251e-04
Loss = 1.1462e-04, PNorm = 50.4095, GNorm = 0.2370, lr_0 = 3.5214e-04
Loss = 1.1644e-04, PNorm = 50.4125, GNorm = 0.3160, lr_0 = 3.5177e-04
Loss = 1.2451e-04, PNorm = 50.4173, GNorm = 0.2208, lr_0 = 3.5140e-04
Loss = 1.2325e-04, PNorm = 50.4263, GNorm = 0.4149, lr_0 = 3.5104e-04
Loss = 1.1510e-04, PNorm = 50.4290, GNorm = 0.2079, lr_0 = 3.5067e-04
Loss = 1.2569e-04, PNorm = 50.4355, GNorm = 0.3109, lr_0 = 3.5030e-04
Loss = 1.4160e-04, PNorm = 50.4404, GNorm = 0.5029, lr_0 = 3.4994e-04
Loss = 1.6254e-04, PNorm = 50.4480, GNorm = 0.2664, lr_0 = 3.4957e-04
Loss = 1.3513e-04, PNorm = 50.4552, GNorm = 0.1575, lr_0 = 3.4921e-04
Loss = 1.4743e-04, PNorm = 50.4583, GNorm = 0.2848, lr_0 = 3.4884e-04
Loss = 1.3336e-04, PNorm = 50.4602, GNorm = 0.3135, lr_0 = 3.4848e-04
Loss = 1.6475e-04, PNorm = 50.4637, GNorm = 0.2797, lr_0 = 3.4812e-04
Loss = 1.3631e-04, PNorm = 50.4689, GNorm = 0.2360, lr_0 = 3.4775e-04
Loss = 1.3495e-04, PNorm = 50.4721, GNorm = 0.2403, lr_0 = 3.4739e-04
Loss = 1.2718e-04, PNorm = 50.4789, GNorm = 0.1986, lr_0 = 3.4703e-04
Loss = 1.3907e-04, PNorm = 50.4851, GNorm = 0.2635, lr_0 = 3.4666e-04
Loss = 1.4373e-04, PNorm = 50.4898, GNorm = 0.2662, lr_0 = 3.4630e-04
Loss = 1.3388e-04, PNorm = 50.4931, GNorm = 0.1676, lr_0 = 3.4594e-04
Validation rmse logD = 0.539838
Validation R2 logD = 0.793741
Validation rmse logP = 0.464405
Validation R2 logP = 0.936871
Epoch 47
Train function
Loss = 1.5357e-04, PNorm = 50.4969, GNorm = 0.5340, lr_0 = 3.4554e-04
Loss = 1.4692e-04, PNorm = 50.5003, GNorm = 0.3638, lr_0 = 3.4518e-04
Loss = 1.1820e-04, PNorm = 50.5064, GNorm = 0.2840, lr_0 = 3.4482e-04
Loss = 1.1922e-04, PNorm = 50.5117, GNorm = 0.1958, lr_0 = 3.4446e-04
Loss = 1.2979e-04, PNorm = 50.5203, GNorm = 0.1899, lr_0 = 3.4410e-04
Loss = 1.2117e-04, PNorm = 50.5230, GNorm = 0.2003, lr_0 = 3.4374e-04
Loss = 1.1359e-04, PNorm = 50.5288, GNorm = 0.1913, lr_0 = 3.4339e-04
Loss = 1.1146e-04, PNorm = 50.5360, GNorm = 0.1894, lr_0 = 3.4303e-04
Loss = 1.1403e-04, PNorm = 50.5400, GNorm = 0.2470, lr_0 = 3.4267e-04
Loss = 1.2530e-04, PNorm = 50.5438, GNorm = 0.2998, lr_0 = 3.4231e-04
Loss = 1.0811e-04, PNorm = 50.5481, GNorm = 0.3678, lr_0 = 3.4195e-04
Loss = 1.5648e-04, PNorm = 50.5524, GNorm = 0.3172, lr_0 = 3.4160e-04
Loss = 1.5600e-04, PNorm = 50.5574, GNorm = 0.4967, lr_0 = 3.4124e-04
Loss = 1.1790e-04, PNorm = 50.5613, GNorm = 0.2638, lr_0 = 3.4088e-04
Loss = 1.0199e-04, PNorm = 50.5647, GNorm = 0.2182, lr_0 = 3.4053e-04
Loss = 1.1631e-04, PNorm = 50.5676, GNorm = 0.2169, lr_0 = 3.4017e-04
Loss = 9.7956e-05, PNorm = 50.5692, GNorm = 0.2292, lr_0 = 3.3982e-04
Loss = 1.0291e-04, PNorm = 50.5752, GNorm = 0.2107, lr_0 = 3.3946e-04
Loss = 1.8276e-04, PNorm = 50.5815, GNorm = 0.2838, lr_0 = 3.3911e-04
Loss = 1.8472e-04, PNorm = 50.5854, GNorm = 0.4080, lr_0 = 3.3876e-04
Loss = 1.2296e-04, PNorm = 50.5913, GNorm = 0.2272, lr_0 = 3.3840e-04
Loss = 1.3577e-04, PNorm = 50.5954, GNorm = 0.2438, lr_0 = 3.3805e-04
Loss = 1.5687e-04, PNorm = 50.5998, GNorm = 0.1561, lr_0 = 3.3770e-04
Validation rmse logD = 0.535865
Validation R2 logD = 0.796765
Validation rmse logP = 0.464761
Validation R2 logP = 0.936774
Epoch 48
Train function
Loss = 1.3377e-04, PNorm = 50.6055, GNorm = 0.2885, lr_0 = 3.3734e-04
Loss = 1.3293e-04, PNorm = 50.6144, GNorm = 0.2767, lr_0 = 3.3699e-04
Loss = 1.1154e-04, PNorm = 50.6183, GNorm = 0.2633, lr_0 = 3.3664e-04
Loss = 1.4026e-04, PNorm = 50.6211, GNorm = 0.2301, lr_0 = 3.3629e-04
Loss = 1.0074e-04, PNorm = 50.6234, GNorm = 0.2071, lr_0 = 3.3594e-04
Loss = 9.5235e-05, PNorm = 50.6260, GNorm = 0.1863, lr_0 = 3.3559e-04
Loss = 1.0823e-04, PNorm = 50.6308, GNorm = 0.2194, lr_0 = 3.3524e-04
Loss = 1.3612e-04, PNorm = 50.6350, GNorm = 0.2064, lr_0 = 3.3489e-04
Loss = 1.4415e-04, PNorm = 50.6393, GNorm = 0.3939, lr_0 = 3.3454e-04
Loss = 1.1633e-04, PNorm = 50.6436, GNorm = 0.2842, lr_0 = 3.3419e-04
Loss = 1.4532e-04, PNorm = 50.6476, GNorm = 0.3613, lr_0 = 3.3384e-04
Loss = 1.3967e-04, PNorm = 50.6512, GNorm = 0.2870, lr_0 = 3.3349e-04
Loss = 1.1597e-04, PNorm = 50.6560, GNorm = 0.2232, lr_0 = 3.3314e-04
Loss = 1.4950e-04, PNorm = 50.6609, GNorm = 0.1865, lr_0 = 3.3280e-04
Loss = 1.2003e-04, PNorm = 50.6662, GNorm = 0.2141, lr_0 = 3.3245e-04
Loss = 1.2634e-04, PNorm = 50.6709, GNorm = 0.2490, lr_0 = 3.3210e-04
Loss = 1.1334e-04, PNorm = 50.6761, GNorm = 0.2090, lr_0 = 3.3175e-04
Loss = 1.3302e-04, PNorm = 50.6803, GNorm = 0.2199, lr_0 = 3.3141e-04
Loss = 1.4017e-04, PNorm = 50.6831, GNorm = 0.2252, lr_0 = 3.3106e-04
Loss = 1.2977e-04, PNorm = 50.6887, GNorm = 0.3196, lr_0 = 3.3072e-04
Loss = 1.9231e-04, PNorm = 50.6922, GNorm = 0.3054, lr_0 = 3.3037e-04
Loss = 1.2350e-04, PNorm = 50.6954, GNorm = 0.2076, lr_0 = 3.3003e-04
Validation rmse logD = 0.535182
Validation R2 logD = 0.797283
Validation rmse logP = 0.465903
Validation R2 logP = 0.936463
Epoch 49
Train function
Loss = 1.1869e-04, PNorm = 50.7002, GNorm = 0.4584, lr_0 = 3.2965e-04
Loss = 1.2665e-04, PNorm = 50.7059, GNorm = 0.1866, lr_0 = 3.2930e-04
Loss = 1.1935e-04, PNorm = 50.7114, GNorm = 0.1642, lr_0 = 3.2896e-04
Loss = 1.1392e-04, PNorm = 50.7151, GNorm = 0.3562, lr_0 = 3.2862e-04
Loss = 1.0316e-04, PNorm = 50.7183, GNorm = 0.2225, lr_0 = 3.2827e-04
Loss = 1.4453e-04, PNorm = 50.7235, GNorm = 0.2612, lr_0 = 3.2793e-04
Loss = 1.2550e-04, PNorm = 50.7281, GNorm = 0.2620, lr_0 = 3.2759e-04
Loss = 1.2187e-04, PNorm = 50.7331, GNorm = 0.2416, lr_0 = 3.2725e-04
Loss = 9.6978e-05, PNorm = 50.7372, GNorm = 0.3812, lr_0 = 3.2691e-04
Loss = 1.0324e-04, PNorm = 50.7417, GNorm = 0.1353, lr_0 = 3.2656e-04
Loss = 1.2434e-04, PNorm = 50.7469, GNorm = 0.2498, lr_0 = 3.2622e-04
Loss = 1.0916e-04, PNorm = 50.7508, GNorm = 0.3596, lr_0 = 3.2588e-04
Loss = 1.4884e-04, PNorm = 50.7539, GNorm = 0.1878, lr_0 = 3.2554e-04
Loss = 1.4622e-04, PNorm = 50.7615, GNorm = 0.3830, lr_0 = 3.2520e-04
Loss = 1.0804e-04, PNorm = 50.7689, GNorm = 0.2148, lr_0 = 3.2486e-04
Loss = 1.4472e-04, PNorm = 50.7723, GNorm = 0.2620, lr_0 = 3.2452e-04
Loss = 1.1880e-04, PNorm = 50.7766, GNorm = 0.3914, lr_0 = 3.2419e-04
Loss = 1.0656e-04, PNorm = 50.7791, GNorm = 0.2138, lr_0 = 3.2385e-04
Loss = 1.6656e-04, PNorm = 50.7859, GNorm = 0.4326, lr_0 = 3.2351e-04
Loss = 1.8992e-04, PNorm = 50.7911, GNorm = 0.3157, lr_0 = 3.2317e-04
Loss = 1.6670e-04, PNorm = 50.7978, GNorm = 0.3325, lr_0 = 3.2283e-04
Loss = 1.8503e-04, PNorm = 50.8038, GNorm = 0.4067, lr_0 = 3.2250e-04
Loss = 1.3829e-04, PNorm = 50.8076, GNorm = 0.3476, lr_0 = 3.2216e-04
Validation rmse logD = 0.539154
Validation R2 logD = 0.794263
Validation rmse logP = 0.461431
Validation R2 logP = 0.937677
Epoch 50
Train function
Loss = 1.5003e-04, PNorm = 50.8139, GNorm = 0.2425, lr_0 = 3.2182e-04
Loss = 1.2540e-04, PNorm = 50.8143, GNorm = 0.3570, lr_0 = 3.2149e-04
Loss = 1.2114e-04, PNorm = 50.8162, GNorm = 0.2068, lr_0 = 3.2115e-04
Loss = 1.4446e-04, PNorm = 50.8212, GNorm = 0.2981, lr_0 = 3.2082e-04
Loss = 1.2805e-04, PNorm = 50.8254, GNorm = 0.2127, lr_0 = 3.2048e-04
Loss = 1.2158e-04, PNorm = 50.8294, GNorm = 0.3452, lr_0 = 3.2015e-04
Loss = 1.0032e-04, PNorm = 50.8366, GNorm = 0.5179, lr_0 = 3.1981e-04
Loss = 1.1707e-04, PNorm = 50.8405, GNorm = 0.2417, lr_0 = 3.1948e-04
Loss = 1.0258e-04, PNorm = 50.8424, GNorm = 0.4073, lr_0 = 3.1915e-04
Loss = 1.2837e-04, PNorm = 50.8434, GNorm = 0.1831, lr_0 = 3.1881e-04
Loss = 1.3217e-04, PNorm = 50.8475, GNorm = 0.3821, lr_0 = 3.1848e-04
Loss = 1.1885e-04, PNorm = 50.8526, GNorm = 0.3032, lr_0 = 3.1815e-04
Loss = 9.2155e-05, PNorm = 50.8556, GNorm = 0.1639, lr_0 = 3.1782e-04
Loss = 1.0432e-04, PNorm = 50.8600, GNorm = 0.1311, lr_0 = 3.1749e-04
Loss = 1.2270e-04, PNorm = 50.8630, GNorm = 0.2079, lr_0 = 3.1715e-04
Loss = 8.3629e-05, PNorm = 50.8669, GNorm = 0.1969, lr_0 = 3.1682e-04
Loss = 9.6269e-05, PNorm = 50.8710, GNorm = 0.2949, lr_0 = 3.1649e-04
Loss = 8.5516e-05, PNorm = 50.8752, GNorm = 0.2367, lr_0 = 3.1616e-04
Loss = 1.5049e-04, PNorm = 50.8794, GNorm = 0.2861, lr_0 = 3.1583e-04
Loss = 1.4900e-04, PNorm = 50.8829, GNorm = 0.3652, lr_0 = 3.1550e-04
Loss = 1.5393e-04, PNorm = 50.8876, GNorm = 0.4733, lr_0 = 3.1517e-04
Loss = 1.3251e-04, PNorm = 50.8929, GNorm = 0.2596, lr_0 = 3.1484e-04
Validation rmse logD = 0.536553
Validation R2 logD = 0.796244
Validation rmse logP = 0.462319
Validation R2 logP = 0.937436
Epoch 51
Train function
Loss = 1.6829e-04, PNorm = 50.8963, GNorm = 0.3540, lr_0 = 3.1448e-04
Loss = 8.2447e-05, PNorm = 50.9017, GNorm = 0.2014, lr_0 = 3.1415e-04
Loss = 1.0472e-04, PNorm = 50.9061, GNorm = 0.1940, lr_0 = 3.1383e-04
Loss = 1.1137e-04, PNorm = 50.9108, GNorm = 0.1960, lr_0 = 3.1350e-04
Loss = 8.9293e-05, PNorm = 50.9140, GNorm = 0.2526, lr_0 = 3.1317e-04
Loss = 1.1742e-04, PNorm = 50.9173, GNorm = 0.2344, lr_0 = 3.1284e-04
Loss = 1.1715e-04, PNorm = 50.9223, GNorm = 0.1983, lr_0 = 3.1252e-04
Loss = 1.1770e-04, PNorm = 50.9271, GNorm = 0.2896, lr_0 = 3.1219e-04
Loss = 9.8576e-05, PNorm = 50.9314, GNorm = 0.1768, lr_0 = 3.1187e-04
Loss = 8.3160e-05, PNorm = 50.9347, GNorm = 0.1672, lr_0 = 3.1154e-04
Loss = 1.2171e-04, PNorm = 50.9382, GNorm = 0.5023, lr_0 = 3.1122e-04
Loss = 1.1779e-04, PNorm = 50.9409, GNorm = 0.1591, lr_0 = 3.1089e-04
Loss = 1.1220e-04, PNorm = 50.9458, GNorm = 0.1976, lr_0 = 3.1057e-04
Loss = 9.5669e-05, PNorm = 50.9493, GNorm = 0.1919, lr_0 = 3.1024e-04
Loss = 1.0164e-04, PNorm = 50.9520, GNorm = 0.2494, lr_0 = 3.0992e-04
Loss = 9.8082e-05, PNorm = 50.9565, GNorm = 0.1708, lr_0 = 3.0959e-04
Loss = 9.7291e-05, PNorm = 50.9585, GNorm = 0.2486, lr_0 = 3.0927e-04
Loss = 8.5730e-05, PNorm = 50.9606, GNorm = 0.1813, lr_0 = 3.0895e-04
Loss = 1.0231e-04, PNorm = 50.9656, GNorm = 0.1295, lr_0 = 3.0863e-04
Loss = 1.2971e-04, PNorm = 50.9696, GNorm = 0.5369, lr_0 = 3.0830e-04
Loss = 1.1504e-04, PNorm = 50.9747, GNorm = 0.2731, lr_0 = 3.0798e-04
Loss = 1.6083e-04, PNorm = 50.9796, GNorm = 0.1681, lr_0 = 3.0766e-04
Loss = 1.1486e-04, PNorm = 50.9864, GNorm = 0.2558, lr_0 = 3.0734e-04
Validation rmse logD = 0.534823
Validation R2 logD = 0.797555
Validation rmse logP = 0.465534
Validation R2 logP = 0.936563
Epoch 52
Train function
Loss = 8.8261e-05, PNorm = 50.9899, GNorm = 0.1839, lr_0 = 3.0699e-04
Loss = 1.1187e-04, PNorm = 50.9920, GNorm = 0.2545, lr_0 = 3.0667e-04
Loss = 9.3891e-05, PNorm = 50.9938, GNorm = 0.2247, lr_0 = 3.0635e-04
Loss = 9.1576e-05, PNorm = 50.9988, GNorm = 0.1806, lr_0 = 3.0603e-04
Loss = 9.0146e-05, PNorm = 51.0022, GNorm = 0.3309, lr_0 = 3.0571e-04
Loss = 8.7713e-05, PNorm = 51.0074, GNorm = 0.1110, lr_0 = 3.0539e-04
Loss = 8.8986e-05, PNorm = 51.0106, GNorm = 0.2014, lr_0 = 3.0507e-04
Loss = 8.2106e-05, PNorm = 51.0143, GNorm = 0.1620, lr_0 = 3.0475e-04
Loss = 8.2788e-05, PNorm = 51.0154, GNorm = 0.2914, lr_0 = 3.0443e-04
Loss = 1.0203e-04, PNorm = 51.0196, GNorm = 0.2758, lr_0 = 3.0412e-04
Loss = 9.6825e-05, PNorm = 51.0233, GNorm = 0.1797, lr_0 = 3.0380e-04
Loss = 1.0127e-04, PNorm = 51.0256, GNorm = 0.2120, lr_0 = 3.0348e-04
Loss = 8.7034e-05, PNorm = 51.0301, GNorm = 0.2639, lr_0 = 3.0316e-04
Loss = 9.0327e-05, PNorm = 51.0342, GNorm = 0.3629, lr_0 = 3.0285e-04
Loss = 9.6775e-05, PNorm = 51.0364, GNorm = 0.2387, lr_0 = 3.0253e-04
Loss = 1.3092e-04, PNorm = 51.0413, GNorm = 0.2307, lr_0 = 3.0222e-04
Loss = 8.9434e-05, PNorm = 51.0468, GNorm = 0.1581, lr_0 = 3.0190e-04
Loss = 9.5150e-05, PNorm = 51.0514, GNorm = 0.1603, lr_0 = 3.0159e-04
Loss = 8.5400e-05, PNorm = 51.0550, GNorm = 0.1666, lr_0 = 3.0127e-04
Loss = 1.2221e-04, PNorm = 51.0604, GNorm = 0.2067, lr_0 = 3.0096e-04
Loss = 1.1830e-04, PNorm = 51.0646, GNorm = 0.3783, lr_0 = 3.0064e-04
Loss = 9.9232e-05, PNorm = 51.0686, GNorm = 0.1984, lr_0 = 3.0033e-04
Loss = 1.1952e-04, PNorm = 51.0727, GNorm = 0.1807, lr_0 = 3.0001e-04
Validation rmse logD = 0.538840
Validation R2 logD = 0.794503
Validation rmse logP = 0.463959
Validation R2 logP = 0.936992
Epoch 53
Train function
Loss = 8.4848e-05, PNorm = 51.0768, GNorm = 0.1976, lr_0 = 2.9970e-04
Loss = 1.1374e-04, PNorm = 51.0811, GNorm = 0.2735, lr_0 = 2.9939e-04
Loss = 9.0912e-05, PNorm = 51.0838, GNorm = 0.2868, lr_0 = 2.9908e-04
Loss = 9.3272e-05, PNorm = 51.0880, GNorm = 0.1251, lr_0 = 2.9876e-04
Loss = 8.2934e-05, PNorm = 51.0927, GNorm = 0.0986, lr_0 = 2.9845e-04
Loss = 8.1455e-05, PNorm = 51.0959, GNorm = 0.1459, lr_0 = 2.9814e-04
Loss = 7.3975e-05, PNorm = 51.0993, GNorm = 0.2000, lr_0 = 2.9783e-04
Loss = 7.6827e-05, PNorm = 51.1034, GNorm = 0.1398, lr_0 = 2.9752e-04
Loss = 8.4326e-05, PNorm = 51.1073, GNorm = 0.1697, lr_0 = 2.9721e-04
Loss = 7.8416e-05, PNorm = 51.1088, GNorm = 0.2595, lr_0 = 2.9690e-04
Loss = 9.0403e-05, PNorm = 51.1102, GNorm = 0.3547, lr_0 = 2.9659e-04
Loss = 9.6173e-05, PNorm = 51.1136, GNorm = 0.2092, lr_0 = 2.9628e-04
Loss = 9.3356e-05, PNorm = 51.1161, GNorm = 0.2537, lr_0 = 2.9597e-04
Loss = 1.0749e-04, PNorm = 51.1195, GNorm = 0.3774, lr_0 = 2.9566e-04
Loss = 1.1570e-04, PNorm = 51.1229, GNorm = 0.3095, lr_0 = 2.9535e-04
Loss = 9.2403e-05, PNorm = 51.1276, GNorm = 0.2009, lr_0 = 2.9504e-04
Loss = 1.0170e-04, PNorm = 51.1313, GNorm = 0.2761, lr_0 = 2.9474e-04
Loss = 1.3124e-04, PNorm = 51.1373, GNorm = 0.3207, lr_0 = 2.9443e-04
Loss = 1.2158e-04, PNorm = 51.1429, GNorm = 0.3175, lr_0 = 2.9412e-04
Loss = 1.0614e-04, PNorm = 51.1492, GNorm = 0.1914, lr_0 = 2.9381e-04
Loss = 1.1965e-04, PNorm = 51.1578, GNorm = 0.1879, lr_0 = 2.9351e-04
Loss = 9.3125e-05, PNorm = 51.1606, GNorm = 0.2372, lr_0 = 2.9320e-04
Validation rmse logD = 0.531462
Validation R2 logD = 0.800092
Validation rmse logP = 0.467429
Validation R2 logP = 0.936046
Epoch 54
Train function
Loss = 1.1227e-04, PNorm = 51.1631, GNorm = 0.1752, lr_0 = 2.9286e-04
Loss = 8.6651e-05, PNorm = 51.1670, GNorm = 0.1789, lr_0 = 2.9256e-04
Loss = 9.1931e-05, PNorm = 51.1714, GNorm = 0.2145, lr_0 = 2.9225e-04
Loss = 7.9491e-05, PNorm = 51.1739, GNorm = 0.2131, lr_0 = 2.9195e-04
Loss = 7.6514e-05, PNorm = 51.1741, GNorm = 0.1493, lr_0 = 2.9164e-04
Loss = 6.9908e-05, PNorm = 51.1755, GNorm = 0.1524, lr_0 = 2.9134e-04
Loss = 8.8207e-05, PNorm = 51.1771, GNorm = 0.1719, lr_0 = 2.9104e-04
Loss = 8.1051e-05, PNorm = 51.1793, GNorm = 0.1382, lr_0 = 2.9073e-04
Loss = 8.7270e-05, PNorm = 51.1822, GNorm = 0.1202, lr_0 = 2.9043e-04
Loss = 7.2441e-05, PNorm = 51.1848, GNorm = 0.1244, lr_0 = 2.9012e-04
Loss = 8.0510e-05, PNorm = 51.1897, GNorm = 0.2888, lr_0 = 2.8982e-04
Loss = 9.5548e-05, PNorm = 51.1938, GNorm = 0.1734, lr_0 = 2.8952e-04
Loss = 7.6579e-05, PNorm = 51.1957, GNorm = 0.2562, lr_0 = 2.8922e-04
Loss = 8.4567e-05, PNorm = 51.1990, GNorm = 0.4191, lr_0 = 2.8892e-04
Loss = 9.6918e-05, PNorm = 51.2005, GNorm = 0.3226, lr_0 = 2.8861e-04
Loss = 1.1077e-04, PNorm = 51.2036, GNorm = 0.1987, lr_0 = 2.8831e-04
Loss = 1.0786e-04, PNorm = 51.2100, GNorm = 0.2957, lr_0 = 2.8801e-04
Loss = 7.7276e-05, PNorm = 51.2148, GNorm = 0.1835, lr_0 = 2.8771e-04
Loss = 9.6153e-05, PNorm = 51.2169, GNorm = 0.2727, lr_0 = 2.8741e-04
Loss = 9.1168e-05, PNorm = 51.2196, GNorm = 0.1611, lr_0 = 2.8711e-04
Loss = 7.8860e-05, PNorm = 51.2228, GNorm = 0.2554, lr_0 = 2.8681e-04
Loss = 8.9953e-05, PNorm = 51.2264, GNorm = 0.4485, lr_0 = 2.8651e-04
Loss = 1.6009e-04, PNorm = 51.2333, GNorm = 0.5906, lr_0 = 2.8621e-04
Validation rmse logD = 0.533648
Validation R2 logD = 0.798444
Validation rmse logP = 0.460741
Validation R2 logP = 0.937863
Epoch 55
Train function
Loss = 1.6013e-04, PNorm = 51.2399, GNorm = 0.2998, lr_0 = 2.8591e-04
Loss = 1.5869e-04, PNorm = 51.2456, GNorm = 0.6006, lr_0 = 2.8562e-04
Loss = 1.3158e-04, PNorm = 51.2511, GNorm = 0.3205, lr_0 = 2.8532e-04
Loss = 1.1561e-04, PNorm = 51.2560, GNorm = 0.2998, lr_0 = 2.8502e-04
Loss = 9.6365e-05, PNorm = 51.2594, GNorm = 0.2264, lr_0 = 2.8472e-04
Loss = 1.0815e-04, PNorm = 51.2625, GNorm = 0.1948, lr_0 = 2.8443e-04
Loss = 1.0496e-04, PNorm = 51.2664, GNorm = 0.2889, lr_0 = 2.8413e-04
Loss = 8.4143e-05, PNorm = 51.2705, GNorm = 0.2433, lr_0 = 2.8383e-04
Loss = 7.9286e-05, PNorm = 51.2751, GNorm = 0.2239, lr_0 = 2.8354e-04
Loss = 7.8291e-05, PNorm = 51.2785, GNorm = 0.2444, lr_0 = 2.8324e-04
Loss = 6.6929e-05, PNorm = 51.2817, GNorm = 0.1256, lr_0 = 2.8294e-04
Loss = 7.5335e-05, PNorm = 51.2849, GNorm = 0.1771, lr_0 = 2.8265e-04
Loss = 9.7201e-05, PNorm = 51.2882, GNorm = 0.2084, lr_0 = 2.8235e-04
Loss = 8.9206e-05, PNorm = 51.2939, GNorm = 0.2368, lr_0 = 2.8206e-04
Loss = 1.0509e-04, PNorm = 51.2964, GNorm = 0.1098, lr_0 = 2.8176e-04
Loss = 7.4930e-05, PNorm = 51.3024, GNorm = 0.1492, lr_0 = 2.8147e-04
Loss = 9.3579e-05, PNorm = 51.3045, GNorm = 0.1791, lr_0 = 2.8118e-04
Loss = 9.8024e-05, PNorm = 51.3081, GNorm = 0.1753, lr_0 = 2.8088e-04
Loss = 8.1742e-05, PNorm = 51.3121, GNorm = 0.2079, lr_0 = 2.8059e-04
Loss = 9.5454e-05, PNorm = 51.3137, GNorm = 0.1887, lr_0 = 2.8030e-04
Loss = 1.0458e-04, PNorm = 51.3182, GNorm = 0.2236, lr_0 = 2.8000e-04
Loss = 8.8188e-05, PNorm = 51.3221, GNorm = 0.2460, lr_0 = 2.7971e-04
Validation rmse logD = 0.540176
Validation R2 logD = 0.793482
Validation rmse logP = 0.462207
Validation R2 logP = 0.937467
Epoch 56
Train function
Loss = 6.8102e-05, PNorm = 51.3243, GNorm = 0.2439, lr_0 = 2.7939e-04
Loss = 7.4781e-05, PNorm = 51.3275, GNorm = 0.2256, lr_0 = 2.7910e-04
Loss = 7.9956e-05, PNorm = 51.3307, GNorm = 0.1982, lr_0 = 2.7881e-04
Loss = 8.0458e-05, PNorm = 51.3330, GNorm = 0.2072, lr_0 = 2.7852e-04
Loss = 7.1773e-05, PNorm = 51.3355, GNorm = 0.2157, lr_0 = 2.7823e-04
Loss = 9.3008e-05, PNorm = 51.3375, GNorm = 0.1950, lr_0 = 2.7794e-04
Loss = 7.2343e-05, PNorm = 51.3419, GNorm = 0.1597, lr_0 = 2.7765e-04
Loss = 7.4029e-05, PNorm = 51.3441, GNorm = 0.2307, lr_0 = 2.7736e-04
Loss = 6.1994e-05, PNorm = 51.3464, GNorm = 0.1256, lr_0 = 2.7707e-04
Loss = 8.0187e-05, PNorm = 51.3488, GNorm = 0.4293, lr_0 = 2.7678e-04
Loss = 8.0824e-05, PNorm = 51.3508, GNorm = 0.1767, lr_0 = 2.7649e-04
Loss = 8.0627e-05, PNorm = 51.3554, GNorm = 0.1341, lr_0 = 2.7620e-04
Loss = 9.5295e-05, PNorm = 51.3589, GNorm = 0.2201, lr_0 = 2.7591e-04
Loss = 9.5980e-05, PNorm = 51.3625, GNorm = 0.1497, lr_0 = 2.7562e-04
Loss = 1.0100e-04, PNorm = 51.3677, GNorm = 0.2703, lr_0 = 2.7534e-04
Loss = 8.0278e-05, PNorm = 51.3722, GNorm = 0.2199, lr_0 = 2.7505e-04
Loss = 8.1408e-05, PNorm = 51.3744, GNorm = 0.1247, lr_0 = 2.7476e-04
Loss = 1.0916e-04, PNorm = 51.3778, GNorm = 0.3926, lr_0 = 2.7448e-04
Loss = 7.8050e-05, PNorm = 51.3818, GNorm = 0.2425, lr_0 = 2.7419e-04
Loss = 8.7947e-05, PNorm = 51.3857, GNorm = 0.2058, lr_0 = 2.7390e-04
Loss = 8.8680e-05, PNorm = 51.3882, GNorm = 0.3404, lr_0 = 2.7362e-04
Loss = 1.0424e-04, PNorm = 51.3928, GNorm = 0.1977, lr_0 = 2.7333e-04
Loss = 1.0600e-04, PNorm = 51.3976, GNorm = 0.4299, lr_0 = 2.7305e-04
Validation rmse logD = 0.543540
Validation R2 logD = 0.790902
Validation rmse logP = 0.474497
Validation R2 logP = 0.934097
Epoch 57
Train function
Loss = 1.4964e-04, PNorm = 51.4018, GNorm = 0.3109, lr_0 = 2.7276e-04
Loss = 1.4963e-04, PNorm = 51.4082, GNorm = 0.3261, lr_0 = 2.7248e-04
Loss = 9.8815e-05, PNorm = 51.4103, GNorm = 0.1781, lr_0 = 2.7219e-04
Loss = 8.9128e-05, PNorm = 51.4127, GNorm = 0.3062, lr_0 = 2.7191e-04
Loss = 8.1060e-05, PNorm = 51.4153, GNorm = 0.1516, lr_0 = 2.7162e-04
Loss = 8.6230e-05, PNorm = 51.4188, GNorm = 0.1709, lr_0 = 2.7134e-04
Loss = 7.5873e-05, PNorm = 51.4222, GNorm = 0.3192, lr_0 = 2.7106e-04
Loss = 7.5944e-05, PNorm = 51.4242, GNorm = 0.1904, lr_0 = 2.7077e-04
Loss = 6.3555e-05, PNorm = 51.4276, GNorm = 0.1506, lr_0 = 2.7049e-04
Loss = 6.6516e-05, PNorm = 51.4293, GNorm = 0.1997, lr_0 = 2.7021e-04
Loss = 6.6203e-05, PNorm = 51.4302, GNorm = 0.1750, lr_0 = 2.6993e-04
Loss = 1.0070e-04, PNorm = 51.4341, GNorm = 0.3612, lr_0 = 2.6965e-04
Loss = 7.9939e-05, PNorm = 51.4346, GNorm = 0.2638, lr_0 = 2.6936e-04
Loss = 8.2656e-05, PNorm = 51.4389, GNorm = 0.1866, lr_0 = 2.6908e-04
Loss = 9.1754e-05, PNorm = 51.4426, GNorm = 0.1747, lr_0 = 2.6880e-04
Loss = 7.5911e-05, PNorm = 51.4448, GNorm = 0.3081, lr_0 = 2.6852e-04
Loss = 8.0527e-05, PNorm = 51.4505, GNorm = 0.1223, lr_0 = 2.6824e-04
Loss = 7.1978e-05, PNorm = 51.4545, GNorm = 0.1359, lr_0 = 2.6796e-04
Loss = 6.9065e-05, PNorm = 51.4561, GNorm = 0.1524, lr_0 = 2.6768e-04
Loss = 8.0580e-05, PNorm = 51.4589, GNorm = 0.1405, lr_0 = 2.6740e-04
Loss = 7.0938e-05, PNorm = 51.4619, GNorm = 0.1348, lr_0 = 2.6712e-04
Loss = 8.3626e-05, PNorm = 51.4639, GNorm = 0.2166, lr_0 = 2.6684e-04
Validation rmse logD = 0.534471
Validation R2 logD = 0.797821
Validation rmse logP = 0.464693
Validation R2 logP = 0.936792
Epoch 58
Train function
Loss = 5.4249e-05, PNorm = 51.4666, GNorm = 0.1639, lr_0 = 2.6654e-04
Loss = 6.1531e-05, PNorm = 51.4690, GNorm = 0.1886, lr_0 = 2.6626e-04
Loss = 7.3686e-05, PNorm = 51.4692, GNorm = 0.2966, lr_0 = 2.6598e-04
Loss = 5.9241e-05, PNorm = 51.4711, GNorm = 0.1570, lr_0 = 2.6570e-04
Loss = 5.0846e-05, PNorm = 51.4737, GNorm = 0.2358, lr_0 = 2.6543e-04
Loss = 7.3692e-05, PNorm = 51.4757, GNorm = 0.1667, lr_0 = 2.6515e-04
Loss = 6.5114e-05, PNorm = 51.4779, GNorm = 0.1919, lr_0 = 2.6487e-04
Loss = 6.9964e-05, PNorm = 51.4802, GNorm = 0.1695, lr_0 = 2.6460e-04
Loss = 6.5166e-05, PNorm = 51.4817, GNorm = 0.2955, lr_0 = 2.6432e-04
Loss = 5.9907e-05, PNorm = 51.4822, GNorm = 0.1799, lr_0 = 2.6405e-04
Loss = 6.8296e-05, PNorm = 51.4826, GNorm = 0.2512, lr_0 = 2.6377e-04
Loss = 8.6703e-05, PNorm = 51.4867, GNorm = 0.4014, lr_0 = 2.6349e-04
Loss = 6.8943e-05, PNorm = 51.4886, GNorm = 0.1960, lr_0 = 2.6322e-04
Loss = 6.1727e-05, PNorm = 51.4919, GNorm = 0.1771, lr_0 = 2.6294e-04
Loss = 7.6285e-05, PNorm = 51.4947, GNorm = 0.2112, lr_0 = 2.6267e-04
Loss = 6.3568e-05, PNorm = 51.4976, GNorm = 0.1397, lr_0 = 2.6240e-04
Loss = 5.9168e-05, PNorm = 51.5009, GNorm = 0.1377, lr_0 = 2.6212e-04
Loss = 9.3605e-05, PNorm = 51.5033, GNorm = 0.1849, lr_0 = 2.6185e-04
Loss = 7.2814e-05, PNorm = 51.5060, GNorm = 0.1008, lr_0 = 2.6158e-04
Loss = 1.0450e-04, PNorm = 51.5081, GNorm = 0.1651, lr_0 = 2.6130e-04
Loss = 6.6151e-05, PNorm = 51.5122, GNorm = 0.1094, lr_0 = 2.6103e-04
Loss = 6.6226e-05, PNorm = 51.5157, GNorm = 0.1836, lr_0 = 2.6076e-04
Loss = 6.5701e-05, PNorm = 51.5198, GNorm = 0.1386, lr_0 = 2.6048e-04
Validation rmse logD = 0.536026
Validation R2 logD = 0.796643
Validation rmse logP = 0.467005
Validation R2 logP = 0.936162
Epoch 59
Train function
Loss = 5.3438e-05, PNorm = 51.5223, GNorm = 0.2155, lr_0 = 2.6021e-04
Loss = 5.4785e-05, PNorm = 51.5243, GNorm = 0.1339, lr_0 = 2.5994e-04
Loss = 5.3137e-05, PNorm = 51.5282, GNorm = 0.1264, lr_0 = 2.5967e-04
Loss = 5.7079e-05, PNorm = 51.5316, GNorm = 0.1376, lr_0 = 2.5940e-04
Loss = 5.4700e-05, PNorm = 51.5346, GNorm = 0.1752, lr_0 = 2.5913e-04
Loss = 6.3005e-05, PNorm = 51.5364, GNorm = 0.1073, lr_0 = 2.5886e-04
Loss = 5.3619e-05, PNorm = 51.5394, GNorm = 0.2542, lr_0 = 2.5859e-04
Loss = 5.4780e-05, PNorm = 51.5416, GNorm = 0.2173, lr_0 = 2.5832e-04
Loss = 5.6888e-05, PNorm = 51.5436, GNorm = 0.2469, lr_0 = 2.5805e-04
Loss = 5.4984e-05, PNorm = 51.5466, GNorm = 0.2345, lr_0 = 2.5778e-04
Loss = 5.8510e-05, PNorm = 51.5497, GNorm = 0.1454, lr_0 = 2.5751e-04
Loss = 6.1320e-05, PNorm = 51.5533, GNorm = 0.2738, lr_0 = 2.5724e-04
Loss = 9.1000e-05, PNorm = 51.5575, GNorm = 0.3026, lr_0 = 2.5697e-04
Loss = 7.7890e-05, PNorm = 51.5612, GNorm = 0.1949, lr_0 = 2.5670e-04
Loss = 6.7498e-05, PNorm = 51.5644, GNorm = 0.1415, lr_0 = 2.5644e-04
Loss = 6.3868e-05, PNorm = 51.5674, GNorm = 0.2401, lr_0 = 2.5617e-04
Loss = 6.9550e-05, PNorm = 51.5693, GNorm = 0.1665, lr_0 = 2.5590e-04
Loss = 6.8209e-05, PNorm = 51.5730, GNorm = 0.1807, lr_0 = 2.5563e-04
Loss = 7.1702e-05, PNorm = 51.5765, GNorm = 0.1844, lr_0 = 2.5537e-04
Loss = 7.9898e-05, PNorm = 51.5780, GNorm = 0.1675, lr_0 = 2.5510e-04
Loss = 7.9599e-05, PNorm = 51.5792, GNorm = 0.1924, lr_0 = 2.5483e-04
Loss = 6.7160e-05, PNorm = 51.5826, GNorm = 0.1976, lr_0 = 2.5457e-04
Validation rmse logD = 0.536107
Validation R2 logD = 0.796581
Validation rmse logP = 0.465683
Validation R2 logP = 0.936523
Epoch 60
Train function
Loss = 8.1168e-05, PNorm = 51.5858, GNorm = 0.2743, lr_0 = 2.5428e-04
Loss = 6.9721e-05, PNorm = 51.5896, GNorm = 0.1775, lr_0 = 2.5401e-04
Loss = 7.7883e-05, PNorm = 51.5922, GNorm = 0.1900, lr_0 = 2.5375e-04
Loss = 7.5401e-05, PNorm = 51.5969, GNorm = 0.2534, lr_0 = 2.5348e-04
Loss = 6.5177e-05, PNorm = 51.5995, GNorm = 0.2106, lr_0 = 2.5322e-04
Loss = 7.9770e-05, PNorm = 51.6019, GNorm = 0.1742, lr_0 = 2.5295e-04
Loss = 5.8201e-05, PNorm = 51.6029, GNorm = 0.2519, lr_0 = 2.5269e-04
Loss = 6.7736e-05, PNorm = 51.6047, GNorm = 0.1782, lr_0 = 2.5242e-04
Loss = 6.4215e-05, PNorm = 51.6076, GNorm = 0.1912, lr_0 = 2.5216e-04
Loss = 6.1121e-05, PNorm = 51.6102, GNorm = 0.1603, lr_0 = 2.5190e-04
Loss = 6.8491e-05, PNorm = 51.6132, GNorm = 0.2428, lr_0 = 2.5163e-04
Loss = 6.3683e-05, PNorm = 51.6167, GNorm = 0.1014, lr_0 = 2.5137e-04
Loss = 5.9419e-05, PNorm = 51.6201, GNorm = 0.2115, lr_0 = 2.5111e-04
Loss = 5.7760e-05, PNorm = 51.6214, GNorm = 0.2219, lr_0 = 2.5085e-04
Loss = 6.2774e-05, PNorm = 51.6245, GNorm = 0.1499, lr_0 = 2.5059e-04
Loss = 5.8217e-05, PNorm = 51.6276, GNorm = 0.1519, lr_0 = 2.5032e-04
Loss = 4.6508e-05, PNorm = 51.6297, GNorm = 0.1163, lr_0 = 2.5006e-04
Loss = 6.0505e-05, PNorm = 51.6335, GNorm = 0.2035, lr_0 = 2.4980e-04
Loss = 5.7511e-05, PNorm = 51.6358, GNorm = 0.1526, lr_0 = 2.4954e-04
Loss = 6.9813e-05, PNorm = 51.6389, GNorm = 0.1874, lr_0 = 2.4928e-04
Loss = 8.1184e-05, PNorm = 51.6430, GNorm = 0.1523, lr_0 = 2.4902e-04
Loss = 7.1168e-05, PNorm = 51.6458, GNorm = 0.3036, lr_0 = 2.4876e-04
Loss = 9.3136e-05, PNorm = 51.6507, GNorm = 0.2464, lr_0 = 2.4850e-04
Validation rmse logD = 0.536321
Validation R2 logD = 0.796419
Validation rmse logP = 0.469964
Validation R2 logP = 0.935350
Epoch 61
Train function
Loss = 9.3393e-05, PNorm = 51.6556, GNorm = 0.3810, lr_0 = 2.4824e-04
Loss = 7.4977e-05, PNorm = 51.6588, GNorm = 0.2323, lr_0 = 2.4798e-04
Loss = 7.8565e-05, PNorm = 51.6600, GNorm = 0.2068, lr_0 = 2.4772e-04
Loss = 8.8443e-05, PNorm = 51.6644, GNorm = 0.1864, lr_0 = 2.4747e-04
Loss = 6.4779e-05, PNorm = 51.6699, GNorm = 0.1544, lr_0 = 2.4721e-04
Loss = 6.6348e-05, PNorm = 51.6736, GNorm = 0.1603, lr_0 = 2.4695e-04
Loss = 6.0651e-05, PNorm = 51.6756, GNorm = 0.1371, lr_0 = 2.4669e-04
Loss = 4.9490e-05, PNorm = 51.6779, GNorm = 0.1921, lr_0 = 2.4643e-04
Loss = 5.1342e-05, PNorm = 51.6774, GNorm = 0.2084, lr_0 = 2.4618e-04
Loss = 5.0246e-05, PNorm = 51.6783, GNorm = 0.2664, lr_0 = 2.4592e-04
Loss = 5.7150e-05, PNorm = 51.6813, GNorm = 0.1798, lr_0 = 2.4566e-04
Loss = 5.8809e-05, PNorm = 51.6841, GNorm = 0.2031, lr_0 = 2.4541e-04
Loss = 5.4802e-05, PNorm = 51.6884, GNorm = 0.1047, lr_0 = 2.4515e-04
Loss = 7.2133e-05, PNorm = 51.6886, GNorm = 0.2867, lr_0 = 2.4489e-04
Loss = 6.0923e-05, PNorm = 51.6913, GNorm = 0.1905, lr_0 = 2.4464e-04
Loss = 5.7232e-05, PNorm = 51.6917, GNorm = 0.1390, lr_0 = 2.4438e-04
Loss = 5.6488e-05, PNorm = 51.6924, GNorm = 0.2054, lr_0 = 2.4413e-04
Loss = 6.8977e-05, PNorm = 51.6956, GNorm = 0.1202, lr_0 = 2.4387e-04
Loss = 5.6082e-05, PNorm = 51.6984, GNorm = 0.2362, lr_0 = 2.4362e-04
Loss = 7.2579e-05, PNorm = 51.7011, GNorm = 0.1898, lr_0 = 2.4337e-04
Loss = 6.9168e-05, PNorm = 51.7052, GNorm = 0.3496, lr_0 = 2.4311e-04
Loss = 7.6252e-05, PNorm = 51.7091, GNorm = 0.2711, lr_0 = 2.4286e-04
Validation rmse logD = 0.538037
Validation R2 logD = 0.795115
Validation rmse logP = 0.463985
Validation R2 logP = 0.936985
Epoch 62
Train function
Loss = 4.5508e-05, PNorm = 51.7105, GNorm = 0.1227, lr_0 = 2.4258e-04
Loss = 4.4398e-05, PNorm = 51.7116, GNorm = 0.1571, lr_0 = 2.4233e-04
Loss = 5.9397e-05, PNorm = 51.7131, GNorm = 0.1568, lr_0 = 2.4207e-04
Loss = 5.2054e-05, PNorm = 51.7158, GNorm = 0.1923, lr_0 = 2.4182e-04
Loss = 6.1817e-05, PNorm = 51.7191, GNorm = 0.1564, lr_0 = 2.4157e-04
Loss = 4.8700e-05, PNorm = 51.7194, GNorm = 0.2229, lr_0 = 2.4132e-04
Loss = 4.6663e-05, PNorm = 51.7217, GNorm = 0.1530, lr_0 = 2.4106e-04
Loss = 5.3583e-05, PNorm = 51.7252, GNorm = 0.1504, lr_0 = 2.4081e-04
Loss = 6.2320e-05, PNorm = 51.7269, GNorm = 0.2071, lr_0 = 2.4056e-04
Loss = 4.8788e-05, PNorm = 51.7293, GNorm = 0.1164, lr_0 = 2.4031e-04
Loss = 5.3862e-05, PNorm = 51.7321, GNorm = 0.1435, lr_0 = 2.4006e-04
Loss = 5.6973e-05, PNorm = 51.7348, GNorm = 0.2777, lr_0 = 2.3981e-04
Loss = 4.4465e-05, PNorm = 51.7374, GNorm = 0.1465, lr_0 = 2.3956e-04
Loss = 6.9153e-05, PNorm = 51.7380, GNorm = 0.1724, lr_0 = 2.3931e-04
Loss = 7.2461e-05, PNorm = 51.7414, GNorm = 0.3260, lr_0 = 2.3906e-04
Loss = 7.9643e-05, PNorm = 51.7462, GNorm = 0.2641, lr_0 = 2.3881e-04
Loss = 7.6340e-05, PNorm = 51.7489, GNorm = 0.1526, lr_0 = 2.3856e-04
Loss = 6.8445e-05, PNorm = 51.7526, GNorm = 0.1620, lr_0 = 2.3831e-04
Loss = 6.2526e-05, PNorm = 51.7549, GNorm = 0.1144, lr_0 = 2.3806e-04
Loss = 5.6239e-05, PNorm = 51.7577, GNorm = 0.1808, lr_0 = 2.3781e-04
Loss = 6.8738e-05, PNorm = 51.7585, GNorm = 0.1696, lr_0 = 2.3756e-04
Loss = 6.6296e-05, PNorm = 51.7600, GNorm = 0.2194, lr_0 = 2.3732e-04
Loss = 7.1635e-05, PNorm = 51.7630, GNorm = 0.2300, lr_0 = 2.3707e-04
Validation rmse logD = 0.534094
Validation R2 logD = 0.798107
Validation rmse logP = 0.472703
Validation R2 logP = 0.934594
Epoch 63
Train function
Loss = 7.2762e-05, PNorm = 51.7649, GNorm = 0.4034, lr_0 = 2.3682e-04
Loss = 6.7169e-05, PNorm = 51.7652, GNorm = 0.1886, lr_0 = 2.3657e-04
Loss = 8.4378e-05, PNorm = 51.7678, GNorm = 0.4215, lr_0 = 2.3633e-04
Loss = 9.3150e-05, PNorm = 51.7708, GNorm = 0.2174, lr_0 = 2.3608e-04
Loss = 8.1684e-05, PNorm = 51.7745, GNorm = 0.1816, lr_0 = 2.3583e-04
Loss = 7.1585e-05, PNorm = 51.7799, GNorm = 0.2856, lr_0 = 2.3559e-04
Loss = 6.1745e-05, PNorm = 51.7825, GNorm = 0.2163, lr_0 = 2.3534e-04
Loss = 5.4665e-05, PNorm = 51.7853, GNorm = 0.1203, lr_0 = 2.3510e-04
Loss = 6.5398e-05, PNorm = 51.7884, GNorm = 0.1128, lr_0 = 2.3485e-04
Loss = 6.2968e-05, PNorm = 51.7910, GNorm = 0.3311, lr_0 = 2.3461e-04
Loss = 8.8729e-05, PNorm = 51.7940, GNorm = 0.2092, lr_0 = 2.3436e-04
Loss = 7.6714e-05, PNorm = 51.7993, GNorm = 0.2597, lr_0 = 2.3412e-04
Loss = 6.4056e-05, PNorm = 51.8017, GNorm = 0.1058, lr_0 = 2.3387e-04
Loss = 8.3031e-05, PNorm = 51.8044, GNorm = 0.3356, lr_0 = 2.3363e-04
Loss = 6.2194e-05, PNorm = 51.8068, GNorm = 0.3326, lr_0 = 2.3338e-04
Loss = 5.9340e-05, PNorm = 51.8094, GNorm = 0.1573, lr_0 = 2.3314e-04
Loss = 7.4912e-05, PNorm = 51.8118, GNorm = 0.2130, lr_0 = 2.3290e-04
Loss = 6.7042e-05, PNorm = 51.8142, GNorm = 0.1740, lr_0 = 2.3265e-04
Loss = 5.2805e-05, PNorm = 51.8169, GNorm = 0.1467, lr_0 = 2.3241e-04
Loss = 8.1650e-05, PNorm = 51.8180, GNorm = 0.4051, lr_0 = 2.3217e-04
Loss = 6.4371e-05, PNorm = 51.8217, GNorm = 0.1088, lr_0 = 2.3193e-04
Loss = 5.6769e-05, PNorm = 51.8230, GNorm = 0.1994, lr_0 = 2.3169e-04
Loss = 5.5236e-05, PNorm = 51.8252, GNorm = 0.1922, lr_0 = 2.3144e-04
Loss = 1.3303e-04, PNorm = 51.8256, GNorm = 0.2973, lr_0 = 2.3142e-04
Validation rmse logD = 0.535168
Validation R2 logD = 0.797294
Validation rmse logP = 0.467896
Validation R2 logP = 0.935918
Epoch 64
Train function
Loss = 5.6877e-05, PNorm = 51.8270, GNorm = 0.1172, lr_0 = 2.3118e-04
Loss = 6.6175e-05, PNorm = 51.8299, GNorm = 0.3253, lr_0 = 2.3094e-04
Loss = 5.9719e-05, PNorm = 51.8321, GNorm = 0.1342, lr_0 = 2.3070e-04
Loss = 5.8657e-05, PNorm = 51.8344, GNorm = 0.0948, lr_0 = 2.3045e-04
Loss = 6.5671e-05, PNorm = 51.8367, GNorm = 0.2492, lr_0 = 2.3021e-04
Loss = 5.5511e-05, PNorm = 51.8397, GNorm = 0.2113, lr_0 = 2.2997e-04
Loss = 6.5312e-05, PNorm = 51.8424, GNorm = 0.2089, lr_0 = 2.2973e-04
Loss = 6.0726e-05, PNorm = 51.8448, GNorm = 0.2307, lr_0 = 2.2949e-04
Loss = 5.7623e-05, PNorm = 51.8472, GNorm = 0.2297, lr_0 = 2.2925e-04
Loss = 6.6306e-05, PNorm = 51.8515, GNorm = 0.1199, lr_0 = 2.2902e-04
Loss = 4.8982e-05, PNorm = 51.8543, GNorm = 0.1457, lr_0 = 2.2878e-04
Loss = 4.8740e-05, PNorm = 51.8568, GNorm = 0.1056, lr_0 = 2.2854e-04
Loss = 5.3854e-05, PNorm = 51.8583, GNorm = 0.1447, lr_0 = 2.2830e-04
Loss = 4.9265e-05, PNorm = 51.8593, GNorm = 0.1809, lr_0 = 2.2806e-04
Loss = 5.7465e-05, PNorm = 51.8598, GNorm = 0.1510, lr_0 = 2.2782e-04
Loss = 5.2490e-05, PNorm = 51.8611, GNorm = 0.2447, lr_0 = 2.2758e-04
Loss = 4.7889e-05, PNorm = 51.8640, GNorm = 0.1177, lr_0 = 2.2735e-04
Loss = 5.0753e-05, PNorm = 51.8667, GNorm = 0.1641, lr_0 = 2.2711e-04
Loss = 5.7495e-05, PNorm = 51.8688, GNorm = 0.1728, lr_0 = 2.2687e-04
Loss = 6.1322e-05, PNorm = 51.8700, GNorm = 0.1529, lr_0 = 2.2664e-04
Loss = 5.8730e-05, PNorm = 51.8708, GNorm = 0.1869, lr_0 = 2.2640e-04
Loss = 6.4597e-05, PNorm = 51.8726, GNorm = 0.3130, lr_0 = 2.2616e-04
Validation rmse logD = 0.535548
Validation R2 logD = 0.797005
Validation rmse logP = 0.465431
Validation R2 logP = 0.936591
Epoch 65
Train function
Loss = 6.1158e-05, PNorm = 51.8746, GNorm = 0.1815, lr_0 = 2.2593e-04
Loss = 6.6352e-05, PNorm = 51.8771, GNorm = 0.1293, lr_0 = 2.2569e-04
Loss = 5.8088e-05, PNorm = 51.8811, GNorm = 0.1235, lr_0 = 2.2546e-04
Loss = 5.3465e-05, PNorm = 51.8852, GNorm = 0.1631, lr_0 = 2.2522e-04
Loss = 6.9225e-05, PNorm = 51.8865, GNorm = 0.2918, lr_0 = 2.2499e-04
Loss = 6.8448e-05, PNorm = 51.8876, GNorm = 0.2994, lr_0 = 2.2475e-04
Loss = 6.6238e-05, PNorm = 51.8906, GNorm = 0.1942, lr_0 = 2.2452e-04
Loss = 4.9017e-05, PNorm = 51.8943, GNorm = 0.1525, lr_0 = 2.2428e-04
Loss = 5.1332e-05, PNorm = 51.8963, GNorm = 0.1763, lr_0 = 2.2405e-04
Loss = 5.5343e-05, PNorm = 51.8983, GNorm = 0.1894, lr_0 = 2.2381e-04
Loss = 7.6619e-05, PNorm = 51.8997, GNorm = 0.1734, lr_0 = 2.2358e-04
Loss = 5.6997e-05, PNorm = 51.8999, GNorm = 0.2194, lr_0 = 2.2335e-04
Loss = 5.9917e-05, PNorm = 51.9029, GNorm = 0.2399, lr_0 = 2.2311e-04
Loss = 5.7747e-05, PNorm = 51.9049, GNorm = 0.1192, lr_0 = 2.2288e-04
Loss = 6.6679e-05, PNorm = 51.9070, GNorm = 0.2750, lr_0 = 2.2265e-04
Loss = 6.1129e-05, PNorm = 51.9105, GNorm = 0.2084, lr_0 = 2.2242e-04
Loss = 6.0762e-05, PNorm = 51.9140, GNorm = 0.1689, lr_0 = 2.2218e-04
Loss = 4.5394e-05, PNorm = 51.9149, GNorm = 0.1735, lr_0 = 2.2195e-04
Loss = 5.0115e-05, PNorm = 51.9177, GNorm = 0.1879, lr_0 = 2.2172e-04
Loss = 4.4974e-05, PNorm = 51.9191, GNorm = 0.1866, lr_0 = 2.2149e-04
Loss = 4.9872e-05, PNorm = 51.9202, GNorm = 0.1418, lr_0 = 2.2126e-04
Loss = 5.2419e-05, PNorm = 51.9223, GNorm = 0.1949, lr_0 = 2.2103e-04
Loss = 3.9306e-05, PNorm = 51.9236, GNorm = 0.1328, lr_0 = 2.2080e-04
Validation rmse logD = 0.537867
Validation R2 logD = 0.795244
Validation rmse logP = 0.464504
Validation R2 logP = 0.936843
Epoch 66
Train function
Loss = 4.1243e-05, PNorm = 51.9261, GNorm = 0.1430, lr_0 = 2.2054e-04
Loss = 5.0751e-05, PNorm = 51.9279, GNorm = 0.1338, lr_0 = 2.2031e-04
Loss = 3.9912e-05, PNorm = 51.9297, GNorm = 0.2302, lr_0 = 2.2008e-04
Loss = 5.7581e-05, PNorm = 51.9308, GNorm = 0.3743, lr_0 = 2.1985e-04
Loss = 5.9719e-05, PNorm = 51.9343, GNorm = 0.1015, lr_0 = 2.1962e-04
Loss = 6.0606e-05, PNorm = 51.9379, GNorm = 0.1822, lr_0 = 2.1939e-04
Loss = 4.6268e-05, PNorm = 51.9401, GNorm = 0.1337, lr_0 = 2.1916e-04
Loss = 5.2108e-05, PNorm = 51.9420, GNorm = 0.1245, lr_0 = 2.1894e-04
Loss = 4.8098e-05, PNorm = 51.9448, GNorm = 0.1135, lr_0 = 2.1871e-04
Loss = 4.5701e-05, PNorm = 51.9469, GNorm = 0.1620, lr_0 = 2.1848e-04
Loss = 4.3407e-05, PNorm = 51.9485, GNorm = 0.0802, lr_0 = 2.1825e-04
Loss = 4.0178e-05, PNorm = 51.9498, GNorm = 0.1326, lr_0 = 2.1802e-04
Loss = 4.5527e-05, PNorm = 51.9488, GNorm = 0.1563, lr_0 = 2.1780e-04
Loss = 4.5059e-05, PNorm = 51.9501, GNorm = 0.1983, lr_0 = 2.1757e-04
Loss = 5.9174e-05, PNorm = 51.9519, GNorm = 0.1338, lr_0 = 2.1734e-04
Loss = 5.9666e-05, PNorm = 51.9544, GNorm = 0.2197, lr_0 = 2.1711e-04
Loss = 4.8239e-05, PNorm = 51.9575, GNorm = 0.1612, lr_0 = 2.1689e-04
Loss = 4.2977e-05, PNorm = 51.9606, GNorm = 0.3000, lr_0 = 2.1666e-04
Loss = 4.8263e-05, PNorm = 51.9621, GNorm = 0.1275, lr_0 = 2.1644e-04
Loss = 4.2236e-05, PNorm = 51.9625, GNorm = 0.1222, lr_0 = 2.1621e-04
Loss = 3.5393e-05, PNorm = 51.9644, GNorm = 0.0884, lr_0 = 2.1598e-04
Loss = 4.1504e-05, PNorm = 51.9662, GNorm = 0.1313, lr_0 = 2.1576e-04
Validation rmse logD = 0.536131
Validation R2 logD = 0.796563
Validation rmse logP = 0.465312
Validation R2 logP = 0.936624
Epoch 67
Train function
Loss = 3.6432e-05, PNorm = 51.9678, GNorm = 0.2281, lr_0 = 2.1553e-04
Loss = 4.3480e-05, PNorm = 51.9708, GNorm = 0.0871, lr_0 = 2.1531e-04
Loss = 4.4431e-05, PNorm = 51.9729, GNorm = 0.1206, lr_0 = 2.1508e-04
Loss = 4.0148e-05, PNorm = 51.9744, GNorm = 0.1050, lr_0 = 2.1486e-04
Loss = 5.0143e-05, PNorm = 51.9762, GNorm = 0.2036, lr_0 = 2.1464e-04
Loss = 4.7283e-05, PNorm = 51.9772, GNorm = 0.2403, lr_0 = 2.1441e-04
Loss = 5.0031e-05, PNorm = 51.9779, GNorm = 0.1244, lr_0 = 2.1419e-04
Loss = 4.9322e-05, PNorm = 51.9800, GNorm = 0.2167, lr_0 = 2.1396e-04
Loss = 5.7174e-05, PNorm = 51.9816, GNorm = 0.1484, lr_0 = 2.1374e-04
Loss = 4.9896e-05, PNorm = 51.9849, GNorm = 0.1985, lr_0 = 2.1352e-04
Loss = 4.3969e-05, PNorm = 51.9871, GNorm = 0.0807, lr_0 = 2.1329e-04
Loss = 3.5801e-05, PNorm = 51.9892, GNorm = 0.0893, lr_0 = 2.1307e-04
Loss = 4.0706e-05, PNorm = 51.9919, GNorm = 0.1277, lr_0 = 2.1285e-04
Loss = 3.8769e-05, PNorm = 51.9932, GNorm = 0.1136, lr_0 = 2.1263e-04
Loss = 4.5315e-05, PNorm = 51.9948, GNorm = 0.1171, lr_0 = 2.1241e-04
Loss = 4.6154e-05, PNorm = 51.9987, GNorm = 0.2226, lr_0 = 2.1218e-04
Loss = 4.5873e-05, PNorm = 52.0006, GNorm = 0.1397, lr_0 = 2.1196e-04
Loss = 5.3144e-05, PNorm = 52.0022, GNorm = 0.1380, lr_0 = 2.1174e-04
Loss = 6.5923e-05, PNorm = 52.0051, GNorm = 0.2851, lr_0 = 2.1152e-04
Loss = 5.1688e-05, PNorm = 52.0065, GNorm = 0.1366, lr_0 = 2.1130e-04
Loss = 4.6538e-05, PNorm = 52.0090, GNorm = 0.1337, lr_0 = 2.1108e-04
Loss = 5.9277e-05, PNorm = 52.0114, GNorm = 0.1020, lr_0 = 2.1086e-04
Loss = 4.5648e-05, PNorm = 52.0125, GNorm = 0.1886, lr_0 = 2.1064e-04
Validation rmse logD = 0.536480
Validation R2 logD = 0.796299
Validation rmse logP = 0.466624
Validation R2 logP = 0.936266
Epoch 68
Train function
Loss = 6.0453e-05, PNorm = 52.0152, GNorm = 0.1312, lr_0 = 2.1040e-04
Loss = 4.7139e-05, PNorm = 52.0177, GNorm = 0.1159, lr_0 = 2.1018e-04
Loss = 6.1164e-05, PNorm = 52.0204, GNorm = 0.1869, lr_0 = 2.0996e-04
Loss = 6.3094e-05, PNorm = 52.0224, GNorm = 0.2329, lr_0 = 2.0974e-04
Loss = 5.0786e-05, PNorm = 52.0237, GNorm = 0.1320, lr_0 = 2.0952e-04
Loss = 4.3620e-05, PNorm = 52.0259, GNorm = 0.1683, lr_0 = 2.0930e-04
Loss = 4.6092e-05, PNorm = 52.0269, GNorm = 0.1090, lr_0 = 2.0908e-04
Loss = 5.2546e-05, PNorm = 52.0276, GNorm = 0.1241, lr_0 = 2.0886e-04
Loss = 5.4760e-05, PNorm = 52.0299, GNorm = 0.2394, lr_0 = 2.0865e-04
Loss = 4.0083e-05, PNorm = 52.0325, GNorm = 0.1623, lr_0 = 2.0843e-04
Loss = 3.5716e-05, PNorm = 52.0336, GNorm = 0.1304, lr_0 = 2.0821e-04
Loss = 4.5147e-05, PNorm = 52.0361, GNorm = 0.1180, lr_0 = 2.0799e-04
Loss = 4.7387e-05, PNorm = 52.0383, GNorm = 0.1228, lr_0 = 2.0778e-04
Loss = 5.0296e-05, PNorm = 52.0410, GNorm = 0.1671, lr_0 = 2.0756e-04
Loss = 5.1135e-05, PNorm = 52.0426, GNorm = 0.2026, lr_0 = 2.0734e-04
Loss = 4.2785e-05, PNorm = 52.0440, GNorm = 0.2000, lr_0 = 2.0713e-04
Loss = 4.5467e-05, PNorm = 52.0465, GNorm = 0.1791, lr_0 = 2.0691e-04
Loss = 5.4827e-05, PNorm = 52.0494, GNorm = 0.1645, lr_0 = 2.0669e-04
Loss = 4.9050e-05, PNorm = 52.0516, GNorm = 0.1749, lr_0 = 2.0648e-04
Loss = 4.4927e-05, PNorm = 52.0530, GNorm = 0.1468, lr_0 = 2.0626e-04
Loss = 4.7005e-05, PNorm = 52.0544, GNorm = 0.1723, lr_0 = 2.0605e-04
Loss = 3.5241e-05, PNorm = 52.0546, GNorm = 0.1517, lr_0 = 2.0583e-04
Validation rmse logD = 0.536609
Validation R2 logD = 0.796201
Validation rmse logP = 0.462518
Validation R2 logP = 0.937383
Epoch 69
Train function
Loss = 2.2293e-05, PNorm = 52.0554, GNorm = 0.1493, lr_0 = 2.0562e-04
Loss = 3.2127e-05, PNorm = 52.0570, GNorm = 0.1059, lr_0 = 2.0540e-04
Loss = 4.0482e-05, PNorm = 52.0585, GNorm = 0.1193, lr_0 = 2.0519e-04
Loss = 3.6095e-05, PNorm = 52.0601, GNorm = 0.1650, lr_0 = 2.0497e-04
Loss = 4.5283e-05, PNorm = 52.0626, GNorm = 0.1578, lr_0 = 2.0476e-04
Loss = 3.8438e-05, PNorm = 52.0632, GNorm = 0.2695, lr_0 = 2.0455e-04
Loss = 4.1320e-05, PNorm = 52.0645, GNorm = 0.1714, lr_0 = 2.0433e-04
Loss = 4.8695e-05, PNorm = 52.0666, GNorm = 0.1265, lr_0 = 2.0412e-04
Loss = 3.8062e-05, PNorm = 52.0687, GNorm = 0.1627, lr_0 = 2.0391e-04
Loss = 4.3261e-05, PNorm = 52.0707, GNorm = 0.2542, lr_0 = 2.0369e-04
Loss = 4.4555e-05, PNorm = 52.0734, GNorm = 0.1313, lr_0 = 2.0348e-04
Loss = 4.9659e-05, PNorm = 52.0753, GNorm = 0.2677, lr_0 = 2.0327e-04
Loss = 5.1172e-05, PNorm = 52.0788, GNorm = 0.1325, lr_0 = 2.0306e-04
Loss = 4.6715e-05, PNorm = 52.0818, GNorm = 0.1448, lr_0 = 2.0285e-04
Loss = 4.7772e-05, PNorm = 52.0834, GNorm = 0.1006, lr_0 = 2.0263e-04
Loss = 5.4146e-05, PNorm = 52.0855, GNorm = 0.1940, lr_0 = 2.0242e-04
Loss = 4.1931e-05, PNorm = 52.0875, GNorm = 0.2749, lr_0 = 2.0221e-04
Loss = 3.5182e-05, PNorm = 52.0889, GNorm = 0.1733, lr_0 = 2.0200e-04
Loss = 4.3198e-05, PNorm = 52.0901, GNorm = 0.2032, lr_0 = 2.0179e-04
Loss = 4.6443e-05, PNorm = 52.0912, GNorm = 0.0958, lr_0 = 2.0158e-04
Loss = 4.6823e-05, PNorm = 52.0917, GNorm = 0.2244, lr_0 = 2.0137e-04
Loss = 4.9601e-05, PNorm = 52.0937, GNorm = 0.1022, lr_0 = 2.0116e-04
Loss = 4.8149e-05, PNorm = 52.0950, GNorm = 0.1235, lr_0 = 2.0095e-04
Validation rmse logD = 0.541752
Validation R2 logD = 0.792276
Validation rmse logP = 0.462703
Validation R2 logP = 0.937332
Epoch 70
Train function
Loss = 3.8524e-05, PNorm = 52.0978, GNorm = 0.0974, lr_0 = 2.0072e-04
Loss = 3.2489e-05, PNorm = 52.0990, GNorm = 0.2546, lr_0 = 2.0051e-04
Loss = 3.9302e-05, PNorm = 52.1009, GNorm = 0.1135, lr_0 = 2.0030e-04
Loss = 3.5999e-05, PNorm = 52.1031, GNorm = 0.1328, lr_0 = 2.0009e-04
Loss = 4.8608e-05, PNorm = 52.1053, GNorm = 0.1287, lr_0 = 1.9988e-04
Loss = 3.4858e-05, PNorm = 52.1060, GNorm = 0.0975, lr_0 = 1.9967e-04
Loss = 3.9659e-05, PNorm = 52.1081, GNorm = 0.1128, lr_0 = 1.9946e-04
Loss = 3.4578e-05, PNorm = 52.1083, GNorm = 0.1280, lr_0 = 1.9926e-04
Loss = 3.7377e-05, PNorm = 52.1076, GNorm = 0.1862, lr_0 = 1.9905e-04
Loss = 4.9399e-05, PNorm = 52.1096, GNorm = 0.1044, lr_0 = 1.9884e-04
Loss = 4.4172e-05, PNorm = 52.1102, GNorm = 0.1670, lr_0 = 1.9863e-04
Loss = 4.0669e-05, PNorm = 52.1118, GNorm = 0.1244, lr_0 = 1.9842e-04
Loss = 3.2509e-05, PNorm = 52.1140, GNorm = 0.0750, lr_0 = 1.9822e-04
Loss = 5.2035e-05, PNorm = 52.1170, GNorm = 0.2115, lr_0 = 1.9801e-04
Loss = 4.2509e-05, PNorm = 52.1192, GNorm = 0.1181, lr_0 = 1.9780e-04
Loss = 4.3085e-05, PNorm = 52.1226, GNorm = 0.1013, lr_0 = 1.9760e-04
Loss = 3.3977e-05, PNorm = 52.1261, GNorm = 0.1253, lr_0 = 1.9739e-04
Loss = 4.1101e-05, PNorm = 52.1289, GNorm = 0.0965, lr_0 = 1.9719e-04
Loss = 4.8443e-05, PNorm = 52.1301, GNorm = 0.1535, lr_0 = 1.9698e-04
Loss = 4.3095e-05, PNorm = 52.1330, GNorm = 0.1597, lr_0 = 1.9677e-04
Loss = 3.9830e-05, PNorm = 52.1347, GNorm = 0.1009, lr_0 = 1.9657e-04
Loss = 3.6892e-05, PNorm = 52.1361, GNorm = 0.1358, lr_0 = 1.9636e-04
Validation rmse logD = 0.537716
Validation R2 logD = 0.795359
Validation rmse logP = 0.465431
Validation R2 logP = 0.936591
Epoch 71
Train function
Loss = 3.8636e-05, PNorm = 52.1381, GNorm = 0.1357, lr_0 = 1.9616e-04
Loss = 3.2717e-05, PNorm = 52.1397, GNorm = 0.1530, lr_0 = 1.9595e-04
Loss = 2.9120e-05, PNorm = 52.1405, GNorm = 0.0896, lr_0 = 1.9575e-04
Loss = 3.1771e-05, PNorm = 52.1428, GNorm = 0.1346, lr_0 = 1.9555e-04
Loss = 3.9753e-05, PNorm = 52.1434, GNorm = 0.1284, lr_0 = 1.9534e-04
Loss = 3.1642e-05, PNorm = 52.1439, GNorm = 0.0795, lr_0 = 1.9514e-04
Loss = 3.3278e-05, PNorm = 52.1457, GNorm = 0.2155, lr_0 = 1.9493e-04
Loss = 4.1436e-05, PNorm = 52.1488, GNorm = 0.1108, lr_0 = 1.9473e-04
Loss = 2.7274e-05, PNorm = 52.1503, GNorm = 0.0876, lr_0 = 1.9453e-04
Loss = 3.3965e-05, PNorm = 52.1519, GNorm = 0.1125, lr_0 = 1.9432e-04
Loss = 3.9759e-05, PNorm = 52.1521, GNorm = 0.1741, lr_0 = 1.9412e-04
Loss = 3.0337e-05, PNorm = 52.1534, GNorm = 0.0924, lr_0 = 1.9392e-04
Loss = 3.7121e-05, PNorm = 52.1540, GNorm = 0.1556, lr_0 = 1.9372e-04
Loss = 3.4680e-05, PNorm = 52.1550, GNorm = 0.1114, lr_0 = 1.9351e-04
Loss = 4.0112e-05, PNorm = 52.1577, GNorm = 0.1744, lr_0 = 1.9331e-04
Loss = 4.4584e-05, PNorm = 52.1601, GNorm = 0.1601, lr_0 = 1.9311e-04
Loss = 3.5697e-05, PNorm = 52.1613, GNorm = 0.1589, lr_0 = 1.9291e-04
Loss = 4.2663e-05, PNorm = 52.1631, GNorm = 0.1046, lr_0 = 1.9271e-04
Loss = 2.8699e-05, PNorm = 52.1645, GNorm = 0.1131, lr_0 = 1.9251e-04
Loss = 3.8978e-05, PNorm = 52.1650, GNorm = 0.1463, lr_0 = 1.9231e-04
Loss = 3.3400e-05, PNorm = 52.1664, GNorm = 0.2942, lr_0 = 1.9210e-04
Loss = 3.4272e-05, PNorm = 52.1675, GNorm = 0.1180, lr_0 = 1.9190e-04
Loss = 3.0210e-05, PNorm = 52.1692, GNorm = 0.0809, lr_0 = 1.9170e-04
Validation rmse logD = 0.537205
Validation R2 logD = 0.795748
Validation rmse logP = 0.465296
Validation R2 logP = 0.936628
Epoch 72
Train function
Loss = 3.6901e-05, PNorm = 52.1715, GNorm = 0.1631, lr_0 = 1.9148e-04
Loss = 2.8382e-05, PNorm = 52.1712, GNorm = 0.1668, lr_0 = 1.9128e-04
Loss = 2.8406e-05, PNorm = 52.1722, GNorm = 0.1185, lr_0 = 1.9108e-04
Loss = 2.7703e-05, PNorm = 52.1727, GNorm = 0.0988, lr_0 = 1.9088e-04
Loss = 2.8425e-05, PNorm = 52.1732, GNorm = 0.1398, lr_0 = 1.9069e-04
Loss = 2.9384e-05, PNorm = 52.1741, GNorm = 0.0914, lr_0 = 1.9049e-04
Loss = 3.3858e-05, PNorm = 52.1768, GNorm = 0.2167, lr_0 = 1.9029e-04
Loss = 3.0687e-05, PNorm = 52.1785, GNorm = 0.1092, lr_0 = 1.9009e-04
Loss = 3.1412e-05, PNorm = 52.1804, GNorm = 0.1673, lr_0 = 1.8989e-04
Loss = 3.0484e-05, PNorm = 52.1824, GNorm = 0.1453, lr_0 = 1.8969e-04
Loss = 3.2121e-05, PNorm = 52.1840, GNorm = 0.0681, lr_0 = 1.8949e-04
Loss = 3.5623e-05, PNorm = 52.1845, GNorm = 0.1892, lr_0 = 1.8930e-04
Loss = 4.4390e-05, PNorm = 52.1863, GNorm = 0.2515, lr_0 = 1.8910e-04
Loss = 6.9734e-05, PNorm = 52.1887, GNorm = 0.3685, lr_0 = 1.8890e-04
Loss = 4.9950e-05, PNorm = 52.1914, GNorm = 0.2146, lr_0 = 1.8870e-04
Loss = 4.0648e-05, PNorm = 52.1931, GNorm = 0.2114, lr_0 = 1.8851e-04
Loss = 4.4423e-05, PNorm = 52.1953, GNorm = 0.2415, lr_0 = 1.8831e-04
Loss = 4.1308e-05, PNorm = 52.1974, GNorm = 0.1383, lr_0 = 1.8811e-04
Loss = 3.2136e-05, PNorm = 52.1997, GNorm = 0.1706, lr_0 = 1.8792e-04
Loss = 3.7653e-05, PNorm = 52.2025, GNorm = 0.1168, lr_0 = 1.8772e-04
Loss = 3.0340e-05, PNorm = 52.2025, GNorm = 0.1069, lr_0 = 1.8753e-04
Loss = 3.9412e-05, PNorm = 52.2038, GNorm = 0.2389, lr_0 = 1.8733e-04
Loss = 4.5529e-05, PNorm = 52.2066, GNorm = 0.1744, lr_0 = 1.8713e-04
Validation rmse logD = 0.536777
Validation R2 logD = 0.796073
Validation rmse logP = 0.464675
Validation R2 logP = 0.936797
Epoch 73
Train function
Loss = 3.2651e-05, PNorm = 52.2076, GNorm = 0.1106, lr_0 = 1.8694e-04
Loss = 3.4048e-05, PNorm = 52.2087, GNorm = 0.1043, lr_0 = 1.8674e-04
Loss = 3.4068e-05, PNorm = 52.2104, GNorm = 0.1541, lr_0 = 1.8655e-04
Loss = 3.8849e-05, PNorm = 52.2133, GNorm = 0.1759, lr_0 = 1.8635e-04
Loss = 3.9702e-05, PNorm = 52.2148, GNorm = 0.1228, lr_0 = 1.8616e-04
Loss = 4.3368e-05, PNorm = 52.2177, GNorm = 0.1627, lr_0 = 1.8597e-04
Loss = 3.6484e-05, PNorm = 52.2198, GNorm = 0.1912, lr_0 = 1.8577e-04
Loss = 4.2301e-05, PNorm = 52.2214, GNorm = 0.2207, lr_0 = 1.8558e-04
Loss = 4.7065e-05, PNorm = 52.2233, GNorm = 0.3880, lr_0 = 1.8538e-04
Loss = 5.1066e-05, PNorm = 52.2256, GNorm = 0.1386, lr_0 = 1.8519e-04
Loss = 3.9952e-05, PNorm = 52.2276, GNorm = 0.1317, lr_0 = 1.8500e-04
Loss = 3.2815e-05, PNorm = 52.2279, GNorm = 0.0948, lr_0 = 1.8480e-04
Loss = 3.0569e-05, PNorm = 52.2297, GNorm = 0.1204, lr_0 = 1.8461e-04
Loss = 3.2885e-05, PNorm = 52.2312, GNorm = 0.0989, lr_0 = 1.8442e-04
Loss = 3.7743e-05, PNorm = 52.2338, GNorm = 0.1353, lr_0 = 1.8423e-04
Loss = 3.6843e-05, PNorm = 52.2355, GNorm = 0.1396, lr_0 = 1.8403e-04
Loss = 2.8284e-05, PNorm = 52.2362, GNorm = 0.0777, lr_0 = 1.8384e-04
Loss = 3.3137e-05, PNorm = 52.2364, GNorm = 0.1554, lr_0 = 1.8365e-04
Loss = 2.6574e-05, PNorm = 52.2383, GNorm = 0.1015, lr_0 = 1.8346e-04
Loss = 3.1700e-05, PNorm = 52.2398, GNorm = 0.1418, lr_0 = 1.8327e-04
Loss = 3.9243e-05, PNorm = 52.2421, GNorm = 0.0912, lr_0 = 1.8308e-04
Loss = 3.2826e-05, PNorm = 52.2449, GNorm = 0.1527, lr_0 = 1.8288e-04
Validation rmse logD = 0.535536
Validation R2 logD = 0.797015
Validation rmse logP = 0.469265
Validation R2 logP = 0.935542
Epoch 74
Train function
Loss = 4.0650e-05, PNorm = 52.2438, GNorm = 0.1856, lr_0 = 1.8267e-04
Loss = 3.7249e-05, PNorm = 52.2444, GNorm = 0.2608, lr_0 = 1.8248e-04
Loss = 3.3246e-05, PNorm = 52.2456, GNorm = 0.1162, lr_0 = 1.8229e-04
Loss = 2.5972e-05, PNorm = 52.2478, GNorm = 0.0768, lr_0 = 1.8210e-04
Loss = 2.9600e-05, PNorm = 52.2494, GNorm = 0.1309, lr_0 = 1.8191e-04
Loss = 3.4678e-05, PNorm = 52.2520, GNorm = 0.1490, lr_0 = 1.8172e-04
Loss = 3.5270e-05, PNorm = 52.2538, GNorm = 0.1678, lr_0 = 1.8153e-04
Loss = 2.5972e-05, PNorm = 52.2557, GNorm = 0.1360, lr_0 = 1.8134e-04
Loss = 3.5889e-05, PNorm = 52.2578, GNorm = 0.0932, lr_0 = 1.8115e-04
Loss = 3.3629e-05, PNorm = 52.2582, GNorm = 0.2360, lr_0 = 1.8097e-04
Loss = 3.9826e-05, PNorm = 52.2605, GNorm = 0.1468, lr_0 = 1.8078e-04
Loss = 4.9783e-05, PNorm = 52.2614, GNorm = 0.1515, lr_0 = 1.8059e-04
Loss = 3.5180e-05, PNorm = 52.2617, GNorm = 0.1163, lr_0 = 1.8040e-04
Loss = 3.5910e-05, PNorm = 52.2637, GNorm = 0.1003, lr_0 = 1.8021e-04
Loss = 3.4948e-05, PNorm = 52.2670, GNorm = 0.1089, lr_0 = 1.8002e-04
Loss = 3.9518e-05, PNorm = 52.2684, GNorm = 0.1335, lr_0 = 1.7984e-04
Loss = 3.5175e-05, PNorm = 52.2698, GNorm = 0.0979, lr_0 = 1.7965e-04
Loss = 2.9892e-05, PNorm = 52.2711, GNorm = 0.1410, lr_0 = 1.7946e-04
Loss = 3.9146e-05, PNorm = 52.2722, GNorm = 0.1407, lr_0 = 1.7927e-04
Loss = 3.7991e-05, PNorm = 52.2746, GNorm = 0.1151, lr_0 = 1.7909e-04
Loss = 3.4107e-05, PNorm = 52.2762, GNorm = 0.1072, lr_0 = 1.7890e-04
Loss = 3.7737e-05, PNorm = 52.2782, GNorm = 0.1412, lr_0 = 1.7871e-04
Loss = 3.4864e-05, PNorm = 52.2797, GNorm = 0.1319, lr_0 = 1.7853e-04
Validation rmse logD = 0.532633
Validation R2 logD = 0.799210
Validation rmse logP = 0.461812
Validation R2 logP = 0.937573
Epoch 75
Train function
Loss = 3.1112e-05, PNorm = 52.2799, GNorm = 0.1809, lr_0 = 1.7834e-04
Loss = 2.8427e-05, PNorm = 52.2807, GNorm = 0.0947, lr_0 = 1.7815e-04
Loss = 3.3868e-05, PNorm = 52.2825, GNorm = 0.1593, lr_0 = 1.7797e-04
Loss = 4.1020e-05, PNorm = 52.2843, GNorm = 0.1083, lr_0 = 1.7778e-04
Loss = 3.9253e-05, PNorm = 52.2855, GNorm = 0.1982, lr_0 = 1.7760e-04
Loss = 3.3729e-05, PNorm = 52.2871, GNorm = 0.0935, lr_0 = 1.7741e-04
Loss = 3.6699e-05, PNorm = 52.2899, GNorm = 0.0942, lr_0 = 1.7723e-04
Loss = 3.2889e-05, PNorm = 52.2910, GNorm = 0.1077, lr_0 = 1.7704e-04
Loss = 3.3972e-05, PNorm = 52.2928, GNorm = 0.2205, lr_0 = 1.7686e-04
Loss = 3.4520e-05, PNorm = 52.2932, GNorm = 0.2556, lr_0 = 1.7667e-04
Loss = 2.6875e-05, PNorm = 52.2947, GNorm = 0.0780, lr_0 = 1.7649e-04
Loss = 3.0379e-05, PNorm = 52.2963, GNorm = 0.1286, lr_0 = 1.7630e-04
Loss = 2.8920e-05, PNorm = 52.2978, GNorm = 0.0924, lr_0 = 1.7612e-04
Loss = 2.1035e-05, PNorm = 52.2993, GNorm = 0.0668, lr_0 = 1.7593e-04
Loss = 2.8475e-05, PNorm = 52.3002, GNorm = 0.1658, lr_0 = 1.7575e-04
Loss = 3.3679e-05, PNorm = 52.3017, GNorm = 0.2277, lr_0 = 1.7557e-04
Loss = 3.6951e-05, PNorm = 52.3038, GNorm = 0.2166, lr_0 = 1.7538e-04
Loss = 4.1768e-05, PNorm = 52.3044, GNorm = 0.1200, lr_0 = 1.7520e-04
Loss = 4.4875e-05, PNorm = 52.3059, GNorm = 0.1477, lr_0 = 1.7502e-04
Loss = 4.1570e-05, PNorm = 52.3087, GNorm = 0.3525, lr_0 = 1.7484e-04
Loss = 5.2401e-05, PNorm = 52.3112, GNorm = 0.2332, lr_0 = 1.7465e-04
Loss = 5.9366e-05, PNorm = 52.3153, GNorm = 0.2837, lr_0 = 1.7447e-04
Validation rmse logD = 0.535617
Validation R2 logD = 0.796953
Validation rmse logP = 0.462934
Validation R2 logP = 0.937270
Epoch 76
Train function
Loss = 2.8463e-05, PNorm = 52.3182, GNorm = 0.1957, lr_0 = 1.7427e-04
Loss = 3.3778e-05, PNorm = 52.3193, GNorm = 0.0868, lr_0 = 1.7409e-04
Loss = 3.5307e-05, PNorm = 52.3192, GNorm = 0.1581, lr_0 = 1.7391e-04
Loss = 3.2494e-05, PNorm = 52.3197, GNorm = 0.2056, lr_0 = 1.7373e-04
Loss = 3.3772e-05, PNorm = 52.3228, GNorm = 0.1545, lr_0 = 1.7354e-04
Loss = 3.5828e-05, PNorm = 52.3240, GNorm = 0.1354, lr_0 = 1.7336e-04
Loss = 3.1617e-05, PNorm = 52.3248, GNorm = 0.1405, lr_0 = 1.7318e-04
Loss = 2.9311e-05, PNorm = 52.3256, GNorm = 0.1696, lr_0 = 1.7300e-04
Loss = 3.9554e-05, PNorm = 52.3274, GNorm = 0.1083, lr_0 = 1.7282e-04
Loss = 4.1762e-05, PNorm = 52.3287, GNorm = 0.1404, lr_0 = 1.7264e-04
Loss = 3.0797e-05, PNorm = 52.3301, GNorm = 0.1369, lr_0 = 1.7246e-04
Loss = 2.7542e-05, PNorm = 52.3321, GNorm = 0.1157, lr_0 = 1.7228e-04
Loss = 3.1728e-05, PNorm = 52.3334, GNorm = 0.1211, lr_0 = 1.7210e-04
Loss = 2.9610e-05, PNorm = 52.3359, GNorm = 0.1043, lr_0 = 1.7192e-04
Loss = 3.1785e-05, PNorm = 52.3390, GNorm = 0.1573, lr_0 = 1.7174e-04
Loss = 2.9628e-05, PNorm = 52.3413, GNorm = 0.0990, lr_0 = 1.7156e-04
Loss = 3.1402e-05, PNorm = 52.3415, GNorm = 0.1348, lr_0 = 1.7138e-04
Loss = 2.3834e-05, PNorm = 52.3419, GNorm = 0.0864, lr_0 = 1.7120e-04
Loss = 3.4882e-05, PNorm = 52.3441, GNorm = 0.1176, lr_0 = 1.7103e-04
Loss = 2.7688e-05, PNorm = 52.3462, GNorm = 0.1044, lr_0 = 1.7085e-04
Loss = 3.1306e-05, PNorm = 52.3470, GNorm = 0.1427, lr_0 = 1.7067e-04
Loss = 3.9113e-05, PNorm = 52.3486, GNorm = 0.0948, lr_0 = 1.7049e-04
Loss = 3.2939e-05, PNorm = 52.3501, GNorm = 0.1187, lr_0 = 1.7031e-04
Validation rmse logD = 0.534459
Validation R2 logD = 0.797831
Validation rmse logP = 0.461621
Validation R2 logP = 0.937625
Epoch 77
Train function
Loss = 3.2142e-05, PNorm = 52.3510, GNorm = 0.1469, lr_0 = 1.7012e-04
Loss = 2.8084e-05, PNorm = 52.3532, GNorm = 0.1017, lr_0 = 1.6994e-04
Loss = 2.6146e-05, PNorm = 52.3537, GNorm = 0.0744, lr_0 = 1.6976e-04
Loss = 3.0605e-05, PNorm = 52.3546, GNorm = 0.1689, lr_0 = 1.6959e-04
Loss = 2.4559e-05, PNorm = 52.3568, GNorm = 0.1049, lr_0 = 1.6941e-04
Loss = 2.4550e-05, PNorm = 52.3566, GNorm = 0.0872, lr_0 = 1.6923e-04
Loss = 2.7584e-05, PNorm = 52.3584, GNorm = 0.1091, lr_0 = 1.6905e-04
Loss = 2.8075e-05, PNorm = 52.3580, GNorm = 0.1706, lr_0 = 1.6888e-04
Loss = 3.6682e-05, PNorm = 52.3593, GNorm = 0.2472, lr_0 = 1.6870e-04
Loss = 3.2580e-05, PNorm = 52.3596, GNorm = 0.1267, lr_0 = 1.6853e-04
Loss = 2.9499e-05, PNorm = 52.3605, GNorm = 0.1218, lr_0 = 1.6835e-04
Loss = 3.0434e-05, PNorm = 52.3623, GNorm = 0.1989, lr_0 = 1.6817e-04
Loss = 3.0689e-05, PNorm = 52.3638, GNorm = 0.0938, lr_0 = 1.6800e-04
Loss = 3.1878e-05, PNorm = 52.3660, GNorm = 0.1684, lr_0 = 1.6782e-04
Loss = 3.0249e-05, PNorm = 52.3679, GNorm = 0.1383, lr_0 = 1.6765e-04
Loss = 3.0061e-05, PNorm = 52.3678, GNorm = 0.1842, lr_0 = 1.6747e-04
Loss = 3.2078e-05, PNorm = 52.3681, GNorm = 0.1512, lr_0 = 1.6730e-04
Loss = 2.7911e-05, PNorm = 52.3686, GNorm = 0.0929, lr_0 = 1.6712e-04
Loss = 2.9171e-05, PNorm = 52.3700, GNorm = 0.1180, lr_0 = 1.6695e-04
Loss = 2.4465e-05, PNorm = 52.3725, GNorm = 0.0848, lr_0 = 1.6678e-04
Loss = 2.6653e-05, PNorm = 52.3734, GNorm = 0.1060, lr_0 = 1.6660e-04
Loss = 2.5405e-05, PNorm = 52.3752, GNorm = 0.0960, lr_0 = 1.6643e-04
Validation rmse logD = 0.534597
Validation R2 logD = 0.797726
Validation rmse logP = 0.463032
Validation R2 logP = 0.937243
Epoch 78
Train function
Loss = 3.3477e-05, PNorm = 52.3776, GNorm = 0.1149, lr_0 = 1.6625e-04
Loss = 1.7999e-05, PNorm = 52.3788, GNorm = 0.0802, lr_0 = 1.6608e-04
Loss = 2.0066e-05, PNorm = 52.3797, GNorm = 0.1109, lr_0 = 1.6591e-04
Loss = 2.2872e-05, PNorm = 52.3810, GNorm = 0.0908, lr_0 = 1.6573e-04
Loss = 2.5723e-05, PNorm = 52.3813, GNorm = 0.1371, lr_0 = 1.6556e-04
Loss = 2.2462e-05, PNorm = 52.3828, GNorm = 0.0536, lr_0 = 1.6539e-04
Loss = 2.3267e-05, PNorm = 52.3840, GNorm = 0.1293, lr_0 = 1.6522e-04
Loss = 2.1317e-05, PNorm = 52.3863, GNorm = 0.1131, lr_0 = 1.6504e-04
Loss = 2.6644e-05, PNorm = 52.3876, GNorm = 0.1232, lr_0 = 1.6487e-04
Loss = 1.9647e-05, PNorm = 52.3898, GNorm = 0.1505, lr_0 = 1.6470e-04
Loss = 2.5747e-05, PNorm = 52.3910, GNorm = 0.1149, lr_0 = 1.6453e-04
Loss = 2.3297e-05, PNorm = 52.3918, GNorm = 0.1170, lr_0 = 1.6435e-04
Loss = 2.9545e-05, PNorm = 52.3936, GNorm = 0.1027, lr_0 = 1.6418e-04
Loss = 2.5535e-05, PNorm = 52.3951, GNorm = 0.1403, lr_0 = 1.6401e-04
Loss = 2.3954e-05, PNorm = 52.3958, GNorm = 0.1103, lr_0 = 1.6384e-04
Loss = 2.6231e-05, PNorm = 52.3954, GNorm = 0.1404, lr_0 = 1.6367e-04
Loss = 2.5015e-05, PNorm = 52.3966, GNorm = 0.1005, lr_0 = 1.6350e-04
Loss = 2.5421e-05, PNorm = 52.3973, GNorm = 0.0927, lr_0 = 1.6333e-04
Loss = 2.7312e-05, PNorm = 52.3987, GNorm = 0.0731, lr_0 = 1.6316e-04
Loss = 2.7357e-05, PNorm = 52.4004, GNorm = 0.1704, lr_0 = 1.6299e-04
Loss = 2.3393e-05, PNorm = 52.4016, GNorm = 0.1086, lr_0 = 1.6282e-04
Loss = 2.8089e-05, PNorm = 52.4032, GNorm = 0.1137, lr_0 = 1.6265e-04
Loss = 2.9323e-05, PNorm = 52.4055, GNorm = 0.1170, lr_0 = 1.6248e-04
Validation rmse logD = 0.535017
Validation R2 logD = 0.797408
Validation rmse logP = 0.465120
Validation R2 logP = 0.936676
Epoch 79
Train function
Loss = 2.9962e-05, PNorm = 52.4067, GNorm = 0.1129, lr_0 = 1.6229e-04
Loss = 3.6275e-05, PNorm = 52.4069, GNorm = 0.2187, lr_0 = 1.6212e-04
Loss = 2.8503e-05, PNorm = 52.4091, GNorm = 0.1285, lr_0 = 1.6195e-04
Loss = 3.3098e-05, PNorm = 52.4103, GNorm = 0.1811, lr_0 = 1.6178e-04
Loss = 2.7066e-05, PNorm = 52.4110, GNorm = 0.1374, lr_0 = 1.6161e-04
Loss = 2.8494e-05, PNorm = 52.4114, GNorm = 0.1609, lr_0 = 1.6145e-04
Loss = 2.3656e-05, PNorm = 52.4139, GNorm = 0.0746, lr_0 = 1.6128e-04
Loss = 2.7551e-05, PNorm = 52.4168, GNorm = 0.0949, lr_0 = 1.6111e-04
Loss = 2.9537e-05, PNorm = 52.4168, GNorm = 0.0864, lr_0 = 1.6094e-04
Loss = 2.5249e-05, PNorm = 52.4177, GNorm = 0.1480, lr_0 = 1.6077e-04
Loss = 2.4755e-05, PNorm = 52.4199, GNorm = 0.0728, lr_0 = 1.6061e-04
Loss = 2.1025e-05, PNorm = 52.4209, GNorm = 0.1132, lr_0 = 1.6044e-04
Loss = 2.1786e-05, PNorm = 52.4210, GNorm = 0.1949, lr_0 = 1.6027e-04
Loss = 2.6823e-05, PNorm = 52.4218, GNorm = 0.1301, lr_0 = 1.6010e-04
Loss = 2.6180e-05, PNorm = 52.4224, GNorm = 0.1822, lr_0 = 1.5994e-04
Loss = 2.7166e-05, PNorm = 52.4249, GNorm = 0.1417, lr_0 = 1.5977e-04
Loss = 2.4356e-05, PNorm = 52.4264, GNorm = 0.0907, lr_0 = 1.5960e-04
Loss = 3.1122e-05, PNorm = 52.4272, GNorm = 0.1072, lr_0 = 1.5944e-04
Loss = 2.7197e-05, PNorm = 52.4296, GNorm = 0.1082, lr_0 = 1.5927e-04
Loss = 2.8772e-05, PNorm = 52.4319, GNorm = 0.1195, lr_0 = 1.5910e-04
Loss = 2.9898e-05, PNorm = 52.4343, GNorm = 0.2114, lr_0 = 1.5894e-04
Loss = 3.3102e-05, PNorm = 52.4355, GNorm = 0.1726, lr_0 = 1.5877e-04
Validation rmse logD = 0.534508
Validation R2 logD = 0.797794
Validation rmse logP = 0.465049
Validation R2 logP = 0.936695
Epoch 80
Train function
Loss = 2.2972e-05, PNorm = 52.4369, GNorm = 0.0650, lr_0 = 1.5861e-04
Loss = 1.9787e-05, PNorm = 52.4382, GNorm = 0.1020, lr_0 = 1.5844e-04
Loss = 2.6609e-05, PNorm = 52.4398, GNorm = 0.1280, lr_0 = 1.5827e-04
Loss = 2.5677e-05, PNorm = 52.4417, GNorm = 0.0942, lr_0 = 1.5811e-04
Loss = 2.3232e-05, PNorm = 52.4441, GNorm = 0.0818, lr_0 = 1.5794e-04
Loss = 2.3454e-05, PNorm = 52.4447, GNorm = 0.0633, lr_0 = 1.5778e-04
Loss = 2.0675e-05, PNorm = 52.4456, GNorm = 0.0798, lr_0 = 1.5761e-04
Loss = 2.1127e-05, PNorm = 52.4466, GNorm = 0.0968, lr_0 = 1.5745e-04
Loss = 2.3195e-05, PNorm = 52.4476, GNorm = 0.0605, lr_0 = 1.5729e-04
Loss = 2.5787e-05, PNorm = 52.4482, GNorm = 0.1754, lr_0 = 1.5712e-04
Loss = 1.9515e-05, PNorm = 52.4500, GNorm = 0.0802, lr_0 = 1.5696e-04
Loss = 2.5685e-05, PNorm = 52.4506, GNorm = 0.1923, lr_0 = 1.5679e-04
Loss = 2.1078e-05, PNorm = 52.4512, GNorm = 0.0784, lr_0 = 1.5663e-04
Loss = 2.4841e-05, PNorm = 52.4526, GNorm = 0.0948, lr_0 = 1.5647e-04
Loss = 2.8239e-05, PNorm = 52.4542, GNorm = 0.0712, lr_0 = 1.5630e-04
Loss = 3.0463e-05, PNorm = 52.4550, GNorm = 0.1587, lr_0 = 1.5614e-04
Loss = 2.4606e-05, PNorm = 52.4557, GNorm = 0.1134, lr_0 = 1.5598e-04
Loss = 2.5044e-05, PNorm = 52.4565, GNorm = 0.0645, lr_0 = 1.5581e-04
Loss = 2.9194e-05, PNorm = 52.4572, GNorm = 0.1296, lr_0 = 1.5565e-04
Loss = 2.5817e-05, PNorm = 52.4589, GNorm = 0.1278, lr_0 = 1.5549e-04
Loss = 2.5485e-05, PNorm = 52.4600, GNorm = 0.1195, lr_0 = 1.5533e-04
Loss = 3.3548e-05, PNorm = 52.4626, GNorm = 0.1184, lr_0 = 1.5516e-04
Loss = 3.1371e-05, PNorm = 52.4636, GNorm = 0.1531, lr_0 = 1.5500e-04
Validation rmse logD = 0.537646
Validation R2 logD = 0.795412
Validation rmse logP = 0.464942
Validation R2 logP = 0.936724
Epoch 81
Train function
Loss = 2.8095e-05, PNorm = 52.4663, GNorm = 0.1758, lr_0 = 1.5483e-04
Loss = 2.5688e-05, PNorm = 52.4675, GNorm = 0.1045, lr_0 = 1.5466e-04
Loss = 2.6123e-05, PNorm = 52.4673, GNorm = 0.1369, lr_0 = 1.5450e-04
Loss = 2.8778e-05, PNorm = 52.4680, GNorm = 0.1528, lr_0 = 1.5434e-04
Loss = 2.6032e-05, PNorm = 52.4689, GNorm = 0.0821, lr_0 = 1.5418e-04
Loss = 2.4628e-05, PNorm = 52.4692, GNorm = 0.0969, lr_0 = 1.5402e-04
Loss = 2.1435e-05, PNorm = 52.4702, GNorm = 0.1178, lr_0 = 1.5386e-04
Loss = 2.2623e-05, PNorm = 52.4704, GNorm = 0.0871, lr_0 = 1.5370e-04
Loss = 2.6954e-05, PNorm = 52.4717, GNorm = 0.0838, lr_0 = 1.5354e-04
Loss = 2.1211e-05, PNorm = 52.4719, GNorm = 0.0824, lr_0 = 1.5338e-04
Loss = 2.5303e-05, PNorm = 52.4730, GNorm = 0.1383, lr_0 = 1.5322e-04
Loss = 2.7588e-05, PNorm = 52.4743, GNorm = 0.1051, lr_0 = 1.5306e-04
Loss = 2.2863e-05, PNorm = 52.4752, GNorm = 0.1303, lr_0 = 1.5290e-04
Loss = 1.9055e-05, PNorm = 52.4764, GNorm = 0.1125, lr_0 = 1.5274e-04
Loss = 1.8091e-05, PNorm = 52.4764, GNorm = 0.1525, lr_0 = 1.5258e-04
Loss = 2.5158e-05, PNorm = 52.4778, GNorm = 0.1406, lr_0 = 1.5242e-04
Loss = 2.0329e-05, PNorm = 52.4787, GNorm = 0.1087, lr_0 = 1.5226e-04
Loss = 1.8290e-05, PNorm = 52.4799, GNorm = 0.0655, lr_0 = 1.5210e-04
Loss = 1.9014e-05, PNorm = 52.4809, GNorm = 0.1062, lr_0 = 1.5194e-04
Loss = 2.5818e-05, PNorm = 52.4821, GNorm = 0.2403, lr_0 = 1.5178e-04
Loss = 2.6280e-05, PNorm = 52.4832, GNorm = 0.0823, lr_0 = 1.5163e-04
Loss = 3.0585e-05, PNorm = 52.4860, GNorm = 0.1589, lr_0 = 1.5147e-04
Validation rmse logD = 0.538038
Validation R2 logD = 0.795114
Validation rmse logP = 0.467515
Validation R2 logP = 0.936022
Epoch 82
Train function
Loss = 3.0625e-05, PNorm = 52.4868, GNorm = 0.1880, lr_0 = 1.5131e-04
Loss = 3.0422e-05, PNorm = 52.4878, GNorm = 0.1646, lr_0 = 1.5115e-04
Loss = 2.6724e-05, PNorm = 52.4886, GNorm = 0.1217, lr_0 = 1.5099e-04
Loss = 2.1153e-05, PNorm = 52.4900, GNorm = 0.1206, lr_0 = 1.5084e-04
Loss = 2.1110e-05, PNorm = 52.4911, GNorm = 0.1171, lr_0 = 1.5068e-04
Loss = 2.4421e-05, PNorm = 52.4934, GNorm = 0.1061, lr_0 = 1.5052e-04
Loss = 2.7441e-05, PNorm = 52.4939, GNorm = 0.2059, lr_0 = 1.5036e-04
Loss = 3.1683e-05, PNorm = 52.4969, GNorm = 0.1108, lr_0 = 1.5021e-04
Loss = 2.2472e-05, PNorm = 52.4971, GNorm = 0.1014, lr_0 = 1.5005e-04
Loss = 1.9235e-05, PNorm = 52.4979, GNorm = 0.0797, lr_0 = 1.4989e-04
Loss = 1.9209e-05, PNorm = 52.4990, GNorm = 0.0847, lr_0 = 1.4974e-04
Loss = 1.8582e-05, PNorm = 52.4993, GNorm = 0.0730, lr_0 = 1.4958e-04
Loss = 1.9514e-05, PNorm = 52.5002, GNorm = 0.0939, lr_0 = 1.4942e-04
Loss = 1.9952e-05, PNorm = 52.5013, GNorm = 0.1851, lr_0 = 1.4927e-04
Loss = 2.0172e-05, PNorm = 52.5025, GNorm = 0.1800, lr_0 = 1.4911e-04
Loss = 2.2894e-05, PNorm = 52.5039, GNorm = 0.1448, lr_0 = 1.4896e-04
Loss = 2.6207e-05, PNorm = 52.5045, GNorm = 0.0871, lr_0 = 1.4880e-04
Loss = 2.5941e-05, PNorm = 52.5060, GNorm = 0.0914, lr_0 = 1.4865e-04
Loss = 1.9028e-05, PNorm = 52.5071, GNorm = 0.0926, lr_0 = 1.4849e-04
Loss = 2.3922e-05, PNorm = 52.5083, GNorm = 0.0745, lr_0 = 1.4834e-04
Loss = 2.3226e-05, PNorm = 52.5094, GNorm = 0.0983, lr_0 = 1.4818e-04
Loss = 1.8146e-05, PNorm = 52.5105, GNorm = 0.0774, lr_0 = 1.4803e-04
Loss = 2.3853e-05, PNorm = 52.5125, GNorm = 0.1373, lr_0 = 1.4787e-04
Validation rmse logD = 0.536806
Validation R2 logD = 0.796051
Validation rmse logP = 0.465136
Validation R2 logP = 0.936672
Epoch 83
Train function
Loss = 2.0938e-05, PNorm = 52.5138, GNorm = 0.0797, lr_0 = 1.4770e-04
Loss = 1.6893e-05, PNorm = 52.5161, GNorm = 0.0908, lr_0 = 1.4755e-04
Loss = 1.7789e-05, PNorm = 52.5175, GNorm = 0.0933, lr_0 = 1.4739e-04
Loss = 1.8490e-05, PNorm = 52.5179, GNorm = 0.0586, lr_0 = 1.4724e-04
Loss = 2.3180e-05, PNorm = 52.5188, GNorm = 0.1250, lr_0 = 1.4709e-04
Loss = 2.5079e-05, PNorm = 52.5205, GNorm = 0.2007, lr_0 = 1.4693e-04
Loss = 2.4020e-05, PNorm = 52.5212, GNorm = 0.1762, lr_0 = 1.4678e-04
Loss = 1.9342e-05, PNorm = 52.5221, GNorm = 0.1018, lr_0 = 1.4663e-04
Loss = 1.9738e-05, PNorm = 52.5232, GNorm = 0.0766, lr_0 = 1.4647e-04
Loss = 1.8083e-05, PNorm = 52.5241, GNorm = 0.1013, lr_0 = 1.4632e-04
Loss = 2.3694e-05, PNorm = 52.5254, GNorm = 0.1067, lr_0 = 1.4617e-04
Loss = 2.4242e-05, PNorm = 52.5260, GNorm = 0.0989, lr_0 = 1.4602e-04
Loss = 2.0889e-05, PNorm = 52.5265, GNorm = 0.1362, lr_0 = 1.4586e-04
Loss = 2.4230e-05, PNorm = 52.5262, GNorm = 0.0918, lr_0 = 1.4571e-04
Loss = 2.6759e-05, PNorm = 52.5262, GNorm = 0.0712, lr_0 = 1.4556e-04
Loss = 3.0290e-05, PNorm = 52.5280, GNorm = 0.1021, lr_0 = 1.4541e-04
Loss = 2.0911e-05, PNorm = 52.5304, GNorm = 0.1322, lr_0 = 1.4526e-04
Loss = 2.2302e-05, PNorm = 52.5322, GNorm = 0.1057, lr_0 = 1.4510e-04
Loss = 1.6551e-05, PNorm = 52.5329, GNorm = 0.0728, lr_0 = 1.4495e-04
Loss = 2.2274e-05, PNorm = 52.5335, GNorm = 0.1005, lr_0 = 1.4480e-04
Loss = 2.1072e-05, PNorm = 52.5346, GNorm = 0.0946, lr_0 = 1.4465e-04
Loss = 2.2174e-05, PNorm = 52.5347, GNorm = 0.1023, lr_0 = 1.4450e-04
Loss = 2.2281e-05, PNorm = 52.5362, GNorm = 0.0867, lr_0 = 1.4435e-04
Validation rmse logD = 0.536854
Validation R2 logD = 0.796015
Validation rmse logP = 0.463934
Validation R2 logP = 0.936999
Epoch 84
Train function
Loss = 1.8612e-05, PNorm = 52.5367, GNorm = 0.1037, lr_0 = 1.4420e-04
Loss = 1.4952e-05, PNorm = 52.5370, GNorm = 0.0825, lr_0 = 1.4405e-04
Loss = 1.6027e-05, PNorm = 52.5373, GNorm = 0.0596, lr_0 = 1.4390e-04
Loss = 1.7050e-05, PNorm = 52.5379, GNorm = 0.0842, lr_0 = 1.4375e-04
Loss = 2.1311e-05, PNorm = 52.5392, GNorm = 0.1048, lr_0 = 1.4360e-04
Loss = 2.1471e-05, PNorm = 52.5401, GNorm = 0.1091, lr_0 = 1.4345e-04
Loss = 2.2372e-05, PNorm = 52.5415, GNorm = 0.1271, lr_0 = 1.4330e-04
Loss = 1.9987e-05, PNorm = 52.5431, GNorm = 0.1390, lr_0 = 1.4315e-04
Loss = 1.6688e-05, PNorm = 52.5441, GNorm = 0.0689, lr_0 = 1.4300e-04
Loss = 2.0252e-05, PNorm = 52.5456, GNorm = 0.0811, lr_0 = 1.4285e-04
Loss = 2.3429e-05, PNorm = 52.5471, GNorm = 0.1482, lr_0 = 1.4270e-04
Loss = 2.4020e-05, PNorm = 52.5470, GNorm = 0.1433, lr_0 = 1.4255e-04
Loss = 2.0390e-05, PNorm = 52.5495, GNorm = 0.0925, lr_0 = 1.4240e-04
Loss = 2.1916e-05, PNorm = 52.5516, GNorm = 0.0908, lr_0 = 1.4225e-04
Loss = 1.6872e-05, PNorm = 52.5521, GNorm = 0.0746, lr_0 = 1.4210e-04
Loss = 2.5933e-05, PNorm = 52.5523, GNorm = 0.1039, lr_0 = 1.4196e-04
Loss = 1.7074e-05, PNorm = 52.5537, GNorm = 0.0734, lr_0 = 1.4181e-04
Loss = 2.4073e-05, PNorm = 52.5552, GNorm = 0.0972, lr_0 = 1.4166e-04
Loss = 1.8847e-05, PNorm = 52.5568, GNorm = 0.1089, lr_0 = 1.4151e-04
Loss = 2.3939e-05, PNorm = 52.5580, GNorm = 0.0937, lr_0 = 1.4136e-04
Loss = 3.0401e-05, PNorm = 52.5575, GNorm = 0.1555, lr_0 = 1.4122e-04
Loss = 2.3568e-05, PNorm = 52.5588, GNorm = 0.0897, lr_0 = 1.4107e-04
Validation rmse logD = 0.538755
Validation R2 logD = 0.794567
Validation rmse logP = 0.465654
Validation R2 logP = 0.936530
Epoch 85
Train function
Loss = 1.7383e-05, PNorm = 52.5596, GNorm = 0.0739, lr_0 = 1.4091e-04
Loss = 2.1604e-05, PNorm = 52.5609, GNorm = 0.1318, lr_0 = 1.4076e-04
Loss = 2.2515e-05, PNorm = 52.5618, GNorm = 0.0487, lr_0 = 1.4061e-04
Loss = 2.4405e-05, PNorm = 52.5624, GNorm = 0.1902, lr_0 = 1.4047e-04
Loss = 2.2694e-05, PNorm = 52.5637, GNorm = 0.1506, lr_0 = 1.4032e-04
Loss = 1.8969e-05, PNorm = 52.5642, GNorm = 0.1108, lr_0 = 1.4017e-04
Loss = 1.5524e-05, PNorm = 52.5654, GNorm = 0.1332, lr_0 = 1.4003e-04
Loss = 1.9385e-05, PNorm = 52.5658, GNorm = 0.1126, lr_0 = 1.3988e-04
Loss = 2.0234e-05, PNorm = 52.5665, GNorm = 0.0930, lr_0 = 1.3974e-04
Loss = 1.5760e-05, PNorm = 52.5676, GNorm = 0.0839, lr_0 = 1.3959e-04
Loss = 1.4322e-05, PNorm = 52.5690, GNorm = 0.1365, lr_0 = 1.3944e-04
Loss = 1.4644e-05, PNorm = 52.5704, GNorm = 0.0848, lr_0 = 1.3930e-04
Loss = 2.1656e-05, PNorm = 52.5715, GNorm = 0.0877, lr_0 = 1.3915e-04
Loss = 1.8917e-05, PNorm = 52.5727, GNorm = 0.1169, lr_0 = 1.3901e-04
Loss = 1.9436e-05, PNorm = 52.5738, GNorm = 0.0974, lr_0 = 1.3886e-04
Loss = 2.4876e-05, PNorm = 52.5738, GNorm = 0.1713, lr_0 = 1.3872e-04
Loss = 2.3757e-05, PNorm = 52.5745, GNorm = 0.1322, lr_0 = 1.3857e-04
Loss = 2.1804e-05, PNorm = 52.5756, GNorm = 0.0844, lr_0 = 1.3843e-04
Loss = 1.8520e-05, PNorm = 52.5769, GNorm = 0.0619, lr_0 = 1.3828e-04
Loss = 2.2804e-05, PNorm = 52.5778, GNorm = 0.0779, lr_0 = 1.3814e-04
Loss = 2.0133e-05, PNorm = 52.5788, GNorm = 0.1120, lr_0 = 1.3800e-04
Loss = 2.3168e-05, PNorm = 52.5810, GNorm = 0.0902, lr_0 = 1.3785e-04
Loss = 2.3213e-05, PNorm = 52.5821, GNorm = 0.0953, lr_0 = 1.3771e-04
Validation rmse logD = 0.537470
Validation R2 logD = 0.795546
Validation rmse logP = 0.463234
Validation R2 logP = 0.937188
Epoch 86
Train function
Loss = 2.1738e-05, PNorm = 52.5832, GNorm = 0.1463, lr_0 = 1.3756e-04
Loss = 2.2034e-05, PNorm = 52.5837, GNorm = 0.0801, lr_0 = 1.3742e-04
Loss = 2.8330e-05, PNorm = 52.5865, GNorm = 0.2063, lr_0 = 1.3728e-04
Loss = 1.9098e-05, PNorm = 52.5891, GNorm = 0.0748, lr_0 = 1.3713e-04
Loss = 2.1861e-05, PNorm = 52.5907, GNorm = 0.0751, lr_0 = 1.3699e-04
Loss = 1.7302e-05, PNorm = 52.5918, GNorm = 0.0636, lr_0 = 1.3685e-04
Loss = 2.1520e-05, PNorm = 52.5924, GNorm = 0.1692, lr_0 = 1.3670e-04
Loss = 1.8644e-05, PNorm = 52.5936, GNorm = 0.0713, lr_0 = 1.3656e-04
Loss = 2.2289e-05, PNorm = 52.5952, GNorm = 0.1431, lr_0 = 1.3642e-04
Loss = 1.8651e-05, PNorm = 52.5964, GNorm = 0.0930, lr_0 = 1.3628e-04
Loss = 2.4121e-05, PNorm = 52.5974, GNorm = 0.1236, lr_0 = 1.3613e-04
Loss = 2.1471e-05, PNorm = 52.5978, GNorm = 0.0964, lr_0 = 1.3599e-04
Loss = 2.4735e-05, PNorm = 52.5992, GNorm = 0.1378, lr_0 = 1.3585e-04
Loss = 2.4074e-05, PNorm = 52.5994, GNorm = 0.1441, lr_0 = 1.3571e-04
Loss = 2.1955e-05, PNorm = 52.6003, GNorm = 0.1126, lr_0 = 1.3557e-04
Loss = 2.5287e-05, PNorm = 52.6010, GNorm = 0.0760, lr_0 = 1.3543e-04
Loss = 2.1481e-05, PNorm = 52.6037, GNorm = 0.0642, lr_0 = 1.3528e-04
Loss = 1.9251e-05, PNorm = 52.6054, GNorm = 0.1144, lr_0 = 1.3514e-04
Loss = 1.9545e-05, PNorm = 52.6052, GNorm = 0.0997, lr_0 = 1.3500e-04
Loss = 1.8792e-05, PNorm = 52.6053, GNorm = 0.0565, lr_0 = 1.3486e-04
Loss = 1.8185e-05, PNorm = 52.6061, GNorm = 0.1298, lr_0 = 1.3472e-04
Loss = 1.8402e-05, PNorm = 52.6057, GNorm = 0.1070, lr_0 = 1.3458e-04
Validation rmse logD = 0.535782
Validation R2 logD = 0.796828
Validation rmse logP = 0.463973
Validation R2 logP = 0.936988
Epoch 87
Train function
Loss = 2.1393e-05, PNorm = 52.6079, GNorm = 0.1759, lr_0 = 1.3443e-04
Loss = 1.7649e-05, PNorm = 52.6092, GNorm = 0.0888, lr_0 = 1.3428e-04
Loss = 1.5753e-05, PNorm = 52.6104, GNorm = 0.0728, lr_0 = 1.3414e-04
Loss = 2.0133e-05, PNorm = 52.6114, GNorm = 0.1477, lr_0 = 1.3400e-04
Loss = 1.8573e-05, PNorm = 52.6127, GNorm = 0.1050, lr_0 = 1.3386e-04
Loss = 1.7210e-05, PNorm = 52.6135, GNorm = 0.0637, lr_0 = 1.3373e-04
Loss = 2.3756e-05, PNorm = 52.6137, GNorm = 0.1228, lr_0 = 1.3359e-04
Loss = 2.3727e-05, PNorm = 52.6151, GNorm = 0.0889, lr_0 = 1.3345e-04
Loss = 1.9612e-05, PNorm = 52.6154, GNorm = 0.0986, lr_0 = 1.3331e-04
Loss = 1.9271e-05, PNorm = 52.6157, GNorm = 0.1677, lr_0 = 1.3317e-04
Loss = 2.0499e-05, PNorm = 52.6164, GNorm = 0.1100, lr_0 = 1.3303e-04
Loss = 1.7574e-05, PNorm = 52.6175, GNorm = 0.1332, lr_0 = 1.3289e-04
Loss = 1.8306e-05, PNorm = 52.6183, GNorm = 0.1191, lr_0 = 1.3275e-04
Loss = 1.7540e-05, PNorm = 52.6191, GNorm = 0.0904, lr_0 = 1.3261e-04
Loss = 1.8526e-05, PNorm = 52.6202, GNorm = 0.0755, lr_0 = 1.3247e-04
Loss = 1.4816e-05, PNorm = 52.6215, GNorm = 0.0563, lr_0 = 1.3234e-04
Loss = 1.7018e-05, PNorm = 52.6218, GNorm = 0.0846, lr_0 = 1.3220e-04
Loss = 1.7760e-05, PNorm = 52.6212, GNorm = 0.0969, lr_0 = 1.3206e-04
Loss = 1.6766e-05, PNorm = 52.6221, GNorm = 0.1389, lr_0 = 1.3192e-04
Loss = 1.3341e-05, PNorm = 52.6228, GNorm = 0.0762, lr_0 = 1.3178e-04
Loss = 1.3647e-05, PNorm = 52.6217, GNorm = 0.0846, lr_0 = 1.3165e-04
Loss = 2.0185e-05, PNorm = 52.6211, GNorm = 0.0960, lr_0 = 1.3151e-04
Loss = 1.8386e-05, PNorm = 52.6210, GNorm = 0.0815, lr_0 = 1.3137e-04
Validation rmse logD = 0.535572
Validation R2 logD = 0.796987
Validation rmse logP = 0.465020
Validation R2 logP = 0.936703
Epoch 88
Train function
Loss = 1.2472e-05, PNorm = 52.6223, GNorm = 0.0748, lr_0 = 1.3124e-04
Loss = 1.5398e-05, PNorm = 52.6236, GNorm = 0.1371, lr_0 = 1.3110e-04
Loss = 1.9273e-05, PNorm = 52.6241, GNorm = 0.1301, lr_0 = 1.3096e-04
Loss = 1.4873e-05, PNorm = 52.6251, GNorm = 0.0849, lr_0 = 1.3082e-04
Loss = 1.4495e-05, PNorm = 52.6249, GNorm = 0.1009, lr_0 = 1.3069e-04
Loss = 1.8717e-05, PNorm = 52.6259, GNorm = 0.1446, lr_0 = 1.3055e-04
Loss = 2.0515e-05, PNorm = 52.6278, GNorm = 0.0727, lr_0 = 1.3042e-04
Loss = 1.9012e-05, PNorm = 52.6284, GNorm = 0.1530, lr_0 = 1.3028e-04
Loss = 1.7584e-05, PNorm = 52.6296, GNorm = 0.1211, lr_0 = 1.3014e-04
Loss = 1.8513e-05, PNorm = 52.6306, GNorm = 0.1409, lr_0 = 1.3001e-04
Loss = 1.8651e-05, PNorm = 52.6315, GNorm = 0.0660, lr_0 = 1.2987e-04
Loss = 1.5280e-05, PNorm = 52.6319, GNorm = 0.0703, lr_0 = 1.2974e-04
Loss = 1.5643e-05, PNorm = 52.6324, GNorm = 0.0871, lr_0 = 1.2960e-04
Loss = 1.5819e-05, PNorm = 52.6336, GNorm = 0.1178, lr_0 = 1.2947e-04
Loss = 1.5528e-05, PNorm = 52.6349, GNorm = 0.0885, lr_0 = 1.2933e-04
Loss = 1.9144e-05, PNorm = 52.6365, GNorm = 0.0889, lr_0 = 1.2920e-04
Loss = 2.0278e-05, PNorm = 52.6373, GNorm = 0.0692, lr_0 = 1.2906e-04
Loss = 2.2963e-05, PNorm = 52.6384, GNorm = 0.1640, lr_0 = 1.2893e-04
Loss = 2.0003e-05, PNorm = 52.6390, GNorm = 0.1479, lr_0 = 1.2879e-04
Loss = 1.8269e-05, PNorm = 52.6400, GNorm = 0.0545, lr_0 = 1.2866e-04
Loss = 1.6737e-05, PNorm = 52.6413, GNorm = 0.0731, lr_0 = 1.2852e-04
Loss = 1.8631e-05, PNorm = 52.6437, GNorm = 0.0548, lr_0 = 1.2839e-04
Validation rmse logD = 0.535121
Validation R2 logD = 0.797329
Validation rmse logP = 0.467290
Validation R2 logP = 0.936084
Epoch 89
Train function
Loss = 2.0388e-05, PNorm = 52.6453, GNorm = 0.0615, lr_0 = 1.2824e-04
Loss = 1.7168e-05, PNorm = 52.6467, GNorm = 0.0975, lr_0 = 1.2811e-04
Loss = 1.3376e-05, PNorm = 52.6474, GNorm = 0.1275, lr_0 = 1.2797e-04
Loss = 1.5379e-05, PNorm = 52.6482, GNorm = 0.0567, lr_0 = 1.2784e-04
Loss = 1.5599e-05, PNorm = 52.6490, GNorm = 0.0777, lr_0 = 1.2771e-04
Loss = 1.5491e-05, PNorm = 52.6498, GNorm = 0.0647, lr_0 = 1.2757e-04
Loss = 1.7811e-05, PNorm = 52.6500, GNorm = 0.0862, lr_0 = 1.2744e-04
Loss = 1.4544e-05, PNorm = 52.6509, GNorm = 0.0503, lr_0 = 1.2731e-04
Loss = 1.3117e-05, PNorm = 52.6511, GNorm = 0.0719, lr_0 = 1.2717e-04
Loss = 1.6935e-05, PNorm = 52.6511, GNorm = 0.0868, lr_0 = 1.2704e-04
Loss = 1.9398e-05, PNorm = 52.6511, GNorm = 0.2294, lr_0 = 1.2691e-04
Loss = 1.7034e-05, PNorm = 52.6525, GNorm = 0.0770, lr_0 = 1.2678e-04
Loss = 1.6000e-05, PNorm = 52.6523, GNorm = 0.1271, lr_0 = 1.2664e-04
Loss = 1.3208e-05, PNorm = 52.6534, GNorm = 0.0704, lr_0 = 1.2651e-04
Loss = 1.5503e-05, PNorm = 52.6544, GNorm = 0.0659, lr_0 = 1.2638e-04
Loss = 1.8231e-05, PNorm = 52.6559, GNorm = 0.1047, lr_0 = 1.2625e-04
Loss = 1.8202e-05, PNorm = 52.6573, GNorm = 0.1846, lr_0 = 1.2612e-04
Loss = 1.7010e-05, PNorm = 52.6576, GNorm = 0.0903, lr_0 = 1.2598e-04
Loss = 1.8725e-05, PNorm = 52.6586, GNorm = 0.0927, lr_0 = 1.2585e-04
Loss = 1.7386e-05, PNorm = 52.6593, GNorm = 0.0867, lr_0 = 1.2572e-04
Loss = 1.9227e-05, PNorm = 52.6606, GNorm = 0.1317, lr_0 = 1.2559e-04
Loss = 2.0201e-05, PNorm = 52.6621, GNorm = 0.1013, lr_0 = 1.2546e-04
Loss = 2.0889e-05, PNorm = 52.6623, GNorm = 0.0827, lr_0 = 1.2533e-04
Validation rmse logD = 0.536833
Validation R2 logD = 0.796030
Validation rmse logP = 0.464747
Validation R2 logP = 0.936777
Epoch 90
Train function
Loss = 2.2724e-05, PNorm = 52.6640, GNorm = 0.1057, lr_0 = 1.2520e-04
Loss = 2.0263e-05, PNorm = 52.6654, GNorm = 0.1389, lr_0 = 1.2507e-04
Loss = 1.6137e-05, PNorm = 52.6659, GNorm = 0.1268, lr_0 = 1.2494e-04
Loss = 1.2931e-05, PNorm = 52.6664, GNorm = 0.0747, lr_0 = 1.2481e-04
Loss = 1.8622e-05, PNorm = 52.6674, GNorm = 0.0758, lr_0 = 1.2468e-04
Loss = 2.2108e-05, PNorm = 52.6682, GNorm = 0.2376, lr_0 = 1.2455e-04
Loss = 1.4267e-05, PNorm = 52.6684, GNorm = 0.0667, lr_0 = 1.2442e-04
Loss = 1.8971e-05, PNorm = 52.6703, GNorm = 0.1326, lr_0 = 1.2429e-04
Loss = 1.3696e-05, PNorm = 52.6703, GNorm = 0.0747, lr_0 = 1.2416e-04
Loss = 1.5211e-05, PNorm = 52.6715, GNorm = 0.0602, lr_0 = 1.2403e-04
Loss = 1.4569e-05, PNorm = 52.6714, GNorm = 0.0653, lr_0 = 1.2390e-04
Loss = 1.6626e-05, PNorm = 52.6714, GNorm = 0.1439, lr_0 = 1.2377e-04
Loss = 1.4721e-05, PNorm = 52.6720, GNorm = 0.0752, lr_0 = 1.2364e-04
Loss = 1.3514e-05, PNorm = 52.6728, GNorm = 0.0856, lr_0 = 1.2351e-04
Loss = 1.6607e-05, PNorm = 52.6732, GNorm = 0.1174, lr_0 = 1.2338e-04
Loss = 1.1241e-05, PNorm = 52.6736, GNorm = 0.1200, lr_0 = 1.2325e-04
Loss = 1.6179e-05, PNorm = 52.6746, GNorm = 0.1154, lr_0 = 1.2312e-04
Loss = 1.5518e-05, PNorm = 52.6756, GNorm = 0.0783, lr_0 = 1.2299e-04
Loss = 1.4444e-05, PNorm = 52.6767, GNorm = 0.1149, lr_0 = 1.2287e-04
Loss = 1.1067e-05, PNorm = 52.6773, GNorm = 0.0530, lr_0 = 1.2274e-04
Loss = 1.2738e-05, PNorm = 52.6784, GNorm = 0.1014, lr_0 = 1.2261e-04
Loss = 1.4366e-05, PNorm = 52.6794, GNorm = 0.1370, lr_0 = 1.2248e-04
Validation rmse logD = 0.534129
Validation R2 logD = 0.798080
Validation rmse logP = 0.464678
Validation R2 logP = 0.936796
Epoch 91
Train function
Loss = 2.2631e-05, PNorm = 52.6797, GNorm = 0.1008, lr_0 = 1.2234e-04
Loss = 2.1635e-05, PNorm = 52.6809, GNorm = 0.1776, lr_0 = 1.2221e-04
Loss = 1.8467e-05, PNorm = 52.6814, GNorm = 0.1438, lr_0 = 1.2209e-04
Loss = 1.4179e-05, PNorm = 52.6825, GNorm = 0.0932, lr_0 = 1.2196e-04
Loss = 1.8689e-05, PNorm = 52.6833, GNorm = 0.0767, lr_0 = 1.2183e-04
Loss = 1.3928e-05, PNorm = 52.6839, GNorm = 0.0842, lr_0 = 1.2170e-04
Loss = 1.5341e-05, PNorm = 52.6849, GNorm = 0.1061, lr_0 = 1.2158e-04
Loss = 1.2955e-05, PNorm = 52.6852, GNorm = 0.0848, lr_0 = 1.2145e-04
Loss = 1.2094e-05, PNorm = 52.6856, GNorm = 0.0903, lr_0 = 1.2132e-04
Loss = 1.2421e-05, PNorm = 52.6860, GNorm = 0.0505, lr_0 = 1.2120e-04
Loss = 1.4294e-05, PNorm = 52.6866, GNorm = 0.1505, lr_0 = 1.2107e-04
Loss = 1.5969e-05, PNorm = 52.6872, GNorm = 0.1380, lr_0 = 1.2094e-04
Loss = 1.5379e-05, PNorm = 52.6879, GNorm = 0.0767, lr_0 = 1.2082e-04
Loss = 1.1610e-05, PNorm = 52.6880, GNorm = 0.0752, lr_0 = 1.2069e-04
Loss = 2.6650e-05, PNorm = 52.6890, GNorm = 0.1310, lr_0 = 1.2057e-04
Loss = 2.0183e-05, PNorm = 52.6903, GNorm = 0.0646, lr_0 = 1.2044e-04
Loss = 1.4750e-05, PNorm = 52.6918, GNorm = 0.1254, lr_0 = 1.2031e-04
Loss = 1.4304e-05, PNorm = 52.6925, GNorm = 0.0860, lr_0 = 1.2019e-04
Loss = 1.6230e-05, PNorm = 52.6938, GNorm = 0.0575, lr_0 = 1.2006e-04
Loss = 1.5917e-05, PNorm = 52.6950, GNorm = 0.0820, lr_0 = 1.1994e-04
Loss = 1.9696e-05, PNorm = 52.6959, GNorm = 0.1232, lr_0 = 1.1981e-04
Loss = 1.6091e-05, PNorm = 52.6974, GNorm = 0.0572, lr_0 = 1.1969e-04
Loss = 1.5675e-05, PNorm = 52.6984, GNorm = 0.0918, lr_0 = 1.1956e-04
Validation rmse logD = 0.535617
Validation R2 logD = 0.796953
Validation rmse logP = 0.464992
Validation R2 logP = 0.936711
Epoch 92
Train function
Loss = 9.3067e-06, PNorm = 52.6989, GNorm = 0.0933, lr_0 = 1.1944e-04
Loss = 1.1395e-05, PNorm = 52.6998, GNorm = 0.0642, lr_0 = 1.1931e-04
Loss = 9.5452e-06, PNorm = 52.7004, GNorm = 0.0956, lr_0 = 1.1919e-04
Loss = 1.5293e-05, PNorm = 52.7015, GNorm = 0.0741, lr_0 = 1.1906e-04
Loss = 1.1398e-05, PNorm = 52.7028, GNorm = 0.0645, lr_0 = 1.1894e-04
Loss = 1.2298e-05, PNorm = 52.7042, GNorm = 0.0536, lr_0 = 1.1882e-04
Loss = 1.0739e-05, PNorm = 52.7042, GNorm = 0.0636, lr_0 = 1.1869e-04
Loss = 1.2008e-05, PNorm = 52.7048, GNorm = 0.0726, lr_0 = 1.1857e-04
Loss = 1.1568e-05, PNorm = 52.7054, GNorm = 0.0699, lr_0 = 1.1844e-04
Loss = 1.0730e-05, PNorm = 52.7057, GNorm = 0.0471, lr_0 = 1.1832e-04
Loss = 1.2775e-05, PNorm = 52.7064, GNorm = 0.0698, lr_0 = 1.1820e-04
Loss = 1.8117e-05, PNorm = 52.7061, GNorm = 0.1337, lr_0 = 1.1807e-04
Loss = 1.8523e-05, PNorm = 52.7073, GNorm = 0.0705, lr_0 = 1.1795e-04
Loss = 1.3277e-05, PNorm = 52.7080, GNorm = 0.0837, lr_0 = 1.1783e-04
Loss = 1.4860e-05, PNorm = 52.7089, GNorm = 0.1143, lr_0 = 1.1770e-04
Loss = 1.4208e-05, PNorm = 52.7097, GNorm = 0.0595, lr_0 = 1.1758e-04
Loss = 1.7165e-05, PNorm = 52.7107, GNorm = 0.1322, lr_0 = 1.1746e-04
Loss = 1.3997e-05, PNorm = 52.7110, GNorm = 0.0953, lr_0 = 1.1734e-04
Loss = 1.3293e-05, PNorm = 52.7106, GNorm = 0.0693, lr_0 = 1.1721e-04
Loss = 1.3027e-05, PNorm = 52.7110, GNorm = 0.0787, lr_0 = 1.1709e-04
Loss = 1.3103e-05, PNorm = 52.7114, GNorm = 0.0772, lr_0 = 1.1697e-04
Loss = 1.4062e-05, PNorm = 52.7112, GNorm = 0.0672, lr_0 = 1.1685e-04
Validation rmse logD = 0.535876
Validation R2 logD = 0.796757
Validation rmse logP = 0.463603
Validation R2 logP = 0.937088
Epoch 93
Train function
Loss = 1.0488e-05, PNorm = 52.7123, GNorm = 0.0641, lr_0 = 1.1671e-04
Loss = 2.0399e-05, PNorm = 52.7139, GNorm = 0.1453, lr_0 = 1.1659e-04
Loss = 1.4604e-05, PNorm = 52.7148, GNorm = 0.0763, lr_0 = 1.1647e-04
Loss = 1.5863e-05, PNorm = 52.7160, GNorm = 0.0768, lr_0 = 1.1635e-04
Loss = 1.8511e-05, PNorm = 52.7161, GNorm = 0.0688, lr_0 = 1.1623e-04
Loss = 1.2825e-05, PNorm = 52.7161, GNorm = 0.0819, lr_0 = 1.1611e-04
Loss = 1.2098e-05, PNorm = 52.7173, GNorm = 0.1078, lr_0 = 1.1598e-04
Loss = 1.3610e-05, PNorm = 52.7184, GNorm = 0.0592, lr_0 = 1.1586e-04
Loss = 1.2055e-05, PNorm = 52.7194, GNorm = 0.0792, lr_0 = 1.1574e-04
Loss = 1.4101e-05, PNorm = 52.7202, GNorm = 0.1100, lr_0 = 1.1562e-04
Loss = 1.4189e-05, PNorm = 52.7203, GNorm = 0.1165, lr_0 = 1.1550e-04
Loss = 1.7182e-05, PNorm = 52.7205, GNorm = 0.0811, lr_0 = 1.1538e-04
Loss = 1.5909e-05, PNorm = 52.7214, GNorm = 0.1177, lr_0 = 1.1526e-04
Loss = 1.4205e-05, PNorm = 52.7227, GNorm = 0.0807, lr_0 = 1.1514e-04
Loss = 1.6146e-05, PNorm = 52.7231, GNorm = 0.1062, lr_0 = 1.1502e-04
Loss = 1.5417e-05, PNorm = 52.7234, GNorm = 0.1442, lr_0 = 1.1490e-04
Loss = 1.1932e-05, PNorm = 52.7241, GNorm = 0.0953, lr_0 = 1.1478e-04
Loss = 1.6220e-05, PNorm = 52.7250, GNorm = 0.1263, lr_0 = 1.1466e-04
Loss = 1.5110e-05, PNorm = 52.7248, GNorm = 0.1070, lr_0 = 1.1454e-04
Loss = 1.6988e-05, PNorm = 52.7251, GNorm = 0.1523, lr_0 = 1.1442e-04
Loss = 1.2879e-05, PNorm = 52.7265, GNorm = 0.1083, lr_0 = 1.1430e-04
Loss = 1.3103e-05, PNorm = 52.7279, GNorm = 0.0798, lr_0 = 1.1418e-04
Loss = 1.5106e-05, PNorm = 52.7287, GNorm = 0.1649, lr_0 = 1.1406e-04
Validation rmse logD = 0.535173
Validation R2 logD = 0.797290
Validation rmse logP = 0.464692
Validation R2 logP = 0.936792
Epoch 94
Train function
Loss = 2.0480e-05, PNorm = 52.7293, GNorm = 0.2093, lr_0 = 1.1394e-04
Loss = 1.1487e-05, PNorm = 52.7296, GNorm = 0.1308, lr_0 = 1.1382e-04
Loss = 1.4912e-05, PNorm = 52.7306, GNorm = 0.0862, lr_0 = 1.1371e-04
Loss = 1.1314e-05, PNorm = 52.7311, GNorm = 0.0772, lr_0 = 1.1359e-04
Loss = 1.2755e-05, PNorm = 52.7315, GNorm = 0.0867, lr_0 = 1.1347e-04
Loss = 1.7733e-05, PNorm = 52.7318, GNorm = 0.1370, lr_0 = 1.1335e-04
Loss = 1.6138e-05, PNorm = 52.7324, GNorm = 0.1028, lr_0 = 1.1323e-04
Loss = 1.3741e-05, PNorm = 52.7338, GNorm = 0.1321, lr_0 = 1.1311e-04
Loss = 1.1568e-05, PNorm = 52.7355, GNorm = 0.0414, lr_0 = 1.1300e-04
Loss = 1.2830e-05, PNorm = 52.7361, GNorm = 0.0878, lr_0 = 1.1288e-04
Loss = 1.0077e-05, PNorm = 52.7369, GNorm = 0.0381, lr_0 = 1.1276e-04
Loss = 1.3680e-05, PNorm = 52.7373, GNorm = 0.0687, lr_0 = 1.1264e-04
Loss = 1.3355e-05, PNorm = 52.7379, GNorm = 0.0604, lr_0 = 1.1252e-04
Loss = 1.2839e-05, PNorm = 52.7379, GNorm = 0.1008, lr_0 = 1.1241e-04
Loss = 1.3682e-05, PNorm = 52.7386, GNorm = 0.0964, lr_0 = 1.1229e-04
Loss = 2.0792e-05, PNorm = 52.7402, GNorm = 0.0676, lr_0 = 1.1217e-04
Loss = 1.9484e-05, PNorm = 52.7415, GNorm = 0.1115, lr_0 = 1.1206e-04
Loss = 1.2982e-05, PNorm = 52.7422, GNorm = 0.0669, lr_0 = 1.1194e-04
Loss = 1.3376e-05, PNorm = 52.7431, GNorm = 0.1002, lr_0 = 1.1182e-04
Loss = 1.0761e-05, PNorm = 52.7439, GNorm = 0.0973, lr_0 = 1.1170e-04
Loss = 1.6037e-05, PNorm = 52.7443, GNorm = 0.1411, lr_0 = 1.1159e-04
Loss = 1.7046e-05, PNorm = 52.7445, GNorm = 0.1214, lr_0 = 1.1147e-04
Loss = 1.7155e-05, PNorm = 52.7462, GNorm = 0.1500, lr_0 = 1.1136e-04
Loss = 1.4228e-05, PNorm = 52.7463, GNorm = 0.0691, lr_0 = 1.1134e-04
Validation rmse logD = 0.534567
Validation R2 logD = 0.797749
Validation rmse logP = 0.463893
Validation R2 logP = 0.937010
Epoch 95
Train function
Loss = 1.2294e-05, PNorm = 52.7476, GNorm = 0.0869, lr_0 = 1.1123e-04
Loss = 1.0773e-05, PNorm = 52.7480, GNorm = 0.0502, lr_0 = 1.1111e-04
Loss = 1.4240e-05, PNorm = 52.7479, GNorm = 0.0881, lr_0 = 1.1100e-04
Loss = 1.0634e-05, PNorm = 52.7480, GNorm = 0.0699, lr_0 = 1.1088e-04
Loss = 1.3520e-05, PNorm = 52.7486, GNorm = 0.1210, lr_0 = 1.1076e-04
Loss = 1.6911e-05, PNorm = 52.7494, GNorm = 0.1936, lr_0 = 1.1065e-04
Loss = 1.8227e-05, PNorm = 52.7514, GNorm = 0.1788, lr_0 = 1.1053e-04
Loss = 1.1834e-05, PNorm = 52.7522, GNorm = 0.0993, lr_0 = 1.1042e-04
Loss = 1.3816e-05, PNorm = 52.7529, GNorm = 0.1048, lr_0 = 1.1030e-04
Loss = 1.6141e-05, PNorm = 52.7537, GNorm = 0.1648, lr_0 = 1.1019e-04
Loss = 1.4988e-05, PNorm = 52.7554, GNorm = 0.0784, lr_0 = 1.1007e-04
Loss = 1.2570e-05, PNorm = 52.7559, GNorm = 0.0515, lr_0 = 1.0996e-04
Loss = 1.0539e-05, PNorm = 52.7568, GNorm = 0.0808, lr_0 = 1.0984e-04
Loss = 1.2986e-05, PNorm = 52.7566, GNorm = 0.1505, lr_0 = 1.0973e-04
Loss = 1.2832e-05, PNorm = 52.7570, GNorm = 0.0369, lr_0 = 1.0961e-04
Loss = 1.7458e-05, PNorm = 52.7572, GNorm = 0.0664, lr_0 = 1.0950e-04
Loss = 1.1496e-05, PNorm = 52.7581, GNorm = 0.0740, lr_0 = 1.0938e-04
Loss = 1.3134e-05, PNorm = 52.7584, GNorm = 0.0807, lr_0 = 1.0927e-04
Loss = 1.2584e-05, PNorm = 52.7586, GNorm = 0.0645, lr_0 = 1.0916e-04
Loss = 1.1607e-05, PNorm = 52.7589, GNorm = 0.1262, lr_0 = 1.0904e-04
Loss = 1.3450e-05, PNorm = 52.7594, GNorm = 0.0813, lr_0 = 1.0893e-04
Loss = 1.8727e-05, PNorm = 52.7601, GNorm = 0.0997, lr_0 = 1.0882e-04
Validation rmse logD = 0.538708
Validation R2 logD = 0.794603
Validation rmse logP = 0.464313
Validation R2 logP = 0.936896
Epoch 96
Train function
Loss = 1.1521e-05, PNorm = 52.7606, GNorm = 0.0793, lr_0 = 1.0870e-04
Loss = 1.8037e-05, PNorm = 52.7614, GNorm = 0.1313, lr_0 = 1.0859e-04
Loss = 1.5323e-05, PNorm = 52.7620, GNorm = 0.1019, lr_0 = 1.0847e-04
Loss = 1.3573e-05, PNorm = 52.7614, GNorm = 0.0649, lr_0 = 1.0836e-04
Loss = 1.2716e-05, PNorm = 52.7620, GNorm = 0.1041, lr_0 = 1.0825e-04
Loss = 1.2150e-05, PNorm = 52.7628, GNorm = 0.1114, lr_0 = 1.0814e-04
Loss = 1.3974e-05, PNorm = 52.7631, GNorm = 0.0609, lr_0 = 1.0802e-04
Loss = 1.2697e-05, PNorm = 52.7635, GNorm = 0.0578, lr_0 = 1.0791e-04
Loss = 1.4274e-05, PNorm = 52.7641, GNorm = 0.0929, lr_0 = 1.0780e-04
Loss = 1.0595e-05, PNorm = 52.7646, GNorm = 0.0765, lr_0 = 1.0768e-04
Loss = 1.1684e-05, PNorm = 52.7650, GNorm = 0.0916, lr_0 = 1.0757e-04
Loss = 1.2323e-05, PNorm = 52.7664, GNorm = 0.0638, lr_0 = 1.0746e-04
Loss = 1.1543e-05, PNorm = 52.7678, GNorm = 0.1064, lr_0 = 1.0735e-04
Loss = 1.2632e-05, PNorm = 52.7681, GNorm = 0.0700, lr_0 = 1.0724e-04
Loss = 1.0263e-05, PNorm = 52.7687, GNorm = 0.0897, lr_0 = 1.0712e-04
Loss = 9.7581e-06, PNorm = 52.7692, GNorm = 0.1308, lr_0 = 1.0701e-04
Loss = 1.2940e-05, PNorm = 52.7701, GNorm = 0.1260, lr_0 = 1.0690e-04
Loss = 1.2136e-05, PNorm = 52.7720, GNorm = 0.1067, lr_0 = 1.0679e-04
Loss = 1.5908e-05, PNorm = 52.7722, GNorm = 0.1125, lr_0 = 1.0668e-04
Loss = 1.9272e-05, PNorm = 52.7725, GNorm = 0.2461, lr_0 = 1.0657e-04
Loss = 1.7332e-05, PNorm = 52.7741, GNorm = 0.0654, lr_0 = 1.0645e-04
Loss = 1.4061e-05, PNorm = 52.7745, GNorm = 0.1078, lr_0 = 1.0634e-04
Loss = 1.6213e-05, PNorm = 52.7757, GNorm = 0.0599, lr_0 = 1.0623e-04
Validation rmse logD = 0.537411
Validation R2 logD = 0.795591
Validation rmse logP = 0.464738
Validation R2 logP = 0.936780
Epoch 97
Train function
Loss = 1.4121e-05, PNorm = 52.7765, GNorm = 0.0693, lr_0 = 1.0611e-04
Loss = 1.7334e-05, PNorm = 52.7779, GNorm = 0.1921, lr_0 = 1.0600e-04
Loss = 2.2245e-05, PNorm = 52.7782, GNorm = 0.1033, lr_0 = 1.0589e-04
Loss = 1.3940e-05, PNorm = 52.7794, GNorm = 0.0806, lr_0 = 1.0578e-04
Loss = 1.2340e-05, PNorm = 52.7799, GNorm = 0.0583, lr_0 = 1.0567e-04
Loss = 1.2134e-05, PNorm = 52.7801, GNorm = 0.0596, lr_0 = 1.0556e-04
Loss = 1.2519e-05, PNorm = 52.7813, GNorm = 0.0535, lr_0 = 1.0545e-04
Loss = 1.0646e-05, PNorm = 52.7822, GNorm = 0.1073, lr_0 = 1.0534e-04
Loss = 1.2056e-05, PNorm = 52.7830, GNorm = 0.1117, lr_0 = 1.0523e-04
Loss = 1.1765e-05, PNorm = 52.7834, GNorm = 0.0536, lr_0 = 1.0512e-04
Loss = 1.5190e-05, PNorm = 52.7839, GNorm = 0.0696, lr_0 = 1.0501e-04
Loss = 1.2803e-05, PNorm = 52.7843, GNorm = 0.0702, lr_0 = 1.0490e-04
Loss = 1.2634e-05, PNorm = 52.7845, GNorm = 0.0858, lr_0 = 1.0479e-04
Loss = 1.2886e-05, PNorm = 52.7854, GNorm = 0.1016, lr_0 = 1.0468e-04
Loss = 9.9671e-06, PNorm = 52.7858, GNorm = 0.0737, lr_0 = 1.0457e-04
Loss = 1.3136e-05, PNorm = 52.7860, GNorm = 0.0939, lr_0 = 1.0446e-04
Loss = 1.1057e-05, PNorm = 52.7863, GNorm = 0.1363, lr_0 = 1.0435e-04
Loss = 9.7084e-06, PNorm = 52.7869, GNorm = 0.0609, lr_0 = 1.0424e-04
Loss = 1.2509e-05, PNorm = 52.7872, GNorm = 0.0691, lr_0 = 1.0413e-04
Loss = 1.6374e-05, PNorm = 52.7879, GNorm = 0.1427, lr_0 = 1.0403e-04
Loss = 1.2822e-05, PNorm = 52.7889, GNorm = 0.1410, lr_0 = 1.0392e-04
Loss = 1.1361e-05, PNorm = 52.7892, GNorm = 0.0621, lr_0 = 1.0381e-04
Validation rmse logD = 0.536018
Validation R2 logD = 0.796649
Validation rmse logP = 0.463505
Validation R2 logP = 0.937115
Epoch 98
Train function
Loss = 8.8092e-06, PNorm = 52.7894, GNorm = 0.0645, lr_0 = 1.0370e-04
Loss = 9.1842e-06, PNorm = 52.7901, GNorm = 0.0641, lr_0 = 1.0359e-04
Loss = 1.0470e-05, PNorm = 52.7904, GNorm = 0.1148, lr_0 = 1.0348e-04
Loss = 1.0263e-05, PNorm = 52.7912, GNorm = 0.0572, lr_0 = 1.0338e-04
Loss = 9.8897e-06, PNorm = 52.7914, GNorm = 0.0737, lr_0 = 1.0327e-04
Loss = 8.2698e-06, PNorm = 52.7920, GNorm = 0.0551, lr_0 = 1.0316e-04
Loss = 1.0825e-05, PNorm = 52.7921, GNorm = 0.0620, lr_0 = 1.0305e-04
Loss = 1.0304e-05, PNorm = 52.7925, GNorm = 0.1166, lr_0 = 1.0295e-04
Loss = 1.0261e-05, PNorm = 52.7927, GNorm = 0.0498, lr_0 = 1.0284e-04
Loss = 9.4012e-06, PNorm = 52.7934, GNorm = 0.0610, lr_0 = 1.0273e-04
Loss = 1.0088e-05, PNorm = 52.7937, GNorm = 0.0629, lr_0 = 1.0262e-04
Loss = 9.6881e-06, PNorm = 52.7938, GNorm = 0.0917, lr_0 = 1.0252e-04
Loss = 1.1969e-05, PNorm = 52.7946, GNorm = 0.0908, lr_0 = 1.0241e-04
Loss = 1.2322e-05, PNorm = 52.7953, GNorm = 0.1067, lr_0 = 1.0230e-04
Loss = 1.1748e-05, PNorm = 52.7955, GNorm = 0.0744, lr_0 = 1.0220e-04
Loss = 1.2503e-05, PNorm = 52.7961, GNorm = 0.0714, lr_0 = 1.0209e-04
Loss = 1.5453e-05, PNorm = 52.7970, GNorm = 0.1105, lr_0 = 1.0198e-04
Loss = 1.5562e-05, PNorm = 52.7983, GNorm = 0.0677, lr_0 = 1.0188e-04
Loss = 1.1304e-05, PNorm = 52.7992, GNorm = 0.0650, lr_0 = 1.0177e-04
Loss = 1.4621e-05, PNorm = 52.8004, GNorm = 0.1001, lr_0 = 1.0166e-04
Loss = 1.2774e-05, PNorm = 52.8012, GNorm = 0.1059, lr_0 = 1.0156e-04
Loss = 8.5443e-06, PNorm = 52.8017, GNorm = 0.0659, lr_0 = 1.0145e-04
Loss = 1.1165e-05, PNorm = 52.8027, GNorm = 0.0717, lr_0 = 1.0135e-04
Validation rmse logD = 0.535804
Validation R2 logD = 0.796812
Validation rmse logP = 0.463415
Validation R2 logP = 0.937139
Epoch 99
Train function
Loss = 7.7992e-06, PNorm = 52.8032, GNorm = 0.0483, lr_0 = 1.0123e-04
Loss = 1.0003e-05, PNorm = 52.8028, GNorm = 0.0592, lr_0 = 1.0112e-04
Loss = 1.5033e-05, PNorm = 52.8032, GNorm = 0.1465, lr_0 = 1.0102e-04
Loss = 8.2195e-06, PNorm = 52.8043, GNorm = 0.0455, lr_0 = 1.0091e-04
Loss = 8.5921e-06, PNorm = 52.8051, GNorm = 0.1118, lr_0 = 1.0081e-04
Loss = 1.1083e-05, PNorm = 52.8053, GNorm = 0.0907, lr_0 = 1.0070e-04
Loss = 9.6588e-06, PNorm = 52.8063, GNorm = 0.0788, lr_0 = 1.0060e-04
Loss = 1.5340e-05, PNorm = 52.8069, GNorm = 0.1243, lr_0 = 1.0049e-04
Loss = 1.2565e-05, PNorm = 52.8068, GNorm = 0.0705, lr_0 = 1.0039e-04
Loss = 1.2423e-05, PNorm = 52.8075, GNorm = 0.0969, lr_0 = 1.0028e-04
Loss = 1.2504e-05, PNorm = 52.8082, GNorm = 0.1071, lr_0 = 1.0018e-04
Loss = 1.4012e-05, PNorm = 52.8088, GNorm = 0.1110, lr_0 = 1.0007e-04
Loss = 1.1978e-05, PNorm = 52.8100, GNorm = 0.0564, lr_0 = 1.0000e-04
Loss = 1.2744e-05, PNorm = 52.8107, GNorm = 0.1426, lr_0 = 1.0000e-04
Loss = 9.7153e-06, PNorm = 52.8111, GNorm = 0.0843, lr_0 = 1.0000e-04
Loss = 9.5465e-06, PNorm = 52.8116, GNorm = 0.0782, lr_0 = 1.0000e-04
Loss = 1.0441e-05, PNorm = 52.8129, GNorm = 0.0764, lr_0 = 1.0000e-04
Loss = 1.2514e-05, PNorm = 52.8138, GNorm = 0.0918, lr_0 = 1.0000e-04
Loss = 1.0799e-05, PNorm = 52.8149, GNorm = 0.1148, lr_0 = 1.0000e-04
Loss = 1.2257e-05, PNorm = 52.8154, GNorm = 0.0863, lr_0 = 1.0000e-04
Loss = 1.2846e-05, PNorm = 52.8157, GNorm = 0.1376, lr_0 = 1.0000e-04
Loss = 1.2565e-05, PNorm = 52.8163, GNorm = 0.0416, lr_0 = 1.0000e-04
Validation rmse logD = 0.534871
Validation R2 logD = 0.797518
Validation rmse logP = 0.464881
Validation R2 logP = 0.936741
Model 0 best validation rmse = 0.497194 on epoch 54
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logD = 0.553774
Model 0 test R2 logD = 0.805500
Model 0 test rmse logP = 0.444645
Model 0 test R2 logP = 0.943213
Ensemble test rmse  logD= 0.553774
Ensemble test R2  logD= 0.805500
Ensemble test rmse  logP= 0.444645
Ensemble test R2  logP= 0.943213
Fold 3
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_best'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 199,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_3',
 'save_smiles_splits': False,
 'seed': 3,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD', 'logP'],
 'task_names': ['logD', 'logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 11274,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 3
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=499, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 414,902
Moving model to cuda
Epoch 0
Train function
Loss = 2.0380e-02, PNorm = 35.0908, GNorm = 6.9093, lr_0 = 1.2200e-04
Loss = 1.6877e-02, PNorm = 35.0929, GNorm = 4.7350, lr_0 = 1.4200e-04
Loss = 1.6629e-02, PNorm = 35.0952, GNorm = 2.8629, lr_0 = 1.6200e-04
Loss = 1.3592e-02, PNorm = 35.0992, GNorm = 3.0481, lr_0 = 1.8200e-04
Loss = 1.1434e-02, PNorm = 35.1048, GNorm = 6.1631, lr_0 = 2.0200e-04
Loss = 1.0383e-02, PNorm = 35.1108, GNorm = 3.9059, lr_0 = 2.2200e-04
Loss = 9.7494e-03, PNorm = 35.1188, GNorm = 4.4863, lr_0 = 2.4200e-04
Loss = 9.2847e-03, PNorm = 35.1248, GNorm = 4.7230, lr_0 = 2.6200e-04
Loss = 8.2885e-03, PNorm = 35.1326, GNorm = 3.4851, lr_0 = 2.8200e-04
Loss = 9.4989e-03, PNorm = 35.1396, GNorm = 8.6550, lr_0 = 3.0200e-04
Loss = 9.4086e-03, PNorm = 35.1469, GNorm = 5.0147, lr_0 = 3.2200e-04
Loss = 9.7746e-03, PNorm = 35.1586, GNorm = 2.8579, lr_0 = 3.4200e-04
Loss = 7.1181e-03, PNorm = 35.1704, GNorm = 1.5043, lr_0 = 3.6200e-04
Loss = 8.0021e-03, PNorm = 35.1824, GNorm = 2.6991, lr_0 = 3.8200e-04
Loss = 7.4672e-03, PNorm = 35.1947, GNorm = 1.9166, lr_0 = 4.0200e-04
Loss = 7.2182e-03, PNorm = 35.2073, GNorm = 3.7836, lr_0 = 4.2200e-04
Loss = 6.8098e-03, PNorm = 35.2207, GNorm = 4.6994, lr_0 = 4.4200e-04
Loss = 6.8405e-03, PNorm = 35.2365, GNorm = 2.7402, lr_0 = 4.6200e-04
Loss = 7.2033e-03, PNorm = 35.2538, GNorm = 2.1435, lr_0 = 4.8200e-04
Loss = 5.9455e-03, PNorm = 35.2706, GNorm = 1.2363, lr_0 = 5.0200e-04
Loss = 5.9773e-03, PNorm = 35.2867, GNorm = 2.1027, lr_0 = 5.2200e-04
Loss = 7.0227e-03, PNorm = 35.3053, GNorm = 3.8406, lr_0 = 5.4200e-04
Validation rmse logD = 0.951841
Validation R2 logD = 0.372510
Validation rmse logP = 0.871212
Validation R2 logP = 0.779713
Epoch 1
Train function
Loss = 5.4468e-03, PNorm = 35.3239, GNorm = 2.5739, lr_0 = 5.6400e-04
Loss = 4.6984e-03, PNorm = 35.3391, GNorm = 1.3349, lr_0 = 5.8400e-04
Loss = 4.9573e-03, PNorm = 35.3602, GNorm = 2.6577, lr_0 = 6.0400e-04
Loss = 5.7674e-03, PNorm = 35.3769, GNorm = 6.5272, lr_0 = 6.2400e-04
Loss = 6.7306e-03, PNorm = 35.3993, GNorm = 2.7031, lr_0 = 6.4400e-04
Loss = 6.1369e-03, PNorm = 35.4235, GNorm = 2.2680, lr_0 = 6.6400e-04
Loss = 5.0594e-03, PNorm = 35.4416, GNorm = 3.5911, lr_0 = 6.8400e-04
Loss = 4.6465e-03, PNorm = 35.4656, GNorm = 2.6501, lr_0 = 7.0400e-04
Loss = 4.9448e-03, PNorm = 35.4900, GNorm = 4.1805, lr_0 = 7.2400e-04
Loss = 4.6976e-03, PNorm = 35.5121, GNorm = 1.4466, lr_0 = 7.4400e-04
Loss = 4.9201e-03, PNorm = 35.5360, GNorm = 1.5399, lr_0 = 7.6400e-04
Loss = 4.6383e-03, PNorm = 35.5590, GNorm = 3.8469, lr_0 = 7.8400e-04
Loss = 5.3030e-03, PNorm = 35.5874, GNorm = 4.4918, lr_0 = 8.0400e-04
Loss = 5.6086e-03, PNorm = 35.6107, GNorm = 3.9135, lr_0 = 8.2400e-04
Loss = 5.3678e-03, PNorm = 35.6434, GNorm = 1.3014, lr_0 = 8.4400e-04
Loss = 5.2026e-03, PNorm = 35.6736, GNorm = 1.1863, lr_0 = 8.6400e-04
Loss = 4.5994e-03, PNorm = 35.7013, GNorm = 1.8341, lr_0 = 8.8400e-04
Loss = 5.3335e-03, PNorm = 35.7313, GNorm = 2.5219, lr_0 = 9.0400e-04
Loss = 4.0824e-03, PNorm = 35.7652, GNorm = 2.4931, lr_0 = 9.2400e-04
Loss = 4.5395e-03, PNorm = 35.7960, GNorm = 2.8973, lr_0 = 9.4400e-04
Loss = 4.8399e-03, PNorm = 35.8293, GNorm = 2.8594, lr_0 = 9.6400e-04
Loss = 4.6811e-03, PNorm = 35.8609, GNorm = 1.2571, lr_0 = 9.8400e-04
Loss = 5.1369e-03, PNorm = 35.9109, GNorm = 1.0286, lr_0 = 9.9979e-04
Loss = 5.9621e-03, PNorm = 35.9165, GNorm = 1.4695, lr_0 = 9.9969e-04
Validation rmse logD = 0.872543
Validation R2 logD = 0.472708
Validation rmse logP = 0.669897
Validation R2 logP = 0.869756
Epoch 2
Train function
Loss = 3.7164e-03, PNorm = 35.9537, GNorm = 1.2033, lr_0 = 9.9864e-04
Loss = 3.5709e-03, PNorm = 35.9851, GNorm = 2.4335, lr_0 = 9.9760e-04
Loss = 4.5563e-03, PNorm = 36.0097, GNorm = 2.3648, lr_0 = 9.9656e-04
Loss = 4.0299e-03, PNorm = 36.0495, GNorm = 2.5292, lr_0 = 9.9552e-04
Loss = 3.8830e-03, PNorm = 36.0839, GNorm = 2.9809, lr_0 = 9.9448e-04
Loss = 4.2163e-03, PNorm = 36.1143, GNorm = 5.6190, lr_0 = 9.9344e-04
Loss = 5.0812e-03, PNorm = 36.1520, GNorm = 0.8744, lr_0 = 9.9241e-04
Loss = 5.2086e-03, PNorm = 36.1875, GNorm = 3.7510, lr_0 = 9.9137e-04
Loss = 4.0385e-03, PNorm = 36.2184, GNorm = 2.4258, lr_0 = 9.9034e-04
Loss = 4.6777e-03, PNorm = 36.2603, GNorm = 4.7918, lr_0 = 9.8930e-04
Loss = 4.6453e-03, PNorm = 36.2988, GNorm = 2.9200, lr_0 = 9.8827e-04
Loss = 4.4223e-03, PNorm = 36.3343, GNorm = 2.1129, lr_0 = 9.8724e-04
Loss = 5.6688e-03, PNorm = 36.3737, GNorm = 1.4117, lr_0 = 9.8621e-04
Loss = 4.2441e-03, PNorm = 36.4097, GNorm = 2.6219, lr_0 = 9.8518e-04
Loss = 4.0355e-03, PNorm = 36.4401, GNorm = 1.6100, lr_0 = 9.8415e-04
Loss = 4.3893e-03, PNorm = 36.4780, GNorm = 1.7435, lr_0 = 9.8312e-04
Loss = 3.8873e-03, PNorm = 36.5145, GNorm = 1.4001, lr_0 = 9.8210e-04
Loss = 3.9972e-03, PNorm = 36.5405, GNorm = 1.0031, lr_0 = 9.8107e-04
Loss = 3.8215e-03, PNorm = 36.5776, GNorm = 1.7410, lr_0 = 9.8005e-04
Loss = 3.9109e-03, PNorm = 36.6092, GNorm = 1.7309, lr_0 = 9.7902e-04
Loss = 3.1576e-03, PNorm = 36.6344, GNorm = 2.3593, lr_0 = 9.7800e-04
Loss = 3.5874e-03, PNorm = 36.6670, GNorm = 3.1472, lr_0 = 9.7698e-04
Validation rmse logD = 0.872480
Validation R2 logD = 0.472784
Validation rmse logP = 0.649152
Validation R2 logP = 0.877698
Epoch 3
Train function
Loss = 3.7608e-03, PNorm = 36.6951, GNorm = 3.5134, lr_0 = 9.7596e-04
Loss = 3.4740e-03, PNorm = 36.7269, GNorm = 1.1870, lr_0 = 9.7494e-04
Loss = 4.2548e-03, PNorm = 36.7596, GNorm = 1.1791, lr_0 = 9.7393e-04
Loss = 3.0748e-03, PNorm = 36.7959, GNorm = 1.5424, lr_0 = 9.7291e-04
Loss = 3.3625e-03, PNorm = 36.8346, GNorm = 3.8715, lr_0 = 9.7189e-04
Loss = 3.3775e-03, PNorm = 36.8614, GNorm = 0.9377, lr_0 = 9.7088e-04
Loss = 3.8667e-03, PNorm = 36.8945, GNorm = 1.2409, lr_0 = 9.6987e-04
Loss = 3.3623e-03, PNorm = 36.9486, GNorm = 0.9204, lr_0 = 9.6885e-04
Loss = 3.1167e-03, PNorm = 36.9855, GNorm = 0.9206, lr_0 = 9.6784e-04
Loss = 3.7041e-03, PNorm = 37.0247, GNorm = 2.4319, lr_0 = 9.6683e-04
Loss = 3.6897e-03, PNorm = 37.0674, GNorm = 1.3532, lr_0 = 9.6582e-04
Loss = 3.0051e-03, PNorm = 37.0988, GNorm = 0.7256, lr_0 = 9.6482e-04
Loss = 3.2074e-03, PNorm = 37.1284, GNorm = 1.9927, lr_0 = 9.6381e-04
Loss = 3.0227e-03, PNorm = 37.1602, GNorm = 1.7447, lr_0 = 9.6280e-04
Loss = 3.2579e-03, PNorm = 37.1928, GNorm = 1.0504, lr_0 = 9.6180e-04
Loss = 3.0385e-03, PNorm = 37.2117, GNorm = 1.2209, lr_0 = 9.6079e-04
Loss = 2.8226e-03, PNorm = 37.2271, GNorm = 1.2191, lr_0 = 9.5979e-04
Loss = 2.7642e-03, PNorm = 37.2514, GNorm = 0.7534, lr_0 = 9.5879e-04
Loss = 2.7710e-03, PNorm = 37.2740, GNorm = 1.5541, lr_0 = 9.5779e-04
Loss = 2.7693e-03, PNorm = 37.3138, GNorm = 0.8502, lr_0 = 9.5679e-04
Loss = 4.0677e-03, PNorm = 37.3399, GNorm = 2.7171, lr_0 = 9.5579e-04
Loss = 3.1872e-03, PNorm = 37.3773, GNorm = 0.6689, lr_0 = 9.5479e-04
Loss = 3.1203e-03, PNorm = 37.4131, GNorm = 1.9672, lr_0 = 9.5380e-04
Validation rmse logD = 0.795705
Validation R2 logD = 0.561487
Validation rmse logP = 0.602184
Validation R2 logP = 0.894755
Epoch 4
Train function
Loss = 2.8444e-03, PNorm = 37.4518, GNorm = 2.2022, lr_0 = 9.5270e-04
Loss = 3.2764e-03, PNorm = 37.4928, GNorm = 1.0282, lr_0 = 9.5171e-04
Loss = 2.8841e-03, PNorm = 37.5241, GNorm = 1.1985, lr_0 = 9.5071e-04
Loss = 2.6347e-03, PNorm = 37.5616, GNorm = 1.6826, lr_0 = 9.4972e-04
Loss = 2.9559e-03, PNorm = 37.5873, GNorm = 2.3006, lr_0 = 9.4873e-04
Loss = 2.7686e-03, PNorm = 37.6089, GNorm = 1.0089, lr_0 = 9.4774e-04
Loss = 2.8127e-03, PNorm = 37.6405, GNorm = 1.6285, lr_0 = 9.4675e-04
Loss = 3.2731e-03, PNorm = 37.6795, GNorm = 0.9364, lr_0 = 9.4576e-04
Loss = 3.5257e-03, PNorm = 37.7117, GNorm = 2.3625, lr_0 = 9.4478e-04
Loss = 2.9299e-03, PNorm = 37.7525, GNorm = 1.6166, lr_0 = 9.4379e-04
Loss = 2.6290e-03, PNorm = 37.7767, GNorm = 1.5505, lr_0 = 9.4280e-04
Loss = 2.8202e-03, PNorm = 37.8053, GNorm = 1.4062, lr_0 = 9.4182e-04
Loss = 2.8597e-03, PNorm = 37.8384, GNorm = 1.4884, lr_0 = 9.4084e-04
Loss = 2.9953e-03, PNorm = 37.8657, GNorm = 0.9742, lr_0 = 9.3986e-04
Loss = 2.4761e-03, PNorm = 37.8942, GNorm = 1.1545, lr_0 = 9.3887e-04
Loss = 2.9641e-03, PNorm = 37.9190, GNorm = 1.0813, lr_0 = 9.3789e-04
Loss = 2.3635e-03, PNorm = 37.9442, GNorm = 1.1878, lr_0 = 9.3692e-04
Loss = 2.6248e-03, PNorm = 37.9686, GNorm = 1.4772, lr_0 = 9.3594e-04
Loss = 2.1341e-03, PNorm = 38.0061, GNorm = 0.8299, lr_0 = 9.3496e-04
Loss = 2.2568e-03, PNorm = 38.0163, GNorm = 1.5768, lr_0 = 9.3399e-04
Loss = 2.5787e-03, PNorm = 38.0358, GNorm = 0.5986, lr_0 = 9.3301e-04
Loss = 2.6152e-03, PNorm = 38.0635, GNorm = 1.0467, lr_0 = 9.3204e-04
Validation rmse logD = 0.659850
Validation R2 logD = 0.698444
Validation rmse logP = 0.577940
Validation R2 logP = 0.903059
Epoch 5
Train function
Loss = 2.1061e-03, PNorm = 38.0850, GNorm = 0.9962, lr_0 = 9.3106e-04
Loss = 2.3731e-03, PNorm = 38.1230, GNorm = 1.2709, lr_0 = 9.3009e-04
Loss = 2.3565e-03, PNorm = 38.1544, GNorm = 2.6988, lr_0 = 9.2912e-04
Loss = 2.8639e-03, PNorm = 38.1803, GNorm = 2.5609, lr_0 = 9.2815e-04
Loss = 2.2914e-03, PNorm = 38.2126, GNorm = 1.2015, lr_0 = 9.2718e-04
Loss = 2.4372e-03, PNorm = 38.2434, GNorm = 1.2790, lr_0 = 9.2622e-04
Loss = 1.8076e-03, PNorm = 38.2703, GNorm = 1.6027, lr_0 = 9.2525e-04
Loss = 2.1699e-03, PNorm = 38.2869, GNorm = 0.9805, lr_0 = 9.2428e-04
Loss = 2.1372e-03, PNorm = 38.3135, GNorm = 0.5015, lr_0 = 9.2332e-04
Loss = 2.2074e-03, PNorm = 38.3521, GNorm = 1.5142, lr_0 = 9.2235e-04
Loss = 1.9720e-03, PNorm = 38.3742, GNorm = 1.0795, lr_0 = 9.2139e-04
Loss = 2.2184e-03, PNorm = 38.3998, GNorm = 1.1873, lr_0 = 9.2043e-04
Loss = 2.9700e-03, PNorm = 38.4294, GNorm = 1.6945, lr_0 = 9.1947e-04
Loss = 2.3800e-03, PNorm = 38.4534, GNorm = 0.7467, lr_0 = 9.1851e-04
Loss = 2.5224e-03, PNorm = 38.4759, GNorm = 3.4821, lr_0 = 9.1755e-04
Loss = 2.8497e-03, PNorm = 38.4992, GNorm = 2.5267, lr_0 = 9.1659e-04
Loss = 2.9794e-03, PNorm = 38.5405, GNorm = 1.3390, lr_0 = 9.1564e-04
Loss = 2.5906e-03, PNorm = 38.5789, GNorm = 2.3419, lr_0 = 9.1468e-04
Loss = 2.6848e-03, PNorm = 38.6134, GNorm = 0.7998, lr_0 = 9.1373e-04
Loss = 2.4257e-03, PNorm = 38.6441, GNorm = 1.1264, lr_0 = 9.1277e-04
Loss = 2.6886e-03, PNorm = 38.6663, GNorm = 1.0202, lr_0 = 9.1182e-04
Loss = 2.3599e-03, PNorm = 38.6921, GNorm = 1.4060, lr_0 = 9.1087e-04
Loss = 2.3782e-03, PNorm = 38.7136, GNorm = 1.3273, lr_0 = 9.0992e-04
Validation rmse logD = 0.678620
Validation R2 logD = 0.681044
Validation rmse logP = 0.577019
Validation R2 logP = 0.903368
Epoch 6
Train function
Loss = 1.8376e-03, PNorm = 38.7502, GNorm = 0.8697, lr_0 = 9.0887e-04
Loss = 1.8755e-03, PNorm = 38.7765, GNorm = 0.7208, lr_0 = 9.0792e-04
Loss = 1.8514e-03, PNorm = 38.7998, GNorm = 1.5494, lr_0 = 9.0698e-04
Loss = 1.9996e-03, PNorm = 38.8177, GNorm = 1.9552, lr_0 = 9.0603e-04
Loss = 2.2360e-03, PNorm = 38.8432, GNorm = 0.6069, lr_0 = 9.0508e-04
Loss = 2.2621e-03, PNorm = 38.8779, GNorm = 0.6049, lr_0 = 9.0414e-04
Loss = 2.0798e-03, PNorm = 38.9078, GNorm = 0.7208, lr_0 = 9.0320e-04
Loss = 2.1154e-03, PNorm = 38.9323, GNorm = 1.4459, lr_0 = 9.0225e-04
Loss = 2.0752e-03, PNorm = 38.9715, GNorm = 0.8281, lr_0 = 9.0131e-04
Loss = 2.3324e-03, PNorm = 38.9985, GNorm = 1.5803, lr_0 = 9.0037e-04
Loss = 2.3013e-03, PNorm = 39.0238, GNorm = 1.9209, lr_0 = 8.9943e-04
Loss = 2.4807e-03, PNorm = 39.0450, GNorm = 1.8031, lr_0 = 8.9849e-04
Loss = 2.4643e-03, PNorm = 39.0780, GNorm = 0.7586, lr_0 = 8.9756e-04
Loss = 2.7399e-03, PNorm = 39.1110, GNorm = 1.3976, lr_0 = 8.9662e-04
Loss = 2.6044e-03, PNorm = 39.1424, GNorm = 1.6481, lr_0 = 8.9568e-04
Loss = 1.9699e-03, PNorm = 39.1774, GNorm = 0.8516, lr_0 = 8.9475e-04
Loss = 2.5521e-03, PNorm = 39.2125, GNorm = 1.1164, lr_0 = 8.9381e-04
Loss = 1.9400e-03, PNorm = 39.2393, GNorm = 1.2243, lr_0 = 8.9288e-04
Loss = 2.0516e-03, PNorm = 39.2710, GNorm = 0.7359, lr_0 = 8.9195e-04
Loss = 2.1219e-03, PNorm = 39.3086, GNorm = 1.1422, lr_0 = 8.9102e-04
Loss = 1.9295e-03, PNorm = 39.3283, GNorm = 0.9886, lr_0 = 8.9009e-04
Loss = 1.9477e-03, PNorm = 39.3554, GNorm = 0.9890, lr_0 = 8.8916e-04
Validation rmse logD = 0.698921
Validation R2 logD = 0.661675
Validation rmse logP = 0.525584
Validation R2 logP = 0.919828
Epoch 7
Train function
Loss = 2.2735e-03, PNorm = 39.3779, GNorm = 0.9150, lr_0 = 8.8823e-04
Loss = 1.7020e-03, PNorm = 39.4150, GNorm = 0.3694, lr_0 = 8.8730e-04
Loss = 1.5061e-03, PNorm = 39.4454, GNorm = 0.8976, lr_0 = 8.8638e-04
Loss = 2.2258e-03, PNorm = 39.4662, GNorm = 1.3266, lr_0 = 8.8545e-04
Loss = 1.7641e-03, PNorm = 39.4993, GNorm = 2.1068, lr_0 = 8.8453e-04
Loss = 2.8451e-03, PNorm = 39.5193, GNorm = 2.8334, lr_0 = 8.8361e-04
Loss = 2.2272e-03, PNorm = 39.5458, GNorm = 1.4944, lr_0 = 8.8268e-04
Loss = 2.2619e-03, PNorm = 39.5703, GNorm = 1.6564, lr_0 = 8.8176e-04
Loss = 2.0357e-03, PNorm = 39.5954, GNorm = 1.6047, lr_0 = 8.8084e-04
Loss = 2.2780e-03, PNorm = 39.6287, GNorm = 2.3995, lr_0 = 8.7992e-04
Loss = 1.9641e-03, PNorm = 39.6473, GNorm = 0.7902, lr_0 = 8.7900e-04
Loss = 1.5369e-03, PNorm = 39.6713, GNorm = 0.5614, lr_0 = 8.7809e-04
Loss = 1.6105e-03, PNorm = 39.6954, GNorm = 0.9750, lr_0 = 8.7717e-04
Loss = 1.8327e-03, PNorm = 39.7239, GNorm = 0.9553, lr_0 = 8.7625e-04
Loss = 1.7042e-03, PNorm = 39.7525, GNorm = 1.2963, lr_0 = 8.7534e-04
Loss = 2.5279e-03, PNorm = 39.7849, GNorm = 2.3698, lr_0 = 8.7443e-04
Loss = 1.5679e-03, PNorm = 39.8150, GNorm = 0.7022, lr_0 = 8.7351e-04
Loss = 1.6850e-03, PNorm = 39.8356, GNorm = 1.5690, lr_0 = 8.7260e-04
Loss = 1.6578e-03, PNorm = 39.8460, GNorm = 0.5201, lr_0 = 8.7169e-04
Loss = 1.5516e-03, PNorm = 39.8775, GNorm = 1.0191, lr_0 = 8.7078e-04
Loss = 1.8526e-03, PNorm = 39.9011, GNorm = 0.7654, lr_0 = 8.6987e-04
Loss = 1.9710e-03, PNorm = 39.9225, GNorm = 0.6927, lr_0 = 8.6896e-04
Loss = 1.9250e-03, PNorm = 39.9486, GNorm = 1.4165, lr_0 = 8.6806e-04
Validation rmse logD = 0.631594
Validation R2 logD = 0.723717
Validation rmse logP = 0.520502
Validation R2 logP = 0.921370
Epoch 8
Train function
Loss = 1.3607e-03, PNorm = 39.9767, GNorm = 1.0244, lr_0 = 8.6706e-04
Loss = 1.8199e-03, PNorm = 39.9936, GNorm = 1.2696, lr_0 = 8.6616e-04
Loss = 1.5879e-03, PNorm = 40.0121, GNorm = 0.9445, lr_0 = 8.6525e-04
Loss = 1.7963e-03, PNorm = 40.0433, GNorm = 0.8537, lr_0 = 8.6435e-04
Loss = 1.5789e-03, PNorm = 40.0700, GNorm = 0.5936, lr_0 = 8.6345e-04
Loss = 1.7132e-03, PNorm = 40.0999, GNorm = 0.7773, lr_0 = 8.6255e-04
Loss = 1.6291e-03, PNorm = 40.1256, GNorm = 0.7576, lr_0 = 8.6165e-04
Loss = 1.5082e-03, PNorm = 40.1460, GNorm = 0.7044, lr_0 = 8.6075e-04
Loss = 1.4861e-03, PNorm = 40.1670, GNorm = 1.2959, lr_0 = 8.5985e-04
Loss = 1.8058e-03, PNorm = 40.1856, GNorm = 0.5159, lr_0 = 8.5895e-04
Loss = 1.5645e-03, PNorm = 40.2099, GNorm = 0.3931, lr_0 = 8.5805e-04
Loss = 1.6140e-03, PNorm = 40.2356, GNorm = 0.8373, lr_0 = 8.5716e-04
Loss = 2.0252e-03, PNorm = 40.2584, GNorm = 1.4252, lr_0 = 8.5626e-04
Loss = 1.7879e-03, PNorm = 40.2795, GNorm = 0.9161, lr_0 = 8.5537e-04
Loss = 1.3786e-03, PNorm = 40.3009, GNorm = 1.1256, lr_0 = 8.5448e-04
Loss = 1.5616e-03, PNorm = 40.3192, GNorm = 0.6405, lr_0 = 8.5359e-04
Loss = 1.8807e-03, PNorm = 40.3418, GNorm = 1.1572, lr_0 = 8.5269e-04
Loss = 1.8053e-03, PNorm = 40.3635, GNorm = 1.5080, lr_0 = 8.5180e-04
Loss = 1.5782e-03, PNorm = 40.3860, GNorm = 0.6380, lr_0 = 8.5092e-04
Loss = 1.6316e-03, PNorm = 40.4055, GNorm = 0.7270, lr_0 = 8.5003e-04
Loss = 1.6911e-03, PNorm = 40.4329, GNorm = 0.8003, lr_0 = 8.4914e-04
Loss = 1.3439e-03, PNorm = 40.4567, GNorm = 0.6996, lr_0 = 8.4825e-04
Validation rmse logD = 0.632055
Validation R2 logD = 0.723314
Validation rmse logP = 0.525214
Validation R2 logP = 0.919940
Epoch 9
Train function
Loss = 2.0197e-03, PNorm = 40.4843, GNorm = 1.0648, lr_0 = 8.4737e-04
Loss = 1.4429e-03, PNorm = 40.5013, GNorm = 0.7403, lr_0 = 8.4648e-04
Loss = 1.4794e-03, PNorm = 40.5296, GNorm = 0.9028, lr_0 = 8.4560e-04
Loss = 1.5776e-03, PNorm = 40.5439, GNorm = 0.8635, lr_0 = 8.4472e-04
Loss = 1.4536e-03, PNorm = 40.5747, GNorm = 1.1865, lr_0 = 8.4384e-04
Loss = 1.6513e-03, PNorm = 40.5967, GNorm = 0.9265, lr_0 = 8.4296e-04
Loss = 1.4469e-03, PNorm = 40.6147, GNorm = 0.7651, lr_0 = 8.4208e-04
Loss = 1.1825e-03, PNorm = 40.6334, GNorm = 0.5999, lr_0 = 8.4120e-04
Loss = 1.4772e-03, PNorm = 40.6544, GNorm = 1.1433, lr_0 = 8.4032e-04
Loss = 1.2188e-03, PNorm = 40.6761, GNorm = 0.5670, lr_0 = 8.3944e-04
Loss = 1.5152e-03, PNorm = 40.7000, GNorm = 0.5993, lr_0 = 8.3857e-04
Loss = 2.0034e-03, PNorm = 40.7354, GNorm = 1.1451, lr_0 = 8.3769e-04
Loss = 1.3313e-03, PNorm = 40.7571, GNorm = 1.5171, lr_0 = 8.3682e-04
Loss = 1.6374e-03, PNorm = 40.7826, GNorm = 1.7328, lr_0 = 8.3594e-04
Loss = 1.4932e-03, PNorm = 40.8056, GNorm = 0.6728, lr_0 = 8.3507e-04
Loss = 1.4046e-03, PNorm = 40.8374, GNorm = 0.9135, lr_0 = 8.3420e-04
Loss = 1.1375e-03, PNorm = 40.8625, GNorm = 1.1769, lr_0 = 8.3333e-04
Loss = 1.4404e-03, PNorm = 40.8856, GNorm = 1.2027, lr_0 = 8.3246e-04
Loss = 1.4852e-03, PNorm = 40.9086, GNorm = 1.5447, lr_0 = 8.3159e-04
Loss = 1.9091e-03, PNorm = 40.9327, GNorm = 0.9160, lr_0 = 8.3072e-04
Loss = 1.5684e-03, PNorm = 40.9564, GNorm = 0.7571, lr_0 = 8.2986e-04
Loss = 1.4998e-03, PNorm = 40.9786, GNorm = 0.5227, lr_0 = 8.2899e-04
Loss = 1.4525e-03, PNorm = 41.0009, GNorm = 0.6606, lr_0 = 8.2812e-04
Validation rmse logD = 0.641476
Validation R2 logD = 0.715004
Validation rmse logP = 0.511177
Validation R2 logP = 0.924163
Epoch 10
Train function
Loss = 1.3136e-03, PNorm = 41.0264, GNorm = 0.6726, lr_0 = 8.2717e-04
Loss = 1.2103e-03, PNorm = 41.0525, GNorm = 0.6277, lr_0 = 8.2631e-04
Loss = 1.3463e-03, PNorm = 41.0722, GNorm = 0.9749, lr_0 = 8.2545e-04
Loss = 1.2789e-03, PNorm = 41.0901, GNorm = 0.7803, lr_0 = 8.2459e-04
Loss = 1.3108e-03, PNorm = 41.1023, GNorm = 0.7036, lr_0 = 8.2373e-04
Loss = 1.3523e-03, PNorm = 41.1330, GNorm = 0.9037, lr_0 = 8.2287e-04
Loss = 1.6207e-03, PNorm = 41.1607, GNorm = 1.1244, lr_0 = 8.2201e-04
Loss = 1.1696e-03, PNorm = 41.1797, GNorm = 0.4837, lr_0 = 8.2115e-04
Loss = 1.4379e-03, PNorm = 41.2027, GNorm = 1.2578, lr_0 = 8.2029e-04
Loss = 1.4350e-03, PNorm = 41.2269, GNorm = 1.1362, lr_0 = 8.1944e-04
Loss = 1.4236e-03, PNorm = 41.2609, GNorm = 0.8477, lr_0 = 8.1858e-04
Loss = 1.3159e-03, PNorm = 41.2871, GNorm = 0.5671, lr_0 = 8.1773e-04
Loss = 1.4243e-03, PNorm = 41.2988, GNorm = 1.2022, lr_0 = 8.1687e-04
Loss = 1.1953e-03, PNorm = 41.3117, GNorm = 0.9926, lr_0 = 8.1602e-04
Loss = 1.1182e-03, PNorm = 41.3344, GNorm = 0.8206, lr_0 = 8.1517e-04
Loss = 1.2808e-03, PNorm = 41.3591, GNorm = 0.9008, lr_0 = 8.1432e-04
Loss = 1.2305e-03, PNorm = 41.3786, GNorm = 0.7439, lr_0 = 8.1347e-04
Loss = 1.1273e-03, PNorm = 41.3904, GNorm = 1.3668, lr_0 = 8.1262e-04
Loss = 1.5679e-03, PNorm = 41.4092, GNorm = 0.8042, lr_0 = 8.1177e-04
Loss = 1.2045e-03, PNorm = 41.4340, GNorm = 0.6878, lr_0 = 8.1092e-04
Loss = 1.2864e-03, PNorm = 41.4489, GNorm = 0.5699, lr_0 = 8.1008e-04
Loss = 1.5035e-03, PNorm = 41.4643, GNorm = 1.0111, lr_0 = 8.0923e-04
Loss = 1.5403e-03, PNorm = 41.4926, GNorm = 1.5457, lr_0 = 8.0839e-04
Validation rmse logD = 0.618104
Validation R2 logD = 0.735394
Validation rmse logP = 0.517576
Validation R2 logP = 0.922252
Epoch 11
Train function
Loss = 1.2116e-03, PNorm = 41.5158, GNorm = 0.7740, lr_0 = 8.0754e-04
Loss = 1.2257e-03, PNorm = 41.5423, GNorm = 1.4248, lr_0 = 8.0670e-04
Loss = 1.2891e-03, PNorm = 41.5659, GNorm = 1.8442, lr_0 = 8.0586e-04
Loss = 1.0569e-03, PNorm = 41.5888, GNorm = 1.2890, lr_0 = 8.0502e-04
Loss = 1.0988e-03, PNorm = 41.6082, GNorm = 0.7013, lr_0 = 8.0418e-04
Loss = 1.3631e-03, PNorm = 41.6231, GNorm = 1.3520, lr_0 = 8.0334e-04
Loss = 1.8231e-03, PNorm = 41.6477, GNorm = 1.7311, lr_0 = 8.0250e-04
Loss = 1.6238e-03, PNorm = 41.6743, GNorm = 1.0956, lr_0 = 8.0166e-04
Loss = 2.0950e-03, PNorm = 41.7081, GNorm = 0.9820, lr_0 = 8.0082e-04
Loss = 1.8079e-03, PNorm = 41.7385, GNorm = 0.8788, lr_0 = 7.9999e-04
Loss = 1.2604e-03, PNorm = 41.7727, GNorm = 1.0030, lr_0 = 7.9915e-04
Loss = 1.2186e-03, PNorm = 41.8032, GNorm = 1.2865, lr_0 = 7.9832e-04
Loss = 9.8920e-04, PNorm = 41.8350, GNorm = 0.7854, lr_0 = 7.9749e-04
Loss = 1.1024e-03, PNorm = 41.8479, GNorm = 0.5345, lr_0 = 7.9665e-04
Loss = 9.5582e-04, PNorm = 41.8659, GNorm = 0.5614, lr_0 = 7.9582e-04
Loss = 1.1955e-03, PNorm = 41.8874, GNorm = 0.5987, lr_0 = 7.9499e-04
Loss = 1.2357e-03, PNorm = 41.9044, GNorm = 0.5679, lr_0 = 7.9416e-04
Loss = 1.5220e-03, PNorm = 41.9150, GNorm = 1.4273, lr_0 = 7.9333e-04
Loss = 1.4096e-03, PNorm = 41.9481, GNorm = 1.1491, lr_0 = 7.9251e-04
Loss = 1.5283e-03, PNorm = 41.9783, GNorm = 0.5638, lr_0 = 7.9168e-04
Loss = 1.2016e-03, PNorm = 42.0069, GNorm = 0.6753, lr_0 = 7.9085e-04
Loss = 1.3467e-03, PNorm = 42.0309, GNorm = 1.1718, lr_0 = 7.9003e-04
Validation rmse logD = 0.609583
Validation R2 logD = 0.742638
Validation rmse logP = 0.491514
Validation R2 logP = 0.929885
Epoch 12
Train function
Loss = 9.7218e-04, PNorm = 42.0502, GNorm = 1.3520, lr_0 = 7.8912e-04
Loss = 1.1055e-03, PNorm = 42.0688, GNorm = 0.8345, lr_0 = 7.8830e-04
Loss = 1.0089e-03, PNorm = 42.0879, GNorm = 0.5691, lr_0 = 7.8747e-04
Loss = 1.0781e-03, PNorm = 42.0924, GNorm = 0.6923, lr_0 = 7.8665e-04
Loss = 1.1200e-03, PNorm = 42.1051, GNorm = 0.6647, lr_0 = 7.8583e-04
Loss = 9.9713e-04, PNorm = 42.1284, GNorm = 0.6051, lr_0 = 7.8501e-04
Loss = 1.2298e-03, PNorm = 42.1459, GNorm = 0.4690, lr_0 = 7.8419e-04
Loss = 1.1260e-03, PNorm = 42.1678, GNorm = 0.6689, lr_0 = 7.8337e-04
Loss = 1.1568e-03, PNorm = 42.1911, GNorm = 1.3508, lr_0 = 7.8255e-04
Loss = 1.2280e-03, PNorm = 42.2240, GNorm = 1.4436, lr_0 = 7.8174e-04
Loss = 1.1007e-03, PNorm = 42.2586, GNorm = 0.5896, lr_0 = 7.8092e-04
Loss = 1.5103e-03, PNorm = 42.2846, GNorm = 1.3365, lr_0 = 7.8011e-04
Loss = 1.2617e-03, PNorm = 42.3047, GNorm = 0.8383, lr_0 = 7.7929e-04
Loss = 1.3126e-03, PNorm = 42.3263, GNorm = 1.3163, lr_0 = 7.7848e-04
Loss = 1.0799e-03, PNorm = 42.3495, GNorm = 0.5698, lr_0 = 7.7767e-04
Loss = 1.1919e-03, PNorm = 42.3656, GNorm = 0.9127, lr_0 = 7.7686e-04
Loss = 1.0748e-03, PNorm = 42.3828, GNorm = 0.5955, lr_0 = 7.7604e-04
Loss = 1.2982e-03, PNorm = 42.3954, GNorm = 0.8595, lr_0 = 7.7523e-04
Loss = 1.0474e-03, PNorm = 42.4107, GNorm = 0.4302, lr_0 = 7.7443e-04
Loss = 1.1441e-03, PNorm = 42.4313, GNorm = 0.8224, lr_0 = 7.7362e-04
Loss = 1.1426e-03, PNorm = 42.4460, GNorm = 0.3817, lr_0 = 7.7281e-04
Loss = 1.4973e-03, PNorm = 42.4688, GNorm = 0.7896, lr_0 = 7.7200e-04
Loss = 9.1368e-04, PNorm = 42.4938, GNorm = 0.6933, lr_0 = 7.7120e-04
Validation rmse logD = 0.605683
Validation R2 logD = 0.745921
Validation rmse logP = 0.515661
Validation R2 logP = 0.922826
Epoch 13
Train function
Loss = 8.2472e-04, PNorm = 42.5124, GNorm = 0.7253, lr_0 = 7.7039e-04
Loss = 1.0659e-03, PNorm = 42.5277, GNorm = 1.0285, lr_0 = 7.6959e-04
Loss = 1.1005e-03, PNorm = 42.5494, GNorm = 1.3473, lr_0 = 7.6879e-04
Loss = 1.1341e-03, PNorm = 42.5669, GNorm = 1.0962, lr_0 = 7.6798e-04
Loss = 1.0069e-03, PNorm = 42.5869, GNorm = 0.6040, lr_0 = 7.6718e-04
Loss = 1.1867e-03, PNorm = 42.6064, GNorm = 1.0306, lr_0 = 7.6638e-04
Loss = 1.0369e-03, PNorm = 42.6245, GNorm = 0.6751, lr_0 = 7.6558e-04
Loss = 1.1203e-03, PNorm = 42.6439, GNorm = 0.9337, lr_0 = 7.6478e-04
Loss = 1.0537e-03, PNorm = 42.6684, GNorm = 0.8007, lr_0 = 7.6398e-04
Loss = 9.2158e-04, PNorm = 42.6795, GNorm = 0.4293, lr_0 = 7.6319e-04
Loss = 1.1108e-03, PNorm = 42.6968, GNorm = 1.0956, lr_0 = 7.6239e-04
Loss = 1.0285e-03, PNorm = 42.7222, GNorm = 1.1163, lr_0 = 7.6159e-04
Loss = 1.0969e-03, PNorm = 42.7332, GNorm = 1.2600, lr_0 = 7.6080e-04
Loss = 1.0197e-03, PNorm = 42.7499, GNorm = 0.6739, lr_0 = 7.6000e-04
Loss = 1.0992e-03, PNorm = 42.7671, GNorm = 1.0258, lr_0 = 7.5921e-04
Loss = 1.0429e-03, PNorm = 42.7873, GNorm = 1.0515, lr_0 = 7.5842e-04
Loss = 9.8904e-04, PNorm = 42.8084, GNorm = 0.7473, lr_0 = 7.5763e-04
Loss = 1.0284e-03, PNorm = 42.8315, GNorm = 0.8613, lr_0 = 7.5684e-04
Loss = 1.0405e-03, PNorm = 42.8467, GNorm = 1.3781, lr_0 = 7.5605e-04
Loss = 8.9828e-04, PNorm = 42.8613, GNorm = 0.9116, lr_0 = 7.5526e-04
Loss = 9.8322e-04, PNorm = 42.8771, GNorm = 0.9382, lr_0 = 7.5447e-04
Loss = 1.1035e-03, PNorm = 42.8970, GNorm = 1.0306, lr_0 = 7.5368e-04
Validation rmse logD = 0.611828
Validation R2 logD = 0.740740
Validation rmse logP = 0.500135
Validation R2 logP = 0.927404
Epoch 14
Train function
Loss = 1.0491e-03, PNorm = 42.9256, GNorm = 0.8793, lr_0 = 7.5282e-04
Loss = 9.5090e-04, PNorm = 42.9419, GNorm = 0.7377, lr_0 = 7.5203e-04
Loss = 8.3922e-04, PNorm = 42.9587, GNorm = 0.5042, lr_0 = 7.5125e-04
Loss = 6.7719e-04, PNorm = 42.9676, GNorm = 0.4015, lr_0 = 7.5046e-04
Loss = 8.2122e-04, PNorm = 42.9779, GNorm = 0.6084, lr_0 = 7.4968e-04
Loss = 9.6228e-04, PNorm = 42.9936, GNorm = 1.5422, lr_0 = 7.4890e-04
Loss = 9.8027e-04, PNorm = 43.0154, GNorm = 0.9514, lr_0 = 7.4811e-04
Loss = 1.0135e-03, PNorm = 43.0368, GNorm = 1.1891, lr_0 = 7.4733e-04
Loss = 9.1046e-04, PNorm = 43.0522, GNorm = 0.4917, lr_0 = 7.4655e-04
Loss = 9.0532e-04, PNorm = 43.0662, GNorm = 0.8174, lr_0 = 7.4577e-04
Loss = 1.0283e-03, PNorm = 43.0762, GNorm = 1.1224, lr_0 = 7.4500e-04
Loss = 8.1048e-04, PNorm = 43.0970, GNorm = 0.9021, lr_0 = 7.4422e-04
Loss = 1.0773e-03, PNorm = 43.1131, GNorm = 0.7001, lr_0 = 7.4344e-04
Loss = 1.0764e-03, PNorm = 43.1282, GNorm = 0.6502, lr_0 = 7.4267e-04
Loss = 9.2570e-04, PNorm = 43.1487, GNorm = 0.5286, lr_0 = 7.4189e-04
Loss = 8.8174e-04, PNorm = 43.1654, GNorm = 0.4157, lr_0 = 7.4112e-04
Loss = 8.9840e-04, PNorm = 43.1819, GNorm = 0.6569, lr_0 = 7.4034e-04
Loss = 9.1857e-04, PNorm = 43.1962, GNorm = 0.7107, lr_0 = 7.3957e-04
Loss = 8.6302e-04, PNorm = 43.2149, GNorm = 0.5561, lr_0 = 7.3880e-04
Loss = 8.6907e-04, PNorm = 43.2356, GNorm = 0.7631, lr_0 = 7.3803e-04
Loss = 9.5580e-04, PNorm = 43.2513, GNorm = 0.9585, lr_0 = 7.3726e-04
Loss = 9.5719e-04, PNorm = 43.2678, GNorm = 0.7308, lr_0 = 7.3649e-04
Loss = 9.5947e-04, PNorm = 43.2868, GNorm = 0.5689, lr_0 = 7.3572e-04
Validation rmse logD = 0.587746
Validation R2 logD = 0.760747
Validation rmse logP = 0.487394
Validation R2 logP = 0.931055
Epoch 15
Train function
Loss = 7.4640e-04, PNorm = 43.3101, GNorm = 0.5238, lr_0 = 7.3495e-04
Loss = 6.6279e-04, PNorm = 43.3310, GNorm = 0.5670, lr_0 = 7.3418e-04
Loss = 1.0249e-03, PNorm = 43.3531, GNorm = 1.2682, lr_0 = 7.3342e-04
Loss = 1.0256e-03, PNorm = 43.3742, GNorm = 0.5423, lr_0 = 7.3265e-04
Loss = 1.0073e-03, PNorm = 43.3885, GNorm = 0.8067, lr_0 = 7.3189e-04
Loss = 8.6343e-04, PNorm = 43.4149, GNorm = 1.0338, lr_0 = 7.3112e-04
Loss = 9.8893e-04, PNorm = 43.4308, GNorm = 1.0667, lr_0 = 7.3036e-04
Loss = 7.6100e-04, PNorm = 43.4513, GNorm = 0.4189, lr_0 = 7.2960e-04
Loss = 9.0967e-04, PNorm = 43.4715, GNorm = 0.5374, lr_0 = 7.2884e-04
Loss = 8.7836e-04, PNorm = 43.4896, GNorm = 1.1577, lr_0 = 7.2808e-04
Loss = 6.7082e-04, PNorm = 43.4998, GNorm = 0.7050, lr_0 = 7.2732e-04
Loss = 9.5313e-04, PNorm = 43.5131, GNorm = 1.4602, lr_0 = 7.2656e-04
Loss = 9.2194e-04, PNorm = 43.5295, GNorm = 0.9153, lr_0 = 7.2580e-04
Loss = 1.0394e-03, PNorm = 43.5398, GNorm = 1.1045, lr_0 = 7.2504e-04
Loss = 7.9588e-04, PNorm = 43.5567, GNorm = 0.5183, lr_0 = 7.2428e-04
Loss = 9.8093e-04, PNorm = 43.5824, GNorm = 0.6988, lr_0 = 7.2353e-04
Loss = 1.0193e-03, PNorm = 43.6051, GNorm = 1.3571, lr_0 = 7.2277e-04
Loss = 9.8525e-04, PNorm = 43.6176, GNorm = 0.5140, lr_0 = 7.2202e-04
Loss = 7.8144e-04, PNorm = 43.6314, GNorm = 0.4952, lr_0 = 7.2127e-04
Loss = 6.1638e-04, PNorm = 43.6430, GNorm = 0.3231, lr_0 = 7.2051e-04
Loss = 8.4491e-04, PNorm = 43.6573, GNorm = 0.7362, lr_0 = 7.1976e-04
Loss = 8.4764e-04, PNorm = 43.6728, GNorm = 0.6633, lr_0 = 7.1901e-04
Validation rmse logD = 0.608577
Validation R2 logD = 0.743487
Validation rmse logP = 0.479751
Validation R2 logP = 0.933201
Epoch 16
Train function
Loss = 6.5687e-04, PNorm = 43.6915, GNorm = 0.5855, lr_0 = 7.1818e-04
Loss = 8.7937e-04, PNorm = 43.7113, GNorm = 0.7987, lr_0 = 7.1743e-04
Loss = 7.4013e-04, PNorm = 43.7340, GNorm = 0.6068, lr_0 = 7.1669e-04
Loss = 7.9354e-04, PNorm = 43.7544, GNorm = 0.6129, lr_0 = 7.1594e-04
Loss = 7.7284e-04, PNorm = 43.7707, GNorm = 0.6256, lr_0 = 7.1519e-04
Loss = 8.2073e-04, PNorm = 43.7904, GNorm = 0.4945, lr_0 = 7.1444e-04
Loss = 7.4041e-04, PNorm = 43.8049, GNorm = 0.4190, lr_0 = 7.1370e-04
Loss = 7.6567e-04, PNorm = 43.8192, GNorm = 0.4148, lr_0 = 7.1295e-04
Loss = 7.7700e-04, PNorm = 43.8313, GNorm = 1.0272, lr_0 = 7.1221e-04
Loss = 1.0170e-03, PNorm = 43.8453, GNorm = 1.0249, lr_0 = 7.1147e-04
Loss = 6.8823e-04, PNorm = 43.8621, GNorm = 0.6093, lr_0 = 7.1072e-04
Loss = 8.1465e-04, PNorm = 43.8760, GNorm = 0.4458, lr_0 = 7.0998e-04
Loss = 8.0673e-04, PNorm = 43.8948, GNorm = 0.4837, lr_0 = 7.0924e-04
Loss = 6.0165e-04, PNorm = 43.9093, GNorm = 0.7897, lr_0 = 7.0850e-04
Loss = 8.8848e-04, PNorm = 43.9325, GNorm = 0.6411, lr_0 = 7.0776e-04
Loss = 7.9181e-04, PNorm = 43.9587, GNorm = 0.3205, lr_0 = 7.0702e-04
Loss = 7.8782e-04, PNorm = 43.9733, GNorm = 0.9473, lr_0 = 7.0628e-04
Loss = 6.9466e-04, PNorm = 43.9914, GNorm = 0.3790, lr_0 = 7.0555e-04
Loss = 8.2929e-04, PNorm = 44.0096, GNorm = 0.7067, lr_0 = 7.0481e-04
Loss = 8.6263e-04, PNorm = 44.0283, GNorm = 1.6678, lr_0 = 7.0408e-04
Loss = 1.0650e-03, PNorm = 44.0389, GNorm = 1.4454, lr_0 = 7.0334e-04
Loss = 8.1146e-04, PNorm = 44.0564, GNorm = 0.9904, lr_0 = 7.0261e-04
Loss = 8.0861e-04, PNorm = 44.0722, GNorm = 0.7712, lr_0 = 7.0187e-04
Validation rmse logD = 0.607253
Validation R2 logD = 0.744602
Validation rmse logP = 0.479664
Validation R2 logP = 0.933225
Epoch 17
Train function
Loss = 6.4624e-04, PNorm = 44.0894, GNorm = 0.4167, lr_0 = 7.0114e-04
Loss = 6.5786e-04, PNorm = 44.1052, GNorm = 0.5894, lr_0 = 7.0041e-04
Loss = 6.1996e-04, PNorm = 44.1219, GNorm = 0.4721, lr_0 = 6.9968e-04
Loss = 6.0753e-04, PNorm = 44.1320, GNorm = 0.4380, lr_0 = 6.9895e-04
Loss = 6.4218e-04, PNorm = 44.1426, GNorm = 0.5368, lr_0 = 6.9822e-04
Loss = 6.3367e-04, PNorm = 44.1539, GNorm = 0.3774, lr_0 = 6.9749e-04
Loss = 6.7488e-04, PNorm = 44.1658, GNorm = 0.6569, lr_0 = 6.9676e-04
Loss = 7.0951e-04, PNorm = 44.1806, GNorm = 0.6441, lr_0 = 6.9603e-04
Loss = 8.5691e-04, PNorm = 44.1931, GNorm = 0.4028, lr_0 = 6.9531e-04
Loss = 7.6968e-04, PNorm = 44.2111, GNorm = 0.6574, lr_0 = 6.9458e-04
Loss = 7.5327e-04, PNorm = 44.2278, GNorm = 0.5687, lr_0 = 6.9386e-04
Loss = 6.9059e-04, PNorm = 44.2444, GNorm = 0.8352, lr_0 = 6.9313e-04
Loss = 6.9987e-04, PNorm = 44.2664, GNorm = 0.8024, lr_0 = 6.9241e-04
Loss = 8.3375e-04, PNorm = 44.2729, GNorm = 0.7893, lr_0 = 6.9169e-04
Loss = 7.8218e-04, PNorm = 44.2934, GNorm = 0.8069, lr_0 = 6.9096e-04
Loss = 7.8619e-04, PNorm = 44.3137, GNorm = 0.9435, lr_0 = 6.9024e-04
Loss = 9.0720e-04, PNorm = 44.3313, GNorm = 0.9903, lr_0 = 6.8952e-04
Loss = 6.8784e-04, PNorm = 44.3499, GNorm = 0.4446, lr_0 = 6.8880e-04
Loss = 7.6879e-04, PNorm = 44.3720, GNorm = 0.4330, lr_0 = 6.8808e-04
Loss = 7.1877e-04, PNorm = 44.3859, GNorm = 0.7156, lr_0 = 6.8737e-04
Loss = 7.1866e-04, PNorm = 44.3993, GNorm = 0.3695, lr_0 = 6.8665e-04
Loss = 8.1299e-04, PNorm = 44.4099, GNorm = 1.0312, lr_0 = 6.8593e-04
Validation rmse logD = 0.617381
Validation R2 logD = 0.736012
Validation rmse logP = 0.498831
Validation R2 logP = 0.927782
Epoch 18
Train function
Loss = 7.5274e-04, PNorm = 44.4289, GNorm = 1.1142, lr_0 = 6.8514e-04
Loss = 6.7539e-04, PNorm = 44.4488, GNorm = 0.5230, lr_0 = 6.8443e-04
Loss = 7.6949e-04, PNorm = 44.4647, GNorm = 1.0347, lr_0 = 6.8372e-04
Loss = 5.7904e-04, PNorm = 44.4717, GNorm = 0.8394, lr_0 = 6.8300e-04
Loss = 6.3402e-04, PNorm = 44.4913, GNorm = 0.4241, lr_0 = 6.8229e-04
Loss = 7.1960e-04, PNorm = 44.5049, GNorm = 1.1647, lr_0 = 6.8158e-04
Loss = 6.9201e-04, PNorm = 44.5205, GNorm = 0.6760, lr_0 = 6.8087e-04
Loss = 6.2452e-04, PNorm = 44.5358, GNorm = 0.7004, lr_0 = 6.8015e-04
Loss = 5.8012e-04, PNorm = 44.5423, GNorm = 0.5084, lr_0 = 6.7944e-04
Loss = 6.0812e-04, PNorm = 44.5584, GNorm = 0.3855, lr_0 = 6.7874e-04
Loss = 8.0427e-04, PNorm = 44.5721, GNorm = 0.7354, lr_0 = 6.7803e-04
Loss = 5.9476e-04, PNorm = 44.5875, GNorm = 0.4157, lr_0 = 6.7732e-04
Loss = 6.7159e-04, PNorm = 44.6025, GNorm = 0.5538, lr_0 = 6.7661e-04
Loss = 6.2941e-04, PNorm = 44.6188, GNorm = 1.3361, lr_0 = 6.7591e-04
Loss = 5.4700e-04, PNorm = 44.6308, GNorm = 0.7576, lr_0 = 6.7520e-04
Loss = 6.8991e-04, PNorm = 44.6447, GNorm = 1.0815, lr_0 = 6.7450e-04
Loss = 7.8131e-04, PNorm = 44.6648, GNorm = 0.6306, lr_0 = 6.7379e-04
Loss = 8.4204e-04, PNorm = 44.6870, GNorm = 0.5073, lr_0 = 6.7309e-04
Loss = 8.5798e-04, PNorm = 44.7037, GNorm = 0.8229, lr_0 = 6.7239e-04
Loss = 6.6513e-04, PNorm = 44.7175, GNorm = 0.4481, lr_0 = 6.7168e-04
Loss = 9.4441e-04, PNorm = 44.7307, GNorm = 0.7165, lr_0 = 6.7098e-04
Loss = 7.9447e-04, PNorm = 44.7524, GNorm = 0.9154, lr_0 = 6.7028e-04
Loss = 7.4658e-04, PNorm = 44.7685, GNorm = 0.5152, lr_0 = 6.6958e-04
Validation rmse logD = 0.581809
Validation R2 logD = 0.765556
Validation rmse logP = 0.485748
Validation R2 logP = 0.931520
Epoch 19
Train function
Loss = 5.8335e-04, PNorm = 44.7877, GNorm = 0.3958, lr_0 = 6.6889e-04
Loss = 6.1851e-04, PNorm = 44.8009, GNorm = 1.3062, lr_0 = 6.6819e-04
Loss = 7.0115e-04, PNorm = 44.8150, GNorm = 0.6872, lr_0 = 6.6749e-04
Loss = 5.7359e-04, PNorm = 44.8338, GNorm = 0.3524, lr_0 = 6.6679e-04
Loss = 5.6633e-04, PNorm = 44.8486, GNorm = 0.4712, lr_0 = 6.6610e-04
Loss = 5.9522e-04, PNorm = 44.8563, GNorm = 0.5634, lr_0 = 6.6540e-04
Loss = 7.5596e-04, PNorm = 44.8726, GNorm = 1.2979, lr_0 = 6.6471e-04
Loss = 5.2980e-04, PNorm = 44.8914, GNorm = 0.3218, lr_0 = 6.6401e-04
Loss = 7.2007e-04, PNorm = 44.9071, GNorm = 0.6162, lr_0 = 6.6332e-04
Loss = 5.5753e-04, PNorm = 44.9231, GNorm = 0.3669, lr_0 = 6.6263e-04
Loss = 5.5780e-04, PNorm = 44.9355, GNorm = 0.8475, lr_0 = 6.6194e-04
Loss = 6.4389e-04, PNorm = 44.9459, GNorm = 0.5799, lr_0 = 6.6125e-04
Loss = 5.4962e-04, PNorm = 44.9563, GNorm = 0.4559, lr_0 = 6.6056e-04
Loss = 5.2280e-04, PNorm = 44.9694, GNorm = 0.7376, lr_0 = 6.5987e-04
Loss = 8.2267e-04, PNorm = 44.9809, GNorm = 1.0112, lr_0 = 6.5918e-04
Loss = 7.9658e-04, PNorm = 44.9947, GNorm = 0.6056, lr_0 = 6.5849e-04
Loss = 7.3518e-04, PNorm = 45.0160, GNorm = 0.6144, lr_0 = 6.5780e-04
Loss = 6.2256e-04, PNorm = 45.0360, GNorm = 0.3870, lr_0 = 6.5712e-04
Loss = 6.3070e-04, PNorm = 45.0547, GNorm = 0.5266, lr_0 = 6.5643e-04
Loss = 5.9264e-04, PNorm = 45.0692, GNorm = 0.4296, lr_0 = 6.5574e-04
Loss = 6.9274e-04, PNorm = 45.0771, GNorm = 0.5251, lr_0 = 6.5506e-04
Loss = 6.8096e-04, PNorm = 45.0900, GNorm = 0.6429, lr_0 = 6.5438e-04
Validation rmse logD = 0.585374
Validation R2 logD = 0.762674
Validation rmse logP = 0.478441
Validation R2 logP = 0.933565
Epoch 20
Train function
Loss = 6.6194e-04, PNorm = 45.1015, GNorm = 0.7227, lr_0 = 6.5363e-04
Loss = 6.7327e-04, PNorm = 45.1171, GNorm = 0.6728, lr_0 = 6.5294e-04
Loss = 6.4515e-04, PNorm = 45.1308, GNorm = 0.6240, lr_0 = 6.5226e-04
Loss = 6.3544e-04, PNorm = 45.1500, GNorm = 0.3325, lr_0 = 6.5158e-04
Loss = 5.0147e-04, PNorm = 45.1633, GNorm = 0.5248, lr_0 = 6.5090e-04
Loss = 5.5426e-04, PNorm = 45.1790, GNorm = 0.5037, lr_0 = 6.5022e-04
Loss = 5.6588e-04, PNorm = 45.1938, GNorm = 0.7628, lr_0 = 6.4954e-04
Loss = 6.0119e-04, PNorm = 45.2098, GNorm = 0.6039, lr_0 = 6.4886e-04
Loss = 5.9426e-04, PNorm = 45.2217, GNorm = 0.5659, lr_0 = 6.4819e-04
Loss = 5.7975e-04, PNorm = 45.2348, GNorm = 0.3134, lr_0 = 6.4751e-04
Loss = 6.8013e-04, PNorm = 45.2475, GNorm = 0.6437, lr_0 = 6.4684e-04
Loss = 6.9361e-04, PNorm = 45.2585, GNorm = 0.7789, lr_0 = 6.4616e-04
Loss = 6.8537e-04, PNorm = 45.2719, GNorm = 0.7531, lr_0 = 6.4549e-04
Loss = 6.6051e-04, PNorm = 45.2862, GNorm = 0.3762, lr_0 = 6.4481e-04
Loss = 7.2464e-04, PNorm = 45.3005, GNorm = 0.5686, lr_0 = 6.4414e-04
Loss = 5.7611e-04, PNorm = 45.3136, GNorm = 0.4582, lr_0 = 6.4347e-04
Loss = 6.0900e-04, PNorm = 45.3299, GNorm = 0.3993, lr_0 = 6.4280e-04
Loss = 7.1712e-04, PNorm = 45.3498, GNorm = 0.4502, lr_0 = 6.4212e-04
Loss = 5.8449e-04, PNorm = 45.3722, GNorm = 0.4097, lr_0 = 6.4145e-04
Loss = 6.0274e-04, PNorm = 45.3896, GNorm = 0.6104, lr_0 = 6.4078e-04
Loss = 4.8248e-04, PNorm = 45.3982, GNorm = 0.2963, lr_0 = 6.4012e-04
Loss = 5.9596e-04, PNorm = 45.4090, GNorm = 1.1253, lr_0 = 6.3945e-04
Loss = 8.4717e-04, PNorm = 45.4274, GNorm = 0.4900, lr_0 = 6.3878e-04
Validation rmse logD = 0.637329
Validation R2 logD = 0.718677
Validation rmse logP = 0.477791
Validation R2 logP = 0.933745
Epoch 21
Train function
Loss = 6.4168e-04, PNorm = 45.4419, GNorm = 0.9883, lr_0 = 6.3811e-04
Loss = 7.1567e-04, PNorm = 45.4646, GNorm = 0.4755, lr_0 = 6.3745e-04
Loss = 5.2640e-04, PNorm = 45.4825, GNorm = 0.4488, lr_0 = 6.3678e-04
Loss = 5.6541e-04, PNorm = 45.4936, GNorm = 0.5917, lr_0 = 6.3612e-04
Loss = 4.6955e-04, PNorm = 45.5078, GNorm = 0.6719, lr_0 = 6.3545e-04
Loss = 4.7448e-04, PNorm = 45.5236, GNorm = 0.5321, lr_0 = 6.3479e-04
Loss = 5.2713e-04, PNorm = 45.5290, GNorm = 0.7279, lr_0 = 6.3413e-04
Loss = 4.9380e-04, PNorm = 45.5391, GNorm = 0.6301, lr_0 = 6.3347e-04
Loss = 4.8722e-04, PNorm = 45.5497, GNorm = 0.4147, lr_0 = 6.3280e-04
Loss = 5.4666e-04, PNorm = 45.5607, GNorm = 0.9367, lr_0 = 6.3214e-04
Loss = 5.4677e-04, PNorm = 45.5758, GNorm = 0.9055, lr_0 = 6.3148e-04
Loss = 6.3227e-04, PNorm = 45.5878, GNorm = 0.8230, lr_0 = 6.3083e-04
Loss = 5.7761e-04, PNorm = 45.6022, GNorm = 0.5733, lr_0 = 6.3017e-04
Loss = 6.0076e-04, PNorm = 45.6173, GNorm = 0.5479, lr_0 = 6.2951e-04
Loss = 5.4856e-04, PNorm = 45.6296, GNorm = 0.6798, lr_0 = 6.2885e-04
Loss = 6.1575e-04, PNorm = 45.6448, GNorm = 0.8843, lr_0 = 6.2820e-04
Loss = 6.1010e-04, PNorm = 45.6627, GNorm = 0.7639, lr_0 = 6.2754e-04
Loss = 6.1624e-04, PNorm = 45.6756, GNorm = 0.7904, lr_0 = 6.2689e-04
Loss = 6.6391e-04, PNorm = 45.6856, GNorm = 0.4273, lr_0 = 6.2623e-04
Loss = 7.9593e-04, PNorm = 45.6994, GNorm = 1.4799, lr_0 = 6.2558e-04
Loss = 6.9828e-04, PNorm = 45.7125, GNorm = 0.5725, lr_0 = 6.2492e-04
Loss = 6.4697e-04, PNorm = 45.7316, GNorm = 0.3862, lr_0 = 6.2427e-04
Loss = 6.7392e-04, PNorm = 45.7493, GNorm = 0.7346, lr_0 = 6.2362e-04
Loss = 6.6760e-04, PNorm = 45.7504, GNorm = 0.5341, lr_0 = 6.2356e-04
Validation rmse logD = 0.589563
Validation R2 logD = 0.759265
Validation rmse logP = 0.481887
Validation R2 logP = 0.932604
Epoch 22
Train function
Loss = 5.9229e-04, PNorm = 45.7659, GNorm = 0.5189, lr_0 = 6.2290e-04
Loss = 4.2418e-04, PNorm = 45.7775, GNorm = 0.3329, lr_0 = 6.2225e-04
Loss = 4.7569e-04, PNorm = 45.7860, GNorm = 0.4768, lr_0 = 6.2161e-04
Loss = 5.4433e-04, PNorm = 45.8055, GNorm = 0.2904, lr_0 = 6.2096e-04
Loss = 4.9225e-04, PNorm = 45.8206, GNorm = 0.9882, lr_0 = 6.2031e-04
Loss = 6.0099e-04, PNorm = 45.8277, GNorm = 0.4504, lr_0 = 6.1966e-04
Loss = 3.9825e-04, PNorm = 45.8360, GNorm = 0.3919, lr_0 = 6.1901e-04
Loss = 4.1220e-04, PNorm = 45.8487, GNorm = 0.2119, lr_0 = 6.1837e-04
Loss = 5.1189e-04, PNorm = 45.8580, GNorm = 0.9234, lr_0 = 6.1772e-04
Loss = 6.1962e-04, PNorm = 45.8719, GNorm = 0.7469, lr_0 = 6.1708e-04
Loss = 4.9295e-04, PNorm = 45.8870, GNorm = 0.5437, lr_0 = 6.1643e-04
Loss = 6.0868e-04, PNorm = 45.9040, GNorm = 0.3078, lr_0 = 6.1579e-04
Loss = 4.4226e-04, PNorm = 45.9154, GNorm = 0.5165, lr_0 = 6.1515e-04
Loss = 4.8718e-04, PNorm = 45.9250, GNorm = 0.5299, lr_0 = 6.1451e-04
Loss = 4.7227e-04, PNorm = 45.9378, GNorm = 0.5713, lr_0 = 6.1386e-04
Loss = 5.6663e-04, PNorm = 45.9509, GNorm = 0.5459, lr_0 = 6.1322e-04
Loss = 4.9143e-04, PNorm = 45.9646, GNorm = 0.5033, lr_0 = 6.1258e-04
Loss = 4.9968e-04, PNorm = 45.9763, GNorm = 0.4744, lr_0 = 6.1194e-04
Loss = 5.4888e-04, PNorm = 45.9859, GNorm = 0.5334, lr_0 = 6.1131e-04
Loss = 6.0247e-04, PNorm = 46.0002, GNorm = 0.7411, lr_0 = 6.1067e-04
Loss = 5.4688e-04, PNorm = 46.0093, GNorm = 0.4678, lr_0 = 6.1003e-04
Loss = 5.5225e-04, PNorm = 46.0237, GNorm = 0.7036, lr_0 = 6.0939e-04
Validation rmse logD = 0.607377
Validation R2 logD = 0.744498
Validation rmse logP = 0.478886
Validation R2 logP = 0.933441
Epoch 23
Train function
Loss = 4.4282e-04, PNorm = 46.0413, GNorm = 0.4292, lr_0 = 6.0876e-04
Loss = 5.1712e-04, PNorm = 46.0541, GNorm = 0.3637, lr_0 = 6.0812e-04
Loss = 4.0448e-04, PNorm = 46.0654, GNorm = 0.4171, lr_0 = 6.0749e-04
Loss = 3.9911e-04, PNorm = 46.0737, GNorm = 0.3088, lr_0 = 6.0685e-04
Loss = 3.9606e-04, PNorm = 46.0867, GNorm = 0.6003, lr_0 = 6.0622e-04
Loss = 4.8102e-04, PNorm = 46.0981, GNorm = 0.7448, lr_0 = 6.0559e-04
Loss = 4.5497e-04, PNorm = 46.1090, GNorm = 0.9868, lr_0 = 6.0496e-04
Loss = 5.1398e-04, PNorm = 46.1207, GNorm = 0.3564, lr_0 = 6.0432e-04
Loss = 3.9427e-04, PNorm = 46.1326, GNorm = 0.4631, lr_0 = 6.0369e-04
Loss = 4.4741e-04, PNorm = 46.1440, GNorm = 0.5877, lr_0 = 6.0306e-04
Loss = 4.7365e-04, PNorm = 46.1575, GNorm = 1.0040, lr_0 = 6.0243e-04
Loss = 4.7003e-04, PNorm = 46.1686, GNorm = 0.8625, lr_0 = 6.0180e-04
Loss = 5.5143e-04, PNorm = 46.1888, GNorm = 0.4683, lr_0 = 6.0118e-04
Loss = 6.9372e-04, PNorm = 46.2043, GNorm = 0.8377, lr_0 = 6.0055e-04
Loss = 5.5030e-04, PNorm = 46.2235, GNorm = 0.3736, lr_0 = 5.9992e-04
Loss = 6.0443e-04, PNorm = 46.2366, GNorm = 0.6969, lr_0 = 5.9930e-04
Loss = 5.9575e-04, PNorm = 46.2500, GNorm = 0.8470, lr_0 = 5.9867e-04
Loss = 7.6418e-04, PNorm = 46.2682, GNorm = 0.6476, lr_0 = 5.9805e-04
Loss = 5.7998e-04, PNorm = 46.2802, GNorm = 1.1247, lr_0 = 5.9742e-04
Loss = 6.1119e-04, PNorm = 46.2962, GNorm = 0.7792, lr_0 = 5.9680e-04
Loss = 4.6595e-04, PNorm = 46.3127, GNorm = 0.3677, lr_0 = 5.9618e-04
Loss = 6.0228e-04, PNorm = 46.3275, GNorm = 0.3457, lr_0 = 5.9555e-04
Loss = 4.7832e-04, PNorm = 46.3343, GNorm = 0.3317, lr_0 = 5.9493e-04
Validation rmse logD = 0.586434
Validation R2 logD = 0.761814
Validation rmse logP = 0.463575
Validation R2 logP = 0.937629
Epoch 24
Train function
Loss = 3.6635e-04, PNorm = 46.3442, GNorm = 0.4019, lr_0 = 5.9425e-04
Loss = 4.4356e-04, PNorm = 46.3537, GNorm = 0.4373, lr_0 = 5.9363e-04
Loss = 4.9563e-04, PNorm = 46.3639, GNorm = 0.7222, lr_0 = 5.9301e-04
Loss = 4.0541e-04, PNorm = 46.3769, GNorm = 0.5401, lr_0 = 5.9239e-04
Loss = 4.4627e-04, PNorm = 46.3880, GNorm = 0.5429, lr_0 = 5.9177e-04
Loss = 4.4542e-04, PNorm = 46.3983, GNorm = 0.3884, lr_0 = 5.9115e-04
Loss = 3.6886e-04, PNorm = 46.4122, GNorm = 0.4601, lr_0 = 5.9054e-04
Loss = 4.8879e-04, PNorm = 46.4259, GNorm = 0.8795, lr_0 = 5.8992e-04
Loss = 5.3085e-04, PNorm = 46.4320, GNorm = 0.6118, lr_0 = 5.8931e-04
Loss = 4.9891e-04, PNorm = 46.4420, GNorm = 1.0466, lr_0 = 5.8869e-04
Loss = 5.1631e-04, PNorm = 46.4575, GNorm = 0.3976, lr_0 = 5.8808e-04
Loss = 5.2034e-04, PNorm = 46.4801, GNorm = 0.3795, lr_0 = 5.8746e-04
Loss = 5.2552e-04, PNorm = 46.4915, GNorm = 0.6892, lr_0 = 5.8685e-04
Loss = 5.9972e-04, PNorm = 46.5104, GNorm = 0.6704, lr_0 = 5.8624e-04
Loss = 4.7862e-04, PNorm = 46.5260, GNorm = 0.6630, lr_0 = 5.8562e-04
Loss = 4.1442e-04, PNorm = 46.5393, GNorm = 0.4919, lr_0 = 5.8501e-04
Loss = 5.1781e-04, PNorm = 46.5552, GNorm = 0.5741, lr_0 = 5.8440e-04
Loss = 4.3137e-04, PNorm = 46.5655, GNorm = 0.4299, lr_0 = 5.8379e-04
Loss = 4.2711e-04, PNorm = 46.5748, GNorm = 0.4719, lr_0 = 5.8318e-04
Loss = 4.2218e-04, PNorm = 46.5861, GNorm = 0.5436, lr_0 = 5.8257e-04
Loss = 3.8814e-04, PNorm = 46.5964, GNorm = 0.4639, lr_0 = 5.8197e-04
Loss = 4.6715e-04, PNorm = 46.6044, GNorm = 0.5934, lr_0 = 5.8136e-04
Validation rmse logD = 0.597729
Validation R2 logD = 0.752550
Validation rmse logP = 0.479697
Validation R2 logP = 0.933216
Epoch 25
Train function
Loss = 4.9250e-04, PNorm = 46.6083, GNorm = 0.6848, lr_0 = 5.8075e-04
Loss = 4.4421e-04, PNorm = 46.6234, GNorm = 0.3298, lr_0 = 5.8015e-04
Loss = 3.8177e-04, PNorm = 46.6298, GNorm = 0.5936, lr_0 = 5.7954e-04
Loss = 4.9227e-04, PNorm = 46.6425, GNorm = 0.4379, lr_0 = 5.7894e-04
Loss = 3.6418e-04, PNorm = 46.6536, GNorm = 0.8196, lr_0 = 5.7833e-04
Loss = 4.4619e-04, PNorm = 46.6631, GNorm = 0.6051, lr_0 = 5.7773e-04
Loss = 3.4666e-04, PNorm = 46.6735, GNorm = 0.2294, lr_0 = 5.7712e-04
Loss = 4.6632e-04, PNorm = 46.6815, GNorm = 0.4154, lr_0 = 5.7652e-04
Loss = 3.9577e-04, PNorm = 46.6978, GNorm = 0.3488, lr_0 = 5.7592e-04
Loss = 4.1215e-04, PNorm = 46.7105, GNorm = 0.3285, lr_0 = 5.7532e-04
Loss = 3.7437e-04, PNorm = 46.7201, GNorm = 0.4407, lr_0 = 5.7472e-04
Loss = 4.3606e-04, PNorm = 46.7319, GNorm = 0.5349, lr_0 = 5.7412e-04
Loss = 3.6499e-04, PNorm = 46.7415, GNorm = 0.4600, lr_0 = 5.7352e-04
Loss = 4.8591e-04, PNorm = 46.7461, GNorm = 0.7137, lr_0 = 5.7292e-04
Loss = 5.2280e-04, PNorm = 46.7593, GNorm = 0.8900, lr_0 = 5.7232e-04
Loss = 3.6839e-04, PNorm = 46.7724, GNorm = 0.3919, lr_0 = 5.7173e-04
Loss = 4.4118e-04, PNorm = 46.7782, GNorm = 0.4787, lr_0 = 5.7113e-04
Loss = 3.7742e-04, PNorm = 46.7885, GNorm = 0.5534, lr_0 = 5.7053e-04
Loss = 4.4794e-04, PNorm = 46.7971, GNorm = 0.4545, lr_0 = 5.6994e-04
Loss = 4.0541e-04, PNorm = 46.8035, GNorm = 0.5230, lr_0 = 5.6934e-04
Loss = 5.0706e-04, PNorm = 46.8164, GNorm = 0.4559, lr_0 = 5.6875e-04
Loss = 4.7464e-04, PNorm = 46.8267, GNorm = 0.6440, lr_0 = 5.6816e-04
Loss = 4.5113e-04, PNorm = 46.8415, GNorm = 0.6458, lr_0 = 5.6756e-04
Validation rmse logD = 0.585267
Validation R2 logD = 0.762761
Validation rmse logP = 0.485050
Validation R2 logP = 0.931717
Epoch 26
Train function
Loss = 3.7441e-04, PNorm = 46.8534, GNorm = 0.5846, lr_0 = 5.6691e-04
Loss = 4.4652e-04, PNorm = 46.8644, GNorm = 0.3825, lr_0 = 5.6632e-04
Loss = 3.8875e-04, PNorm = 46.8753, GNorm = 0.4239, lr_0 = 5.6573e-04
Loss = 3.0058e-04, PNorm = 46.8885, GNorm = 0.5675, lr_0 = 5.6514e-04
Loss = 3.3739e-04, PNorm = 46.8971, GNorm = 0.5030, lr_0 = 5.6455e-04
Loss = 3.4353e-04, PNorm = 46.9021, GNorm = 0.5428, lr_0 = 5.6396e-04
Loss = 4.1051e-04, PNorm = 46.9130, GNorm = 0.5254, lr_0 = 5.6337e-04
Loss = 3.8222e-04, PNorm = 46.9231, GNorm = 0.6111, lr_0 = 5.6278e-04
Loss = 3.4907e-04, PNorm = 46.9342, GNorm = 0.5275, lr_0 = 5.6219e-04
Loss = 3.6788e-04, PNorm = 46.9451, GNorm = 0.7681, lr_0 = 5.6161e-04
Loss = 3.3430e-04, PNorm = 46.9539, GNorm = 0.3388, lr_0 = 5.6102e-04
Loss = 3.9789e-04, PNorm = 46.9643, GNorm = 0.4941, lr_0 = 5.6044e-04
Loss = 4.1228e-04, PNorm = 46.9739, GNorm = 0.5075, lr_0 = 5.5985e-04
Loss = 3.6180e-04, PNorm = 46.9885, GNorm = 0.2933, lr_0 = 5.5927e-04
Loss = 3.4262e-04, PNorm = 46.9964, GNorm = 0.4755, lr_0 = 5.5868e-04
Loss = 4.7827e-04, PNorm = 47.0093, GNorm = 0.7097, lr_0 = 5.5810e-04
Loss = 4.4128e-04, PNorm = 47.0180, GNorm = 0.2951, lr_0 = 5.5752e-04
Loss = 3.8138e-04, PNorm = 47.0289, GNorm = 0.4753, lr_0 = 5.5694e-04
Loss = 5.5784e-04, PNorm = 47.0365, GNorm = 0.7985, lr_0 = 5.5635e-04
Loss = 5.0702e-04, PNorm = 47.0455, GNorm = 0.5154, lr_0 = 5.5577e-04
Loss = 5.3627e-04, PNorm = 47.0546, GNorm = 0.6930, lr_0 = 5.5519e-04
Loss = 4.9862e-04, PNorm = 47.0744, GNorm = 0.6151, lr_0 = 5.5461e-04
Validation rmse logD = 0.599951
Validation R2 logD = 0.750707
Validation rmse logP = 0.476629
Validation R2 logP = 0.934067
Epoch 27
Train function
Loss = 3.6511e-04, PNorm = 47.0889, GNorm = 0.3408, lr_0 = 5.5398e-04
Loss = 3.8652e-04, PNorm = 47.0992, GNorm = 0.3094, lr_0 = 5.5340e-04
Loss = 3.7550e-04, PNorm = 47.1098, GNorm = 0.8678, lr_0 = 5.5282e-04
Loss = 3.7868e-04, PNorm = 47.1226, GNorm = 0.6421, lr_0 = 5.5224e-04
Loss = 3.9978e-04, PNorm = 47.1332, GNorm = 0.4424, lr_0 = 5.5167e-04
Loss = 3.4983e-04, PNorm = 47.1453, GNorm = 0.4784, lr_0 = 5.5109e-04
Loss = 3.2443e-04, PNorm = 47.1545, GNorm = 0.6305, lr_0 = 5.5052e-04
Loss = 4.3565e-04, PNorm = 47.1640, GNorm = 0.9436, lr_0 = 5.4994e-04
Loss = 5.0492e-04, PNorm = 47.1759, GNorm = 0.5455, lr_0 = 5.4937e-04
Loss = 4.1034e-04, PNorm = 47.1882, GNorm = 0.5984, lr_0 = 5.4880e-04
Loss = 3.5436e-04, PNorm = 47.2027, GNorm = 0.4171, lr_0 = 5.4822e-04
Loss = 4.6932e-04, PNorm = 47.2113, GNorm = 0.3354, lr_0 = 5.4765e-04
Loss = 4.6673e-04, PNorm = 47.2246, GNorm = 0.3329, lr_0 = 5.4708e-04
Loss = 4.6181e-04, PNorm = 47.2334, GNorm = 0.4454, lr_0 = 5.4651e-04
Loss = 4.5260e-04, PNorm = 47.2443, GNorm = 0.4162, lr_0 = 5.4594e-04
Loss = 3.8102e-04, PNorm = 47.2550, GNorm = 0.5596, lr_0 = 5.4537e-04
Loss = 3.4242e-04, PNorm = 47.2621, GNorm = 0.5797, lr_0 = 5.4480e-04
Loss = 3.7152e-04, PNorm = 47.2726, GNorm = 0.3144, lr_0 = 5.4423e-04
Loss = 3.8286e-04, PNorm = 47.2846, GNorm = 0.6738, lr_0 = 5.4366e-04
Loss = 3.6709e-04, PNorm = 47.2954, GNorm = 0.3151, lr_0 = 5.4309e-04
Loss = 4.1454e-04, PNorm = 47.3080, GNorm = 0.3224, lr_0 = 5.4253e-04
Loss = 3.4107e-04, PNorm = 47.3261, GNorm = 0.4664, lr_0 = 5.4196e-04
Loss = 3.5700e-04, PNorm = 47.3364, GNorm = 0.3196, lr_0 = 5.4140e-04
Validation rmse logD = 0.588215
Validation R2 logD = 0.760365
Validation rmse logP = 0.469835
Validation R2 logP = 0.935933
Epoch 28
Train function
Loss = 2.9960e-04, PNorm = 47.3427, GNorm = 0.4017, lr_0 = 5.4083e-04
Loss = 3.0822e-04, PNorm = 47.3529, GNorm = 0.5076, lr_0 = 5.4027e-04
Loss = 3.2467e-04, PNorm = 47.3651, GNorm = 0.3387, lr_0 = 5.3970e-04
Loss = 2.7341e-04, PNorm = 47.3729, GNorm = 0.4016, lr_0 = 5.3914e-04
Loss = 2.7192e-04, PNorm = 47.3782, GNorm = 0.4754, lr_0 = 5.3858e-04
Loss = 2.7352e-04, PNorm = 47.3870, GNorm = 0.2765, lr_0 = 5.3801e-04
Loss = 3.9469e-04, PNorm = 47.3954, GNorm = 0.6269, lr_0 = 5.3745e-04
Loss = 3.4165e-04, PNorm = 47.4036, GNorm = 0.6344, lr_0 = 5.3689e-04
Loss = 3.2803e-04, PNorm = 47.4132, GNorm = 0.3468, lr_0 = 5.3633e-04
Loss = 3.2208e-04, PNorm = 47.4219, GNorm = 0.4867, lr_0 = 5.3577e-04
Loss = 3.1146e-04, PNorm = 47.4283, GNorm = 0.3580, lr_0 = 5.3521e-04
Loss = 3.2785e-04, PNorm = 47.4355, GNorm = 0.5357, lr_0 = 5.3465e-04
Loss = 4.3901e-04, PNorm = 47.4479, GNorm = 0.4005, lr_0 = 5.3410e-04
Loss = 3.3190e-04, PNorm = 47.4572, GNorm = 0.3469, lr_0 = 5.3354e-04
Loss = 2.8086e-04, PNorm = 47.4694, GNorm = 0.2557, lr_0 = 5.3298e-04
Loss = 2.9207e-04, PNorm = 47.4789, GNorm = 0.3858, lr_0 = 5.3243e-04
Loss = 3.6004e-04, PNorm = 47.4876, GNorm = 0.6014, lr_0 = 5.3187e-04
Loss = 3.3528e-04, PNorm = 47.4948, GNorm = 0.3511, lr_0 = 5.3131e-04
Loss = 2.8351e-04, PNorm = 47.5036, GNorm = 0.3222, lr_0 = 5.3076e-04
Loss = 3.2561e-04, PNorm = 47.5097, GNorm = 0.4274, lr_0 = 5.3021e-04
Loss = 5.2289e-04, PNorm = 47.5178, GNorm = 0.5484, lr_0 = 5.2965e-04
Loss = 4.4802e-04, PNorm = 47.5312, GNorm = 0.3150, lr_0 = 5.2910e-04
Validation rmse logD = 0.582381
Validation R2 logD = 0.765095
Validation rmse logP = 0.464314
Validation R2 logP = 0.937430
Epoch 29
Train function
Loss = 2.6903e-04, PNorm = 47.5399, GNorm = 0.3519, lr_0 = 5.2849e-04
Loss = 3.0924e-04, PNorm = 47.5535, GNorm = 0.2924, lr_0 = 5.2794e-04
Loss = 3.8613e-04, PNorm = 47.5624, GNorm = 0.3816, lr_0 = 5.2739e-04
Loss = 2.9658e-04, PNorm = 47.5693, GNorm = 0.3741, lr_0 = 5.2684e-04
Loss = 3.0961e-04, PNorm = 47.5803, GNorm = 0.3366, lr_0 = 5.2629e-04
Loss = 3.1305e-04, PNorm = 47.5902, GNorm = 0.5184, lr_0 = 5.2574e-04
Loss = 3.9023e-04, PNorm = 47.6019, GNorm = 0.2764, lr_0 = 5.2519e-04
Loss = 4.3573e-04, PNorm = 47.6099, GNorm = 0.4238, lr_0 = 5.2464e-04
Loss = 3.0733e-04, PNorm = 47.6204, GNorm = 0.3692, lr_0 = 5.2410e-04
Loss = 3.1203e-04, PNorm = 47.6323, GNorm = 0.9944, lr_0 = 5.2355e-04
Loss = 3.6649e-04, PNorm = 47.6366, GNorm = 0.3338, lr_0 = 5.2300e-04
Loss = 4.4480e-04, PNorm = 47.6468, GNorm = 0.9038, lr_0 = 5.2246e-04
Loss = 3.7434e-04, PNorm = 47.6625, GNorm = 0.3412, lr_0 = 5.2191e-04
Loss = 3.0088e-04, PNorm = 47.6756, GNorm = 0.3751, lr_0 = 5.2137e-04
Loss = 3.2148e-04, PNorm = 47.6864, GNorm = 0.3544, lr_0 = 5.2082e-04
Loss = 3.6770e-04, PNorm = 47.6964, GNorm = 0.7041, lr_0 = 5.2028e-04
Loss = 4.6199e-04, PNorm = 47.7063, GNorm = 0.7334, lr_0 = 5.1974e-04
Loss = 4.0011e-04, PNorm = 47.7143, GNorm = 0.3056, lr_0 = 5.1919e-04
Loss = 3.2545e-04, PNorm = 47.7216, GNorm = 0.4771, lr_0 = 5.1865e-04
Loss = 3.7299e-04, PNorm = 47.7323, GNorm = 0.3887, lr_0 = 5.1811e-04
Loss = 4.0506e-04, PNorm = 47.7404, GNorm = 0.6214, lr_0 = 5.1757e-04
Loss = 3.4623e-04, PNorm = 47.7503, GNorm = 0.4129, lr_0 = 5.1703e-04
Loss = 3.2740e-04, PNorm = 47.7601, GNorm = 0.3229, lr_0 = 5.1649e-04
Validation rmse logD = 0.592101
Validation R2 logD = 0.757188
Validation rmse logP = 0.481851
Validation R2 logP = 0.932614
Epoch 30
Train function
Loss = 3.0194e-04, PNorm = 47.7696, GNorm = 0.8998, lr_0 = 5.1595e-04
Loss = 3.3936e-04, PNorm = 47.7798, GNorm = 0.4159, lr_0 = 5.1541e-04
Loss = 3.6384e-04, PNorm = 47.7919, GNorm = 0.8805, lr_0 = 5.1487e-04
Loss = 3.1842e-04, PNorm = 47.8049, GNorm = 0.4531, lr_0 = 5.1434e-04
Loss = 2.3622e-04, PNorm = 47.8143, GNorm = 0.3298, lr_0 = 5.1380e-04
Loss = 3.0263e-04, PNorm = 47.8221, GNorm = 0.5001, lr_0 = 5.1326e-04
Loss = 2.7524e-04, PNorm = 47.8335, GNorm = 0.3110, lr_0 = 5.1273e-04
Loss = 3.1177e-04, PNorm = 47.8343, GNorm = 0.3544, lr_0 = 5.1219e-04
Loss = 2.8452e-04, PNorm = 47.8410, GNorm = 0.3175, lr_0 = 5.1166e-04
Loss = 3.0011e-04, PNorm = 47.8510, GNorm = 0.3459, lr_0 = 5.1112e-04
Loss = 2.4429e-04, PNorm = 47.8594, GNorm = 0.4893, lr_0 = 5.1059e-04
Loss = 3.2049e-04, PNorm = 47.8665, GNorm = 0.3985, lr_0 = 5.1006e-04
Loss = 2.5506e-04, PNorm = 47.8743, GNorm = 0.4013, lr_0 = 5.0953e-04
Loss = 2.4448e-04, PNorm = 47.8782, GNorm = 0.4378, lr_0 = 5.0899e-04
Loss = 3.6286e-04, PNorm = 47.8793, GNorm = 0.3620, lr_0 = 5.0846e-04
Loss = 2.9819e-04, PNorm = 47.8925, GNorm = 0.4732, lr_0 = 5.0793e-04
Loss = 2.8199e-04, PNorm = 47.9009, GNorm = 0.3006, lr_0 = 5.0740e-04
Loss = 3.4895e-04, PNorm = 47.9069, GNorm = 0.3338, lr_0 = 5.0687e-04
Loss = 3.0606e-04, PNorm = 47.9129, GNorm = 0.4296, lr_0 = 5.0634e-04
Loss = 2.8035e-04, PNorm = 47.9190, GNorm = 0.3340, lr_0 = 5.0581e-04
Loss = 2.8226e-04, PNorm = 47.9310, GNorm = 0.3830, lr_0 = 5.0529e-04
Loss = 2.9073e-04, PNorm = 47.9409, GNorm = 0.3001, lr_0 = 5.0476e-04
Validation rmse logD = 0.601512
Validation R2 logD = 0.749408
Validation rmse logP = 0.465544
Validation R2 logP = 0.937098
Epoch 31
Train function
Loss = 1.8010e-04, PNorm = 47.9485, GNorm = 0.2417, lr_0 = 5.0418e-04
Loss = 2.7466e-04, PNorm = 47.9524, GNorm = 0.2644, lr_0 = 5.0365e-04
Loss = 2.0955e-04, PNorm = 47.9586, GNorm = 0.3185, lr_0 = 5.0313e-04
Loss = 2.3817e-04, PNorm = 47.9651, GNorm = 0.3265, lr_0 = 5.0260e-04
Loss = 2.2453e-04, PNorm = 47.9765, GNorm = 0.2092, lr_0 = 5.0208e-04
Loss = 3.2653e-04, PNorm = 47.9858, GNorm = 0.5623, lr_0 = 5.0155e-04
Loss = 2.7303e-04, PNorm = 47.9948, GNorm = 0.4091, lr_0 = 5.0103e-04
Loss = 2.3016e-04, PNorm = 48.0050, GNorm = 0.4497, lr_0 = 5.0051e-04
Loss = 2.7283e-04, PNorm = 48.0104, GNorm = 0.4059, lr_0 = 4.9998e-04
Loss = 2.4117e-04, PNorm = 48.0159, GNorm = 0.3509, lr_0 = 4.9946e-04
Loss = 2.3698e-04, PNorm = 48.0229, GNorm = 0.4389, lr_0 = 4.9894e-04
Loss = 2.7192e-04, PNorm = 48.0281, GNorm = 0.3276, lr_0 = 4.9842e-04
Loss = 2.6521e-04, PNorm = 48.0344, GNorm = 0.4026, lr_0 = 4.9790e-04
Loss = 2.9178e-04, PNorm = 48.0430, GNorm = 0.4109, lr_0 = 4.9738e-04
Loss = 2.5226e-04, PNorm = 48.0497, GNorm = 0.3198, lr_0 = 4.9686e-04
Loss = 2.9467e-04, PNorm = 48.0583, GNorm = 0.4328, lr_0 = 4.9634e-04
Loss = 2.7993e-04, PNorm = 48.0666, GNorm = 0.2680, lr_0 = 4.9583e-04
Loss = 3.1511e-04, PNorm = 48.0747, GNorm = 0.4708, lr_0 = 4.9531e-04
Loss = 2.9664e-04, PNorm = 48.0852, GNorm = 0.4369, lr_0 = 4.9479e-04
Loss = 3.1477e-04, PNorm = 48.0903, GNorm = 0.5137, lr_0 = 4.9427e-04
Loss = 2.8152e-04, PNorm = 48.0981, GNorm = 0.4154, lr_0 = 4.9376e-04
Loss = 3.4248e-04, PNorm = 48.1093, GNorm = 0.2055, lr_0 = 4.9324e-04
Loss = 3.1252e-04, PNorm = 48.1156, GNorm = 0.3068, lr_0 = 4.9273e-04
Validation rmse logD = 0.584510
Validation R2 logD = 0.763374
Validation rmse logP = 0.465248
Validation R2 logP = 0.937178
Epoch 32
Train function
Loss = 1.9869e-04, PNorm = 48.1224, GNorm = 0.3006, lr_0 = 4.9221e-04
Loss = 2.0762e-04, PNorm = 48.1287, GNorm = 0.2731, lr_0 = 4.9170e-04
Loss = 1.7624e-04, PNorm = 48.1357, GNorm = 0.2694, lr_0 = 4.9119e-04
Loss = 2.7730e-04, PNorm = 48.1400, GNorm = 0.4843, lr_0 = 4.9067e-04
Loss = 3.2584e-04, PNorm = 48.1482, GNorm = 0.6744, lr_0 = 4.9016e-04
Loss = 2.7280e-04, PNorm = 48.1553, GNorm = 0.2342, lr_0 = 4.8965e-04
Loss = 2.9945e-04, PNorm = 48.1619, GNorm = 0.3216, lr_0 = 4.8914e-04
Loss = 2.5639e-04, PNorm = 48.1702, GNorm = 0.3286, lr_0 = 4.8863e-04
Loss = 2.5099e-04, PNorm = 48.1790, GNorm = 0.2826, lr_0 = 4.8812e-04
Loss = 3.0729e-04, PNorm = 48.1893, GNorm = 0.4208, lr_0 = 4.8761e-04
Loss = 2.5600e-04, PNorm = 48.1985, GNorm = 0.3462, lr_0 = 4.8710e-04
Loss = 2.9098e-04, PNorm = 48.2096, GNorm = 0.2732, lr_0 = 4.8659e-04
Loss = 2.6654e-04, PNorm = 48.2169, GNorm = 0.4217, lr_0 = 4.8608e-04
Loss = 2.4927e-04, PNorm = 48.2255, GNorm = 0.3330, lr_0 = 4.8558e-04
Loss = 3.1434e-04, PNorm = 48.2321, GNorm = 0.2930, lr_0 = 4.8507e-04
Loss = 2.2799e-04, PNorm = 48.2411, GNorm = 0.3347, lr_0 = 4.8456e-04
Loss = 3.2245e-04, PNorm = 48.2501, GNorm = 0.3400, lr_0 = 4.8406e-04
Loss = 2.5075e-04, PNorm = 48.2595, GNorm = 0.3181, lr_0 = 4.8355e-04
Loss = 2.7070e-04, PNorm = 48.2667, GNorm = 0.3715, lr_0 = 4.8305e-04
Loss = 2.5278e-04, PNorm = 48.2709, GNorm = 0.2865, lr_0 = 4.8254e-04
Loss = 3.1873e-04, PNorm = 48.2763, GNorm = 0.3131, lr_0 = 4.8204e-04
Loss = 2.7497e-04, PNorm = 48.2867, GNorm = 0.2266, lr_0 = 4.8154e-04
Loss = 2.8514e-04, PNorm = 48.2902, GNorm = 0.3809, lr_0 = 4.8104e-04
Loss = 5.3410e-04, PNorm = 48.2909, GNorm = 0.4691, lr_0 = 4.8098e-04
Validation rmse logD = 0.586572
Validation R2 logD = 0.761702
Validation rmse logP = 0.473113
Validation R2 logP = 0.935036
Epoch 33
Train function
Loss = 2.4764e-04, PNorm = 48.2993, GNorm = 0.4031, lr_0 = 4.8048e-04
Loss = 2.1700e-04, PNorm = 48.3092, GNorm = 0.6629, lr_0 = 4.7998e-04
Loss = 2.4587e-04, PNorm = 48.3190, GNorm = 0.3629, lr_0 = 4.7948e-04
Loss = 2.5952e-04, PNorm = 48.3300, GNorm = 0.3529, lr_0 = 4.7898e-04
Loss = 2.6218e-04, PNorm = 48.3362, GNorm = 0.2601, lr_0 = 4.7848e-04
Loss = 2.2429e-04, PNorm = 48.3411, GNorm = 0.3383, lr_0 = 4.7798e-04
Loss = 2.5079e-04, PNorm = 48.3488, GNorm = 0.5520, lr_0 = 4.7748e-04
Loss = 2.1157e-04, PNorm = 48.3573, GNorm = 0.3944, lr_0 = 4.7698e-04
Loss = 2.0247e-04, PNorm = 48.3692, GNorm = 0.2884, lr_0 = 4.7649e-04
Loss = 2.4539e-04, PNorm = 48.3764, GNorm = 0.6153, lr_0 = 4.7599e-04
Loss = 2.6720e-04, PNorm = 48.3808, GNorm = 0.4710, lr_0 = 4.7549e-04
Loss = 2.5819e-04, PNorm = 48.3923, GNorm = 0.1985, lr_0 = 4.7500e-04
Loss = 2.3169e-04, PNorm = 48.4038, GNorm = 0.2906, lr_0 = 4.7450e-04
Loss = 2.1893e-04, PNorm = 48.4073, GNorm = 0.2449, lr_0 = 4.7400e-04
Loss = 2.6332e-04, PNorm = 48.4138, GNorm = 0.5649, lr_0 = 4.7351e-04
Loss = 2.8058e-04, PNorm = 48.4199, GNorm = 0.5658, lr_0 = 4.7302e-04
Loss = 2.6582e-04, PNorm = 48.4305, GNorm = 0.4860, lr_0 = 4.7252e-04
Loss = 2.6154e-04, PNorm = 48.4368, GNorm = 0.2309, lr_0 = 4.7203e-04
Loss = 2.3150e-04, PNorm = 48.4460, GNorm = 0.4712, lr_0 = 4.7154e-04
Loss = 3.4057e-04, PNorm = 48.4576, GNorm = 0.5202, lr_0 = 4.7104e-04
Loss = 3.3602e-04, PNorm = 48.4629, GNorm = 0.3525, lr_0 = 4.7055e-04
Loss = 2.9868e-04, PNorm = 48.4687, GNorm = 0.3009, lr_0 = 4.7006e-04
Validation rmse logD = 0.585078
Validation R2 logD = 0.762915
Validation rmse logP = 0.476538
Validation R2 logP = 0.934092
Epoch 34
Train function
Loss = 2.3811e-04, PNorm = 48.4827, GNorm = 0.3060, lr_0 = 4.6957e-04
Loss = 2.5582e-04, PNorm = 48.4924, GNorm = 0.4407, lr_0 = 4.6908e-04
Loss = 2.5127e-04, PNorm = 48.5003, GNorm = 0.3802, lr_0 = 4.6859e-04
Loss = 2.1708e-04, PNorm = 48.5038, GNorm = 0.3268, lr_0 = 4.6810e-04
Loss = 2.5320e-04, PNorm = 48.5129, GNorm = 0.4121, lr_0 = 4.6761e-04
Loss = 2.1211e-04, PNorm = 48.5249, GNorm = 0.3288, lr_0 = 4.6712e-04
Loss = 2.2377e-04, PNorm = 48.5326, GNorm = 0.2015, lr_0 = 4.6664e-04
Loss = 2.3407e-04, PNorm = 48.5400, GNorm = 0.3553, lr_0 = 4.6615e-04
Loss = 2.3684e-04, PNorm = 48.5474, GNorm = 0.3798, lr_0 = 4.6566e-04
Loss = 2.3663e-04, PNorm = 48.5562, GNorm = 0.6333, lr_0 = 4.6518e-04
Loss = 2.3291e-04, PNorm = 48.5638, GNorm = 0.1860, lr_0 = 4.6469e-04
Loss = 2.2959e-04, PNorm = 48.5717, GNorm = 0.2940, lr_0 = 4.6421e-04
Loss = 2.1175e-04, PNorm = 48.5740, GNorm = 0.6013, lr_0 = 4.6372e-04
Loss = 2.5609e-04, PNorm = 48.5828, GNorm = 0.3670, lr_0 = 4.6324e-04
Loss = 2.8277e-04, PNorm = 48.5854, GNorm = 0.2638, lr_0 = 4.6276e-04
Loss = 2.3355e-04, PNorm = 48.5891, GNorm = 0.2175, lr_0 = 4.6227e-04
Loss = 2.3175e-04, PNorm = 48.5960, GNorm = 0.3983, lr_0 = 4.6179e-04
Loss = 2.7960e-04, PNorm = 48.6037, GNorm = 0.4706, lr_0 = 4.6131e-04
Loss = 3.0695e-04, PNorm = 48.6135, GNorm = 0.7041, lr_0 = 4.6083e-04
Loss = 2.6059e-04, PNorm = 48.6237, GNorm = 0.3800, lr_0 = 4.6035e-04
Loss = 2.2990e-04, PNorm = 48.6340, GNorm = 0.4230, lr_0 = 4.5987e-04
Loss = 3.2799e-04, PNorm = 48.6446, GNorm = 0.6101, lr_0 = 4.5939e-04
Loss = 2.6790e-04, PNorm = 48.6528, GNorm = 0.5907, lr_0 = 4.5891e-04
Validation rmse logD = 0.584067
Validation R2 logD = 0.763733
Validation rmse logP = 0.472547
Validation R2 logP = 0.935192
Epoch 35
Train function
Loss = 2.2095e-04, PNorm = 48.6584, GNorm = 0.2645, lr_0 = 4.5838e-04
Loss = 2.2332e-04, PNorm = 48.6655, GNorm = 0.3476, lr_0 = 4.5790e-04
Loss = 2.2063e-04, PNorm = 48.6715, GNorm = 0.6100, lr_0 = 4.5742e-04
Loss = 1.6493e-04, PNorm = 48.6796, GNorm = 0.2628, lr_0 = 4.5695e-04
Loss = 2.3834e-04, PNorm = 48.6889, GNorm = 0.4675, lr_0 = 4.5647e-04
Loss = 2.2355e-04, PNorm = 48.6912, GNorm = 0.2871, lr_0 = 4.5599e-04
Loss = 1.9734e-04, PNorm = 48.6953, GNorm = 0.3097, lr_0 = 4.5552e-04
Loss = 2.1163e-04, PNorm = 48.7004, GNorm = 0.4341, lr_0 = 4.5504e-04
Loss = 1.7570e-04, PNorm = 48.7057, GNorm = 0.2876, lr_0 = 4.5457e-04
Loss = 2.0755e-04, PNorm = 48.7136, GNorm = 0.4031, lr_0 = 4.5409e-04
Loss = 1.9372e-04, PNorm = 48.7211, GNorm = 0.5789, lr_0 = 4.5362e-04
Loss = 2.5682e-04, PNorm = 48.7298, GNorm = 0.4600, lr_0 = 4.5314e-04
Loss = 2.3219e-04, PNorm = 48.7366, GNorm = 0.3325, lr_0 = 4.5267e-04
Loss = 2.3805e-04, PNorm = 48.7435, GNorm = 0.4750, lr_0 = 4.5220e-04
Loss = 2.7524e-04, PNorm = 48.7535, GNorm = 0.6818, lr_0 = 4.5173e-04
Loss = 2.9206e-04, PNorm = 48.7620, GNorm = 0.3384, lr_0 = 4.5125e-04
Loss = 2.4664e-04, PNorm = 48.7664, GNorm = 0.2541, lr_0 = 4.5078e-04
Loss = 2.4098e-04, PNorm = 48.7761, GNorm = 0.2510, lr_0 = 4.5031e-04
Loss = 2.5183e-04, PNorm = 48.7855, GNorm = 0.3183, lr_0 = 4.4984e-04
Loss = 2.6127e-04, PNorm = 48.7975, GNorm = 0.4997, lr_0 = 4.4937e-04
Loss = 2.6352e-04, PNorm = 48.8108, GNorm = 0.2293, lr_0 = 4.4890e-04
Loss = 2.4220e-04, PNorm = 48.8255, GNorm = 0.2281, lr_0 = 4.4844e-04
Validation rmse logD = 0.588104
Validation R2 logD = 0.760455
Validation rmse logP = 0.479373
Validation R2 logP = 0.933306
Epoch 36
Train function
Loss = 2.1315e-04, PNorm = 48.8277, GNorm = 0.2239, lr_0 = 4.4797e-04
Loss = 2.4659e-04, PNorm = 48.8345, GNorm = 0.2681, lr_0 = 4.4750e-04
Loss = 2.7016e-04, PNorm = 48.8436, GNorm = 0.3363, lr_0 = 4.4703e-04
Loss = 2.2762e-04, PNorm = 48.8560, GNorm = 0.5019, lr_0 = 4.4657e-04
Loss = 2.4396e-04, PNorm = 48.8631, GNorm = 0.2558, lr_0 = 4.4610e-04
Loss = 2.2662e-04, PNorm = 48.8706, GNorm = 0.3322, lr_0 = 4.4564e-04
Loss = 2.5896e-04, PNorm = 48.8819, GNorm = 0.8722, lr_0 = 4.4517e-04
Loss = 2.5039e-04, PNorm = 48.8876, GNorm = 0.6195, lr_0 = 4.4471e-04
Loss = 2.5029e-04, PNorm = 48.8964, GNorm = 0.3123, lr_0 = 4.4424e-04
Loss = 2.3026e-04, PNorm = 48.9038, GNorm = 0.3001, lr_0 = 4.4378e-04
Loss = 2.5332e-04, PNorm = 48.9155, GNorm = 0.6789, lr_0 = 4.4331e-04
Loss = 2.6389e-04, PNorm = 48.9247, GNorm = 0.5416, lr_0 = 4.4285e-04
Loss = 2.2466e-04, PNorm = 48.9302, GNorm = 0.3269, lr_0 = 4.4239e-04
Loss = 1.9973e-04, PNorm = 48.9385, GNorm = 0.2783, lr_0 = 4.4193e-04
Loss = 1.9995e-04, PNorm = 48.9470, GNorm = 0.2664, lr_0 = 4.4147e-04
Loss = 2.2395e-04, PNorm = 48.9567, GNorm = 0.5091, lr_0 = 4.4101e-04
Loss = 3.1239e-04, PNorm = 48.9637, GNorm = 0.4785, lr_0 = 4.4055e-04
Loss = 2.1297e-04, PNorm = 48.9728, GNorm = 0.2808, lr_0 = 4.4009e-04
Loss = 2.8571e-04, PNorm = 48.9814, GNorm = 0.6959, lr_0 = 4.3963e-04
Loss = 2.4846e-04, PNorm = 48.9905, GNorm = 0.2899, lr_0 = 4.3917e-04
Loss = 2.3402e-04, PNorm = 48.9993, GNorm = 0.3508, lr_0 = 4.3871e-04
Loss = 2.5136e-04, PNorm = 49.0078, GNorm = 0.2204, lr_0 = 4.3825e-04
Loss = 2.3563e-04, PNorm = 49.0124, GNorm = 0.6817, lr_0 = 4.3779e-04
Validation rmse logD = 0.579301
Validation R2 logD = 0.767573
Validation rmse logP = 0.465899
Validation R2 logP = 0.937002
Epoch 37
Train function
Loss = 1.6489e-04, PNorm = 49.0165, GNorm = 0.2051, lr_0 = 4.3729e-04
Loss = 2.1687e-04, PNorm = 49.0246, GNorm = 0.3408, lr_0 = 4.3684e-04
Loss = 1.9912e-04, PNorm = 49.0293, GNorm = 0.3071, lr_0 = 4.3638e-04
Loss = 2.4689e-04, PNorm = 49.0372, GNorm = 0.5508, lr_0 = 4.3592e-04
Loss = 1.8756e-04, PNorm = 49.0429, GNorm = 0.2992, lr_0 = 4.3547e-04
Loss = 2.0543e-04, PNorm = 49.0535, GNorm = 0.2732, lr_0 = 4.3501e-04
Loss = 1.9924e-04, PNorm = 49.0608, GNorm = 0.1672, lr_0 = 4.3456e-04
Loss = 1.8773e-04, PNorm = 49.0653, GNorm = 0.3030, lr_0 = 4.3411e-04
Loss = 1.8620e-04, PNorm = 49.0680, GNorm = 0.3289, lr_0 = 4.3365e-04
Loss = 3.3646e-04, PNorm = 49.0750, GNorm = 0.4105, lr_0 = 4.3320e-04
Loss = 3.1499e-04, PNorm = 49.0863, GNorm = 0.7297, lr_0 = 4.3275e-04
Loss = 3.7420e-04, PNorm = 49.1032, GNorm = 0.7595, lr_0 = 4.3230e-04
Loss = 3.6816e-04, PNorm = 49.1168, GNorm = 0.2716, lr_0 = 4.3185e-04
Loss = 2.5498e-04, PNorm = 49.1302, GNorm = 0.2753, lr_0 = 4.3140e-04
Loss = 2.1481e-04, PNorm = 49.1425, GNorm = 0.3381, lr_0 = 4.3094e-04
Loss = 2.0793e-04, PNorm = 49.1496, GNorm = 0.3563, lr_0 = 4.3050e-04
Loss = 2.3744e-04, PNorm = 49.1563, GNorm = 0.4137, lr_0 = 4.3005e-04
Loss = 2.3904e-04, PNorm = 49.1682, GNorm = 0.3283, lr_0 = 4.2960e-04
Loss = 2.3039e-04, PNorm = 49.1750, GNorm = 0.2513, lr_0 = 4.2915e-04
Loss = 2.5376e-04, PNorm = 49.1813, GNorm = 0.3346, lr_0 = 4.2870e-04
Loss = 2.2781e-04, PNorm = 49.1885, GNorm = 0.2477, lr_0 = 4.2825e-04
Loss = 2.4910e-04, PNorm = 49.1952, GNorm = 0.4096, lr_0 = 4.2781e-04
Validation rmse logD = 0.587376
Validation R2 logD = 0.761048
Validation rmse logP = 0.465596
Validation R2 logP = 0.937084
Epoch 38
Train function
Loss = 1.5993e-04, PNorm = 49.2041, GNorm = 0.3087, lr_0 = 4.2736e-04
Loss = 2.0303e-04, PNorm = 49.2134, GNorm = 0.3096, lr_0 = 4.2691e-04
Loss = 2.5831e-04, PNorm = 49.2229, GNorm = 0.5475, lr_0 = 4.2647e-04
Loss = 1.6410e-04, PNorm = 49.2313, GNorm = 0.2931, lr_0 = 4.2602e-04
Loss = 1.8916e-04, PNorm = 49.2377, GNorm = 0.3701, lr_0 = 4.2558e-04
Loss = 1.9824e-04, PNorm = 49.2450, GNorm = 0.3730, lr_0 = 4.2513e-04
Loss = 2.0546e-04, PNorm = 49.2519, GNorm = 0.2416, lr_0 = 4.2469e-04
Loss = 1.4849e-04, PNorm = 49.2555, GNorm = 0.2508, lr_0 = 4.2425e-04
Loss = 1.9109e-04, PNorm = 49.2604, GNorm = 0.4438, lr_0 = 4.2380e-04
Loss = 1.8283e-04, PNorm = 49.2687, GNorm = 0.6499, lr_0 = 4.2336e-04
Loss = 2.9673e-04, PNorm = 49.2631, GNorm = 0.5478, lr_0 = 4.2292e-04
Loss = 2.1939e-04, PNorm = 49.2746, GNorm = 0.2127, lr_0 = 4.2248e-04
Loss = 2.5043e-04, PNorm = 49.2878, GNorm = 0.5877, lr_0 = 4.2204e-04
Loss = 2.4685e-04, PNorm = 49.2962, GNorm = 0.2644, lr_0 = 4.2160e-04
Loss = 2.5281e-04, PNorm = 49.3029, GNorm = 0.2723, lr_0 = 4.2116e-04
Loss = 1.9441e-04, PNorm = 49.3093, GNorm = 0.2647, lr_0 = 4.2072e-04
Loss = 1.7484e-04, PNorm = 49.3157, GNorm = 0.2425, lr_0 = 4.2028e-04
Loss = 2.7834e-04, PNorm = 49.3248, GNorm = 0.2764, lr_0 = 4.1984e-04
Loss = 2.1126e-04, PNorm = 49.3316, GNorm = 0.2324, lr_0 = 4.1940e-04
Loss = 2.6001e-04, PNorm = 49.3423, GNorm = 0.3675, lr_0 = 4.1896e-04
Loss = 2.1509e-04, PNorm = 49.3541, GNorm = 0.2601, lr_0 = 4.1853e-04
Loss = 2.5270e-04, PNorm = 49.3593, GNorm = 0.3158, lr_0 = 4.1809e-04
Loss = 2.5972e-04, PNorm = 49.3673, GNorm = 0.4474, lr_0 = 4.1765e-04
Validation rmse logD = 0.587601
Validation R2 logD = 0.760865
Validation rmse logP = 0.480667
Validation R2 logP = 0.932945
Epoch 39
Train function
Loss = 2.1764e-04, PNorm = 49.3739, GNorm = 0.2300, lr_0 = 4.1717e-04
Loss = 2.0443e-04, PNorm = 49.3812, GNorm = 0.5350, lr_0 = 4.1674e-04
Loss = 1.9409e-04, PNorm = 49.3888, GNorm = 0.3421, lr_0 = 4.1630e-04
Loss = 1.5762e-04, PNorm = 49.3936, GNorm = 0.2005, lr_0 = 4.1587e-04
Loss = 1.8837e-04, PNorm = 49.3976, GNorm = 0.2348, lr_0 = 4.1544e-04
Loss = 1.5783e-04, PNorm = 49.4022, GNorm = 0.5753, lr_0 = 4.1500e-04
Loss = 1.7877e-04, PNorm = 49.4088, GNorm = 0.3254, lr_0 = 4.1457e-04
Loss = 1.8872e-04, PNorm = 49.4157, GNorm = 0.3616, lr_0 = 4.1414e-04
Loss = 2.1843e-04, PNorm = 49.4222, GNorm = 0.3974, lr_0 = 4.1370e-04
Loss = 1.9347e-04, PNorm = 49.4250, GNorm = 0.4643, lr_0 = 4.1327e-04
Loss = 1.6170e-04, PNorm = 49.4303, GNorm = 0.2964, lr_0 = 4.1284e-04
Loss = 1.6557e-04, PNorm = 49.4368, GNorm = 0.2617, lr_0 = 4.1241e-04
Loss = 2.1778e-04, PNorm = 49.4436, GNorm = 0.6741, lr_0 = 4.1198e-04
Loss = 2.4344e-04, PNorm = 49.4481, GNorm = 0.5325, lr_0 = 4.1155e-04
Loss = 1.6798e-04, PNorm = 49.4502, GNorm = 0.2458, lr_0 = 4.1112e-04
Loss = 1.6903e-04, PNorm = 49.4565, GNorm = 0.2596, lr_0 = 4.1069e-04
Loss = 1.7614e-04, PNorm = 49.4628, GNorm = 0.4023, lr_0 = 4.1026e-04
Loss = 2.0507e-04, PNorm = 49.4692, GNorm = 0.3779, lr_0 = 4.0983e-04
Loss = 1.6024e-04, PNorm = 49.4727, GNorm = 0.2357, lr_0 = 4.0941e-04
Loss = 2.2544e-04, PNorm = 49.4834, GNorm = 0.4071, lr_0 = 4.0898e-04
Loss = 1.6348e-04, PNorm = 49.4890, GNorm = 0.4992, lr_0 = 4.0855e-04
Loss = 2.5757e-04, PNorm = 49.5009, GNorm = 0.3840, lr_0 = 4.0813e-04
Validation rmse logD = 0.584917
Validation R2 logD = 0.763045
Validation rmse logP = 0.468729
Validation R2 logP = 0.936235
Epoch 40
Train function
Loss = 2.2464e-04, PNorm = 49.5101, GNorm = 0.4484, lr_0 = 4.0770e-04
Loss = 1.8199e-04, PNorm = 49.5185, GNorm = 0.3770, lr_0 = 4.0727e-04
Loss = 1.5744e-04, PNorm = 49.5249, GNorm = 0.1863, lr_0 = 4.0685e-04
Loss = 1.5090e-04, PNorm = 49.5317, GNorm = 0.2966, lr_0 = 4.0642e-04
Loss = 1.6151e-04, PNorm = 49.5400, GNorm = 0.3380, lr_0 = 4.0600e-04
Loss = 2.0572e-04, PNorm = 49.5459, GNorm = 0.2458, lr_0 = 4.0558e-04
Loss = 1.9002e-04, PNorm = 49.5489, GNorm = 0.3891, lr_0 = 4.0515e-04
Loss = 2.0106e-04, PNorm = 49.5519, GNorm = 0.3093, lr_0 = 4.0473e-04
Loss = 1.4964e-04, PNorm = 49.5541, GNorm = 0.4293, lr_0 = 4.0431e-04
Loss = 1.7971e-04, PNorm = 49.5582, GNorm = 0.4187, lr_0 = 4.0389e-04
Loss = 1.4428e-04, PNorm = 49.5658, GNorm = 0.2323, lr_0 = 4.0346e-04
Loss = 1.6944e-04, PNorm = 49.5713, GNorm = 0.3593, lr_0 = 4.0304e-04
Loss = 1.5110e-04, PNorm = 49.5759, GNorm = 0.3675, lr_0 = 4.0262e-04
Loss = 1.6984e-04, PNorm = 49.5816, GNorm = 0.3469, lr_0 = 4.0220e-04
Loss = 1.9096e-04, PNorm = 49.5869, GNorm = 0.3878, lr_0 = 4.0178e-04
Loss = 1.5656e-04, PNorm = 49.5933, GNorm = 0.4260, lr_0 = 4.0136e-04
Loss = 1.6428e-04, PNorm = 49.5981, GNorm = 0.2801, lr_0 = 4.0094e-04
Loss = 1.4812e-04, PNorm = 49.6033, GNorm = 0.3833, lr_0 = 4.0053e-04
Loss = 2.2702e-04, PNorm = 49.6097, GNorm = 0.9691, lr_0 = 4.0011e-04
Loss = 2.0955e-04, PNorm = 49.6188, GNorm = 0.5130, lr_0 = 3.9969e-04
Loss = 2.0140e-04, PNorm = 49.6255, GNorm = 0.2179, lr_0 = 3.9927e-04
Loss = 1.7691e-04, PNorm = 49.6290, GNorm = 0.3702, lr_0 = 3.9886e-04
Loss = 2.0421e-04, PNorm = 49.6340, GNorm = 0.3831, lr_0 = 3.9844e-04
Validation rmse logD = 0.586412
Validation R2 logD = 0.761832
Validation rmse logP = 0.465759
Validation R2 logP = 0.937040
Epoch 41
Train function
Loss = 2.1771e-04, PNorm = 49.6394, GNorm = 0.2767, lr_0 = 3.9798e-04
Loss = 1.5024e-04, PNorm = 49.6452, GNorm = 0.3994, lr_0 = 3.9757e-04
Loss = 1.7621e-04, PNorm = 49.6521, GNorm = 0.4762, lr_0 = 3.9715e-04
Loss = 1.5712e-04, PNorm = 49.6618, GNorm = 0.3669, lr_0 = 3.9674e-04
Loss = 1.9275e-04, PNorm = 49.6685, GNorm = 0.4361, lr_0 = 3.9632e-04
Loss = 1.8490e-04, PNorm = 49.6737, GNorm = 0.3702, lr_0 = 3.9591e-04
Loss = 2.0512e-04, PNorm = 49.6834, GNorm = 0.4512, lr_0 = 3.9550e-04
Loss = 1.8277e-04, PNorm = 49.6881, GNorm = 0.4168, lr_0 = 3.9508e-04
Loss = 1.4119e-04, PNorm = 49.6951, GNorm = 0.3849, lr_0 = 3.9467e-04
Loss = 1.5751e-04, PNorm = 49.7021, GNorm = 0.2933, lr_0 = 3.9426e-04
Loss = 1.4065e-04, PNorm = 49.7043, GNorm = 0.3508, lr_0 = 3.9385e-04
Loss = 1.6363e-04, PNorm = 49.7093, GNorm = 0.3429, lr_0 = 3.9344e-04
Loss = 1.3734e-04, PNorm = 49.7145, GNorm = 0.3276, lr_0 = 3.9303e-04
Loss = 1.5862e-04, PNorm = 49.7213, GNorm = 0.2196, lr_0 = 3.9262e-04
Loss = 1.4926e-04, PNorm = 49.7263, GNorm = 0.1912, lr_0 = 3.9221e-04
Loss = 1.8114e-04, PNorm = 49.7294, GNorm = 0.4637, lr_0 = 3.9180e-04
Loss = 1.8146e-04, PNorm = 49.7333, GNorm = 0.4230, lr_0 = 3.9139e-04
Loss = 2.8446e-04, PNorm = 49.7376, GNorm = 1.0646, lr_0 = 3.9098e-04
Loss = 2.9809e-04, PNorm = 49.7465, GNorm = 0.2674, lr_0 = 3.9057e-04
Loss = 2.8964e-04, PNorm = 49.7536, GNorm = 0.4631, lr_0 = 3.9016e-04
Loss = 1.9565e-04, PNorm = 49.7671, GNorm = 0.2707, lr_0 = 3.8976e-04
Loss = 1.9875e-04, PNorm = 49.7750, GNorm = 0.2816, lr_0 = 3.8935e-04
Loss = 2.3951e-04, PNorm = 49.7792, GNorm = 0.4673, lr_0 = 3.8894e-04
Validation rmse logD = 0.593622
Validation R2 logD = 0.755939
Validation rmse logP = 0.461172
Validation R2 logP = 0.938274
Epoch 42
Train function
Loss = 1.6117e-04, PNorm = 49.7857, GNorm = 0.1996, lr_0 = 3.8854e-04
Loss = 1.4690e-04, PNorm = 49.7901, GNorm = 0.2648, lr_0 = 3.8813e-04
Loss = 1.5793e-04, PNorm = 49.7953, GNorm = 0.1911, lr_0 = 3.8773e-04
Loss = 1.2099e-04, PNorm = 49.7996, GNorm = 0.2156, lr_0 = 3.8732e-04
Loss = 1.7401e-04, PNorm = 49.8038, GNorm = 0.2884, lr_0 = 3.8692e-04
Loss = 1.4502e-04, PNorm = 49.8087, GNorm = 0.3225, lr_0 = 3.8651e-04
Loss = 1.4082e-04, PNorm = 49.8117, GNorm = 0.3013, lr_0 = 3.8611e-04
Loss = 1.4739e-04, PNorm = 49.8142, GNorm = 0.4898, lr_0 = 3.8571e-04
Loss = 1.5156e-04, PNorm = 49.8206, GNorm = 0.2370, lr_0 = 3.8531e-04
Loss = 1.8243e-04, PNorm = 49.8270, GNorm = 0.4162, lr_0 = 3.8490e-04
Loss = 1.4775e-04, PNorm = 49.8316, GNorm = 0.1837, lr_0 = 3.8450e-04
Loss = 1.4755e-04, PNorm = 49.8371, GNorm = 0.2264, lr_0 = 3.8410e-04
Loss = 1.4551e-04, PNorm = 49.8434, GNorm = 0.2188, lr_0 = 3.8370e-04
Loss = 1.4280e-04, PNorm = 49.8493, GNorm = 0.3315, lr_0 = 3.8330e-04
Loss = 1.2870e-04, PNorm = 49.8528, GNorm = 0.2648, lr_0 = 3.8290e-04
Loss = 1.4613e-04, PNorm = 49.8577, GNorm = 0.4492, lr_0 = 3.8250e-04
Loss = 1.6248e-04, PNorm = 49.8627, GNorm = 0.2959, lr_0 = 3.8210e-04
Loss = 1.8803e-04, PNorm = 49.8682, GNorm = 0.4357, lr_0 = 3.8170e-04
Loss = 1.6649e-04, PNorm = 49.8723, GNorm = 0.1966, lr_0 = 3.8130e-04
Loss = 1.7292e-04, PNorm = 49.8780, GNorm = 0.3009, lr_0 = 3.8090e-04
Loss = 1.3588e-04, PNorm = 49.8827, GNorm = 0.2022, lr_0 = 3.8051e-04
Loss = 1.7462e-04, PNorm = 49.8886, GNorm = 0.2605, lr_0 = 3.8011e-04
Validation rmse logD = 0.581590
Validation R2 logD = 0.765733
Validation rmse logP = 0.466458
Validation R2 logP = 0.936851
Epoch 43
Train function
Loss = 1.3741e-04, PNorm = 49.8952, GNorm = 0.2218, lr_0 = 3.7967e-04
Loss = 1.5695e-04, PNorm = 49.8976, GNorm = 0.2657, lr_0 = 3.7928e-04
Loss = 1.4557e-04, PNorm = 49.9002, GNorm = 0.3033, lr_0 = 3.7888e-04
Loss = 1.3638e-04, PNorm = 49.9053, GNorm = 0.2885, lr_0 = 3.7849e-04
Loss = 1.1730e-04, PNorm = 49.9080, GNorm = 0.2284, lr_0 = 3.7809e-04
Loss = 1.4031e-04, PNorm = 49.9137, GNorm = 0.3097, lr_0 = 3.7770e-04
Loss = 1.5480e-04, PNorm = 49.9163, GNorm = 0.3692, lr_0 = 3.7730e-04
Loss = 1.2580e-04, PNorm = 49.9192, GNorm = 0.3226, lr_0 = 3.7691e-04
Loss = 1.0756e-04, PNorm = 49.9235, GNorm = 0.2228, lr_0 = 3.7652e-04
Loss = 1.0507e-04, PNorm = 49.9271, GNorm = 0.2314, lr_0 = 3.7612e-04
Loss = 1.0201e-04, PNorm = 49.9319, GNorm = 0.1851, lr_0 = 3.7573e-04
Loss = 1.4392e-04, PNorm = 49.9373, GNorm = 0.2329, lr_0 = 3.7534e-04
Loss = 1.5661e-04, PNorm = 49.9421, GNorm = 0.3300, lr_0 = 3.7495e-04
Loss = 1.2709e-04, PNorm = 49.9463, GNorm = 0.1507, lr_0 = 3.7455e-04
Loss = 1.3330e-04, PNorm = 49.9497, GNorm = 0.1551, lr_0 = 3.7416e-04
Loss = 1.2392e-04, PNorm = 49.9539, GNorm = 0.2195, lr_0 = 3.7377e-04
Loss = 1.8363e-04, PNorm = 49.9585, GNorm = 0.2987, lr_0 = 3.7338e-04
Loss = 1.4970e-04, PNorm = 49.9641, GNorm = 0.3167, lr_0 = 3.7299e-04
Loss = 1.5638e-04, PNorm = 49.9736, GNorm = 0.3059, lr_0 = 3.7260e-04
Loss = 1.5126e-04, PNorm = 49.9811, GNorm = 0.2587, lr_0 = 3.7221e-04
Loss = 1.2636e-04, PNorm = 49.9854, GNorm = 0.2717, lr_0 = 3.7183e-04
Loss = 1.3851e-04, PNorm = 49.9879, GNorm = 0.2506, lr_0 = 3.7144e-04
Loss = 1.3857e-04, PNorm = 49.9928, GNorm = 0.2297, lr_0 = 3.7105e-04
Validation rmse logD = 0.584601
Validation R2 logD = 0.763301
Validation rmse logP = 0.465487
Validation R2 logP = 0.937114
Epoch 44
Train function
Loss = 1.3196e-04, PNorm = 49.9976, GNorm = 0.3138, lr_0 = 3.7066e-04
Loss = 1.1863e-04, PNorm = 50.0023, GNorm = 0.1574, lr_0 = 3.7028e-04
Loss = 1.1556e-04, PNorm = 50.0065, GNorm = 0.1761, lr_0 = 3.6989e-04
Loss = 1.2821e-04, PNorm = 50.0091, GNorm = 0.2486, lr_0 = 3.6950e-04
Loss = 1.1502e-04, PNorm = 50.0131, GNorm = 0.1704, lr_0 = 3.6912e-04
Loss = 1.1516e-04, PNorm = 50.0180, GNorm = 0.3124, lr_0 = 3.6873e-04
Loss = 1.3982e-04, PNorm = 50.0236, GNorm = 0.2485, lr_0 = 3.6835e-04
Loss = 1.2269e-04, PNorm = 50.0293, GNorm = 0.2534, lr_0 = 3.6796e-04
Loss = 1.2223e-04, PNorm = 50.0332, GNorm = 0.1863, lr_0 = 3.6758e-04
Loss = 1.0982e-04, PNorm = 50.0417, GNorm = 0.2688, lr_0 = 3.6720e-04
Loss = 1.2587e-04, PNorm = 50.0469, GNorm = 0.2293, lr_0 = 3.6681e-04
Loss = 1.2618e-04, PNorm = 50.0528, GNorm = 0.1714, lr_0 = 3.6643e-04
Loss = 1.4861e-04, PNorm = 50.0556, GNorm = 0.4896, lr_0 = 3.6605e-04
Loss = 1.3539e-04, PNorm = 50.0615, GNorm = 0.3386, lr_0 = 3.6567e-04
Loss = 1.4867e-04, PNorm = 50.0666, GNorm = 0.1797, lr_0 = 3.6528e-04
Loss = 1.5304e-04, PNorm = 50.0702, GNorm = 0.1723, lr_0 = 3.6490e-04
Loss = 1.6145e-04, PNorm = 50.0763, GNorm = 0.2052, lr_0 = 3.6452e-04
Loss = 1.3275e-04, PNorm = 50.0813, GNorm = 0.2409, lr_0 = 3.6414e-04
Loss = 1.4299e-04, PNorm = 50.0856, GNorm = 0.4642, lr_0 = 3.6376e-04
Loss = 1.2944e-04, PNorm = 50.0872, GNorm = 0.2229, lr_0 = 3.6338e-04
Loss = 1.5334e-04, PNorm = 50.0916, GNorm = 0.2558, lr_0 = 3.6300e-04
Loss = 1.2242e-04, PNorm = 50.0977, GNorm = 0.1645, lr_0 = 3.6262e-04
Validation rmse logD = 0.584075
Validation R2 logD = 0.763727
Validation rmse logP = 0.463668
Validation R2 logP = 0.937604
Epoch 45
Train function
Loss = 1.0556e-04, PNorm = 50.1041, GNorm = 0.3003, lr_0 = 3.6221e-04
Loss = 1.1460e-04, PNorm = 50.1119, GNorm = 0.2746, lr_0 = 3.6183e-04
Loss = 1.2022e-04, PNorm = 50.1153, GNorm = 0.3319, lr_0 = 3.6145e-04
Loss = 1.0630e-04, PNorm = 50.1225, GNorm = 0.3015, lr_0 = 3.6107e-04
Loss = 1.2216e-04, PNorm = 50.1279, GNorm = 0.2460, lr_0 = 3.6070e-04
Loss = 1.1504e-04, PNorm = 50.1319, GNorm = 0.1718, lr_0 = 3.6032e-04
Loss = 1.0220e-04, PNorm = 50.1351, GNorm = 0.2094, lr_0 = 3.5994e-04
Loss = 1.2414e-04, PNorm = 50.1386, GNorm = 0.3227, lr_0 = 3.5957e-04
Loss = 1.1322e-04, PNorm = 50.1419, GNorm = 0.2062, lr_0 = 3.5919e-04
Loss = 1.1476e-04, PNorm = 50.1474, GNorm = 0.3232, lr_0 = 3.5882e-04
Loss = 1.3573e-04, PNorm = 50.1529, GNorm = 0.2427, lr_0 = 3.5844e-04
Loss = 1.1740e-04, PNorm = 50.1564, GNorm = 0.4645, lr_0 = 3.5807e-04
Loss = 1.3562e-04, PNorm = 50.1627, GNorm = 0.1777, lr_0 = 3.5770e-04
Loss = 1.0356e-04, PNorm = 50.1670, GNorm = 0.2146, lr_0 = 3.5732e-04
Loss = 1.3529e-04, PNorm = 50.1693, GNorm = 0.2496, lr_0 = 3.5695e-04
Loss = 1.3264e-04, PNorm = 50.1745, GNorm = 0.2534, lr_0 = 3.5658e-04
Loss = 1.2582e-04, PNorm = 50.1782, GNorm = 0.3917, lr_0 = 3.5621e-04
Loss = 1.4839e-04, PNorm = 50.1815, GNorm = 0.3335, lr_0 = 3.5583e-04
Loss = 1.3661e-04, PNorm = 50.1875, GNorm = 0.1940, lr_0 = 3.5546e-04
Loss = 1.4065e-04, PNorm = 50.1898, GNorm = 0.2020, lr_0 = 3.5509e-04
Loss = 1.4081e-04, PNorm = 50.1946, GNorm = 0.3155, lr_0 = 3.5472e-04
Loss = 1.4653e-04, PNorm = 50.2003, GNorm = 0.2778, lr_0 = 3.5435e-04
Loss = 1.3961e-04, PNorm = 50.2079, GNorm = 0.2904, lr_0 = 3.5398e-04
Validation rmse logD = 0.591170
Validation R2 logD = 0.757952
Validation rmse logP = 0.460710
Validation R2 logP = 0.938398
Epoch 46
Train function
Loss = 1.2047e-04, PNorm = 50.2128, GNorm = 0.2642, lr_0 = 3.5361e-04
Loss = 1.4228e-04, PNorm = 50.2155, GNorm = 0.4019, lr_0 = 3.5324e-04
Loss = 1.1686e-04, PNorm = 50.2191, GNorm = 0.2146, lr_0 = 3.5287e-04
Loss = 1.0490e-04, PNorm = 50.2236, GNorm = 0.3457, lr_0 = 3.5251e-04
Loss = 1.1839e-04, PNorm = 50.2299, GNorm = 0.2576, lr_0 = 3.5214e-04
Loss = 8.8595e-05, PNorm = 50.2335, GNorm = 0.2709, lr_0 = 3.5177e-04
Loss = 1.5942e-04, PNorm = 50.2374, GNorm = 0.2656, lr_0 = 3.5140e-04
Loss = 1.1393e-04, PNorm = 50.2395, GNorm = 0.2612, lr_0 = 3.5104e-04
Loss = 1.2883e-04, PNorm = 50.2433, GNorm = 0.3195, lr_0 = 3.5067e-04
Loss = 1.3676e-04, PNorm = 50.2467, GNorm = 0.3300, lr_0 = 3.5030e-04
Loss = 1.2375e-04, PNorm = 50.2545, GNorm = 0.3315, lr_0 = 3.4994e-04
Loss = 1.2433e-04, PNorm = 50.2587, GNorm = 0.1557, lr_0 = 3.4957e-04
Loss = 1.3483e-04, PNorm = 50.2633, GNorm = 0.2905, lr_0 = 3.4921e-04
Loss = 1.5845e-04, PNorm = 50.2682, GNorm = 0.4065, lr_0 = 3.4884e-04
Loss = 1.5768e-04, PNorm = 50.2738, GNorm = 0.5012, lr_0 = 3.4848e-04
Loss = 1.3073e-04, PNorm = 50.2812, GNorm = 0.3370, lr_0 = 3.4812e-04
Loss = 1.3754e-04, PNorm = 50.2899, GNorm = 0.2194, lr_0 = 3.4775e-04
Loss = 1.5255e-04, PNorm = 50.2957, GNorm = 0.1894, lr_0 = 3.4739e-04
Loss = 1.3278e-04, PNorm = 50.3012, GNorm = 0.4004, lr_0 = 3.4703e-04
Loss = 1.5309e-04, PNorm = 50.3066, GNorm = 0.3585, lr_0 = 3.4666e-04
Loss = 1.7840e-04, PNorm = 50.3152, GNorm = 0.5892, lr_0 = 3.4630e-04
Loss = 1.4154e-04, PNorm = 50.3184, GNorm = 0.3408, lr_0 = 3.4594e-04
Validation rmse logD = 0.584823
Validation R2 logD = 0.763121
Validation rmse logP = 0.469424
Validation R2 logP = 0.936046
Epoch 47
Train function
Loss = 1.0756e-04, PNorm = 50.3260, GNorm = 0.4001, lr_0 = 3.4554e-04
Loss = 1.4696e-04, PNorm = 50.3296, GNorm = 0.1806, lr_0 = 3.4518e-04
Loss = 1.4014e-04, PNorm = 50.3337, GNorm = 0.4863, lr_0 = 3.4482e-04
Loss = 1.1851e-04, PNorm = 50.3389, GNorm = 0.2154, lr_0 = 3.4446e-04
Loss = 1.1561e-04, PNorm = 50.3422, GNorm = 0.2808, lr_0 = 3.4410e-04
Loss = 1.3427e-04, PNorm = 50.3478, GNorm = 0.3382, lr_0 = 3.4374e-04
Loss = 1.9965e-04, PNorm = 50.3528, GNorm = 0.8839, lr_0 = 3.4339e-04
Loss = 1.2127e-04, PNorm = 50.3576, GNorm = 0.2126, lr_0 = 3.4303e-04
Loss = 1.0643e-04, PNorm = 50.3621, GNorm = 0.2847, lr_0 = 3.4267e-04
Loss = 1.0688e-04, PNorm = 50.3648, GNorm = 0.1342, lr_0 = 3.4231e-04
Loss = 1.6699e-04, PNorm = 50.3695, GNorm = 0.1504, lr_0 = 3.4195e-04
Loss = 9.5927e-05, PNorm = 50.3762, GNorm = 0.1917, lr_0 = 3.4160e-04
Loss = 1.2110e-04, PNorm = 50.3770, GNorm = 0.4155, lr_0 = 3.4124e-04
Loss = 1.2508e-04, PNorm = 50.3767, GNorm = 0.1688, lr_0 = 3.4088e-04
Loss = 1.1794e-04, PNorm = 50.3811, GNorm = 0.4482, lr_0 = 3.4053e-04
Loss = 1.0651e-04, PNorm = 50.3839, GNorm = 0.2296, lr_0 = 3.4017e-04
Loss = 1.1239e-04, PNorm = 50.3904, GNorm = 0.2302, lr_0 = 3.3982e-04
Loss = 1.4555e-04, PNorm = 50.3986, GNorm = 0.3926, lr_0 = 3.3946e-04
Loss = 1.3136e-04, PNorm = 50.4015, GNorm = 0.1979, lr_0 = 3.3911e-04
Loss = 1.2289e-04, PNorm = 50.4061, GNorm = 0.3569, lr_0 = 3.3876e-04
Loss = 1.3776e-04, PNorm = 50.4124, GNorm = 0.2108, lr_0 = 3.3840e-04
Loss = 1.2093e-04, PNorm = 50.4179, GNorm = 0.4978, lr_0 = 3.3805e-04
Loss = 1.0463e-04, PNorm = 50.4211, GNorm = 0.2316, lr_0 = 3.3770e-04
Validation rmse logD = 0.584390
Validation R2 logD = 0.763472
Validation rmse logP = 0.464574
Validation R2 logP = 0.937360
Epoch 48
Train function
Loss = 1.1243e-04, PNorm = 50.4254, GNorm = 0.5500, lr_0 = 3.3734e-04
Loss = 1.3416e-04, PNorm = 50.4299, GNorm = 0.3108, lr_0 = 3.3699e-04
Loss = 1.1241e-04, PNorm = 50.4351, GNorm = 0.3458, lr_0 = 3.3664e-04
Loss = 1.0794e-04, PNorm = 50.4407, GNorm = 0.2222, lr_0 = 3.3629e-04
Loss = 1.1429e-04, PNorm = 50.4485, GNorm = 0.1119, lr_0 = 3.3594e-04
Loss = 1.3109e-04, PNorm = 50.4547, GNorm = 0.1895, lr_0 = 3.3559e-04
Loss = 9.3408e-05, PNorm = 50.4586, GNorm = 0.2278, lr_0 = 3.3524e-04
Loss = 1.2973e-04, PNorm = 50.4630, GNorm = 0.1652, lr_0 = 3.3489e-04
Loss = 1.0544e-04, PNorm = 50.4673, GNorm = 0.2673, lr_0 = 3.3454e-04
Loss = 1.0599e-04, PNorm = 50.4700, GNorm = 0.3083, lr_0 = 3.3419e-04
Loss = 1.2041e-04, PNorm = 50.4749, GNorm = 0.2079, lr_0 = 3.3384e-04
Loss = 1.2809e-04, PNorm = 50.4812, GNorm = 0.1777, lr_0 = 3.3349e-04
Loss = 1.4608e-04, PNorm = 50.4840, GNorm = 0.2782, lr_0 = 3.3314e-04
Loss = 1.2911e-04, PNorm = 50.4905, GNorm = 0.3069, lr_0 = 3.3280e-04
Loss = 1.0975e-04, PNorm = 50.4955, GNorm = 0.1290, lr_0 = 3.3245e-04
Loss = 1.1988e-04, PNorm = 50.5003, GNorm = 0.1608, lr_0 = 3.3210e-04
Loss = 1.1335e-04, PNorm = 50.5047, GNorm = 0.2120, lr_0 = 3.3175e-04
Loss = 1.0441e-04, PNorm = 50.5087, GNorm = 0.1218, lr_0 = 3.3141e-04
Loss = 1.1670e-04, PNorm = 50.5126, GNorm = 0.1680, lr_0 = 3.3106e-04
Loss = 1.2168e-04, PNorm = 50.5137, GNorm = 0.2804, lr_0 = 3.3072e-04
Loss = 1.0336e-04, PNorm = 50.5180, GNorm = 0.2714, lr_0 = 3.3037e-04
Loss = 1.2414e-04, PNorm = 50.5212, GNorm = 0.2217, lr_0 = 3.3003e-04
Validation rmse logD = 0.589892
Validation R2 logD = 0.758997
Validation rmse logP = 0.466884
Validation R2 logP = 0.936736
Epoch 49
Train function
Loss = 9.8191e-05, PNorm = 50.5247, GNorm = 0.3739, lr_0 = 3.2965e-04
Loss = 1.3085e-04, PNorm = 50.5272, GNorm = 0.3086, lr_0 = 3.2930e-04
Loss = 1.0819e-04, PNorm = 50.5300, GNorm = 0.2300, lr_0 = 3.2896e-04
Loss = 9.4671e-05, PNorm = 50.5361, GNorm = 0.2961, lr_0 = 3.2862e-04
Loss = 9.9125e-05, PNorm = 50.5402, GNorm = 0.3162, lr_0 = 3.2827e-04
Loss = 1.3823e-04, PNorm = 50.5430, GNorm = 0.3949, lr_0 = 3.2793e-04
Loss = 1.0293e-04, PNorm = 50.5473, GNorm = 0.3548, lr_0 = 3.2759e-04
Loss = 8.7331e-05, PNorm = 50.5535, GNorm = 0.1760, lr_0 = 3.2725e-04
Loss = 7.9965e-05, PNorm = 50.5557, GNorm = 0.1872, lr_0 = 3.2691e-04
Loss = 9.1343e-05, PNorm = 50.5572, GNorm = 0.3307, lr_0 = 3.2656e-04
Loss = 1.1427e-04, PNorm = 50.5623, GNorm = 0.1749, lr_0 = 3.2622e-04
Loss = 1.5037e-04, PNorm = 50.5679, GNorm = 0.5330, lr_0 = 3.2588e-04
Loss = 1.5728e-04, PNorm = 50.5747, GNorm = 0.4927, lr_0 = 3.2554e-04
Loss = 1.2979e-04, PNorm = 50.5814, GNorm = 0.2783, lr_0 = 3.2520e-04
Loss = 1.1025e-04, PNorm = 50.5873, GNorm = 0.3109, lr_0 = 3.2486e-04
Loss = 1.3104e-04, PNorm = 50.5927, GNorm = 0.2129, lr_0 = 3.2452e-04
Loss = 1.4679e-04, PNorm = 50.5992, GNorm = 0.4477, lr_0 = 3.2419e-04
Loss = 1.2612e-04, PNorm = 50.6060, GNorm = 0.4115, lr_0 = 3.2385e-04
Loss = 1.1340e-04, PNorm = 50.6109, GNorm = 0.1636, lr_0 = 3.2351e-04
Loss = 1.1768e-04, PNorm = 50.6143, GNorm = 0.1881, lr_0 = 3.2317e-04
Loss = 1.2311e-04, PNorm = 50.6186, GNorm = 0.4329, lr_0 = 3.2283e-04
Loss = 1.0941e-04, PNorm = 50.6218, GNorm = 0.2785, lr_0 = 3.2250e-04
Loss = 9.0307e-05, PNorm = 50.6277, GNorm = 0.2171, lr_0 = 3.2216e-04
Validation rmse logD = 0.594486
Validation R2 logD = 0.755229
Validation rmse logP = 0.471222
Validation R2 logP = 0.935555
Epoch 50
Train function
Loss = 9.7906e-05, PNorm = 50.6330, GNorm = 0.1093, lr_0 = 3.2182e-04
Loss = 9.7374e-05, PNorm = 50.6357, GNorm = 0.1646, lr_0 = 3.2149e-04
Loss = 1.1982e-04, PNorm = 50.6378, GNorm = 0.3362, lr_0 = 3.2115e-04
Loss = 9.4020e-05, PNorm = 50.6432, GNorm = 0.2226, lr_0 = 3.2082e-04
Loss = 1.0744e-04, PNorm = 50.6469, GNorm = 0.2920, lr_0 = 3.2048e-04
Loss = 9.2741e-05, PNorm = 50.6515, GNorm = 0.2194, lr_0 = 3.2015e-04
Loss = 1.1921e-04, PNorm = 50.6537, GNorm = 0.4152, lr_0 = 3.1981e-04
Loss = 9.7987e-05, PNorm = 50.6595, GNorm = 0.3100, lr_0 = 3.1948e-04
Loss = 1.1014e-04, PNorm = 50.6628, GNorm = 0.3429, lr_0 = 3.1915e-04
Loss = 1.1092e-04, PNorm = 50.6666, GNorm = 0.3486, lr_0 = 3.1881e-04
Loss = 9.6657e-05, PNorm = 50.6693, GNorm = 0.1958, lr_0 = 3.1848e-04
Loss = 1.0562e-04, PNorm = 50.6733, GNorm = 0.2563, lr_0 = 3.1815e-04
Loss = 9.8027e-05, PNorm = 50.6760, GNorm = 0.1534, lr_0 = 3.1782e-04
Loss = 8.3144e-05, PNorm = 50.6792, GNorm = 0.2534, lr_0 = 3.1749e-04
Loss = 1.0368e-04, PNorm = 50.6840, GNorm = 0.2436, lr_0 = 3.1715e-04
Loss = 8.9221e-05, PNorm = 50.6884, GNorm = 0.3278, lr_0 = 3.1682e-04
Loss = 1.1892e-04, PNorm = 50.6904, GNorm = 0.2007, lr_0 = 3.1649e-04
Loss = 9.9391e-05, PNorm = 50.6953, GNorm = 0.2738, lr_0 = 3.1616e-04
Loss = 8.0753e-05, PNorm = 50.6994, GNorm = 0.2156, lr_0 = 3.1583e-04
Loss = 8.9561e-05, PNorm = 50.7022, GNorm = 0.2125, lr_0 = 3.1550e-04
Loss = 1.0467e-04, PNorm = 50.7047, GNorm = 0.1500, lr_0 = 3.1517e-04
Loss = 1.0622e-04, PNorm = 50.7085, GNorm = 0.4838, lr_0 = 3.1484e-04
Validation rmse logD = 0.584659
Validation R2 logD = 0.763254
Validation rmse logP = 0.465189
Validation R2 logP = 0.937194
Epoch 51
Train function
Loss = 4.4540e-05, PNorm = 50.7112, GNorm = 0.1608, lr_0 = 3.1448e-04
Loss = 7.2766e-05, PNorm = 50.7153, GNorm = 0.1537, lr_0 = 3.1415e-04
Loss = 8.1493e-05, PNorm = 50.7173, GNorm = 0.2497, lr_0 = 3.1383e-04
Loss = 8.6101e-05, PNorm = 50.7177, GNorm = 0.3158, lr_0 = 3.1350e-04
Loss = 9.3045e-05, PNorm = 50.7210, GNorm = 0.2966, lr_0 = 3.1317e-04
Loss = 8.4474e-05, PNorm = 50.7274, GNorm = 0.1486, lr_0 = 3.1284e-04
Loss = 7.7835e-05, PNorm = 50.7306, GNorm = 0.2474, lr_0 = 3.1252e-04
Loss = 8.7786e-05, PNorm = 50.7331, GNorm = 0.3225, lr_0 = 3.1219e-04
Loss = 1.1403e-04, PNorm = 50.7384, GNorm = 0.3177, lr_0 = 3.1187e-04
Loss = 8.3028e-05, PNorm = 50.7415, GNorm = 0.2656, lr_0 = 3.1154e-04
Loss = 8.7045e-05, PNorm = 50.7448, GNorm = 0.3392, lr_0 = 3.1122e-04
Loss = 1.0352e-04, PNorm = 50.7489, GNorm = 0.3896, lr_0 = 3.1089e-04
Loss = 1.0636e-04, PNorm = 50.7524, GNorm = 0.2948, lr_0 = 3.1057e-04
Loss = 1.1128e-04, PNorm = 50.7544, GNorm = 0.3175, lr_0 = 3.1024e-04
Loss = 9.0773e-05, PNorm = 50.7565, GNorm = 0.1970, lr_0 = 3.0992e-04
Loss = 1.1304e-04, PNorm = 50.7587, GNorm = 0.2662, lr_0 = 3.0959e-04
Loss = 1.1952e-04, PNorm = 50.7641, GNorm = 0.4208, lr_0 = 3.0927e-04
Loss = 1.1800e-04, PNorm = 50.7684, GNorm = 0.3130, lr_0 = 3.0895e-04
Loss = 1.0305e-04, PNorm = 50.7739, GNorm = 0.2726, lr_0 = 3.0863e-04
Loss = 1.0751e-04, PNorm = 50.7778, GNorm = 0.1483, lr_0 = 3.0830e-04
Loss = 1.0512e-04, PNorm = 50.7817, GNorm = 0.1540, lr_0 = 3.0798e-04
Loss = 1.0516e-04, PNorm = 50.7853, GNorm = 0.2049, lr_0 = 3.0766e-04
Loss = 1.0122e-04, PNorm = 50.7914, GNorm = 0.2917, lr_0 = 3.0734e-04
Validation rmse logD = 0.588967
Validation R2 logD = 0.759752
Validation rmse logP = 0.470456
Validation R2 logP = 0.935764
Epoch 52
Train function
Loss = 7.9948e-05, PNorm = 50.7959, GNorm = 0.2647, lr_0 = 3.0699e-04
Loss = 6.7843e-05, PNorm = 50.7993, GNorm = 0.1410, lr_0 = 3.0667e-04
Loss = 8.0580e-05, PNorm = 50.8032, GNorm = 0.2907, lr_0 = 3.0635e-04
Loss = 7.8992e-05, PNorm = 50.8070, GNorm = 0.2282, lr_0 = 3.0603e-04
Loss = 7.0857e-05, PNorm = 50.8085, GNorm = 0.1281, lr_0 = 3.0571e-04
Loss = 8.5004e-05, PNorm = 50.8103, GNorm = 0.1185, lr_0 = 3.0539e-04
Loss = 9.0815e-05, PNorm = 50.8144, GNorm = 0.4031, lr_0 = 3.0507e-04
Loss = 1.0606e-04, PNorm = 50.8179, GNorm = 0.2107, lr_0 = 3.0475e-04
Loss = 8.2723e-05, PNorm = 50.8211, GNorm = 0.2556, lr_0 = 3.0443e-04
Loss = 8.8048e-05, PNorm = 50.8238, GNorm = 0.3244, lr_0 = 3.0412e-04
Loss = 9.4125e-05, PNorm = 50.8242, GNorm = 0.2087, lr_0 = 3.0380e-04
Loss = 9.7239e-05, PNorm = 50.8267, GNorm = 0.2462, lr_0 = 3.0348e-04
Loss = 9.2184e-05, PNorm = 50.8331, GNorm = 0.2405, lr_0 = 3.0316e-04
Loss = 1.0485e-04, PNorm = 50.8367, GNorm = 0.2447, lr_0 = 3.0285e-04
Loss = 8.1049e-05, PNorm = 50.8377, GNorm = 0.1741, lr_0 = 3.0253e-04
Loss = 8.9788e-05, PNorm = 50.8412, GNorm = 0.1972, lr_0 = 3.0222e-04
Loss = 1.0135e-04, PNorm = 50.8443, GNorm = 0.2105, lr_0 = 3.0190e-04
Loss = 9.1836e-05, PNorm = 50.8466, GNorm = 0.2626, lr_0 = 3.0159e-04
Loss = 7.6578e-05, PNorm = 50.8476, GNorm = 0.1754, lr_0 = 3.0127e-04
Loss = 8.7016e-05, PNorm = 50.8537, GNorm = 0.1690, lr_0 = 3.0096e-04
Loss = 8.7719e-05, PNorm = 50.8603, GNorm = 0.1472, lr_0 = 3.0064e-04
Loss = 1.2068e-04, PNorm = 50.8630, GNorm = 0.3626, lr_0 = 3.0033e-04
Loss = 1.0427e-04, PNorm = 50.8671, GNorm = 0.3067, lr_0 = 3.0001e-04
Validation rmse logD = 0.584347
Validation R2 logD = 0.763506
Validation rmse logP = 0.475116
Validation R2 logP = 0.934485
Epoch 53
Train function
Loss = 7.7825e-05, PNorm = 50.8718, GNorm = 0.2264, lr_0 = 2.9970e-04
Loss = 8.8142e-05, PNorm = 50.8739, GNorm = 0.2958, lr_0 = 2.9939e-04
Loss = 1.0348e-04, PNorm = 50.8781, GNorm = 0.2236, lr_0 = 2.9908e-04
Loss = 8.1915e-05, PNorm = 50.8821, GNorm = 0.2587, lr_0 = 2.9876e-04
Loss = 9.7181e-05, PNorm = 50.8854, GNorm = 0.1781, lr_0 = 2.9845e-04
Loss = 7.9525e-05, PNorm = 50.8908, GNorm = 0.1984, lr_0 = 2.9814e-04
Loss = 7.3658e-05, PNorm = 50.8949, GNorm = 0.2129, lr_0 = 2.9783e-04
Loss = 7.1187e-05, PNorm = 50.8980, GNorm = 0.1653, lr_0 = 2.9752e-04
Loss = 7.9569e-05, PNorm = 50.9011, GNorm = 0.3195, lr_0 = 2.9721e-04
Loss = 8.0961e-05, PNorm = 50.9016, GNorm = 0.2098, lr_0 = 2.9690e-04
Loss = 9.3779e-05, PNorm = 50.9036, GNorm = 0.3288, lr_0 = 2.9659e-04
Loss = 9.0398e-05, PNorm = 50.9076, GNorm = 0.2249, lr_0 = 2.9628e-04
Loss = 1.2947e-04, PNorm = 50.9122, GNorm = 0.2597, lr_0 = 2.9597e-04
Loss = 1.4396e-04, PNorm = 50.9171, GNorm = 0.4104, lr_0 = 2.9566e-04
Loss = 1.1101e-04, PNorm = 50.9234, GNorm = 0.2121, lr_0 = 2.9535e-04
Loss = 1.0251e-04, PNorm = 50.9268, GNorm = 0.3872, lr_0 = 2.9504e-04
Loss = 9.3862e-05, PNorm = 50.9297, GNorm = 0.2370, lr_0 = 2.9474e-04
Loss = 9.3367e-05, PNorm = 50.9369, GNorm = 0.2266, lr_0 = 2.9443e-04
Loss = 1.0362e-04, PNorm = 50.9407, GNorm = 0.4554, lr_0 = 2.9412e-04
Loss = 8.7113e-05, PNorm = 50.9464, GNorm = 0.3189, lr_0 = 2.9381e-04
Loss = 9.6238e-05, PNorm = 50.9501, GNorm = 0.1958, lr_0 = 2.9351e-04
Loss = 9.6804e-05, PNorm = 50.9528, GNorm = 0.2618, lr_0 = 2.9320e-04
Validation rmse logD = 0.586073
Validation R2 logD = 0.762107
Validation rmse logP = 0.466562
Validation R2 logP = 0.936823
Epoch 54
Train function
Loss = 1.0013e-04, PNorm = 50.9579, GNorm = 0.2190, lr_0 = 2.9286e-04
Loss = 1.0437e-04, PNorm = 50.9626, GNorm = 0.2328, lr_0 = 2.9256e-04
Loss = 8.2074e-05, PNorm = 50.9678, GNorm = 0.1971, lr_0 = 2.9225e-04
Loss = 6.9063e-05, PNorm = 50.9723, GNorm = 0.1313, lr_0 = 2.9195e-04
Loss = 9.2685e-05, PNorm = 50.9753, GNorm = 0.2029, lr_0 = 2.9164e-04
Loss = 8.2261e-05, PNorm = 50.9776, GNorm = 0.2779, lr_0 = 2.9134e-04
Loss = 8.9863e-05, PNorm = 50.9797, GNorm = 0.1657, lr_0 = 2.9104e-04
Loss = 9.3612e-05, PNorm = 50.9819, GNorm = 0.2200, lr_0 = 2.9073e-04
Loss = 7.6302e-05, PNorm = 50.9841, GNorm = 0.2525, lr_0 = 2.9043e-04
Loss = 7.8291e-05, PNorm = 50.9880, GNorm = 0.2978, lr_0 = 2.9012e-04
Loss = 7.4549e-05, PNorm = 50.9932, GNorm = 0.3264, lr_0 = 2.8982e-04
Loss = 1.0183e-04, PNorm = 50.9962, GNorm = 0.4030, lr_0 = 2.8952e-04
Loss = 1.1780e-04, PNorm = 51.0009, GNorm = 0.3481, lr_0 = 2.8922e-04
Loss = 9.4949e-05, PNorm = 51.0023, GNorm = 0.3688, lr_0 = 2.8892e-04
Loss = 1.1299e-04, PNorm = 51.0054, GNorm = 0.1876, lr_0 = 2.8861e-04
Loss = 9.2694e-05, PNorm = 51.0095, GNorm = 0.3348, lr_0 = 2.8831e-04
Loss = 1.1736e-04, PNorm = 51.0150, GNorm = 0.2084, lr_0 = 2.8801e-04
Loss = 9.7582e-05, PNorm = 51.0168, GNorm = 0.1874, lr_0 = 2.8771e-04
Loss = 8.8747e-05, PNorm = 51.0181, GNorm = 0.2499, lr_0 = 2.8741e-04
Loss = 1.1906e-04, PNorm = 51.0229, GNorm = 0.2307, lr_0 = 2.8711e-04
Loss = 1.2857e-04, PNorm = 51.0301, GNorm = 0.2222, lr_0 = 2.8681e-04
Loss = 1.2314e-04, PNorm = 51.0339, GNorm = 0.5636, lr_0 = 2.8651e-04
Loss = 1.6872e-04, PNorm = 51.0419, GNorm = 0.5797, lr_0 = 2.8621e-04
Validation rmse logD = 0.584620
Validation R2 logD = 0.763285
Validation rmse logP = 0.473115
Validation R2 logP = 0.935036
Epoch 55
Train function
Loss = 1.3060e-04, PNorm = 51.0483, GNorm = 0.3123, lr_0 = 2.8591e-04
Loss = 1.2421e-04, PNorm = 51.0525, GNorm = 0.3675, lr_0 = 2.8562e-04
Loss = 1.2448e-04, PNorm = 51.0564, GNorm = 0.2020, lr_0 = 2.8532e-04
Loss = 9.1200e-05, PNorm = 51.0603, GNorm = 0.1586, lr_0 = 2.8502e-04
Loss = 1.1179e-04, PNorm = 51.0656, GNorm = 0.3707, lr_0 = 2.8472e-04
Loss = 1.2429e-04, PNorm = 51.0722, GNorm = 0.3271, lr_0 = 2.8443e-04
Loss = 1.2203e-04, PNorm = 51.0783, GNorm = 0.4343, lr_0 = 2.8413e-04
Loss = 1.1762e-04, PNorm = 51.0847, GNorm = 0.2928, lr_0 = 2.8383e-04
Loss = 8.0386e-05, PNorm = 51.0905, GNorm = 0.3289, lr_0 = 2.8354e-04
Loss = 9.4336e-05, PNorm = 51.0936, GNorm = 0.2780, lr_0 = 2.8324e-04
Loss = 7.4570e-05, PNorm = 51.0956, GNorm = 0.2323, lr_0 = 2.8294e-04
Loss = 7.3051e-05, PNorm = 51.0971, GNorm = 0.1266, lr_0 = 2.8265e-04
Loss = 8.5313e-05, PNorm = 51.0982, GNorm = 0.1447, lr_0 = 2.8235e-04
Loss = 1.0013e-04, PNorm = 51.1031, GNorm = 0.2858, lr_0 = 2.8206e-04
Loss = 8.8917e-05, PNorm = 51.1072, GNorm = 0.1647, lr_0 = 2.8176e-04
Loss = 9.2837e-05, PNorm = 51.1101, GNorm = 0.1689, lr_0 = 2.8147e-04
Loss = 9.4256e-05, PNorm = 51.1122, GNorm = 0.2448, lr_0 = 2.8118e-04
Loss = 8.2739e-05, PNorm = 51.1162, GNorm = 0.3817, lr_0 = 2.8088e-04
Loss = 7.9260e-05, PNorm = 51.1194, GNorm = 0.1883, lr_0 = 2.8059e-04
Loss = 6.4215e-05, PNorm = 51.1214, GNorm = 0.2773, lr_0 = 2.8030e-04
Loss = 9.0088e-05, PNorm = 51.1247, GNorm = 0.2345, lr_0 = 2.8000e-04
Loss = 7.3134e-05, PNorm = 51.1263, GNorm = 0.1596, lr_0 = 2.7971e-04
Validation rmse logD = 0.588406
Validation R2 logD = 0.760209
Validation rmse logP = 0.467036
Validation R2 logP = 0.936694
Epoch 56
Train function
Loss = 6.3465e-05, PNorm = 51.1289, GNorm = 0.1525, lr_0 = 2.7939e-04
Loss = 6.8151e-05, PNorm = 51.1303, GNorm = 0.1859, lr_0 = 2.7910e-04
Loss = 6.7558e-05, PNorm = 51.1308, GNorm = 0.1137, lr_0 = 2.7881e-04
Loss = 6.5170e-05, PNorm = 51.1329, GNorm = 0.1595, lr_0 = 2.7852e-04
Loss = 8.0272e-05, PNorm = 51.1341, GNorm = 0.1770, lr_0 = 2.7823e-04
Loss = 6.0531e-05, PNorm = 51.1359, GNorm = 0.1594, lr_0 = 2.7794e-04
Loss = 7.4120e-05, PNorm = 51.1402, GNorm = 0.1714, lr_0 = 2.7765e-04
Loss = 7.4449e-05, PNorm = 51.1435, GNorm = 0.1768, lr_0 = 2.7736e-04
Loss = 7.8690e-05, PNorm = 51.1449, GNorm = 0.1669, lr_0 = 2.7707e-04
Loss = 8.1230e-05, PNorm = 51.1457, GNorm = 0.1952, lr_0 = 2.7678e-04
Loss = 7.3106e-05, PNorm = 51.1465, GNorm = 0.2170, lr_0 = 2.7649e-04
Loss = 6.7693e-05, PNorm = 51.1491, GNorm = 0.1816, lr_0 = 2.7620e-04
Loss = 6.3920e-05, PNorm = 51.1508, GNorm = 0.1958, lr_0 = 2.7591e-04
Loss = 5.7023e-05, PNorm = 51.1556, GNorm = 0.1199, lr_0 = 2.7562e-04
Loss = 6.9284e-05, PNorm = 51.1582, GNorm = 0.2502, lr_0 = 2.7534e-04
Loss = 6.6455e-05, PNorm = 51.1604, GNorm = 0.2007, lr_0 = 2.7505e-04
Loss = 6.4970e-05, PNorm = 51.1625, GNorm = 0.1540, lr_0 = 2.7476e-04
Loss = 7.9054e-05, PNorm = 51.1649, GNorm = 0.2446, lr_0 = 2.7448e-04
Loss = 8.9447e-05, PNorm = 51.1677, GNorm = 0.2769, lr_0 = 2.7419e-04
Loss = 6.9422e-05, PNorm = 51.1706, GNorm = 0.2008, lr_0 = 2.7390e-04
Loss = 6.4864e-05, PNorm = 51.1747, GNorm = 0.1102, lr_0 = 2.7362e-04
Loss = 6.0451e-05, PNorm = 51.1785, GNorm = 0.1774, lr_0 = 2.7333e-04
Loss = 8.4430e-05, PNorm = 51.1823, GNorm = 0.1536, lr_0 = 2.7305e-04
Validation rmse logD = 0.584093
Validation R2 logD = 0.763712
Validation rmse logP = 0.468781
Validation R2 logP = 0.936221
Epoch 57
Train function
Loss = 7.2264e-05, PNorm = 51.1847, GNorm = 0.3394, lr_0 = 2.7276e-04
Loss = 8.0347e-05, PNorm = 51.1882, GNorm = 0.1460, lr_0 = 2.7248e-04
Loss = 6.0757e-05, PNorm = 51.1898, GNorm = 0.1542, lr_0 = 2.7219e-04
Loss = 5.9924e-05, PNorm = 51.1923, GNorm = 0.2313, lr_0 = 2.7191e-04
Loss = 6.3431e-05, PNorm = 51.1960, GNorm = 0.1192, lr_0 = 2.7162e-04
Loss = 7.0991e-05, PNorm = 51.1995, GNorm = 0.1070, lr_0 = 2.7134e-04
Loss = 6.8746e-05, PNorm = 51.2009, GNorm = 0.2845, lr_0 = 2.7106e-04
Loss = 6.1416e-05, PNorm = 51.2022, GNorm = 0.1527, lr_0 = 2.7077e-04
Loss = 6.3208e-05, PNorm = 51.2055, GNorm = 0.2911, lr_0 = 2.7049e-04
Loss = 8.7023e-05, PNorm = 51.2087, GNorm = 0.3087, lr_0 = 2.7021e-04
Loss = 7.7564e-05, PNorm = 51.2124, GNorm = 0.3048, lr_0 = 2.6993e-04
Loss = 7.0921e-05, PNorm = 51.2144, GNorm = 0.1763, lr_0 = 2.6965e-04
Loss = 5.3714e-05, PNorm = 51.2161, GNorm = 0.1786, lr_0 = 2.6936e-04
Loss = 6.5724e-05, PNorm = 51.2192, GNorm = 0.1390, lr_0 = 2.6908e-04
Loss = 7.9211e-05, PNorm = 51.2227, GNorm = 0.1640, lr_0 = 2.6880e-04
Loss = 7.0377e-05, PNorm = 51.2244, GNorm = 0.1806, lr_0 = 2.6852e-04
Loss = 6.9753e-05, PNorm = 51.2255, GNorm = 0.1722, lr_0 = 2.6824e-04
Loss = 7.5728e-05, PNorm = 51.2265, GNorm = 0.2168, lr_0 = 2.6796e-04
Loss = 6.9472e-05, PNorm = 51.2300, GNorm = 0.3405, lr_0 = 2.6768e-04
Loss = 7.2406e-05, PNorm = 51.2332, GNorm = 0.1570, lr_0 = 2.6740e-04
Loss = 7.4101e-05, PNorm = 51.2354, GNorm = 0.1659, lr_0 = 2.6712e-04
Loss = 6.5673e-05, PNorm = 51.2370, GNorm = 0.1856, lr_0 = 2.6684e-04
Validation rmse logD = 0.585516
Validation R2 logD = 0.762559
Validation rmse logP = 0.468942
Validation R2 logP = 0.936177
Epoch 58
Train function
Loss = 8.3526e-05, PNorm = 51.2406, GNorm = 0.2853, lr_0 = 2.6654e-04
Loss = 6.4574e-05, PNorm = 51.2444, GNorm = 0.3032, lr_0 = 2.6626e-04
Loss = 6.5924e-05, PNorm = 51.2481, GNorm = 0.2271, lr_0 = 2.6598e-04
Loss = 6.4819e-05, PNorm = 51.2500, GNorm = 0.1544, lr_0 = 2.6570e-04
Loss = 6.5722e-05, PNorm = 51.2518, GNorm = 0.1469, lr_0 = 2.6543e-04
Loss = 5.0133e-05, PNorm = 51.2553, GNorm = 0.1030, lr_0 = 2.6515e-04
Loss = 6.1134e-05, PNorm = 51.2591, GNorm = 0.2853, lr_0 = 2.6487e-04
Loss = 6.8651e-05, PNorm = 51.2613, GNorm = 0.1623, lr_0 = 2.6460e-04
Loss = 6.8536e-05, PNorm = 51.2638, GNorm = 0.1945, lr_0 = 2.6432e-04
Loss = 6.7548e-05, PNorm = 51.2653, GNorm = 0.2755, lr_0 = 2.6405e-04
Loss = 6.2178e-05, PNorm = 51.2673, GNorm = 0.1661, lr_0 = 2.6377e-04
Loss = 5.3671e-05, PNorm = 51.2716, GNorm = 0.2372, lr_0 = 2.6349e-04
Loss = 6.6904e-05, PNorm = 51.2751, GNorm = 0.3159, lr_0 = 2.6322e-04
Loss = 6.6300e-05, PNorm = 51.2790, GNorm = 0.1417, lr_0 = 2.6294e-04
Loss = 8.8423e-05, PNorm = 51.2800, GNorm = 0.1405, lr_0 = 2.6267e-04
Loss = 7.4170e-05, PNorm = 51.2839, GNorm = 0.1873, lr_0 = 2.6240e-04
Loss = 7.9287e-05, PNorm = 51.2878, GNorm = 0.4940, lr_0 = 2.6212e-04
Loss = 6.5279e-05, PNorm = 51.2937, GNorm = 0.2235, lr_0 = 2.6185e-04
Loss = 6.0048e-05, PNorm = 51.2958, GNorm = 0.2919, lr_0 = 2.6158e-04
Loss = 7.4208e-05, PNorm = 51.2986, GNorm = 0.1642, lr_0 = 2.6130e-04
Loss = 6.9828e-05, PNorm = 51.3005, GNorm = 0.1607, lr_0 = 2.6103e-04
Loss = 7.9128e-05, PNorm = 51.3054, GNorm = 0.2091, lr_0 = 2.6076e-04
Loss = 8.2675e-05, PNorm = 51.3043, GNorm = 0.1821, lr_0 = 2.6048e-04
Validation rmse logD = 0.584793
Validation R2 logD = 0.763146
Validation rmse logP = 0.466873
Validation R2 logP = 0.936739
Epoch 59
Train function
Loss = 7.2608e-05, PNorm = 51.3075, GNorm = 0.1728, lr_0 = 2.6021e-04
Loss = 6.3009e-05, PNorm = 51.3092, GNorm = 0.1639, lr_0 = 2.5994e-04
Loss = 6.3356e-05, PNorm = 51.3092, GNorm = 0.1848, lr_0 = 2.5967e-04
Loss = 6.0353e-05, PNorm = 51.3119, GNorm = 0.2028, lr_0 = 2.5940e-04
Loss = 5.5244e-05, PNorm = 51.3151, GNorm = 0.2222, lr_0 = 2.5913e-04
Loss = 6.1370e-05, PNorm = 51.3167, GNorm = 0.1906, lr_0 = 2.5886e-04
Loss = 5.6578e-05, PNorm = 51.3203, GNorm = 0.1378, lr_0 = 2.5859e-04
Loss = 6.5658e-05, PNorm = 51.3222, GNorm = 0.2007, lr_0 = 2.5832e-04
Loss = 5.3177e-05, PNorm = 51.3239, GNorm = 0.1336, lr_0 = 2.5805e-04
Loss = 5.1141e-05, PNorm = 51.3267, GNorm = 0.1233, lr_0 = 2.5778e-04
Loss = 7.6448e-05, PNorm = 51.3279, GNorm = 0.2655, lr_0 = 2.5751e-04
Loss = 7.7537e-05, PNorm = 51.3313, GNorm = 0.1484, lr_0 = 2.5724e-04
Loss = 5.7764e-05, PNorm = 51.3334, GNorm = 0.0949, lr_0 = 2.5697e-04
Loss = 7.0617e-05, PNorm = 51.3352, GNorm = 0.1725, lr_0 = 2.5670e-04
Loss = 6.3623e-05, PNorm = 51.3402, GNorm = 0.1785, lr_0 = 2.5644e-04
Loss = 6.7803e-05, PNorm = 51.3439, GNorm = 0.1431, lr_0 = 2.5617e-04
Loss = 6.8806e-05, PNorm = 51.3475, GNorm = 0.1535, lr_0 = 2.5590e-04
Loss = 6.3232e-05, PNorm = 51.3504, GNorm = 0.1442, lr_0 = 2.5563e-04
Loss = 5.8576e-05, PNorm = 51.3544, GNorm = 0.2516, lr_0 = 2.5537e-04
Loss = 6.2475e-05, PNorm = 51.3571, GNorm = 0.1414, lr_0 = 2.5510e-04
Loss = 5.5352e-05, PNorm = 51.3598, GNorm = 0.1734, lr_0 = 2.5483e-04
Loss = 6.0814e-05, PNorm = 51.3634, GNorm = 0.1015, lr_0 = 2.5457e-04
Validation rmse logD = 0.586390
Validation R2 logD = 0.761850
Validation rmse logP = 0.464154
Validation R2 logP = 0.937473
Epoch 60
Train function
Loss = 7.8188e-05, PNorm = 51.3651, GNorm = 0.2417, lr_0 = 2.5428e-04
Loss = 5.8246e-05, PNorm = 51.3665, GNorm = 0.1654, lr_0 = 2.5401e-04
Loss = 6.4711e-05, PNorm = 51.3697, GNorm = 0.1733, lr_0 = 2.5375e-04
Loss = 6.1861e-05, PNorm = 51.3709, GNorm = 0.2183, lr_0 = 2.5348e-04
Loss = 6.6856e-05, PNorm = 51.3735, GNorm = 0.1540, lr_0 = 2.5322e-04
Loss = 5.5157e-05, PNorm = 51.3764, GNorm = 0.1613, lr_0 = 2.5295e-04
Loss = 5.7646e-05, PNorm = 51.3802, GNorm = 0.0999, lr_0 = 2.5269e-04
Loss = 5.8359e-05, PNorm = 51.3832, GNorm = 0.1298, lr_0 = 2.5242e-04
Loss = 4.8648e-05, PNorm = 51.3870, GNorm = 0.2065, lr_0 = 2.5216e-04
Loss = 5.3611e-05, PNorm = 51.3890, GNorm = 0.4004, lr_0 = 2.5190e-04
Loss = 6.7182e-05, PNorm = 51.3891, GNorm = 0.2077, lr_0 = 2.5163e-04
Loss = 6.0684e-05, PNorm = 51.3922, GNorm = 0.1599, lr_0 = 2.5137e-04
Loss = 5.8891e-05, PNorm = 51.3958, GNorm = 0.1073, lr_0 = 2.5111e-04
Loss = 5.8668e-05, PNorm = 51.3987, GNorm = 0.2658, lr_0 = 2.5085e-04
Loss = 6.6259e-05, PNorm = 51.3999, GNorm = 0.1820, lr_0 = 2.5059e-04
Loss = 7.7278e-05, PNorm = 51.4032, GNorm = 0.2678, lr_0 = 2.5032e-04
Loss = 6.0402e-05, PNorm = 51.4044, GNorm = 0.1491, lr_0 = 2.5006e-04
Loss = 7.8768e-05, PNorm = 51.4059, GNorm = 0.2943, lr_0 = 2.4980e-04
Loss = 9.1373e-05, PNorm = 51.4100, GNorm = 0.2240, lr_0 = 2.4954e-04
Loss = 6.7695e-05, PNorm = 51.4133, GNorm = 0.2604, lr_0 = 2.4928e-04
Loss = 1.0846e-04, PNorm = 51.4169, GNorm = 0.2806, lr_0 = 2.4902e-04
Loss = 6.7465e-05, PNorm = 51.4211, GNorm = 0.2572, lr_0 = 2.4876e-04
Loss = 5.5361e-05, PNorm = 51.4250, GNorm = 0.1692, lr_0 = 2.4850e-04
Validation rmse logD = 0.585115
Validation R2 logD = 0.762885
Validation rmse logP = 0.468085
Validation R2 logP = 0.936410
Epoch 61
Train function
Loss = 5.9807e-05, PNorm = 51.4282, GNorm = 0.1893, lr_0 = 2.4824e-04
Loss = 7.9260e-05, PNorm = 51.4304, GNorm = 0.1226, lr_0 = 2.4798e-04
Loss = 6.9160e-05, PNorm = 51.4348, GNorm = 0.3166, lr_0 = 2.4772e-04
Loss = 6.6795e-05, PNorm = 51.4363, GNorm = 0.2261, lr_0 = 2.4747e-04
Loss = 5.4458e-05, PNorm = 51.4390, GNorm = 0.1345, lr_0 = 2.4721e-04
Loss = 5.4908e-05, PNorm = 51.4443, GNorm = 0.1649, lr_0 = 2.4695e-04
Loss = 4.7091e-05, PNorm = 51.4465, GNorm = 0.1217, lr_0 = 2.4669e-04
Loss = 5.1019e-05, PNorm = 51.4481, GNorm = 0.1222, lr_0 = 2.4643e-04
Loss = 5.3645e-05, PNorm = 51.4510, GNorm = 0.1500, lr_0 = 2.4618e-04
Loss = 5.4627e-05, PNorm = 51.4519, GNorm = 0.2232, lr_0 = 2.4592e-04
Loss = 5.3394e-05, PNorm = 51.4539, GNorm = 0.1663, lr_0 = 2.4566e-04
Loss = 5.3294e-05, PNorm = 51.4559, GNorm = 0.1502, lr_0 = 2.4541e-04
Loss = 5.6775e-05, PNorm = 51.4596, GNorm = 0.1770, lr_0 = 2.4515e-04
Loss = 6.2437e-05, PNorm = 51.4625, GNorm = 0.4048, lr_0 = 2.4489e-04
Loss = 6.1370e-05, PNorm = 51.4648, GNorm = 0.3229, lr_0 = 2.4464e-04
Loss = 5.5831e-05, PNorm = 51.4683, GNorm = 0.1268, lr_0 = 2.4438e-04
Loss = 5.4775e-05, PNorm = 51.4693, GNorm = 0.2165, lr_0 = 2.4413e-04
Loss = 5.8308e-05, PNorm = 51.4706, GNorm = 0.1153, lr_0 = 2.4387e-04
Loss = 6.2838e-05, PNorm = 51.4740, GNorm = 0.3074, lr_0 = 2.4362e-04
Loss = 7.5336e-05, PNorm = 51.4780, GNorm = 0.1906, lr_0 = 2.4337e-04
Loss = 6.3494e-05, PNorm = 51.4824, GNorm = 0.1504, lr_0 = 2.4311e-04
Loss = 7.0413e-05, PNorm = 51.4843, GNorm = 0.2145, lr_0 = 2.4286e-04
Validation rmse logD = 0.591524
Validation R2 logD = 0.757662
Validation rmse logP = 0.465880
Validation R2 logP = 0.937008
Epoch 62
Train function
Loss = 9.1891e-05, PNorm = 51.4851, GNorm = 0.3302, lr_0 = 2.4258e-04
Loss = 6.2546e-05, PNorm = 51.4895, GNorm = 0.2768, lr_0 = 2.4233e-04
Loss = 6.3146e-05, PNorm = 51.4917, GNorm = 0.2320, lr_0 = 2.4207e-04
Loss = 5.4283e-05, PNorm = 51.4941, GNorm = 0.1355, lr_0 = 2.4182e-04
Loss = 5.4448e-05, PNorm = 51.4975, GNorm = 0.1888, lr_0 = 2.4157e-04
Loss = 5.8601e-05, PNorm = 51.5015, GNorm = 0.2173, lr_0 = 2.4132e-04
Loss = 6.0523e-05, PNorm = 51.5056, GNorm = 0.1443, lr_0 = 2.4106e-04
Loss = 5.5389e-05, PNorm = 51.5087, GNorm = 0.1555, lr_0 = 2.4081e-04
Loss = 5.3467e-05, PNorm = 51.5131, GNorm = 0.1058, lr_0 = 2.4056e-04
Loss = 4.8651e-05, PNorm = 51.5140, GNorm = 0.1762, lr_0 = 2.4031e-04
Loss = 4.1413e-05, PNorm = 51.5161, GNorm = 0.1809, lr_0 = 2.4006e-04
Loss = 5.7662e-05, PNorm = 51.5176, GNorm = 0.2597, lr_0 = 2.3981e-04
Loss = 8.2479e-05, PNorm = 51.5203, GNorm = 0.3761, lr_0 = 2.3956e-04
Loss = 6.7072e-05, PNorm = 51.5214, GNorm = 0.2117, lr_0 = 2.3931e-04
Loss = 7.6377e-05, PNorm = 51.5245, GNorm = 0.2053, lr_0 = 2.3906e-04
Loss = 6.3915e-05, PNorm = 51.5288, GNorm = 0.2503, lr_0 = 2.3881e-04
Loss = 5.3146e-05, PNorm = 51.5317, GNorm = 0.1777, lr_0 = 2.3856e-04
Loss = 6.1680e-05, PNorm = 51.5339, GNorm = 0.1192, lr_0 = 2.3831e-04
Loss = 5.9214e-05, PNorm = 51.5363, GNorm = 0.1500, lr_0 = 2.3806e-04
Loss = 5.0697e-05, PNorm = 51.5397, GNorm = 0.1838, lr_0 = 2.3781e-04
Loss = 4.2745e-05, PNorm = 51.5405, GNorm = 0.1889, lr_0 = 2.3756e-04
Loss = 5.4016e-05, PNorm = 51.5423, GNorm = 0.1223, lr_0 = 2.3732e-04
Loss = 4.2366e-05, PNorm = 51.5441, GNorm = 0.1094, lr_0 = 2.3707e-04
Validation rmse logD = 0.586797
Validation R2 logD = 0.761519
Validation rmse logP = 0.464781
Validation R2 logP = 0.937304
Epoch 63
Train function
Loss = 5.0370e-05, PNorm = 51.5455, GNorm = 0.2875, lr_0 = 2.3682e-04
Loss = 5.9118e-05, PNorm = 51.5477, GNorm = 0.0923, lr_0 = 2.3657e-04
Loss = 5.2859e-05, PNorm = 51.5504, GNorm = 0.2024, lr_0 = 2.3633e-04
Loss = 4.5953e-05, PNorm = 51.5512, GNorm = 0.1972, lr_0 = 2.3608e-04
Loss = 4.4291e-05, PNorm = 51.5538, GNorm = 0.1353, lr_0 = 2.3583e-04
Loss = 5.3210e-05, PNorm = 51.5556, GNorm = 0.2110, lr_0 = 2.3559e-04
Loss = 4.7794e-05, PNorm = 51.5574, GNorm = 0.0910, lr_0 = 2.3534e-04
Loss = 5.5512e-05, PNorm = 51.5612, GNorm = 0.1705, lr_0 = 2.3510e-04
Loss = 4.6557e-05, PNorm = 51.5645, GNorm = 0.0882, lr_0 = 2.3485e-04
Loss = 4.9582e-05, PNorm = 51.5641, GNorm = 0.1563, lr_0 = 2.3461e-04
Loss = 5.2826e-05, PNorm = 51.5666, GNorm = 0.2368, lr_0 = 2.3436e-04
Loss = 6.0304e-05, PNorm = 51.5687, GNorm = 0.1175, lr_0 = 2.3412e-04
Loss = 4.6186e-05, PNorm = 51.5695, GNorm = 0.1527, lr_0 = 2.3387e-04
Loss = 4.5809e-05, PNorm = 51.5707, GNorm = 0.1510, lr_0 = 2.3363e-04
Loss = 5.2678e-05, PNorm = 51.5717, GNorm = 0.1792, lr_0 = 2.3338e-04
Loss = 6.2507e-05, PNorm = 51.5737, GNorm = 0.1611, lr_0 = 2.3314e-04
Loss = 6.6213e-05, PNorm = 51.5754, GNorm = 0.1150, lr_0 = 2.3290e-04
Loss = 8.4558e-05, PNorm = 51.5795, GNorm = 0.1762, lr_0 = 2.3265e-04
Loss = 8.5201e-05, PNorm = 51.5832, GNorm = 0.2701, lr_0 = 2.3241e-04
Loss = 8.0040e-05, PNorm = 51.5866, GNorm = 0.3878, lr_0 = 2.3217e-04
Loss = 9.2737e-05, PNorm = 51.5912, GNorm = 0.2178, lr_0 = 2.3193e-04
Loss = 9.4637e-05, PNorm = 51.5966, GNorm = 0.4002, lr_0 = 2.3169e-04
Loss = 8.5581e-05, PNorm = 51.5998, GNorm = 0.2211, lr_0 = 2.3144e-04
Loss = 1.8588e-04, PNorm = 51.6004, GNorm = 0.3463, lr_0 = 2.3142e-04
Validation rmse logD = 0.617071
Validation R2 logD = 0.736277
Validation rmse logP = 0.469375
Validation R2 logP = 0.936059
Epoch 64
Train function
Loss = 1.0494e-04, PNorm = 51.6067, GNorm = 0.1808, lr_0 = 2.3118e-04
Loss = 6.4791e-05, PNorm = 51.6089, GNorm = 0.1830, lr_0 = 2.3094e-04
Loss = 6.8241e-05, PNorm = 51.6090, GNorm = 0.1557, lr_0 = 2.3070e-04
Loss = 6.5160e-05, PNorm = 51.6100, GNorm = 0.2184, lr_0 = 2.3045e-04
Loss = 5.9802e-05, PNorm = 51.6128, GNorm = 0.2541, lr_0 = 2.3021e-04
Loss = 6.2340e-05, PNorm = 51.6145, GNorm = 0.1485, lr_0 = 2.2997e-04
Loss = 5.4745e-05, PNorm = 51.6176, GNorm = 0.1517, lr_0 = 2.2973e-04
Loss = 4.4622e-05, PNorm = 51.6206, GNorm = 0.1614, lr_0 = 2.2949e-04
Loss = 4.7251e-05, PNorm = 51.6229, GNorm = 0.1743, lr_0 = 2.2925e-04
Loss = 5.3426e-05, PNorm = 51.6246, GNorm = 0.2339, lr_0 = 2.2902e-04
Loss = 6.1541e-05, PNorm = 51.6249, GNorm = 0.2572, lr_0 = 2.2878e-04
Loss = 4.6773e-05, PNorm = 51.6260, GNorm = 0.2204, lr_0 = 2.2854e-04
Loss = 5.9817e-05, PNorm = 51.6292, GNorm = 0.1575, lr_0 = 2.2830e-04
Loss = 4.9176e-05, PNorm = 51.6317, GNorm = 0.2527, lr_0 = 2.2806e-04
Loss = 5.6537e-05, PNorm = 51.6337, GNorm = 0.1877, lr_0 = 2.2782e-04
Loss = 5.4263e-05, PNorm = 51.6377, GNorm = 0.1988, lr_0 = 2.2758e-04
Loss = 5.2383e-05, PNorm = 51.6392, GNorm = 0.1806, lr_0 = 2.2735e-04
Loss = 5.4564e-05, PNorm = 51.6397, GNorm = 0.1686, lr_0 = 2.2711e-04
Loss = 5.3373e-05, PNorm = 51.6423, GNorm = 0.1869, lr_0 = 2.2687e-04
Loss = 4.9568e-05, PNorm = 51.6444, GNorm = 0.1449, lr_0 = 2.2664e-04
Loss = 3.7649e-05, PNorm = 51.6461, GNorm = 0.1667, lr_0 = 2.2640e-04
Loss = 4.9130e-05, PNorm = 51.6475, GNorm = 0.2178, lr_0 = 2.2616e-04
Validation rmse logD = 0.582756
Validation R2 logD = 0.764793
Validation rmse logP = 0.466228
Validation R2 logP = 0.936913
Epoch 65
Train function
Loss = 3.8096e-05, PNorm = 51.6512, GNorm = 0.1436, lr_0 = 2.2593e-04
Loss = 4.4856e-05, PNorm = 51.6543, GNorm = 0.1196, lr_0 = 2.2569e-04
Loss = 3.6678e-05, PNorm = 51.6557, GNorm = 0.1310, lr_0 = 2.2546e-04
Loss = 4.4617e-05, PNorm = 51.6560, GNorm = 0.1763, lr_0 = 2.2522e-04
Loss = 4.7236e-05, PNorm = 51.6591, GNorm = 0.1387, lr_0 = 2.2499e-04
Loss = 3.6407e-05, PNorm = 51.6628, GNorm = 0.1324, lr_0 = 2.2475e-04
Loss = 4.8712e-05, PNorm = 51.6640, GNorm = 0.1171, lr_0 = 2.2452e-04
Loss = 5.6187e-05, PNorm = 51.6644, GNorm = 0.1869, lr_0 = 2.2428e-04
Loss = 4.9082e-05, PNorm = 51.6641, GNorm = 0.1887, lr_0 = 2.2405e-04
Loss = 5.7179e-05, PNorm = 51.6644, GNorm = 0.2822, lr_0 = 2.2381e-04
Loss = 5.5003e-05, PNorm = 51.6649, GNorm = 0.2720, lr_0 = 2.2358e-04
Loss = 5.3435e-05, PNorm = 51.6669, GNorm = 0.1973, lr_0 = 2.2335e-04
Loss = 4.6784e-05, PNorm = 51.6685, GNorm = 0.1026, lr_0 = 2.2311e-04
Loss = 4.6609e-05, PNorm = 51.6694, GNorm = 0.1245, lr_0 = 2.2288e-04
Loss = 4.8504e-05, PNorm = 51.6723, GNorm = 0.2589, lr_0 = 2.2265e-04
Loss = 5.3591e-05, PNorm = 51.6751, GNorm = 0.1544, lr_0 = 2.2242e-04
Loss = 4.9702e-05, PNorm = 51.6772, GNorm = 0.1364, lr_0 = 2.2218e-04
Loss = 4.7641e-05, PNorm = 51.6782, GNorm = 0.1890, lr_0 = 2.2195e-04
Loss = 4.3875e-05, PNorm = 51.6803, GNorm = 0.1320, lr_0 = 2.2172e-04
Loss = 5.3872e-05, PNorm = 51.6823, GNorm = 0.1293, lr_0 = 2.2149e-04
Loss = 4.3892e-05, PNorm = 51.6862, GNorm = 0.1140, lr_0 = 2.2126e-04
Loss = 4.1123e-05, PNorm = 51.6875, GNorm = 0.0826, lr_0 = 2.2103e-04
Loss = 5.0601e-05, PNorm = 51.6893, GNorm = 0.1915, lr_0 = 2.2080e-04
Validation rmse logD = 0.585844
Validation R2 logD = 0.762293
Validation rmse logP = 0.468461
Validation R2 logP = 0.936308
Epoch 66
Train function
Loss = 3.8278e-05, PNorm = 51.6910, GNorm = 0.2492, lr_0 = 2.2054e-04
Loss = 4.1089e-05, PNorm = 51.6950, GNorm = 0.1074, lr_0 = 2.2031e-04
Loss = 4.0634e-05, PNorm = 51.6990, GNorm = 0.1219, lr_0 = 2.2008e-04
Loss = 3.2417e-05, PNorm = 51.7020, GNorm = 0.2042, lr_0 = 2.1985e-04
Loss = 4.8165e-05, PNorm = 51.7029, GNorm = 0.2096, lr_0 = 2.1962e-04
Loss = 5.1166e-05, PNorm = 51.7038, GNorm = 0.1606, lr_0 = 2.1939e-04
Loss = 4.2507e-05, PNorm = 51.7068, GNorm = 0.2757, lr_0 = 2.1916e-04
Loss = 4.8670e-05, PNorm = 51.7074, GNorm = 0.1442, lr_0 = 2.1894e-04
Loss = 3.9552e-05, PNorm = 51.7096, GNorm = 0.0854, lr_0 = 2.1871e-04
Loss = 4.0894e-05, PNorm = 51.7109, GNorm = 0.1773, lr_0 = 2.1848e-04
Loss = 3.6772e-05, PNorm = 51.7143, GNorm = 0.1959, lr_0 = 2.1825e-04
Loss = 4.7234e-05, PNorm = 51.7160, GNorm = 0.2391, lr_0 = 2.1802e-04
Loss = 3.1871e-05, PNorm = 51.7179, GNorm = 0.1041, lr_0 = 2.1780e-04
Loss = 4.3640e-05, PNorm = 51.7201, GNorm = 0.1095, lr_0 = 2.1757e-04
Loss = 5.8224e-05, PNorm = 51.7224, GNorm = 0.0966, lr_0 = 2.1734e-04
Loss = 4.8050e-05, PNorm = 51.7250, GNorm = 0.1867, lr_0 = 2.1711e-04
Loss = 3.7580e-05, PNorm = 51.7261, GNorm = 0.1839, lr_0 = 2.1689e-04
Loss = 4.1297e-05, PNorm = 51.7279, GNorm = 0.1234, lr_0 = 2.1666e-04
Loss = 4.9826e-05, PNorm = 51.7285, GNorm = 0.1020, lr_0 = 2.1644e-04
Loss = 5.8360e-05, PNorm = 51.7306, GNorm = 0.1136, lr_0 = 2.1621e-04
Loss = 4.2892e-05, PNorm = 51.7318, GNorm = 0.1639, lr_0 = 2.1598e-04
Loss = 4.5067e-05, PNorm = 51.7321, GNorm = 0.3335, lr_0 = 2.1576e-04
Validation rmse logD = 0.582788
Validation R2 logD = 0.764766
Validation rmse logP = 0.465518
Validation R2 logP = 0.937105
Epoch 67
Train function
Loss = 4.7181e-05, PNorm = 51.7356, GNorm = 0.2578, lr_0 = 2.1553e-04
Loss = 5.1767e-05, PNorm = 51.7376, GNorm = 0.2374, lr_0 = 2.1531e-04
Loss = 3.9048e-05, PNorm = 51.7380, GNorm = 0.1168, lr_0 = 2.1508e-04
Loss = 4.6442e-05, PNorm = 51.7391, GNorm = 0.1668, lr_0 = 2.1486e-04
Loss = 3.8180e-05, PNorm = 51.7413, GNorm = 0.1498, lr_0 = 2.1464e-04
Loss = 4.5302e-05, PNorm = 51.7434, GNorm = 0.1089, lr_0 = 2.1441e-04
Loss = 3.2765e-05, PNorm = 51.7443, GNorm = 0.1532, lr_0 = 2.1419e-04
Loss = 3.6077e-05, PNorm = 51.7463, GNorm = 0.1853, lr_0 = 2.1396e-04
Loss = 3.8869e-05, PNorm = 51.7482, GNorm = 0.0801, lr_0 = 2.1374e-04
Loss = 3.4029e-05, PNorm = 51.7486, GNorm = 0.1456, lr_0 = 2.1352e-04
Loss = 4.4982e-05, PNorm = 51.7484, GNorm = 0.1476, lr_0 = 2.1329e-04
Loss = 4.1039e-05, PNorm = 51.7505, GNorm = 0.1014, lr_0 = 2.1307e-04
Loss = 4.1628e-05, PNorm = 51.7540, GNorm = 0.1406, lr_0 = 2.1285e-04
Loss = 3.9462e-05, PNorm = 51.7561, GNorm = 0.1627, lr_0 = 2.1263e-04
Loss = 4.8529e-05, PNorm = 51.7561, GNorm = 0.1273, lr_0 = 2.1241e-04
Loss = 4.7141e-05, PNorm = 51.7602, GNorm = 0.1943, lr_0 = 2.1218e-04
Loss = 3.9483e-05, PNorm = 51.7624, GNorm = 0.0957, lr_0 = 2.1196e-04
Loss = 4.9240e-05, PNorm = 51.7633, GNorm = 0.1405, lr_0 = 2.1174e-04
Loss = 3.8244e-05, PNorm = 51.7653, GNorm = 0.1360, lr_0 = 2.1152e-04
Loss = 5.2405e-05, PNorm = 51.7670, GNorm = 0.2895, lr_0 = 2.1130e-04
Loss = 4.9740e-05, PNorm = 51.7692, GNorm = 0.1346, lr_0 = 2.1108e-04
Loss = 5.0057e-05, PNorm = 51.7728, GNorm = 0.1132, lr_0 = 2.1086e-04
Loss = 3.9124e-05, PNorm = 51.7733, GNorm = 0.0904, lr_0 = 2.1064e-04
Validation rmse logD = 0.580794
Validation R2 logD = 0.766374
Validation rmse logP = 0.467629
Validation R2 logP = 0.936534
Epoch 68
Train function
Loss = 3.8461e-05, PNorm = 51.7758, GNorm = 0.2643, lr_0 = 2.1040e-04
Loss = 3.3990e-05, PNorm = 51.7782, GNorm = 0.1218, lr_0 = 2.1018e-04
Loss = 4.3731e-05, PNorm = 51.7784, GNorm = 0.1750, lr_0 = 2.0996e-04
Loss = 4.1491e-05, PNorm = 51.7800, GNorm = 0.1969, lr_0 = 2.0974e-04
Loss = 3.7422e-05, PNorm = 51.7818, GNorm = 0.2170, lr_0 = 2.0952e-04
Loss = 3.5038e-05, PNorm = 51.7831, GNorm = 0.2078, lr_0 = 2.0930e-04
Loss = 3.8420e-05, PNorm = 51.7865, GNorm = 0.1093, lr_0 = 2.0908e-04
Loss = 3.4689e-05, PNorm = 51.7893, GNorm = 0.1037, lr_0 = 2.0886e-04
Loss = 3.9209e-05, PNorm = 51.7909, GNorm = 0.1398, lr_0 = 2.0865e-04
Loss = 3.1206e-05, PNorm = 51.7909, GNorm = 0.1081, lr_0 = 2.0843e-04
Loss = 3.1295e-05, PNorm = 51.7920, GNorm = 0.1824, lr_0 = 2.0821e-04
Loss = 3.7681e-05, PNorm = 51.7945, GNorm = 0.1699, lr_0 = 2.0799e-04
Loss = 3.0721e-05, PNorm = 51.7957, GNorm = 0.1745, lr_0 = 2.0778e-04
Loss = 3.7184e-05, PNorm = 51.7973, GNorm = 0.1980, lr_0 = 2.0756e-04
Loss = 3.5672e-05, PNorm = 51.7999, GNorm = 0.1404, lr_0 = 2.0734e-04
Loss = 3.6898e-05, PNorm = 51.8014, GNorm = 0.0915, lr_0 = 2.0713e-04
Loss = 3.9504e-05, PNorm = 51.8052, GNorm = 0.1457, lr_0 = 2.0691e-04
Loss = 3.8009e-05, PNorm = 51.8067, GNorm = 0.1222, lr_0 = 2.0669e-04
Loss = 4.5675e-05, PNorm = 51.8072, GNorm = 0.1367, lr_0 = 2.0648e-04
Loss = 5.9631e-05, PNorm = 51.8112, GNorm = 0.2025, lr_0 = 2.0626e-04
Loss = 5.5685e-05, PNorm = 51.8138, GNorm = 0.2161, lr_0 = 2.0605e-04
Loss = 5.5387e-05, PNorm = 51.8157, GNorm = 0.1821, lr_0 = 2.0583e-04
Validation rmse logD = 0.580033
Validation R2 logD = 0.766985
Validation rmse logP = 0.469481
Validation R2 logP = 0.936030
Epoch 69
Train function
Loss = 6.1263e-05, PNorm = 51.8183, GNorm = 0.1869, lr_0 = 2.0562e-04
Loss = 5.1245e-05, PNorm = 51.8208, GNorm = 0.1622, lr_0 = 2.0540e-04
Loss = 5.1497e-05, PNorm = 51.8244, GNorm = 0.2004, lr_0 = 2.0519e-04
Loss = 5.3292e-05, PNorm = 51.8280, GNorm = 0.2804, lr_0 = 2.0497e-04
Loss = 6.3150e-05, PNorm = 51.8307, GNorm = 0.2185, lr_0 = 2.0476e-04
Loss = 6.0263e-05, PNorm = 51.8332, GNorm = 0.2396, lr_0 = 2.0455e-04
Loss = 5.7780e-05, PNorm = 51.8358, GNorm = 0.3564, lr_0 = 2.0433e-04
Loss = 4.4773e-05, PNorm = 51.8375, GNorm = 0.1237, lr_0 = 2.0412e-04
Loss = 4.6494e-05, PNorm = 51.8406, GNorm = 0.1529, lr_0 = 2.0391e-04
Loss = 5.2663e-05, PNorm = 51.8430, GNorm = 0.1521, lr_0 = 2.0369e-04
Loss = 3.7204e-05, PNorm = 51.8452, GNorm = 0.0990, lr_0 = 2.0348e-04
Loss = 5.0431e-05, PNorm = 51.8457, GNorm = 0.2546, lr_0 = 2.0327e-04
Loss = 3.9328e-05, PNorm = 51.8465, GNorm = 0.1617, lr_0 = 2.0306e-04
Loss = 4.1388e-05, PNorm = 51.8486, GNorm = 0.1354, lr_0 = 2.0285e-04
Loss = 4.0937e-05, PNorm = 51.8501, GNorm = 0.1915, lr_0 = 2.0263e-04
Loss = 4.0301e-05, PNorm = 51.8501, GNorm = 0.1763, lr_0 = 2.0242e-04
Loss = 3.8470e-05, PNorm = 51.8517, GNorm = 0.1703, lr_0 = 2.0221e-04
Loss = 3.5343e-05, PNorm = 51.8537, GNorm = 0.1769, lr_0 = 2.0200e-04
Loss = 4.4125e-05, PNorm = 51.8556, GNorm = 0.2315, lr_0 = 2.0179e-04
Loss = 4.0690e-05, PNorm = 51.8580, GNorm = 0.1299, lr_0 = 2.0158e-04
Loss = 4.0523e-05, PNorm = 51.8598, GNorm = 0.0893, lr_0 = 2.0137e-04
Loss = 3.4357e-05, PNorm = 51.8618, GNorm = 0.1348, lr_0 = 2.0116e-04
Loss = 3.3111e-05, PNorm = 51.8631, GNorm = 0.1685, lr_0 = 2.0095e-04
Validation rmse logD = 0.582049
Validation R2 logD = 0.765363
Validation rmse logP = 0.469575
Validation R2 logP = 0.936004
Epoch 70
Train function
Loss = 4.1387e-05, PNorm = 51.8654, GNorm = 0.1351, lr_0 = 2.0072e-04
Loss = 2.9452e-05, PNorm = 51.8690, GNorm = 0.1119, lr_0 = 2.0051e-04
Loss = 3.3735e-05, PNorm = 51.8695, GNorm = 0.1461, lr_0 = 2.0030e-04
Loss = 2.8731e-05, PNorm = 51.8714, GNorm = 0.1147, lr_0 = 2.0009e-04
Loss = 3.0065e-05, PNorm = 51.8734, GNorm = 0.1475, lr_0 = 1.9988e-04
Loss = 3.5217e-05, PNorm = 51.8736, GNorm = 0.1021, lr_0 = 1.9967e-04
Loss = 3.9433e-05, PNorm = 51.8749, GNorm = 0.2292, lr_0 = 1.9946e-04
Loss = 4.7470e-05, PNorm = 51.8786, GNorm = 0.2773, lr_0 = 1.9926e-04
Loss = 5.1567e-05, PNorm = 51.8792, GNorm = 0.1201, lr_0 = 1.9905e-04
Loss = 4.1835e-05, PNorm = 51.8817, GNorm = 0.1745, lr_0 = 1.9884e-04
Loss = 4.5198e-05, PNorm = 51.8849, GNorm = 0.2106, lr_0 = 1.9863e-04
Loss = 4.2285e-05, PNorm = 51.8864, GNorm = 0.2884, lr_0 = 1.9842e-04
Loss = 4.3169e-05, PNorm = 51.8888, GNorm = 0.1188, lr_0 = 1.9822e-04
Loss = 4.3777e-05, PNorm = 51.8890, GNorm = 0.2188, lr_0 = 1.9801e-04
Loss = 4.1544e-05, PNorm = 51.8907, GNorm = 0.2162, lr_0 = 1.9780e-04
Loss = 4.8386e-05, PNorm = 51.8920, GNorm = 0.1728, lr_0 = 1.9760e-04
Loss = 4.9375e-05, PNorm = 51.8931, GNorm = 0.1170, lr_0 = 1.9739e-04
Loss = 4.5654e-05, PNorm = 51.8942, GNorm = 0.2020, lr_0 = 1.9719e-04
Loss = 4.9165e-05, PNorm = 51.8968, GNorm = 0.2980, lr_0 = 1.9698e-04
Loss = 6.5531e-05, PNorm = 51.9015, GNorm = 0.3890, lr_0 = 1.9677e-04
Loss = 6.4705e-05, PNorm = 51.9035, GNorm = 0.2371, lr_0 = 1.9657e-04
Loss = 6.5199e-05, PNorm = 51.9062, GNorm = 0.2744, lr_0 = 1.9636e-04
Validation rmse logD = 0.586326
Validation R2 logD = 0.761902
Validation rmse logP = 0.468969
Validation R2 logP = 0.936169
Epoch 71
Train function
Loss = 4.1937e-05, PNorm = 51.9092, GNorm = 0.1234, lr_0 = 1.9616e-04
Loss = 4.5099e-05, PNorm = 51.9129, GNorm = 0.1226, lr_0 = 1.9595e-04
Loss = 4.6633e-05, PNorm = 51.9146, GNorm = 0.2061, lr_0 = 1.9575e-04
Loss = 5.1581e-05, PNorm = 51.9170, GNorm = 0.2327, lr_0 = 1.9555e-04
Loss = 6.4728e-05, PNorm = 51.9217, GNorm = 0.2655, lr_0 = 1.9534e-04
Loss = 7.7006e-05, PNorm = 51.9229, GNorm = 0.3010, lr_0 = 1.9514e-04
Loss = 4.9729e-05, PNorm = 51.9253, GNorm = 0.1682, lr_0 = 1.9493e-04
Loss = 4.9124e-05, PNorm = 51.9287, GNorm = 0.1570, lr_0 = 1.9473e-04
Loss = 5.0694e-05, PNorm = 51.9292, GNorm = 0.0828, lr_0 = 1.9453e-04
Loss = 3.9367e-05, PNorm = 51.9312, GNorm = 0.0900, lr_0 = 1.9432e-04
Loss = 3.9567e-05, PNorm = 51.9336, GNorm = 0.0899, lr_0 = 1.9412e-04
Loss = 4.1488e-05, PNorm = 51.9358, GNorm = 0.1313, lr_0 = 1.9392e-04
Loss = 3.7505e-05, PNorm = 51.9389, GNorm = 0.1170, lr_0 = 1.9372e-04
Loss = 4.2825e-05, PNorm = 51.9411, GNorm = 0.1466, lr_0 = 1.9351e-04
Loss = 3.9422e-05, PNorm = 51.9422, GNorm = 0.1517, lr_0 = 1.9331e-04
Loss = 2.8734e-05, PNorm = 51.9434, GNorm = 0.0852, lr_0 = 1.9311e-04
Loss = 3.1702e-05, PNorm = 51.9439, GNorm = 0.0973, lr_0 = 1.9291e-04
Loss = 3.7234e-05, PNorm = 51.9445, GNorm = 0.1743, lr_0 = 1.9271e-04
Loss = 3.6778e-05, PNorm = 51.9463, GNorm = 0.1396, lr_0 = 1.9251e-04
Loss = 3.5088e-05, PNorm = 51.9487, GNorm = 0.1730, lr_0 = 1.9231e-04
Loss = 3.3888e-05, PNorm = 51.9504, GNorm = 0.1413, lr_0 = 1.9210e-04
Loss = 3.7537e-05, PNorm = 51.9508, GNorm = 0.1794, lr_0 = 1.9190e-04
Loss = 3.6938e-05, PNorm = 51.9521, GNorm = 0.1284, lr_0 = 1.9170e-04
Validation rmse logD = 0.581848
Validation R2 logD = 0.765525
Validation rmse logP = 0.470726
Validation R2 logP = 0.935690
Epoch 72
Train function
Loss = 4.2778e-05, PNorm = 51.9534, GNorm = 0.1012, lr_0 = 1.9148e-04
Loss = 3.6567e-05, PNorm = 51.9549, GNorm = 0.1052, lr_0 = 1.9128e-04
Loss = 3.0362e-05, PNorm = 51.9553, GNorm = 0.0954, lr_0 = 1.9108e-04
Loss = 3.7419e-05, PNorm = 51.9569, GNorm = 0.1779, lr_0 = 1.9088e-04
Loss = 3.7076e-05, PNorm = 51.9584, GNorm = 0.1707, lr_0 = 1.9069e-04
Loss = 2.6532e-05, PNorm = 51.9591, GNorm = 0.1057, lr_0 = 1.9049e-04
Loss = 2.9838e-05, PNorm = 51.9607, GNorm = 0.1209, lr_0 = 1.9029e-04
Loss = 2.4493e-05, PNorm = 51.9619, GNorm = 0.1163, lr_0 = 1.9009e-04
Loss = 3.0421e-05, PNorm = 51.9628, GNorm = 0.2746, lr_0 = 1.8989e-04
Loss = 4.2241e-05, PNorm = 51.9643, GNorm = 0.1767, lr_0 = 1.8969e-04
Loss = 3.6607e-05, PNorm = 51.9663, GNorm = 0.1745, lr_0 = 1.8949e-04
Loss = 4.1107e-05, PNorm = 51.9677, GNorm = 0.1694, lr_0 = 1.8930e-04
Loss = 3.3668e-05, PNorm = 51.9704, GNorm = 0.1100, lr_0 = 1.8910e-04
Loss = 4.4414e-05, PNorm = 51.9730, GNorm = 0.1427, lr_0 = 1.8890e-04
Loss = 3.8615e-05, PNorm = 51.9732, GNorm = 0.1706, lr_0 = 1.8870e-04
Loss = 5.0265e-05, PNorm = 51.9755, GNorm = 0.1537, lr_0 = 1.8851e-04
Loss = 3.2852e-05, PNorm = 51.9776, GNorm = 0.1395, lr_0 = 1.8831e-04
Loss = 3.5685e-05, PNorm = 51.9792, GNorm = 0.1708, lr_0 = 1.8811e-04
Loss = 4.0691e-05, PNorm = 51.9811, GNorm = 0.1819, lr_0 = 1.8792e-04
Loss = 3.6589e-05, PNorm = 51.9827, GNorm = 0.0975, lr_0 = 1.8772e-04
Loss = 3.3999e-05, PNorm = 51.9851, GNorm = 0.1373, lr_0 = 1.8753e-04
Loss = 3.3859e-05, PNorm = 51.9877, GNorm = 0.1298, lr_0 = 1.8733e-04
Loss = 4.4689e-05, PNorm = 51.9904, GNorm = 0.3734, lr_0 = 1.8713e-04
Validation rmse logD = 0.579514
Validation R2 logD = 0.767402
Validation rmse logP = 0.468546
Validation R2 logP = 0.936284
Epoch 73
Train function
Loss = 3.1495e-05, PNorm = 51.9924, GNorm = 0.1868, lr_0 = 1.8694e-04
Loss = 2.7712e-05, PNorm = 51.9940, GNorm = 0.1940, lr_0 = 1.8674e-04
Loss = 2.5439e-05, PNorm = 51.9947, GNorm = 0.0990, lr_0 = 1.8655e-04
Loss = 2.5875e-05, PNorm = 51.9950, GNorm = 0.0976, lr_0 = 1.8635e-04
Loss = 2.7115e-05, PNorm = 51.9964, GNorm = 0.1242, lr_0 = 1.8616e-04
Loss = 2.4219e-05, PNorm = 51.9970, GNorm = 0.1331, lr_0 = 1.8597e-04
Loss = 2.4153e-05, PNorm = 51.9994, GNorm = 0.0996, lr_0 = 1.8577e-04
Loss = 2.8126e-05, PNorm = 52.0000, GNorm = 0.1180, lr_0 = 1.8558e-04
Loss = 2.6886e-05, PNorm = 52.0014, GNorm = 0.1096, lr_0 = 1.8538e-04
Loss = 3.0035e-05, PNorm = 52.0021, GNorm = 0.1882, lr_0 = 1.8519e-04
Loss = 3.2255e-05, PNorm = 52.0038, GNorm = 0.1185, lr_0 = 1.8500e-04
Loss = 3.2823e-05, PNorm = 52.0059, GNorm = 0.0768, lr_0 = 1.8480e-04
Loss = 3.6144e-05, PNorm = 52.0073, GNorm = 0.1956, lr_0 = 1.8461e-04
Loss = 3.5602e-05, PNorm = 52.0095, GNorm = 0.1677, lr_0 = 1.8442e-04
Loss = 3.8363e-05, PNorm = 52.0104, GNorm = 0.1804, lr_0 = 1.8423e-04
Loss = 3.4370e-05, PNorm = 52.0122, GNorm = 0.1827, lr_0 = 1.8403e-04
Loss = 3.6000e-05, PNorm = 52.0140, GNorm = 0.1153, lr_0 = 1.8384e-04
Loss = 3.3803e-05, PNorm = 52.0160, GNorm = 0.1754, lr_0 = 1.8365e-04
Loss = 3.0967e-05, PNorm = 52.0174, GNorm = 0.1803, lr_0 = 1.8346e-04
Loss = 3.2724e-05, PNorm = 52.0182, GNorm = 0.1836, lr_0 = 1.8327e-04
Loss = 3.4361e-05, PNorm = 52.0194, GNorm = 0.1931, lr_0 = 1.8308e-04
Loss = 4.7383e-05, PNorm = 52.0229, GNorm = 0.2358, lr_0 = 1.8288e-04
Validation rmse logD = 0.582846
Validation R2 logD = 0.764720
Validation rmse logP = 0.469629
Validation R2 logP = 0.935990
Epoch 74
Train function
Loss = 3.4256e-05, PNorm = 52.0235, GNorm = 0.1098, lr_0 = 1.8267e-04
Loss = 3.3768e-05, PNorm = 52.0235, GNorm = 0.1344, lr_0 = 1.8248e-04
Loss = 2.8838e-05, PNorm = 52.0254, GNorm = 0.1068, lr_0 = 1.8229e-04
Loss = 3.2679e-05, PNorm = 52.0270, GNorm = 0.1462, lr_0 = 1.8210e-04
Loss = 4.2374e-05, PNorm = 52.0263, GNorm = 0.3452, lr_0 = 1.8191e-04
Loss = 3.7445e-05, PNorm = 52.0274, GNorm = 0.1623, lr_0 = 1.8172e-04
Loss = 4.0384e-05, PNorm = 52.0293, GNorm = 0.2619, lr_0 = 1.8153e-04
Loss = 4.1992e-05, PNorm = 52.0323, GNorm = 0.2880, lr_0 = 1.8134e-04
Loss = 3.3556e-05, PNorm = 52.0342, GNorm = 0.2438, lr_0 = 1.8115e-04
Loss = 3.2949e-05, PNorm = 52.0351, GNorm = 0.1080, lr_0 = 1.8097e-04
Loss = 2.8466e-05, PNorm = 52.0364, GNorm = 0.0987, lr_0 = 1.8078e-04
Loss = 3.1888e-05, PNorm = 52.0393, GNorm = 0.0910, lr_0 = 1.8059e-04
Loss = 3.2228e-05, PNorm = 52.0400, GNorm = 0.1819, lr_0 = 1.8040e-04
Loss = 3.2350e-05, PNorm = 52.0410, GNorm = 0.1398, lr_0 = 1.8021e-04
Loss = 2.7384e-05, PNorm = 52.0419, GNorm = 0.1560, lr_0 = 1.8002e-04
Loss = 2.8327e-05, PNorm = 52.0437, GNorm = 0.0805, lr_0 = 1.7984e-04
Loss = 3.1045e-05, PNorm = 52.0439, GNorm = 0.0897, lr_0 = 1.7965e-04
Loss = 3.4785e-05, PNorm = 52.0439, GNorm = 0.1267, lr_0 = 1.7946e-04
Loss = 3.4239e-05, PNorm = 52.0452, GNorm = 0.1440, lr_0 = 1.7927e-04
Loss = 2.5131e-05, PNorm = 52.0462, GNorm = 0.0699, lr_0 = 1.7909e-04
Loss = 3.5635e-05, PNorm = 52.0475, GNorm = 0.3127, lr_0 = 1.7890e-04
Loss = 4.0467e-05, PNorm = 52.0502, GNorm = 0.1378, lr_0 = 1.7871e-04
Loss = 4.0422e-05, PNorm = 52.0520, GNorm = 0.1298, lr_0 = 1.7853e-04
Validation rmse logD = 0.587209
Validation R2 logD = 0.761184
Validation rmse logP = 0.469200
Validation R2 logP = 0.936106
Epoch 75
Train function
Loss = 3.7169e-05, PNorm = 52.0555, GNorm = 0.1785, lr_0 = 1.7834e-04
Loss = 3.7135e-05, PNorm = 52.0555, GNorm = 0.1588, lr_0 = 1.7815e-04
Loss = 2.9481e-05, PNorm = 52.0569, GNorm = 0.1299, lr_0 = 1.7797e-04
Loss = 3.0164e-05, PNorm = 52.0583, GNorm = 0.1351, lr_0 = 1.7778e-04
Loss = 3.4840e-05, PNorm = 52.0596, GNorm = 0.1008, lr_0 = 1.7760e-04
Loss = 2.4958e-05, PNorm = 52.0597, GNorm = 0.0771, lr_0 = 1.7741e-04
Loss = 3.4437e-05, PNorm = 52.0601, GNorm = 0.1516, lr_0 = 1.7723e-04
Loss = 3.2865e-05, PNorm = 52.0615, GNorm = 0.1375, lr_0 = 1.7704e-04
Loss = 2.6851e-05, PNorm = 52.0629, GNorm = 0.0785, lr_0 = 1.7686e-04
Loss = 2.0265e-05, PNorm = 52.0646, GNorm = 0.0876, lr_0 = 1.7667e-04
Loss = 2.6451e-05, PNorm = 52.0657, GNorm = 0.1421, lr_0 = 1.7649e-04
Loss = 2.6608e-05, PNorm = 52.0669, GNorm = 0.1676, lr_0 = 1.7630e-04
Loss = 2.5279e-05, PNorm = 52.0678, GNorm = 0.1289, lr_0 = 1.7612e-04
Loss = 2.7498e-05, PNorm = 52.0692, GNorm = 0.0807, lr_0 = 1.7593e-04
Loss = 2.5042e-05, PNorm = 52.0693, GNorm = 0.1068, lr_0 = 1.7575e-04
Loss = 2.3943e-05, PNorm = 52.0710, GNorm = 0.1086, lr_0 = 1.7557e-04
Loss = 2.5534e-05, PNorm = 52.0730, GNorm = 0.1639, lr_0 = 1.7538e-04
Loss = 2.8199e-05, PNorm = 52.0739, GNorm = 0.0839, lr_0 = 1.7520e-04
Loss = 3.1023e-05, PNorm = 52.0749, GNorm = 0.1106, lr_0 = 1.7502e-04
Loss = 3.2040e-05, PNorm = 52.0768, GNorm = 0.1179, lr_0 = 1.7484e-04
Loss = 3.0022e-05, PNorm = 52.0794, GNorm = 0.1398, lr_0 = 1.7465e-04
Loss = 2.6079e-05, PNorm = 52.0816, GNorm = 0.1323, lr_0 = 1.7447e-04
Validation rmse logD = 0.581249
Validation R2 logD = 0.766007
Validation rmse logP = 0.467723
Validation R2 logP = 0.936508
Epoch 76
Train function
Loss = 2.4606e-05, PNorm = 52.0839, GNorm = 0.1305, lr_0 = 1.7427e-04
Loss = 3.0027e-05, PNorm = 52.0849, GNorm = 0.1102, lr_0 = 1.7409e-04
Loss = 3.1776e-05, PNorm = 52.0864, GNorm = 0.1362, lr_0 = 1.7391e-04
Loss = 2.3760e-05, PNorm = 52.0876, GNorm = 0.0995, lr_0 = 1.7373e-04
Loss = 2.3971e-05, PNorm = 52.0877, GNorm = 0.1716, lr_0 = 1.7354e-04
Loss = 2.3083e-05, PNorm = 52.0878, GNorm = 0.0961, lr_0 = 1.7336e-04
Loss = 2.3185e-05, PNorm = 52.0887, GNorm = 0.0840, lr_0 = 1.7318e-04
Loss = 2.5727e-05, PNorm = 52.0899, GNorm = 0.0995, lr_0 = 1.7300e-04
Loss = 2.2566e-05, PNorm = 52.0914, GNorm = 0.0920, lr_0 = 1.7282e-04
Loss = 2.7016e-05, PNorm = 52.0933, GNorm = 0.1827, lr_0 = 1.7264e-04
Loss = 2.6077e-05, PNorm = 52.0953, GNorm = 0.1125, lr_0 = 1.7246e-04
Loss = 2.2402e-05, PNorm = 52.0958, GNorm = 0.0755, lr_0 = 1.7228e-04
Loss = 2.6298e-05, PNorm = 52.0963, GNorm = 0.1949, lr_0 = 1.7210e-04
Loss = 3.0638e-05, PNorm = 52.0972, GNorm = 0.0567, lr_0 = 1.7192e-04
Loss = 3.1097e-05, PNorm = 52.0996, GNorm = 0.1034, lr_0 = 1.7174e-04
Loss = 3.3660e-05, PNorm = 52.1011, GNorm = 0.1430, lr_0 = 1.7156e-04
Loss = 2.6974e-05, PNorm = 52.1016, GNorm = 0.2186, lr_0 = 1.7138e-04
Loss = 2.2169e-05, PNorm = 52.1026, GNorm = 0.1128, lr_0 = 1.7120e-04
Loss = 2.5973e-05, PNorm = 52.1043, GNorm = 0.0690, lr_0 = 1.7103e-04
Loss = 2.6146e-05, PNorm = 52.1050, GNorm = 0.0752, lr_0 = 1.7085e-04
Loss = 2.8371e-05, PNorm = 52.1051, GNorm = 0.1683, lr_0 = 1.7067e-04
Loss = 3.5370e-05, PNorm = 52.1062, GNorm = 0.1472, lr_0 = 1.7049e-04
Loss = 2.8049e-05, PNorm = 52.1080, GNorm = 0.1502, lr_0 = 1.7031e-04
Validation rmse logD = 0.579250
Validation R2 logD = 0.767614
Validation rmse logP = 0.468086
Validation R2 logP = 0.936409
Epoch 77
Train function
Loss = 3.1043e-05, PNorm = 52.1107, GNorm = 0.1381, lr_0 = 1.7012e-04
Loss = 2.7919e-05, PNorm = 52.1117, GNorm = 0.1114, lr_0 = 1.6994e-04
Loss = 2.4146e-05, PNorm = 52.1132, GNorm = 0.0968, lr_0 = 1.6976e-04
Loss = 2.4174e-05, PNorm = 52.1141, GNorm = 0.1303, lr_0 = 1.6959e-04
Loss = 2.0512e-05, PNorm = 52.1168, GNorm = 0.1098, lr_0 = 1.6941e-04
Loss = 2.2439e-05, PNorm = 52.1181, GNorm = 0.0878, lr_0 = 1.6923e-04
Loss = 2.8792e-05, PNorm = 52.1190, GNorm = 0.1202, lr_0 = 1.6905e-04
Loss = 1.7125e-05, PNorm = 52.1201, GNorm = 0.0642, lr_0 = 1.6888e-04
Loss = 2.9548e-05, PNorm = 52.1208, GNorm = 0.2127, lr_0 = 1.6870e-04
Loss = 3.1743e-05, PNorm = 52.1223, GNorm = 0.3135, lr_0 = 1.6853e-04
Loss = 3.7601e-05, PNorm = 52.1228, GNorm = 0.2266, lr_0 = 1.6835e-04
Loss = 4.8806e-05, PNorm = 52.1263, GNorm = 0.1595, lr_0 = 1.6817e-04
Loss = 4.9008e-05, PNorm = 52.1283, GNorm = 0.2920, lr_0 = 1.6800e-04
Loss = 3.2952e-05, PNorm = 52.1301, GNorm = 0.1301, lr_0 = 1.6782e-04
Loss = 2.8590e-05, PNorm = 52.1306, GNorm = 0.0855, lr_0 = 1.6765e-04
Loss = 3.0434e-05, PNorm = 52.1327, GNorm = 0.1535, lr_0 = 1.6747e-04
Loss = 3.1791e-05, PNorm = 52.1361, GNorm = 0.1925, lr_0 = 1.6730e-04
Loss = 3.2971e-05, PNorm = 52.1372, GNorm = 0.1923, lr_0 = 1.6712e-04
Loss = 3.2706e-05, PNorm = 52.1376, GNorm = 0.1836, lr_0 = 1.6695e-04
Loss = 2.9439e-05, PNorm = 52.1403, GNorm = 0.1123, lr_0 = 1.6678e-04
Loss = 2.6230e-05, PNorm = 52.1429, GNorm = 0.1160, lr_0 = 1.6660e-04
Loss = 3.1415e-05, PNorm = 52.1438, GNorm = 0.1405, lr_0 = 1.6643e-04
Validation rmse logD = 0.582563
Validation R2 logD = 0.764948
Validation rmse logP = 0.469991
Validation R2 logP = 0.935891
Epoch 78
Train function
Loss = 3.0242e-05, PNorm = 52.1455, GNorm = 0.1498, lr_0 = 1.6625e-04
Loss = 2.5075e-05, PNorm = 52.1467, GNorm = 0.1189, lr_0 = 1.6608e-04
Loss = 2.3611e-05, PNorm = 52.1461, GNorm = 0.1571, lr_0 = 1.6591e-04
Loss = 2.4867e-05, PNorm = 52.1467, GNorm = 0.0894, lr_0 = 1.6573e-04
Loss = 2.5000e-05, PNorm = 52.1477, GNorm = 0.1148, lr_0 = 1.6556e-04
Loss = 2.0414e-05, PNorm = 52.1485, GNorm = 0.1824, lr_0 = 1.6539e-04
Loss = 2.2642e-05, PNorm = 52.1486, GNorm = 0.0780, lr_0 = 1.6522e-04
Loss = 2.5365e-05, PNorm = 52.1504, GNorm = 0.0906, lr_0 = 1.6504e-04
Loss = 1.9219e-05, PNorm = 52.1521, GNorm = 0.0773, lr_0 = 1.6487e-04
Loss = 2.2941e-05, PNorm = 52.1539, GNorm = 0.1395, lr_0 = 1.6470e-04
Loss = 2.5045e-05, PNorm = 52.1550, GNorm = 0.0830, lr_0 = 1.6453e-04
Loss = 2.3867e-05, PNorm = 52.1559, GNorm = 0.1018, lr_0 = 1.6435e-04
Loss = 3.6496e-05, PNorm = 52.1562, GNorm = 0.2437, lr_0 = 1.6418e-04
Loss = 3.4185e-05, PNorm = 52.1569, GNorm = 0.1734, lr_0 = 1.6401e-04
Loss = 3.1237e-05, PNorm = 52.1601, GNorm = 0.1594, lr_0 = 1.6384e-04
Loss = 2.6812e-05, PNorm = 52.1617, GNorm = 0.1657, lr_0 = 1.6367e-04
Loss = 2.7342e-05, PNorm = 52.1628, GNorm = 0.1371, lr_0 = 1.6350e-04
Loss = 2.5916e-05, PNorm = 52.1638, GNorm = 0.0911, lr_0 = 1.6333e-04
Loss = 3.1995e-05, PNorm = 52.1650, GNorm = 0.1195, lr_0 = 1.6316e-04
Loss = 3.1783e-05, PNorm = 52.1652, GNorm = 0.2679, lr_0 = 1.6299e-04
Loss = 3.3582e-05, PNorm = 52.1669, GNorm = 0.0967, lr_0 = 1.6282e-04
Loss = 3.8139e-05, PNorm = 52.1700, GNorm = 0.0995, lr_0 = 1.6265e-04
Loss = 3.5305e-05, PNorm = 52.1713, GNorm = 0.1335, lr_0 = 1.6248e-04
Validation rmse logD = 0.584389
Validation R2 logD = 0.763472
Validation rmse logP = 0.471971
Validation R2 logP = 0.935350
Epoch 79
Train function
Loss = 3.7324e-05, PNorm = 52.1725, GNorm = 0.1498, lr_0 = 1.6229e-04
Loss = 3.6605e-05, PNorm = 52.1730, GNorm = 0.2517, lr_0 = 1.6212e-04
Loss = 3.2516e-05, PNorm = 52.1748, GNorm = 0.1023, lr_0 = 1.6195e-04
Loss = 3.2776e-05, PNorm = 52.1751, GNorm = 0.1439, lr_0 = 1.6178e-04
Loss = 2.6140e-05, PNorm = 52.1761, GNorm = 0.0760, lr_0 = 1.6161e-04
Loss = 2.5603e-05, PNorm = 52.1788, GNorm = 0.1414, lr_0 = 1.6145e-04
Loss = 2.5763e-05, PNorm = 52.1791, GNorm = 0.1063, lr_0 = 1.6128e-04
Loss = 2.8087e-05, PNorm = 52.1801, GNorm = 0.1062, lr_0 = 1.6111e-04
Loss = 2.3211e-05, PNorm = 52.1796, GNorm = 0.1013, lr_0 = 1.6094e-04
Loss = 2.6425e-05, PNorm = 52.1803, GNorm = 0.0763, lr_0 = 1.6077e-04
Loss = 2.5834e-05, PNorm = 52.1814, GNorm = 0.1028, lr_0 = 1.6061e-04
Loss = 2.8001e-05, PNorm = 52.1831, GNorm = 0.0961, lr_0 = 1.6044e-04
Loss = 3.0573e-05, PNorm = 52.1845, GNorm = 0.1501, lr_0 = 1.6027e-04
Loss = 2.6346e-05, PNorm = 52.1861, GNorm = 0.1048, lr_0 = 1.6010e-04
Loss = 3.2115e-05, PNorm = 52.1872, GNorm = 0.1457, lr_0 = 1.5994e-04
Loss = 2.9464e-05, PNorm = 52.1877, GNorm = 0.1582, lr_0 = 1.5977e-04
Loss = 2.8471e-05, PNorm = 52.1889, GNorm = 0.1112, lr_0 = 1.5960e-04
Loss = 2.2410e-05, PNorm = 52.1907, GNorm = 0.0758, lr_0 = 1.5944e-04
Loss = 2.0772e-05, PNorm = 52.1921, GNorm = 0.0942, lr_0 = 1.5927e-04
Loss = 2.2379e-05, PNorm = 52.1936, GNorm = 0.1556, lr_0 = 1.5910e-04
Loss = 2.4986e-05, PNorm = 52.1939, GNorm = 0.1031, lr_0 = 1.5894e-04
Loss = 3.0817e-05, PNorm = 52.1949, GNorm = 0.0800, lr_0 = 1.5877e-04
Validation rmse logD = 0.581603
Validation R2 logD = 0.765722
Validation rmse logP = 0.466181
Validation R2 logP = 0.936926
Epoch 80
Train function
Loss = 3.0290e-05, PNorm = 52.1958, GNorm = 0.0660, lr_0 = 1.5861e-04
Loss = 2.2273e-05, PNorm = 52.1969, GNorm = 0.0982, lr_0 = 1.5844e-04
Loss = 2.1907e-05, PNorm = 52.1978, GNorm = 0.1081, lr_0 = 1.5827e-04
Loss = 1.6910e-05, PNorm = 52.1992, GNorm = 0.0929, lr_0 = 1.5811e-04
Loss = 1.7615e-05, PNorm = 52.2003, GNorm = 0.1016, lr_0 = 1.5794e-04
Loss = 1.8093e-05, PNorm = 52.2008, GNorm = 0.0646, lr_0 = 1.5778e-04
Loss = 1.8678e-05, PNorm = 52.2013, GNorm = 0.0899, lr_0 = 1.5761e-04
Loss = 2.1157e-05, PNorm = 52.2029, GNorm = 0.0878, lr_0 = 1.5745e-04
Loss = 2.4551e-05, PNorm = 52.2035, GNorm = 0.1857, lr_0 = 1.5729e-04
Loss = 2.6210e-05, PNorm = 52.2055, GNorm = 0.1557, lr_0 = 1.5712e-04
Loss = 2.7061e-05, PNorm = 52.2082, GNorm = 0.0953, lr_0 = 1.5696e-04
Loss = 1.9924e-05, PNorm = 52.2099, GNorm = 0.1159, lr_0 = 1.5679e-04
Loss = 2.4786e-05, PNorm = 52.2100, GNorm = 0.2012, lr_0 = 1.5663e-04
Loss = 2.5786e-05, PNorm = 52.2116, GNorm = 0.1629, lr_0 = 1.5647e-04
Loss = 2.4228e-05, PNorm = 52.2137, GNorm = 0.0867, lr_0 = 1.5630e-04
Loss = 2.1727e-05, PNorm = 52.2147, GNorm = 0.0556, lr_0 = 1.5614e-04
Loss = 2.4387e-05, PNorm = 52.2156, GNorm = 0.0885, lr_0 = 1.5598e-04
Loss = 2.2522e-05, PNorm = 52.2169, GNorm = 0.0710, lr_0 = 1.5581e-04
Loss = 2.3318e-05, PNorm = 52.2176, GNorm = 0.1081, lr_0 = 1.5565e-04
Loss = 2.3507e-05, PNorm = 52.2181, GNorm = 0.0643, lr_0 = 1.5549e-04
Loss = 2.1300e-05, PNorm = 52.2186, GNorm = 0.1065, lr_0 = 1.5533e-04
Loss = 1.9465e-05, PNorm = 52.2194, GNorm = 0.1838, lr_0 = 1.5516e-04
Loss = 2.5238e-05, PNorm = 52.2204, GNorm = 0.1229, lr_0 = 1.5500e-04
Validation rmse logD = 0.584151
Validation R2 logD = 0.763665
Validation rmse logP = 0.469712
Validation R2 logP = 0.935967
Epoch 81
Train function
Loss = 1.6364e-05, PNorm = 52.2215, GNorm = 0.0825, lr_0 = 1.5483e-04
Loss = 2.4970e-05, PNorm = 52.2221, GNorm = 0.1082, lr_0 = 1.5466e-04
Loss = 1.9990e-05, PNorm = 52.2229, GNorm = 0.1525, lr_0 = 1.5450e-04
Loss = 2.0986e-05, PNorm = 52.2231, GNorm = 0.1288, lr_0 = 1.5434e-04
Loss = 2.4987e-05, PNorm = 52.2246, GNorm = 0.1306, lr_0 = 1.5418e-04
Loss = 2.6104e-05, PNorm = 52.2262, GNorm = 0.1014, lr_0 = 1.5402e-04
Loss = 2.3285e-05, PNorm = 52.2285, GNorm = 0.1966, lr_0 = 1.5386e-04
Loss = 2.4302e-05, PNorm = 52.2286, GNorm = 0.2000, lr_0 = 1.5370e-04
Loss = 2.6509e-05, PNorm = 52.2306, GNorm = 0.1362, lr_0 = 1.5354e-04
Loss = 2.5318e-05, PNorm = 52.2308, GNorm = 0.0944, lr_0 = 1.5338e-04
Loss = 2.4708e-05, PNorm = 52.2312, GNorm = 0.1056, lr_0 = 1.5322e-04
Loss = 2.7314e-05, PNorm = 52.2323, GNorm = 0.1831, lr_0 = 1.5306e-04
Loss = 2.5447e-05, PNorm = 52.2336, GNorm = 0.1857, lr_0 = 1.5290e-04
Loss = 2.8188e-05, PNorm = 52.2328, GNorm = 0.1474, lr_0 = 1.5274e-04
Loss = 2.7918e-05, PNorm = 52.2329, GNorm = 0.1063, lr_0 = 1.5258e-04
Loss = 3.1310e-05, PNorm = 52.2337, GNorm = 0.1349, lr_0 = 1.5242e-04
Loss = 3.3079e-05, PNorm = 52.2358, GNorm = 0.1851, lr_0 = 1.5226e-04
Loss = 2.3197e-05, PNorm = 52.2366, GNorm = 0.1003, lr_0 = 1.5210e-04
Loss = 2.9573e-05, PNorm = 52.2382, GNorm = 0.1199, lr_0 = 1.5194e-04
Loss = 3.4996e-05, PNorm = 52.2393, GNorm = 0.3184, lr_0 = 1.5178e-04
Loss = 2.6664e-05, PNorm = 52.2391, GNorm = 0.1013, lr_0 = 1.5163e-04
Loss = 2.6175e-05, PNorm = 52.2403, GNorm = 0.1113, lr_0 = 1.5147e-04
Validation rmse logD = 0.582422
Validation R2 logD = 0.765062
Validation rmse logP = 0.468514
Validation R2 logP = 0.936293
Epoch 82
Train function
Loss = 2.7856e-05, PNorm = 52.2410, GNorm = 0.1332, lr_0 = 1.5131e-04
Loss = 2.1177e-05, PNorm = 52.2413, GNorm = 0.1280, lr_0 = 1.5115e-04
Loss = 2.1088e-05, PNorm = 52.2414, GNorm = 0.0760, lr_0 = 1.5099e-04
Loss = 2.5122e-05, PNorm = 52.2427, GNorm = 0.0940, lr_0 = 1.5084e-04
Loss = 2.0206e-05, PNorm = 52.2455, GNorm = 0.1074, lr_0 = 1.5068e-04
Loss = 1.6510e-05, PNorm = 52.2478, GNorm = 0.0847, lr_0 = 1.5052e-04
Loss = 2.2065e-05, PNorm = 52.2489, GNorm = 0.0780, lr_0 = 1.5036e-04
Loss = 1.8518e-05, PNorm = 52.2502, GNorm = 0.0654, lr_0 = 1.5021e-04
Loss = 2.2522e-05, PNorm = 52.2516, GNorm = 0.1459, lr_0 = 1.5005e-04
Loss = 1.8474e-05, PNorm = 52.2527, GNorm = 0.1368, lr_0 = 1.4989e-04
Loss = 2.3511e-05, PNorm = 52.2542, GNorm = 0.1293, lr_0 = 1.4974e-04
Loss = 2.3362e-05, PNorm = 52.2551, GNorm = 0.1351, lr_0 = 1.4958e-04
Loss = 2.1219e-05, PNorm = 52.2574, GNorm = 0.1229, lr_0 = 1.4942e-04
Loss = 2.4948e-05, PNorm = 52.2584, GNorm = 0.0802, lr_0 = 1.4927e-04
Loss = 1.8685e-05, PNorm = 52.2601, GNorm = 0.0798, lr_0 = 1.4911e-04
Loss = 2.0562e-05, PNorm = 52.2612, GNorm = 0.1015, lr_0 = 1.4896e-04
Loss = 1.9023e-05, PNorm = 52.2630, GNorm = 0.0799, lr_0 = 1.4880e-04
Loss = 2.2503e-05, PNorm = 52.2642, GNorm = 0.0920, lr_0 = 1.4865e-04
Loss = 2.1635e-05, PNorm = 52.2651, GNorm = 0.0790, lr_0 = 1.4849e-04
Loss = 2.0906e-05, PNorm = 52.2673, GNorm = 0.1721, lr_0 = 1.4834e-04
Loss = 2.1674e-05, PNorm = 52.2678, GNorm = 0.1587, lr_0 = 1.4818e-04
Loss = 2.0991e-05, PNorm = 52.2692, GNorm = 0.1474, lr_0 = 1.4803e-04
Loss = 2.3730e-05, PNorm = 52.2697, GNorm = 0.1009, lr_0 = 1.4787e-04
Validation rmse logD = 0.582491
Validation R2 logD = 0.765006
Validation rmse logP = 0.469085
Validation R2 logP = 0.936138
Epoch 83
Train function
Loss = 2.6887e-05, PNorm = 52.2713, GNorm = 0.1904, lr_0 = 1.4770e-04
Loss = 2.4222e-05, PNorm = 52.2727, GNorm = 0.1366, lr_0 = 1.4755e-04
Loss = 2.2026e-05, PNorm = 52.2731, GNorm = 0.0812, lr_0 = 1.4739e-04
Loss = 1.7699e-05, PNorm = 52.2742, GNorm = 0.1097, lr_0 = 1.4724e-04
Loss = 1.8653e-05, PNorm = 52.2754, GNorm = 0.0864, lr_0 = 1.4709e-04
Loss = 1.7377e-05, PNorm = 52.2761, GNorm = 0.0730, lr_0 = 1.4693e-04
Loss = 1.6901e-05, PNorm = 52.2773, GNorm = 0.0532, lr_0 = 1.4678e-04
Loss = 1.9312e-05, PNorm = 52.2783, GNorm = 0.0565, lr_0 = 1.4663e-04
Loss = 1.9450e-05, PNorm = 52.2782, GNorm = 0.1842, lr_0 = 1.4647e-04
Loss = 2.0072e-05, PNorm = 52.2795, GNorm = 0.1220, lr_0 = 1.4632e-04
Loss = 1.7838e-05, PNorm = 52.2813, GNorm = 0.0882, lr_0 = 1.4617e-04
Loss = 2.9065e-05, PNorm = 52.2821, GNorm = 0.1341, lr_0 = 1.4602e-04
Loss = 1.9707e-05, PNorm = 52.2829, GNorm = 0.0670, lr_0 = 1.4586e-04
Loss = 2.1152e-05, PNorm = 52.2828, GNorm = 0.1063, lr_0 = 1.4571e-04
Loss = 1.6455e-05, PNorm = 52.2830, GNorm = 0.0888, lr_0 = 1.4556e-04
Loss = 1.5625e-05, PNorm = 52.2829, GNorm = 0.1246, lr_0 = 1.4541e-04
Loss = 1.4714e-05, PNorm = 52.2830, GNorm = 0.0749, lr_0 = 1.4526e-04
Loss = 1.7267e-05, PNorm = 52.2845, GNorm = 0.0720, lr_0 = 1.4510e-04
Loss = 2.6058e-05, PNorm = 52.2848, GNorm = 0.1086, lr_0 = 1.4495e-04
Loss = 1.8245e-05, PNorm = 52.2861, GNorm = 0.0612, lr_0 = 1.4480e-04
Loss = 1.8487e-05, PNorm = 52.2876, GNorm = 0.1007, lr_0 = 1.4465e-04
Loss = 1.8121e-05, PNorm = 52.2885, GNorm = 0.1746, lr_0 = 1.4450e-04
Loss = 2.1690e-05, PNorm = 52.2890, GNorm = 0.1899, lr_0 = 1.4435e-04
Validation rmse logD = 0.581731
Validation R2 logD = 0.765619
Validation rmse logP = 0.470124
Validation R2 logP = 0.935854
Epoch 84
Train function
Loss = 1.8105e-05, PNorm = 52.2886, GNorm = 0.1014, lr_0 = 1.4420e-04
Loss = 2.2620e-05, PNorm = 52.2896, GNorm = 0.0835, lr_0 = 1.4405e-04
Loss = 1.4216e-05, PNorm = 52.2906, GNorm = 0.0558, lr_0 = 1.4390e-04
Loss = 1.7207e-05, PNorm = 52.2920, GNorm = 0.0611, lr_0 = 1.4375e-04
Loss = 2.0218e-05, PNorm = 52.2931, GNorm = 0.0583, lr_0 = 1.4360e-04
Loss = 2.1464e-05, PNorm = 52.2942, GNorm = 0.0550, lr_0 = 1.4345e-04
Loss = 1.5592e-05, PNorm = 52.2945, GNorm = 0.1462, lr_0 = 1.4330e-04
Loss = 1.7953e-05, PNorm = 52.2953, GNorm = 0.1499, lr_0 = 1.4315e-04
Loss = 1.9677e-05, PNorm = 52.2967, GNorm = 0.0858, lr_0 = 1.4300e-04
Loss = 2.0771e-05, PNorm = 52.2984, GNorm = 0.1171, lr_0 = 1.4285e-04
Loss = 1.7637e-05, PNorm = 52.3005, GNorm = 0.0942, lr_0 = 1.4270e-04
Loss = 1.2147e-05, PNorm = 52.3020, GNorm = 0.0734, lr_0 = 1.4255e-04
Loss = 1.7104e-05, PNorm = 52.3040, GNorm = 0.0777, lr_0 = 1.4240e-04
Loss = 1.6103e-05, PNorm = 52.3049, GNorm = 0.1026, lr_0 = 1.4225e-04
Loss = 1.6970e-05, PNorm = 52.3065, GNorm = 0.0646, lr_0 = 1.4210e-04
Loss = 1.7039e-05, PNorm = 52.3075, GNorm = 0.0551, lr_0 = 1.4196e-04
Loss = 2.0970e-05, PNorm = 52.3086, GNorm = 0.1140, lr_0 = 1.4181e-04
Loss = 1.7055e-05, PNorm = 52.3092, GNorm = 0.0935, lr_0 = 1.4166e-04
Loss = 1.6745e-05, PNorm = 52.3094, GNorm = 0.0619, lr_0 = 1.4151e-04
Loss = 2.4006e-05, PNorm = 52.3105, GNorm = 0.1325, lr_0 = 1.4136e-04
Loss = 2.2723e-05, PNorm = 52.3118, GNorm = 0.1001, lr_0 = 1.4122e-04
Loss = 2.1310e-05, PNorm = 52.3136, GNorm = 0.0584, lr_0 = 1.4107e-04
Validation rmse logD = 0.581860
Validation R2 logD = 0.765515
Validation rmse logP = 0.467539
Validation R2 logP = 0.936558
Epoch 85
Train function
Loss = 1.5908e-05, PNorm = 52.3148, GNorm = 0.1057, lr_0 = 1.4091e-04
Loss = 2.2210e-05, PNorm = 52.3157, GNorm = 0.0924, lr_0 = 1.4076e-04
Loss = 2.1300e-05, PNorm = 52.3169, GNorm = 0.0830, lr_0 = 1.4061e-04
Loss = 2.1016e-05, PNorm = 52.3186, GNorm = 0.1213, lr_0 = 1.4047e-04
Loss = 1.5629e-05, PNorm = 52.3200, GNorm = 0.0839, lr_0 = 1.4032e-04
Loss = 1.6780e-05, PNorm = 52.3204, GNorm = 0.0774, lr_0 = 1.4017e-04
Loss = 1.9789e-05, PNorm = 52.3209, GNorm = 0.0781, lr_0 = 1.4003e-04
Loss = 1.8780e-05, PNorm = 52.3215, GNorm = 0.0764, lr_0 = 1.3988e-04
Loss = 1.5752e-05, PNorm = 52.3234, GNorm = 0.2019, lr_0 = 1.3974e-04
Loss = 1.7968e-05, PNorm = 52.3238, GNorm = 0.0940, lr_0 = 1.3959e-04
Loss = 1.5597e-05, PNorm = 52.3253, GNorm = 0.0916, lr_0 = 1.3944e-04
Loss = 1.5811e-05, PNorm = 52.3257, GNorm = 0.1385, lr_0 = 1.3930e-04
Loss = 1.4359e-05, PNorm = 52.3264, GNorm = 0.1163, lr_0 = 1.3915e-04
Loss = 1.3777e-05, PNorm = 52.3273, GNorm = 0.0869, lr_0 = 1.3901e-04
Loss = 1.8157e-05, PNorm = 52.3284, GNorm = 0.0623, lr_0 = 1.3886e-04
Loss = 2.0258e-05, PNorm = 52.3286, GNorm = 0.0623, lr_0 = 1.3872e-04
Loss = 2.1728e-05, PNorm = 52.3299, GNorm = 0.0651, lr_0 = 1.3857e-04
Loss = 1.9197e-05, PNorm = 52.3310, GNorm = 0.1498, lr_0 = 1.3843e-04
Loss = 1.5838e-05, PNorm = 52.3315, GNorm = 0.1450, lr_0 = 1.3828e-04
Loss = 1.8864e-05, PNorm = 52.3313, GNorm = 0.1091, lr_0 = 1.3814e-04
Loss = 1.6892e-05, PNorm = 52.3318, GNorm = 0.1603, lr_0 = 1.3800e-04
Loss = 1.7843e-05, PNorm = 52.3331, GNorm = 0.1018, lr_0 = 1.3785e-04
Loss = 1.7980e-05, PNorm = 52.3340, GNorm = 0.0940, lr_0 = 1.3771e-04
Validation rmse logD = 0.581879
Validation R2 logD = 0.765500
Validation rmse logP = 0.472109
Validation R2 logP = 0.935312
Epoch 86
Train function
Loss = 2.1285e-05, PNorm = 52.3357, GNorm = 0.0963, lr_0 = 1.3756e-04
Loss = 2.1676e-05, PNorm = 52.3367, GNorm = 0.0775, lr_0 = 1.3742e-04
Loss = 1.5956e-05, PNorm = 52.3369, GNorm = 0.0747, lr_0 = 1.3728e-04
Loss = 2.3972e-05, PNorm = 52.3378, GNorm = 0.1674, lr_0 = 1.3713e-04
Loss = 1.6013e-05, PNorm = 52.3380, GNorm = 0.1253, lr_0 = 1.3699e-04
Loss = 1.8056e-05, PNorm = 52.3394, GNorm = 0.0936, lr_0 = 1.3685e-04
Loss = 1.7910e-05, PNorm = 52.3401, GNorm = 0.0875, lr_0 = 1.3670e-04
Loss = 2.6034e-05, PNorm = 52.3417, GNorm = 0.0792, lr_0 = 1.3656e-04
Loss = 1.4291e-05, PNorm = 52.3423, GNorm = 0.1037, lr_0 = 1.3642e-04
Loss = 1.8605e-05, PNorm = 52.3435, GNorm = 0.0819, lr_0 = 1.3628e-04
Loss = 1.4821e-05, PNorm = 52.3443, GNorm = 0.0703, lr_0 = 1.3613e-04
Loss = 1.8077e-05, PNorm = 52.3444, GNorm = 0.1185, lr_0 = 1.3599e-04
Loss = 1.8054e-05, PNorm = 52.3452, GNorm = 0.1259, lr_0 = 1.3585e-04
Loss = 1.6593e-05, PNorm = 52.3469, GNorm = 0.1277, lr_0 = 1.3571e-04
Loss = 1.4923e-05, PNorm = 52.3475, GNorm = 0.0943, lr_0 = 1.3557e-04
Loss = 1.4707e-05, PNorm = 52.3484, GNorm = 0.0771, lr_0 = 1.3543e-04
Loss = 1.8115e-05, PNorm = 52.3492, GNorm = 0.0885, lr_0 = 1.3528e-04
Loss = 1.6069e-05, PNorm = 52.3503, GNorm = 0.1412, lr_0 = 1.3514e-04
Loss = 1.6836e-05, PNorm = 52.3506, GNorm = 0.0564, lr_0 = 1.3500e-04
Loss = 1.3012e-05, PNorm = 52.3512, GNorm = 0.1101, lr_0 = 1.3486e-04
Loss = 1.7894e-05, PNorm = 52.3521, GNorm = 0.1191, lr_0 = 1.3472e-04
Loss = 1.5629e-05, PNorm = 52.3531, GNorm = 0.0703, lr_0 = 1.3458e-04
Validation rmse logD = 0.581624
Validation R2 logD = 0.765705
Validation rmse logP = 0.468845
Validation R2 logP = 0.936203
Epoch 87
Train function
Loss = 1.2389e-05, PNorm = 52.3545, GNorm = 0.0694, lr_0 = 1.3443e-04
Loss = 1.4795e-05, PNorm = 52.3553, GNorm = 0.0701, lr_0 = 1.3428e-04
Loss = 1.3582e-05, PNorm = 52.3559, GNorm = 0.0769, lr_0 = 1.3414e-04
Loss = 1.8826e-05, PNorm = 52.3564, GNorm = 0.0932, lr_0 = 1.3400e-04
Loss = 1.5478e-05, PNorm = 52.3571, GNorm = 0.1950, lr_0 = 1.3386e-04
Loss = 1.7346e-05, PNorm = 52.3578, GNorm = 0.1103, lr_0 = 1.3373e-04
Loss = 1.6748e-05, PNorm = 52.3581, GNorm = 0.0947, lr_0 = 1.3359e-04
Loss = 1.8448e-05, PNorm = 52.3589, GNorm = 0.0751, lr_0 = 1.3345e-04
Loss = 1.5601e-05, PNorm = 52.3601, GNorm = 0.1244, lr_0 = 1.3331e-04
Loss = 1.8597e-05, PNorm = 52.3606, GNorm = 0.1154, lr_0 = 1.3317e-04
Loss = 1.9702e-05, PNorm = 52.3604, GNorm = 0.0963, lr_0 = 1.3303e-04
Loss = 1.6580e-05, PNorm = 52.3607, GNorm = 0.0995, lr_0 = 1.3289e-04
Loss = 1.9725e-05, PNorm = 52.3622, GNorm = 0.0844, lr_0 = 1.3275e-04
Loss = 2.1827e-05, PNorm = 52.3624, GNorm = 0.1064, lr_0 = 1.3261e-04
Loss = 1.6623e-05, PNorm = 52.3644, GNorm = 0.1300, lr_0 = 1.3247e-04
Loss = 1.5215e-05, PNorm = 52.3660, GNorm = 0.1005, lr_0 = 1.3234e-04
Loss = 1.7835e-05, PNorm = 52.3672, GNorm = 0.1423, lr_0 = 1.3220e-04
Loss = 2.0664e-05, PNorm = 52.3670, GNorm = 0.0677, lr_0 = 1.3206e-04
Loss = 1.5201e-05, PNorm = 52.3677, GNorm = 0.0552, lr_0 = 1.3192e-04
Loss = 1.5214e-05, PNorm = 52.3688, GNorm = 0.1037, lr_0 = 1.3178e-04
Loss = 1.5561e-05, PNorm = 52.3702, GNorm = 0.0558, lr_0 = 1.3165e-04
Loss = 1.3575e-05, PNorm = 52.3708, GNorm = 0.0719, lr_0 = 1.3151e-04
Loss = 1.8583e-05, PNorm = 52.3714, GNorm = 0.1045, lr_0 = 1.3137e-04
Validation rmse logD = 0.581574
Validation R2 logD = 0.765746
Validation rmse logP = 0.469270
Validation R2 logP = 0.936087
Epoch 88
Train function
Loss = 1.3256e-05, PNorm = 52.3726, GNorm = 0.0834, lr_0 = 1.3124e-04
Loss = 1.2867e-05, PNorm = 52.3727, GNorm = 0.0738, lr_0 = 1.3110e-04
Loss = 1.5831e-05, PNorm = 52.3736, GNorm = 0.0856, lr_0 = 1.3096e-04
Loss = 1.9929e-05, PNorm = 52.3744, GNorm = 0.1749, lr_0 = 1.3082e-04
Loss = 1.4918e-05, PNorm = 52.3756, GNorm = 0.1170, lr_0 = 1.3069e-04
Loss = 2.0418e-05, PNorm = 52.3774, GNorm = 0.1555, lr_0 = 1.3055e-04
Loss = 1.5615e-05, PNorm = 52.3793, GNorm = 0.1595, lr_0 = 1.3042e-04
Loss = 1.6005e-05, PNorm = 52.3804, GNorm = 0.1160, lr_0 = 1.3028e-04
Loss = 1.9973e-05, PNorm = 52.3806, GNorm = 0.0998, lr_0 = 1.3014e-04
Loss = 2.2228e-05, PNorm = 52.3824, GNorm = 0.1558, lr_0 = 1.3001e-04
Loss = 1.7617e-05, PNorm = 52.3832, GNorm = 0.0993, lr_0 = 1.2987e-04
Loss = 1.5065e-05, PNorm = 52.3841, GNorm = 0.1117, lr_0 = 1.2974e-04
Loss = 1.8819e-05, PNorm = 52.3857, GNorm = 0.0784, lr_0 = 1.2960e-04
Loss = 1.7406e-05, PNorm = 52.3877, GNorm = 0.1170, lr_0 = 1.2947e-04
Loss = 1.4623e-05, PNorm = 52.3889, GNorm = 0.0616, lr_0 = 1.2933e-04
Loss = 1.4712e-05, PNorm = 52.3901, GNorm = 0.1000, lr_0 = 1.2920e-04
Loss = 2.1336e-05, PNorm = 52.3906, GNorm = 0.0596, lr_0 = 1.2906e-04
Loss = 1.9071e-05, PNorm = 52.3913, GNorm = 0.0788, lr_0 = 1.2893e-04
Loss = 1.6318e-05, PNorm = 52.3924, GNorm = 0.1136, lr_0 = 1.2879e-04
Loss = 1.4905e-05, PNorm = 52.3929, GNorm = 0.1024, lr_0 = 1.2866e-04
Loss = 1.7618e-05, PNorm = 52.3939, GNorm = 0.1584, lr_0 = 1.2852e-04
Loss = 2.1780e-05, PNorm = 52.3949, GNorm = 0.1390, lr_0 = 1.2839e-04
Validation rmse logD = 0.582698
Validation R2 logD = 0.764839
Validation rmse logP = 0.469963
Validation R2 logP = 0.935898
Epoch 89
Train function
Loss = 1.3854e-05, PNorm = 52.3969, GNorm = 0.0748, lr_0 = 1.2824e-04
Loss = 1.6171e-05, PNorm = 52.3974, GNorm = 0.1028, lr_0 = 1.2811e-04
Loss = 1.4700e-05, PNorm = 52.3979, GNorm = 0.1275, lr_0 = 1.2797e-04
Loss = 1.5284e-05, PNorm = 52.3985, GNorm = 0.0843, lr_0 = 1.2784e-04
Loss = 1.7037e-05, PNorm = 52.3988, GNorm = 0.0485, lr_0 = 1.2771e-04
Loss = 1.3734e-05, PNorm = 52.4000, GNorm = 0.1088, lr_0 = 1.2757e-04
Loss = 1.7126e-05, PNorm = 52.4010, GNorm = 0.0562, lr_0 = 1.2744e-04
Loss = 1.4477e-05, PNorm = 52.4022, GNorm = 0.0975, lr_0 = 1.2731e-04
Loss = 1.7467e-05, PNorm = 52.4031, GNorm = 0.1197, lr_0 = 1.2717e-04
Loss = 1.4764e-05, PNorm = 52.4032, GNorm = 0.0739, lr_0 = 1.2704e-04
Loss = 1.6020e-05, PNorm = 52.4031, GNorm = 0.0744, lr_0 = 1.2691e-04
Loss = 1.7889e-05, PNorm = 52.4037, GNorm = 0.1149, lr_0 = 1.2678e-04
Loss = 1.8402e-05, PNorm = 52.4038, GNorm = 0.0807, lr_0 = 1.2664e-04
Loss = 1.7531e-05, PNorm = 52.4045, GNorm = 0.1365, lr_0 = 1.2651e-04
Loss = 1.8511e-05, PNorm = 52.4059, GNorm = 0.2075, lr_0 = 1.2638e-04
Loss = 1.4793e-05, PNorm = 52.4068, GNorm = 0.1186, lr_0 = 1.2625e-04
Loss = 1.6602e-05, PNorm = 52.4076, GNorm = 0.0929, lr_0 = 1.2612e-04
Loss = 1.5225e-05, PNorm = 52.4085, GNorm = 0.0693, lr_0 = 1.2598e-04
Loss = 1.4248e-05, PNorm = 52.4090, GNorm = 0.0603, lr_0 = 1.2585e-04
Loss = 1.4214e-05, PNorm = 52.4090, GNorm = 0.1293, lr_0 = 1.2572e-04
Loss = 1.4201e-05, PNorm = 52.4109, GNorm = 0.0728, lr_0 = 1.2559e-04
Loss = 1.5813e-05, PNorm = 52.4113, GNorm = 0.1621, lr_0 = 1.2546e-04
Loss = 1.4774e-05, PNorm = 52.4123, GNorm = 0.0712, lr_0 = 1.2533e-04
Validation rmse logD = 0.580824
Validation R2 logD = 0.766349
Validation rmse logP = 0.469557
Validation R2 logP = 0.936009
Epoch 90
Train function
Loss = 1.3483e-05, PNorm = 52.4129, GNorm = 0.0841, lr_0 = 1.2520e-04
Loss = 1.2101e-05, PNorm = 52.4131, GNorm = 0.0868, lr_0 = 1.2507e-04
Loss = 1.5917e-05, PNorm = 52.4136, GNorm = 0.0752, lr_0 = 1.2494e-04
Loss = 1.0765e-05, PNorm = 52.4136, GNorm = 0.0716, lr_0 = 1.2481e-04
Loss = 1.6465e-05, PNorm = 52.4149, GNorm = 0.1106, lr_0 = 1.2468e-04
Loss = 1.4102e-05, PNorm = 52.4161, GNorm = 0.1167, lr_0 = 1.2455e-04
Loss = 1.2404e-05, PNorm = 52.4173, GNorm = 0.0653, lr_0 = 1.2442e-04
Loss = 1.3554e-05, PNorm = 52.4176, GNorm = 0.0608, lr_0 = 1.2429e-04
Loss = 1.0925e-05, PNorm = 52.4184, GNorm = 0.0582, lr_0 = 1.2416e-04
Loss = 1.4049e-05, PNorm = 52.4197, GNorm = 0.0839, lr_0 = 1.2403e-04
Loss = 1.4665e-05, PNorm = 52.4212, GNorm = 0.0687, lr_0 = 1.2390e-04
Loss = 1.2164e-05, PNorm = 52.4221, GNorm = 0.0779, lr_0 = 1.2377e-04
Loss = 1.5787e-05, PNorm = 52.4229, GNorm = 0.0673, lr_0 = 1.2364e-04
Loss = 1.2030e-05, PNorm = 52.4233, GNorm = 0.0652, lr_0 = 1.2351e-04
Loss = 1.1227e-05, PNorm = 52.4235, GNorm = 0.1111, lr_0 = 1.2338e-04
Loss = 1.4562e-05, PNorm = 52.4250, GNorm = 0.0910, lr_0 = 1.2325e-04
Loss = 1.6691e-05, PNorm = 52.4263, GNorm = 0.1053, lr_0 = 1.2312e-04
Loss = 2.1966e-05, PNorm = 52.4273, GNorm = 0.1875, lr_0 = 1.2299e-04
Loss = 1.5813e-05, PNorm = 52.4287, GNorm = 0.0762, lr_0 = 1.2287e-04
Loss = 2.0605e-05, PNorm = 52.4312, GNorm = 0.1333, lr_0 = 1.2274e-04
Loss = 1.6693e-05, PNorm = 52.4321, GNorm = 0.0973, lr_0 = 1.2261e-04
Loss = 1.3974e-05, PNorm = 52.4330, GNorm = 0.0980, lr_0 = 1.2248e-04
Validation rmse logD = 0.580262
Validation R2 logD = 0.766801
Validation rmse logP = 0.469033
Validation R2 logP = 0.936152
Epoch 91
Train function
Loss = 9.6110e-06, PNorm = 52.4336, GNorm = 0.0680, lr_0 = 1.2234e-04
Loss = 1.4828e-05, PNorm = 52.4342, GNorm = 0.0720, lr_0 = 1.2221e-04
Loss = 1.1508e-05, PNorm = 52.4338, GNorm = 0.0863, lr_0 = 1.2209e-04
Loss = 1.0936e-05, PNorm = 52.4342, GNorm = 0.0648, lr_0 = 1.2196e-04
Loss = 1.1595e-05, PNorm = 52.4345, GNorm = 0.0708, lr_0 = 1.2183e-04
Loss = 1.1793e-05, PNorm = 52.4345, GNorm = 0.1699, lr_0 = 1.2170e-04
Loss = 1.4085e-05, PNorm = 52.4353, GNorm = 0.0566, lr_0 = 1.2158e-04
Loss = 2.3495e-05, PNorm = 52.4356, GNorm = 0.0986, lr_0 = 1.2145e-04
Loss = 1.8084e-05, PNorm = 52.4362, GNorm = 0.0671, lr_0 = 1.2132e-04
Loss = 1.8996e-05, PNorm = 52.4369, GNorm = 0.0537, lr_0 = 1.2120e-04
Loss = 1.8993e-05, PNorm = 52.4378, GNorm = 0.0837, lr_0 = 1.2107e-04
Loss = 1.3416e-05, PNorm = 52.4385, GNorm = 0.0642, lr_0 = 1.2094e-04
Loss = 1.5972e-05, PNorm = 52.4399, GNorm = 0.0785, lr_0 = 1.2082e-04
Loss = 1.3201e-05, PNorm = 52.4415, GNorm = 0.0544, lr_0 = 1.2069e-04
Loss = 1.8112e-05, PNorm = 52.4424, GNorm = 0.1232, lr_0 = 1.2057e-04
Loss = 1.2680e-05, PNorm = 52.4441, GNorm = 0.0816, lr_0 = 1.2044e-04
Loss = 1.5997e-05, PNorm = 52.4446, GNorm = 0.0954, lr_0 = 1.2031e-04
Loss = 1.3066e-05, PNorm = 52.4458, GNorm = 0.0696, lr_0 = 1.2019e-04
Loss = 1.1161e-05, PNorm = 52.4468, GNorm = 0.0713, lr_0 = 1.2006e-04
Loss = 1.4856e-05, PNorm = 52.4477, GNorm = 0.0791, lr_0 = 1.1994e-04
Loss = 1.5542e-05, PNorm = 52.4487, GNorm = 0.0613, lr_0 = 1.1981e-04
Loss = 1.7108e-05, PNorm = 52.4498, GNorm = 0.1435, lr_0 = 1.1969e-04
Loss = 1.5733e-05, PNorm = 52.4496, GNorm = 0.0743, lr_0 = 1.1956e-04
Validation rmse logD = 0.582050
Validation R2 logD = 0.765362
Validation rmse logP = 0.469340
Validation R2 logP = 0.936068
Epoch 92
Train function
Loss = 1.5222e-05, PNorm = 52.4489, GNorm = 0.0843, lr_0 = 1.1944e-04
Loss = 1.6105e-05, PNorm = 52.4494, GNorm = 0.1289, lr_0 = 1.1931e-04
Loss = 1.3007e-05, PNorm = 52.4496, GNorm = 0.0865, lr_0 = 1.1919e-04
Loss = 1.9933e-05, PNorm = 52.4500, GNorm = 0.0953, lr_0 = 1.1906e-04
Loss = 1.6581e-05, PNorm = 52.4505, GNorm = 0.0569, lr_0 = 1.1894e-04
Loss = 1.8599e-05, PNorm = 52.4514, GNorm = 0.0944, lr_0 = 1.1882e-04
Loss = 1.5797e-05, PNorm = 52.4534, GNorm = 0.0812, lr_0 = 1.1869e-04
Loss = 1.4256e-05, PNorm = 52.4552, GNorm = 0.1253, lr_0 = 1.1857e-04
Loss = 1.3472e-05, PNorm = 52.4562, GNorm = 0.1498, lr_0 = 1.1844e-04
Loss = 1.4584e-05, PNorm = 52.4573, GNorm = 0.1091, lr_0 = 1.1832e-04
Loss = 1.1648e-05, PNorm = 52.4574, GNorm = 0.0757, lr_0 = 1.1820e-04
Loss = 1.2649e-05, PNorm = 52.4575, GNorm = 0.0696, lr_0 = 1.1807e-04
Loss = 1.4163e-05, PNorm = 52.4578, GNorm = 0.0943, lr_0 = 1.1795e-04
Loss = 1.5003e-05, PNorm = 52.4576, GNorm = 0.0793, lr_0 = 1.1783e-04
Loss = 1.1465e-05, PNorm = 52.4583, GNorm = 0.0537, lr_0 = 1.1770e-04
Loss = 1.4137e-05, PNorm = 52.4594, GNorm = 0.1043, lr_0 = 1.1758e-04
Loss = 1.1192e-05, PNorm = 52.4597, GNorm = 0.0617, lr_0 = 1.1746e-04
Loss = 1.2798e-05, PNorm = 52.4593, GNorm = 0.1238, lr_0 = 1.1734e-04
Loss = 1.1922e-05, PNorm = 52.4601, GNorm = 0.0802, lr_0 = 1.1721e-04
Loss = 1.2257e-05, PNorm = 52.4616, GNorm = 0.1062, lr_0 = 1.1709e-04
Loss = 1.7150e-05, PNorm = 52.4626, GNorm = 0.1073, lr_0 = 1.1697e-04
Loss = 1.5382e-05, PNorm = 52.4644, GNorm = 0.0980, lr_0 = 1.1685e-04
Validation rmse logD = 0.583548
Validation R2 logD = 0.764153
Validation rmse logP = 0.469039
Validation R2 logP = 0.936150
Epoch 93
Train function
Loss = 3.3731e-05, PNorm = 52.4655, GNorm = 0.0960, lr_0 = 1.1671e-04
Loss = 1.6994e-05, PNorm = 52.4660, GNorm = 0.1656, lr_0 = 1.1659e-04
Loss = 3.0001e-05, PNorm = 52.4664, GNorm = 0.1552, lr_0 = 1.1647e-04
Loss = 2.6770e-05, PNorm = 52.4657, GNorm = 0.0983, lr_0 = 1.1635e-04
Loss = 2.2528e-05, PNorm = 52.4667, GNorm = 0.1238, lr_0 = 1.1623e-04
Loss = 1.9706e-05, PNorm = 52.4673, GNorm = 0.0715, lr_0 = 1.1611e-04
Loss = 1.7267e-05, PNorm = 52.4678, GNorm = 0.0907, lr_0 = 1.1598e-04
Loss = 1.4755e-05, PNorm = 52.4682, GNorm = 0.1786, lr_0 = 1.1586e-04
Loss = 1.7531e-05, PNorm = 52.4695, GNorm = 0.0946, lr_0 = 1.1574e-04
Loss = 1.7434e-05, PNorm = 52.4707, GNorm = 0.1205, lr_0 = 1.1562e-04
Loss = 1.5289e-05, PNorm = 52.4709, GNorm = 0.0819, lr_0 = 1.1550e-04
Loss = 1.4532e-05, PNorm = 52.4715, GNorm = 0.0934, lr_0 = 1.1538e-04
Loss = 1.7526e-05, PNorm = 52.4727, GNorm = 0.1239, lr_0 = 1.1526e-04
Loss = 1.3747e-05, PNorm = 52.4740, GNorm = 0.0823, lr_0 = 1.1514e-04
Loss = 1.3996e-05, PNorm = 52.4751, GNorm = 0.0649, lr_0 = 1.1502e-04
Loss = 1.3294e-05, PNorm = 52.4760, GNorm = 0.1429, lr_0 = 1.1490e-04
Loss = 1.2483e-05, PNorm = 52.4765, GNorm = 0.0755, lr_0 = 1.1478e-04
Loss = 1.2227e-05, PNorm = 52.4771, GNorm = 0.1034, lr_0 = 1.1466e-04
Loss = 1.4889e-05, PNorm = 52.4781, GNorm = 0.1449, lr_0 = 1.1454e-04
Loss = 1.6631e-05, PNorm = 52.4784, GNorm = 0.1312, lr_0 = 1.1442e-04
Loss = 2.0405e-05, PNorm = 52.4801, GNorm = 0.1875, lr_0 = 1.1430e-04
Loss = 2.2192e-05, PNorm = 52.4820, GNorm = 0.1628, lr_0 = 1.1418e-04
Loss = 1.5521e-05, PNorm = 52.4828, GNorm = 0.0791, lr_0 = 1.1406e-04
Validation rmse logD = 0.583109
Validation R2 logD = 0.764507
Validation rmse logP = 0.468690
Validation R2 logP = 0.936245
Epoch 94
Train function
Loss = 1.5545e-05, PNorm = 52.4844, GNorm = 0.1012, lr_0 = 1.1394e-04
Loss = 1.6379e-05, PNorm = 52.4852, GNorm = 0.0864, lr_0 = 1.1382e-04
Loss = 1.0797e-05, PNorm = 52.4862, GNorm = 0.0971, lr_0 = 1.1371e-04
Loss = 1.4273e-05, PNorm = 52.4861, GNorm = 0.0648, lr_0 = 1.1359e-04
Loss = 1.3390e-05, PNorm = 52.4866, GNorm = 0.1802, lr_0 = 1.1347e-04
Loss = 1.5831e-05, PNorm = 52.4881, GNorm = 0.0714, lr_0 = 1.1335e-04
Loss = 1.7196e-05, PNorm = 52.4893, GNorm = 0.1320, lr_0 = 1.1323e-04
Loss = 1.4029e-05, PNorm = 52.4897, GNorm = 0.0950, lr_0 = 1.1311e-04
Loss = 1.4190e-05, PNorm = 52.4903, GNorm = 0.1047, lr_0 = 1.1300e-04
Loss = 1.6257e-05, PNorm = 52.4904, GNorm = 0.1254, lr_0 = 1.1288e-04
Loss = 1.0026e-05, PNorm = 52.4906, GNorm = 0.0573, lr_0 = 1.1276e-04
Loss = 1.3384e-05, PNorm = 52.4919, GNorm = 0.0601, lr_0 = 1.1264e-04
Loss = 1.6915e-05, PNorm = 52.4926, GNorm = 0.1477, lr_0 = 1.1252e-04
Loss = 1.1313e-05, PNorm = 52.4929, GNorm = 0.0837, lr_0 = 1.1241e-04
Loss = 1.4091e-05, PNorm = 52.4941, GNorm = 0.1116, lr_0 = 1.1229e-04
Loss = 1.6593e-05, PNorm = 52.4946, GNorm = 0.0914, lr_0 = 1.1217e-04
Loss = 1.5581e-05, PNorm = 52.4945, GNorm = 0.0798, lr_0 = 1.1206e-04
Loss = 1.3848e-05, PNorm = 52.4957, GNorm = 0.1206, lr_0 = 1.1194e-04
Loss = 1.0835e-05, PNorm = 52.4970, GNorm = 0.0615, lr_0 = 1.1182e-04
Loss = 1.3395e-05, PNorm = 52.4970, GNorm = 0.0626, lr_0 = 1.1170e-04
Loss = 1.2856e-05, PNorm = 52.4977, GNorm = 0.0722, lr_0 = 1.1159e-04
Loss = 1.2195e-05, PNorm = 52.4980, GNorm = 0.0623, lr_0 = 1.1147e-04
Loss = 1.6475e-05, PNorm = 52.4994, GNorm = 0.0582, lr_0 = 1.1136e-04
Loss = 2.4826e-05, PNorm = 52.4996, GNorm = 0.0790, lr_0 = 1.1134e-04
Validation rmse logD = 0.582374
Validation R2 logD = 0.765101
Validation rmse logP = 0.467985
Validation R2 logP = 0.936437
Epoch 95
Train function
Loss = 1.3649e-05, PNorm = 52.5006, GNorm = 0.1140, lr_0 = 1.1123e-04
Loss = 1.3588e-05, PNorm = 52.5014, GNorm = 0.1608, lr_0 = 1.1111e-04
Loss = 1.6073e-05, PNorm = 52.5028, GNorm = 0.0752, lr_0 = 1.1100e-04
Loss = 1.7692e-05, PNorm = 52.5032, GNorm = 0.1499, lr_0 = 1.1088e-04
Loss = 1.5683e-05, PNorm = 52.5043, GNorm = 0.1241, lr_0 = 1.1076e-04
Loss = 1.1076e-05, PNorm = 52.5045, GNorm = 0.0975, lr_0 = 1.1065e-04
Loss = 1.2983e-05, PNorm = 52.5047, GNorm = 0.0599, lr_0 = 1.1053e-04
Loss = 1.0284e-05, PNorm = 52.5057, GNorm = 0.0914, lr_0 = 1.1042e-04
Loss = 1.2510e-05, PNorm = 52.5062, GNorm = 0.0726, lr_0 = 1.1030e-04
Loss = 1.3334e-05, PNorm = 52.5066, GNorm = 0.0663, lr_0 = 1.1019e-04
Loss = 1.1287e-05, PNorm = 52.5073, GNorm = 0.0655, lr_0 = 1.1007e-04
Loss = 1.0144e-05, PNorm = 52.5077, GNorm = 0.0816, lr_0 = 1.0996e-04
Loss = 1.1517e-05, PNorm = 52.5083, GNorm = 0.0697, lr_0 = 1.0984e-04
Loss = 1.1766e-05, PNorm = 52.5085, GNorm = 0.0580, lr_0 = 1.0973e-04
Loss = 1.3094e-05, PNorm = 52.5095, GNorm = 0.0792, lr_0 = 1.0961e-04
Loss = 1.0987e-05, PNorm = 52.5104, GNorm = 0.0984, lr_0 = 1.0950e-04
Loss = 9.6321e-06, PNorm = 52.5113, GNorm = 0.1060, lr_0 = 1.0938e-04
Loss = 1.3582e-05, PNorm = 52.5119, GNorm = 0.0578, lr_0 = 1.0927e-04
Loss = 1.0025e-05, PNorm = 52.5120, GNorm = 0.0741, lr_0 = 1.0916e-04
Loss = 1.0498e-05, PNorm = 52.5125, GNorm = 0.0744, lr_0 = 1.0904e-04
Loss = 1.0622e-05, PNorm = 52.5127, GNorm = 0.0490, lr_0 = 1.0893e-04
Loss = 1.2685e-05, PNorm = 52.5134, GNorm = 0.0882, lr_0 = 1.0882e-04
Validation rmse logD = 0.583454
Validation R2 logD = 0.764228
Validation rmse logP = 0.468824
Validation R2 logP = 0.936209
Epoch 96
Train function
Loss = 1.3070e-05, PNorm = 52.5144, GNorm = 0.1134, lr_0 = 1.0870e-04
Loss = 1.0024e-05, PNorm = 52.5146, GNorm = 0.0752, lr_0 = 1.0859e-04
Loss = 8.9842e-06, PNorm = 52.5146, GNorm = 0.0474, lr_0 = 1.0847e-04
Loss = 8.4064e-06, PNorm = 52.5153, GNorm = 0.1024, lr_0 = 1.0836e-04
Loss = 1.2780e-05, PNorm = 52.5171, GNorm = 0.0874, lr_0 = 1.0825e-04
Loss = 1.0381e-05, PNorm = 52.5179, GNorm = 0.0630, lr_0 = 1.0814e-04
Loss = 1.0184e-05, PNorm = 52.5186, GNorm = 0.0555, lr_0 = 1.0802e-04
Loss = 9.3016e-06, PNorm = 52.5198, GNorm = 0.0809, lr_0 = 1.0791e-04
Loss = 1.1896e-05, PNorm = 52.5210, GNorm = 0.0894, lr_0 = 1.0780e-04
Loss = 1.0509e-05, PNorm = 52.5225, GNorm = 0.0986, lr_0 = 1.0768e-04
Loss = 7.5689e-06, PNorm = 52.5222, GNorm = 0.0723, lr_0 = 1.0757e-04
Loss = 1.0970e-05, PNorm = 52.5225, GNorm = 0.1230, lr_0 = 1.0746e-04
Loss = 1.2352e-05, PNorm = 52.5228, GNorm = 0.0550, lr_0 = 1.0735e-04
Loss = 1.0949e-05, PNorm = 52.5228, GNorm = 0.0941, lr_0 = 1.0724e-04
Loss = 1.1967e-05, PNorm = 52.5233, GNorm = 0.0792, lr_0 = 1.0712e-04
Loss = 1.3865e-05, PNorm = 52.5240, GNorm = 0.1091, lr_0 = 1.0701e-04
Loss = 1.1978e-05, PNorm = 52.5247, GNorm = 0.0994, lr_0 = 1.0690e-04
Loss = 1.0882e-05, PNorm = 52.5255, GNorm = 0.0488, lr_0 = 1.0679e-04
Loss = 1.0026e-05, PNorm = 52.5261, GNorm = 0.0593, lr_0 = 1.0668e-04
Loss = 1.3880e-05, PNorm = 52.5275, GNorm = 0.0603, lr_0 = 1.0657e-04
Loss = 1.1490e-05, PNorm = 52.5285, GNorm = 0.1230, lr_0 = 1.0645e-04
Loss = 1.2012e-05, PNorm = 52.5284, GNorm = 0.0984, lr_0 = 1.0634e-04
Loss = 1.0365e-05, PNorm = 52.5293, GNorm = 0.0510, lr_0 = 1.0623e-04
Validation rmse logD = 0.581762
Validation R2 logD = 0.765594
Validation rmse logP = 0.467890
Validation R2 logP = 0.936463
Epoch 97
Train function
Loss = 1.0424e-05, PNorm = 52.5295, GNorm = 0.0769, lr_0 = 1.0611e-04
Loss = 9.3655e-06, PNorm = 52.5296, GNorm = 0.0886, lr_0 = 1.0600e-04
Loss = 1.1252e-05, PNorm = 52.5301, GNorm = 0.0883, lr_0 = 1.0589e-04
Loss = 1.2091e-05, PNorm = 52.5309, GNorm = 0.0527, lr_0 = 1.0578e-04
Loss = 1.1398e-05, PNorm = 52.5313, GNorm = 0.1127, lr_0 = 1.0567e-04
Loss = 9.9809e-06, PNorm = 52.5317, GNorm = 0.0594, lr_0 = 1.0556e-04
Loss = 8.2148e-06, PNorm = 52.5326, GNorm = 0.0708, lr_0 = 1.0545e-04
Loss = 1.0268e-05, PNorm = 52.5328, GNorm = 0.0851, lr_0 = 1.0534e-04
Loss = 1.3230e-05, PNorm = 52.5332, GNorm = 0.1248, lr_0 = 1.0523e-04
Loss = 1.3437e-05, PNorm = 52.5336, GNorm = 0.1446, lr_0 = 1.0512e-04
Loss = 1.0431e-05, PNorm = 52.5348, GNorm = 0.0896, lr_0 = 1.0501e-04
Loss = 8.5469e-06, PNorm = 52.5354, GNorm = 0.0645, lr_0 = 1.0490e-04
Loss = 9.5275e-06, PNorm = 52.5358, GNorm = 0.1074, lr_0 = 1.0479e-04
Loss = 1.5283e-05, PNorm = 52.5363, GNorm = 0.1012, lr_0 = 1.0468e-04
Loss = 1.0404e-05, PNorm = 52.5373, GNorm = 0.0683, lr_0 = 1.0457e-04
Loss = 8.9337e-06, PNorm = 52.5379, GNorm = 0.0648, lr_0 = 1.0446e-04
Loss = 9.6369e-06, PNorm = 52.5386, GNorm = 0.1767, lr_0 = 1.0435e-04
Loss = 1.3850e-05, PNorm = 52.5389, GNorm = 0.1190, lr_0 = 1.0424e-04
Loss = 1.1751e-05, PNorm = 52.5400, GNorm = 0.0304, lr_0 = 1.0413e-04
Loss = 1.2480e-05, PNorm = 52.5406, GNorm = 0.0520, lr_0 = 1.0403e-04
Loss = 1.2230e-05, PNorm = 52.5406, GNorm = 0.0971, lr_0 = 1.0392e-04
Loss = 1.0852e-05, PNorm = 52.5414, GNorm = 0.0685, lr_0 = 1.0381e-04
Validation rmse logD = 0.582717
Validation R2 logD = 0.764824
Validation rmse logP = 0.468515
Validation R2 logP = 0.936293
Epoch 98
Train function
Loss = 7.6770e-06, PNorm = 52.5419, GNorm = 0.1151, lr_0 = 1.0370e-04
Loss = 1.1049e-05, PNorm = 52.5422, GNorm = 0.1283, lr_0 = 1.0359e-04
Loss = 1.7182e-05, PNorm = 52.5429, GNorm = 0.0771, lr_0 = 1.0348e-04
Loss = 8.6525e-06, PNorm = 52.5440, GNorm = 0.0692, lr_0 = 1.0338e-04
Loss = 8.7097e-06, PNorm = 52.5444, GNorm = 0.0590, lr_0 = 1.0327e-04
Loss = 7.4724e-06, PNorm = 52.5447, GNorm = 0.0524, lr_0 = 1.0316e-04
Loss = 1.0464e-05, PNorm = 52.5451, GNorm = 0.0879, lr_0 = 1.0305e-04
Loss = 1.1489e-05, PNorm = 52.5445, GNorm = 0.0627, lr_0 = 1.0295e-04
Loss = 1.2726e-05, PNorm = 52.5457, GNorm = 0.0633, lr_0 = 1.0284e-04
Loss = 9.2980e-06, PNorm = 52.5463, GNorm = 0.0516, lr_0 = 1.0273e-04
Loss = 8.3093e-06, PNorm = 52.5469, GNorm = 0.0860, lr_0 = 1.0262e-04
Loss = 8.4098e-06, PNorm = 52.5479, GNorm = 0.0578, lr_0 = 1.0252e-04
Loss = 9.6692e-06, PNorm = 52.5484, GNorm = 0.0760, lr_0 = 1.0241e-04
Loss = 1.0407e-05, PNorm = 52.5495, GNorm = 0.0543, lr_0 = 1.0230e-04
Loss = 1.1507e-05, PNorm = 52.5502, GNorm = 0.0653, lr_0 = 1.0220e-04
Loss = 1.2629e-05, PNorm = 52.5513, GNorm = 0.0513, lr_0 = 1.0209e-04
Loss = 9.9393e-06, PNorm = 52.5520, GNorm = 0.0554, lr_0 = 1.0198e-04
Loss = 1.2977e-05, PNorm = 52.5528, GNorm = 0.1691, lr_0 = 1.0188e-04
Loss = 1.1963e-05, PNorm = 52.5547, GNorm = 0.0605, lr_0 = 1.0177e-04
Loss = 1.1082e-05, PNorm = 52.5554, GNorm = 0.2320, lr_0 = 1.0166e-04
Loss = 1.2177e-05, PNorm = 52.5558, GNorm = 0.1036, lr_0 = 1.0156e-04
Loss = 1.1464e-05, PNorm = 52.5568, GNorm = 0.0624, lr_0 = 1.0145e-04
Loss = 1.2748e-05, PNorm = 52.5567, GNorm = 0.1611, lr_0 = 1.0135e-04
Validation rmse logD = 0.581058
Validation R2 logD = 0.766161
Validation rmse logP = 0.469614
Validation R2 logP = 0.935994
Epoch 99
Train function
Loss = 1.6110e-05, PNorm = 52.5569, GNorm = 0.1155, lr_0 = 1.0123e-04
Loss = 1.3959e-05, PNorm = 52.5566, GNorm = 0.0970, lr_0 = 1.0112e-04
Loss = 1.0599e-05, PNorm = 52.5573, GNorm = 0.0545, lr_0 = 1.0102e-04
Loss = 8.7542e-06, PNorm = 52.5583, GNorm = 0.0796, lr_0 = 1.0091e-04
Loss = 9.0596e-06, PNorm = 52.5592, GNorm = 0.0925, lr_0 = 1.0081e-04
Loss = 9.5771e-06, PNorm = 52.5598, GNorm = 0.0981, lr_0 = 1.0070e-04
Loss = 9.1323e-06, PNorm = 52.5602, GNorm = 0.0528, lr_0 = 1.0060e-04
Loss = 1.0767e-05, PNorm = 52.5606, GNorm = 0.0724, lr_0 = 1.0049e-04
Loss = 1.1855e-05, PNorm = 52.5613, GNorm = 0.0882, lr_0 = 1.0039e-04
Loss = 1.2178e-05, PNorm = 52.5616, GNorm = 0.1320, lr_0 = 1.0028e-04
Loss = 1.2409e-05, PNorm = 52.5621, GNorm = 0.1029, lr_0 = 1.0018e-04
Loss = 1.2307e-05, PNorm = 52.5627, GNorm = 0.0776, lr_0 = 1.0007e-04
Loss = 1.0154e-05, PNorm = 52.5634, GNorm = 0.1173, lr_0 = 1.0000e-04
Loss = 1.0217e-05, PNorm = 52.5640, GNorm = 0.0624, lr_0 = 1.0000e-04
Loss = 9.1370e-06, PNorm = 52.5647, GNorm = 0.0546, lr_0 = 1.0000e-04
Loss = 8.7647e-06, PNorm = 52.5651, GNorm = 0.0773, lr_0 = 1.0000e-04
Loss = 8.1910e-06, PNorm = 52.5649, GNorm = 0.0996, lr_0 = 1.0000e-04
Loss = 7.2337e-06, PNorm = 52.5654, GNorm = 0.0474, lr_0 = 1.0000e-04
Loss = 8.2179e-06, PNorm = 52.5659, GNorm = 0.0761, lr_0 = 1.0000e-04
Loss = 1.0284e-05, PNorm = 52.5660, GNorm = 0.0545, lr_0 = 1.0000e-04
Loss = 1.3038e-05, PNorm = 52.5665, GNorm = 0.0741, lr_0 = 1.0000e-04
Loss = 1.1578e-05, PNorm = 52.5675, GNorm = 0.0772, lr_0 = 1.0000e-04
Validation rmse logD = 0.583348
Validation R2 logD = 0.764314
Validation rmse logP = 0.469713
Validation R2 logP = 0.935967
Model 0 best validation rmse = 0.522600 on epoch 36
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logD = 0.534958
Model 0 test R2 logD = 0.818493
Model 0 test rmse logP = 0.448863
Model 0 test R2 logP = 0.942131
Ensemble test rmse  logD= 0.534958
Ensemble test R2  logD= 0.818493
Ensemble test rmse  logP= 0.448863
Ensemble test R2  logP= 0.942131
4-fold cross validation
	Seed 0 ==> test rmse = 0.505588
	Seed 0 ==> test R2 = 0.869058
	Seed 1 ==> test rmse = 0.496867
	Seed 1 ==> test R2 = 0.874192
	Seed 2 ==> test rmse = 0.499210
	Seed 2 ==> test R2 = 0.874357
	Seed 3 ==> test rmse = 0.491910
	Seed 3 ==> test R2 = 0.880312
Overall val rmse logD= 0.567204 +/- 0.026378
Overall val R2 logD = 0.772124 +/- 0.019013
Overall test rmse logD = 0.553920 +/- 0.012476
Overall test R2 logD = 0.805299 +/- 0.008731
Overall val rmse logP= 0.456631 +/- 0.007101
Overall val R2 logP = 0.940107 +/- 0.002774
Overall test rmse logP = 0.442868 +/- 0.004489
Overall test R2 logP = 0.943660 +/- 0.001142
Elapsed time = 4:14:02
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_best'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD', 'logP'],
 'task_names': ['logD', 'logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_best'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD', 'logP'],
 'task_names': ['logD', 'logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD'],
 'task_names': ['logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 3,541 | train size = 2,655 | val size = 886 | test size = 625
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=1, bias=True)
  )
)
Number of parameters = 354,901
Moving model to cuda
Epoch 0
Train function
Loss = 2.0493e-02, PNorm = 34.0110, GNorm = 5.4938, lr_0 = 1.0849e-04
Loss = 1.9860e-02, PNorm = 34.0119, GNorm = 2.3289, lr_0 = 1.0849e-04
Loss = 1.7722e-02, PNorm = 34.0135, GNorm = 6.2600, lr_0 = 1.0849e-04
Loss = 1.8290e-02, PNorm = 34.0156, GNorm = 2.7477, lr_0 = 1.0849e-04
Loss = 1.9026e-02, PNorm = 34.0177, GNorm = 4.6016, lr_0 = 1.0849e-04
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_best'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD', 'logP'],
 'task_names': ['logD', 'logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=499, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 414,902
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_wo_MolLogP'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD'],
 'task_names': ['logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 3,541 | train size = 2,655 | val size = 886 | test size = 625
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=499, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=1, bias=True)
  )
)
Number of parameters = 414,601
Moving model to cuda
Epoch 0
Train function
Loss = 2.8252e-02, PNorm = 35.0058, GNorm = 6.3649, lr_0 = 1.0849e-04
Loss = 2.0975e-02, PNorm = 35.0054, GNorm = 6.2100, lr_0 = 1.0849e-04
Loss = 1.8422e-02, PNorm = 35.0061, GNorm = 3.1628, lr_0 = 1.0849e-04
Loss = 1.7571e-02, PNorm = 35.0074, GNorm = 2.2595, lr_0 = 1.0849e-04
Loss = 1.7731e-02, PNorm = 35.0085, GNorm = 3.0250, lr_0 = 1.0849e-04
Validation rmse logD = 1.085169
Validation R2 logD = 0.180012
Epoch 1
Train function
Loss = 1.5315e-02, PNorm = 35.0100, GNorm = 1.9139, lr_0 = 1.0849e-04
Loss = 1.5058e-02, PNorm = 35.0120, GNorm = 1.6984, lr_0 = 1.0849e-04
Loss = 1.5388e-02, PNorm = 35.0138, GNorm = 2.5091, lr_0 = 1.0849e-04
Loss = 1.5452e-02, PNorm = 35.0162, GNorm = 2.6036, lr_0 = 1.0849e-04
Loss = 1.5136e-02, PNorm = 35.0187, GNorm = 4.6189, lr_0 = 1.0849e-04
Validation rmse logD = 1.016627
Validation R2 logD = 0.280325
Epoch 2
Train function
Loss = 1.7563e-02, PNorm = 35.0224, GNorm = 6.7419, lr_0 = 1.0849e-04
Loss = 1.3424e-02, PNorm = 35.0258, GNorm = 2.5288, lr_0 = 1.0849e-04
Loss = 1.2971e-02, PNorm = 35.0288, GNorm = 2.0841, lr_0 = 1.0849e-04
Loss = 1.4302e-02, PNorm = 35.0313, GNorm = 2.5320, lr_0 = 1.0849e-04
Loss = 1.2332e-02, PNorm = 35.0337, GNorm = 2.7913, lr_0 = 1.0849e-04
Validation rmse logD = 0.964078
Validation R2 logD = 0.352801
Epoch 3
Train function
Loss = 1.3470e-02, PNorm = 35.0369, GNorm = 1.9321, lr_0 = 1.0849e-04
Loss = 1.2649e-02, PNorm = 35.0404, GNorm = 3.8279, lr_0 = 1.0849e-04
Loss = 1.2827e-02, PNorm = 35.0441, GNorm = 2.5174, lr_0 = 1.0849e-04
Loss = 1.2564e-02, PNorm = 35.0472, GNorm = 2.0034, lr_0 = 1.0849e-04
Loss = 1.1792e-02, PNorm = 35.0496, GNorm = 2.0524, lr_0 = 1.0849e-04
Loss = 1.2236e-02, PNorm = 35.0530, GNorm = 4.3902, lr_0 = 1.0849e-04
Validation rmse logD = 0.930093
Validation R2 logD = 0.397627
Epoch 4
Train function
Loss = 1.2942e-02, PNorm = 35.0569, GNorm = 4.6581, lr_0 = 1.0849e-04
Loss = 1.2701e-02, PNorm = 35.0609, GNorm = 4.4028, lr_0 = 1.0849e-04
Loss = 9.8586e-03, PNorm = 35.0652, GNorm = 2.4944, lr_0 = 1.0849e-04
Loss = 1.1042e-02, PNorm = 35.0690, GNorm = 1.5788, lr_0 = 1.0849e-04
Loss = 1.1087e-02, PNorm = 35.0727, GNorm = 8.7399, lr_0 = 1.0849e-04
Validation rmse logD = 0.907776
Validation R2 logD = 0.426187
Epoch 5
Train function
Loss = 1.0098e-02, PNorm = 35.0760, GNorm = 8.6131, lr_0 = 1.0849e-04
Loss = 1.1555e-02, PNorm = 35.0792, GNorm = 1.7403, lr_0 = 1.0849e-04
Loss = 9.9559e-03, PNorm = 35.0834, GNorm = 1.6446, lr_0 = 1.0849e-04
Loss = 1.0090e-02, PNorm = 35.0876, GNorm = 4.1560, lr_0 = 1.0849e-04
Loss = 1.1017e-02, PNorm = 35.0925, GNorm = 8.6506, lr_0 = 1.0849e-04
Validation rmse logD = 0.890599
Validation R2 logD = 0.447696
Epoch 6
Train function
Loss = 1.2160e-02, PNorm = 35.0987, GNorm = 15.4786, lr_0 = 1.0849e-04
Loss = 9.9090e-03, PNorm = 35.1039, GNorm = 4.4291, lr_0 = 1.0849e-04
Loss = 1.0523e-02, PNorm = 35.1078, GNorm = 1.9455, lr_0 = 1.0849e-04
Loss = 9.6890e-03, PNorm = 35.1128, GNorm = 2.2957, lr_0 = 1.0849e-04
Loss = 1.0397e-02, PNorm = 35.1175, GNorm = 5.8155, lr_0 = 1.0849e-04
Loss = 9.3424e-03, PNorm = 35.1215, GNorm = 5.7653, lr_0 = 1.0849e-04
Validation rmse logD = 0.868406
Validation R2 logD = 0.474880
Epoch 7
Train function
Loss = 1.0960e-02, PNorm = 35.1251, GNorm = 6.2474, lr_0 = 1.0849e-04
Loss = 1.1329e-02, PNorm = 35.1290, GNorm = 3.2013, lr_0 = 1.0849e-04
Loss = 9.0295e-03, PNorm = 35.1333, GNorm = 2.3841, lr_0 = 1.0849e-04
Loss = 1.0155e-02, PNorm = 35.1380, GNorm = 7.0080, lr_0 = 1.0849e-04
Loss = 8.6096e-03, PNorm = 35.1422, GNorm = 2.6734, lr_0 = 1.0849e-04
Validation rmse logD = 0.843244
Validation R2 logD = 0.504869
Epoch 8
Train function
Loss = 8.7263e-03, PNorm = 35.1476, GNorm = 4.8038, lr_0 = 1.0849e-04
Loss = 8.9625e-03, PNorm = 35.1523, GNorm = 4.1522, lr_0 = 1.0849e-04
Loss = 8.8462e-03, PNorm = 35.1568, GNorm = 2.8371, lr_0 = 1.0849e-04
Loss = 9.5481e-03, PNorm = 35.1624, GNorm = 5.1688, lr_0 = 1.0849e-04
Loss = 8.7414e-03, PNorm = 35.1676, GNorm = 5.5836, lr_0 = 1.0849e-04
Validation rmse logD = 0.822853
Validation R2 logD = 0.528526
Epoch 9
Train function
Loss = 9.1725e-03, PNorm = 35.1727, GNorm = 7.0594, lr_0 = 1.0849e-04
Loss = 8.9633e-03, PNorm = 35.1775, GNorm = 5.0572, lr_0 = 1.0849e-04
Loss = 7.6303e-03, PNorm = 35.1824, GNorm = 8.4276, lr_0 = 1.0849e-04
Loss = 9.5800e-03, PNorm = 35.1864, GNorm = 7.0751, lr_0 = 1.0849e-04
Loss = 8.7766e-03, PNorm = 35.1918, GNorm = 4.6446, lr_0 = 1.0849e-04
Loss = 8.6880e-03, PNorm = 35.1981, GNorm = 4.3383, lr_0 = 1.0849e-04
Validation rmse logD = 0.819468
Validation R2 logD = 0.532398
Epoch 10
Train function
Loss = 7.4554e-03, PNorm = 35.2044, GNorm = 2.2849, lr_0 = 1.0849e-04
Loss = 7.2182e-03, PNorm = 35.2102, GNorm = 3.7649, lr_0 = 1.0849e-04
Loss = 9.3685e-03, PNorm = 35.2162, GNorm = 5.3614, lr_0 = 1.0849e-04
Loss = 8.7609e-03, PNorm = 35.2214, GNorm = 1.9385, lr_0 = 1.0849e-04
Loss = 8.9691e-03, PNorm = 35.2263, GNorm = 3.6606, lr_0 = 1.0849e-04
Validation rmse logD = 0.801214
Validation R2 logD = 0.552997
Epoch 11
Train function
Loss = 7.7266e-03, PNorm = 35.2323, GNorm = 2.9024, lr_0 = 1.0849e-04
Loss = 7.9873e-03, PNorm = 35.2380, GNorm = 8.4017, lr_0 = 1.0849e-04
Loss = 7.4925e-03, PNorm = 35.2436, GNorm = 3.8716, lr_0 = 1.0849e-04
Loss = 8.3598e-03, PNorm = 35.2498, GNorm = 3.9225, lr_0 = 1.0849e-04
Loss = 8.1799e-03, PNorm = 35.2554, GNorm = 3.4980, lr_0 = 1.0849e-04
Validation rmse logD = 0.779195
Validation R2 logD = 0.577229
Epoch 12
Train function
Loss = 7.9561e-03, PNorm = 35.2621, GNorm = 4.4820, lr_0 = 1.0849e-04
Loss = 6.9374e-03, PNorm = 35.2676, GNorm = 2.4604, lr_0 = 1.0849e-04
Loss = 7.4699e-03, PNorm = 35.2728, GNorm = 9.3015, lr_0 = 1.0849e-04
Loss = 7.2576e-03, PNorm = 35.2795, GNorm = 2.9733, lr_0 = 1.0849e-04
Loss = 7.8253e-03, PNorm = 35.2861, GNorm = 4.0716, lr_0 = 1.0849e-04
Loss = 7.3947e-03, PNorm = 35.2917, GNorm = 3.0572, lr_0 = 1.0849e-04
Loss = 1.6268e-01, PNorm = 35.2924, GNorm = 15.0693, lr_0 = 1.0849e-04
Validation rmse logD = 0.777836
Validation R2 logD = 0.578702
Epoch 13
Train function
Loss = 7.7426e-03, PNorm = 35.2976, GNorm = 1.8957, lr_0 = 1.0849e-04
Loss = 7.3271e-03, PNorm = 35.3039, GNorm = 2.4468, lr_0 = 1.0849e-04
Loss = 7.2520e-03, PNorm = 35.3105, GNorm = 4.9542, lr_0 = 1.0849e-04
Loss = 7.5835e-03, PNorm = 35.3166, GNorm = 7.2501, lr_0 = 1.0849e-04
Loss = 6.5373e-03, PNorm = 35.3212, GNorm = 10.0673, lr_0 = 1.0849e-04
Validation rmse logD = 0.888249
Validation R2 logD = 0.450608
Epoch 14
Train function
Loss = 7.8557e-03, PNorm = 35.3266, GNorm = 5.1445, lr_0 = 1.0849e-04
Loss = 5.8721e-03, PNorm = 35.3319, GNorm = 1.5929, lr_0 = 1.0849e-04
Loss = 6.8493e-03, PNorm = 35.3368, GNorm = 1.6069, lr_0 = 1.0849e-04
Loss = 7.3609e-03, PNorm = 35.3420, GNorm = 3.0053, lr_0 = 1.0849e-04
Loss = 7.1841e-03, PNorm = 35.3482, GNorm = 4.2335, lr_0 = 1.0849e-04
Validation rmse logD = 0.743657
Validation R2 logD = 0.614914
Epoch 15
Train function
Loss = 6.1159e-03, PNorm = 35.3545, GNorm = 12.4055, lr_0 = 1.0849e-04
Loss = 6.5344e-03, PNorm = 35.3601, GNorm = 3.7773, lr_0 = 1.0849e-04
Loss = 6.4370e-03, PNorm = 35.3649, GNorm = 2.4710, lr_0 = 1.0849e-04
Loss = 8.0378e-03, PNorm = 35.3694, GNorm = 8.5964, lr_0 = 1.0849e-04
Loss = 8.1111e-03, PNorm = 35.3740, GNorm = 5.2740, lr_0 = 1.0849e-04
Validation rmse logD = 0.789609
Validation R2 logD = 0.565853
Epoch 16
Train function
Loss = 5.4293e-03, PNorm = 35.3807, GNorm = 5.2406, lr_0 = 1.0849e-04
Loss = 6.9640e-03, PNorm = 35.3878, GNorm = 2.0502, lr_0 = 1.0849e-04
Loss = 6.7554e-03, PNorm = 35.3937, GNorm = 1.8575, lr_0 = 1.0849e-04
Loss = 6.3826e-03, PNorm = 35.3984, GNorm = 1.5058, lr_0 = 1.0849e-04
Loss = 5.8811e-03, PNorm = 35.4032, GNorm = 2.8688, lr_0 = 1.0849e-04
Loss = 6.5309e-03, PNorm = 35.4082, GNorm = 5.1579, lr_0 = 1.0849e-04
Validation rmse logD = 0.800045
Validation R2 logD = 0.554301
Epoch 17
Train function
Loss = 7.6844e-03, PNorm = 35.4127, GNorm = 7.5015, lr_0 = 1.0849e-04
Loss = 6.7110e-03, PNorm = 35.4174, GNorm = 4.2836, lr_0 = 1.0849e-04
Loss = 7.0019e-03, PNorm = 35.4219, GNorm = 8.0188, lr_0 = 1.0849e-04
Loss = 6.2805e-03, PNorm = 35.4275, GNorm = 3.3684, lr_0 = 1.0849e-04
Loss = 5.7391e-03, PNorm = 35.4336, GNorm = 6.2740, lr_0 = 1.0849e-04
Validation rmse logD = 0.742282
Validation R2 logD = 0.616336
Epoch 18
Train function
Loss = 6.0090e-03, PNorm = 35.4399, GNorm = 4.4895, lr_0 = 1.0849e-04
Loss = 5.9489e-03, PNorm = 35.4450, GNorm = 11.7071, lr_0 = 1.0849e-04
Loss = 5.6847e-03, PNorm = 35.4483, GNorm = 10.1017, lr_0 = 1.0849e-04
Loss = 6.4000e-03, PNorm = 35.4532, GNorm = 3.0046, lr_0 = 1.0849e-04
Loss = 7.3879e-03, PNorm = 35.4582, GNorm = 2.0840, lr_0 = 1.0849e-04
Validation rmse logD = 0.713016
Validation R2 logD = 0.645994
Epoch 19
Train function
Loss = 8.8580e-03, PNorm = 35.4637, GNorm = 8.9000, lr_0 = 1.0849e-04
Loss = 5.2737e-03, PNorm = 35.4691, GNorm = 6.9538, lr_0 = 1.0849e-04
Loss = 5.7282e-03, PNorm = 35.4739, GNorm = 2.8309, lr_0 = 1.0849e-04
Loss = 5.5248e-03, PNorm = 35.4784, GNorm = 3.8873, lr_0 = 1.0849e-04
Loss = 5.7917e-03, PNorm = 35.4829, GNorm = 9.9661, lr_0 = 1.0849e-04
Loss = 6.7134e-03, PNorm = 35.4872, GNorm = 12.6319, lr_0 = 1.0849e-04
Validation rmse logD = 0.716102
Validation R2 logD = 0.642922
Epoch 20
Train function
Loss = 7.1524e-03, PNorm = 35.4929, GNorm = 7.1837, lr_0 = 1.0849e-04
Loss = 5.7681e-03, PNorm = 35.4983, GNorm = 2.0602, lr_0 = 1.0849e-04
Loss = 5.4353e-03, PNorm = 35.5034, GNorm = 4.7506, lr_0 = 1.0849e-04
Loss = 5.6604e-03, PNorm = 35.5084, GNorm = 12.0647, lr_0 = 1.0849e-04
Loss = 5.9503e-03, PNorm = 35.5123, GNorm = 3.6183, lr_0 = 1.0849e-04
Validation rmse logD = 0.715513
Validation R2 logD = 0.643510
Epoch 21
Train function
Loss = 6.2151e-03, PNorm = 35.5164, GNorm = 3.3853, lr_0 = 1.0849e-04
Loss = 5.5415e-03, PNorm = 35.5210, GNorm = 5.6635, lr_0 = 1.0849e-04
Loss = 5.5027e-03, PNorm = 35.5269, GNorm = 3.2693, lr_0 = 1.0849e-04
Loss = 5.1148e-03, PNorm = 35.5326, GNorm = 2.4116, lr_0 = 1.0849e-04
Loss = 5.8149e-03, PNorm = 35.5376, GNorm = 8.0509, lr_0 = 1.0849e-04
Validation rmse logD = 0.706326
Validation R2 logD = 0.652605
Epoch 22
Train function
Loss = 4.8428e-03, PNorm = 35.5426, GNorm = 2.3376, lr_0 = 1.0849e-04
Loss = 5.3944e-03, PNorm = 35.5462, GNorm = 5.0091, lr_0 = 1.0849e-04
Loss = 5.7190e-03, PNorm = 35.5498, GNorm = 3.1219, lr_0 = 1.0849e-04
Loss = 5.4529e-03, PNorm = 35.5543, GNorm = 4.8616, lr_0 = 1.0849e-04
Loss = 5.7248e-03, PNorm = 35.5585, GNorm = 7.3089, lr_0 = 1.0849e-04
Loss = 6.2704e-03, PNorm = 35.5636, GNorm = 5.4877, lr_0 = 1.0849e-04
Validation rmse logD = 0.721517
Validation R2 logD = 0.637501
Epoch 23
Train function
Loss = 5.6839e-03, PNorm = 35.5682, GNorm = 1.3622, lr_0 = 1.0849e-04
Loss = 5.6509e-03, PNorm = 35.5729, GNorm = 5.7238, lr_0 = 1.0849e-04
Loss = 5.5397e-03, PNorm = 35.5781, GNorm = 3.7202, lr_0 = 1.0849e-04
Loss = 5.1308e-03, PNorm = 35.5820, GNorm = 3.9179, lr_0 = 1.0849e-04
Loss = 4.9099e-03, PNorm = 35.5865, GNorm = 3.7519, lr_0 = 1.0849e-04
Validation rmse logD = 0.684633
Validation R2 logD = 0.673616
Epoch 24
Train function
Loss = 5.0463e-03, PNorm = 35.5917, GNorm = 3.1880, lr_0 = 1.0849e-04
Loss = 4.4042e-03, PNorm = 35.5966, GNorm = 9.9630, lr_0 = 1.0849e-04
Loss = 5.2824e-03, PNorm = 35.6011, GNorm = 2.3190, lr_0 = 1.0849e-04
Loss = 5.4261e-03, PNorm = 35.6056, GNorm = 2.9892, lr_0 = 1.0849e-04
Loss = 4.9925e-03, PNorm = 35.6100, GNorm = 2.1387, lr_0 = 1.0849e-04
Validation rmse logD = 0.737030
Validation R2 logD = 0.621746
Epoch 25
Train function
Loss = 6.2566e-03, PNorm = 35.6140, GNorm = 10.6756, lr_0 = 1.0849e-04
Loss = 4.9537e-03, PNorm = 35.6183, GNorm = 4.7790, lr_0 = 1.0849e-04
Loss = 5.0961e-03, PNorm = 35.6232, GNorm = 3.5028, lr_0 = 1.0849e-04
Loss = 4.7621e-03, PNorm = 35.6281, GNorm = 5.3648, lr_0 = 1.0849e-04
Loss = 4.8523e-03, PNorm = 35.6329, GNorm = 8.1015, lr_0 = 1.0849e-04
Loss = 5.1567e-03, PNorm = 35.6374, GNorm = 6.4512, lr_0 = 1.0849e-04
Loss = 1.6885e-02, PNorm = 35.6380, GNorm = 3.9086, lr_0 = 1.0849e-04
Validation rmse logD = 0.703015
Validation R2 logD = 0.655854
Epoch 26
Train function
Loss = 3.9222e-03, PNorm = 35.6423, GNorm = 2.4814, lr_0 = 1.0849e-04
Loss = 4.5518e-03, PNorm = 35.6468, GNorm = 6.2781, lr_0 = 1.0849e-04
Loss = 5.0636e-03, PNorm = 35.6508, GNorm = 6.1491, lr_0 = 1.0849e-04
Loss = 5.1629e-03, PNorm = 35.6557, GNorm = 8.3937, lr_0 = 1.0849e-04
Loss = 5.1091e-03, PNorm = 35.6605, GNorm = 6.1458, lr_0 = 1.0849e-04
Validation rmse logD = 0.663731
Validation R2 logD = 0.693241
Epoch 27
Train function
Loss = 4.6180e-03, PNorm = 35.6658, GNorm = 4.3437, lr_0 = 1.0849e-04
Loss = 4.2641e-03, PNorm = 35.6705, GNorm = 4.7021, lr_0 = 1.0849e-04
Loss = 4.1340e-03, PNorm = 35.6743, GNorm = 2.8689, lr_0 = 1.0849e-04
Loss = 3.9984e-03, PNorm = 35.6785, GNorm = 3.5943, lr_0 = 1.0849e-04
Loss = 5.0045e-03, PNorm = 35.6831, GNorm = 2.0308, lr_0 = 1.0849e-04
Validation rmse logD = 0.667414
Validation R2 logD = 0.689828
Epoch 28
Train function
Loss = 3.9385e-03, PNorm = 35.6868, GNorm = 7.5301, lr_0 = 1.0849e-04
Loss = 4.4909e-03, PNorm = 35.6907, GNorm = 1.9223, lr_0 = 1.0849e-04
Loss = 4.9660e-03, PNorm = 35.6948, GNorm = 2.7352, lr_0 = 1.0849e-04
Loss = 4.2066e-03, PNorm = 35.6996, GNorm = 6.2635, lr_0 = 1.0849e-04
Loss = 4.2990e-03, PNorm = 35.7042, GNorm = 1.9777, lr_0 = 1.0849e-04
Validation rmse logD = 0.720691
Validation R2 logD = 0.638331
Epoch 29
Train function
Loss = 5.2833e-03, PNorm = 35.7103, GNorm = 11.5770, lr_0 = 1.0849e-04
Loss = 5.9146e-03, PNorm = 35.7135, GNorm = 14.1899, lr_0 = 1.0849e-04
Loss = 4.0851e-03, PNorm = 35.7172, GNorm = 4.8908, lr_0 = 1.0849e-04
Loss = 4.2876e-03, PNorm = 35.7212, GNorm = 9.6086, lr_0 = 1.0849e-04
Loss = 4.1241e-03, PNorm = 35.7260, GNorm = 2.6761, lr_0 = 1.0849e-04
Loss = 4.6931e-03, PNorm = 35.7304, GNorm = 4.8632, lr_0 = 1.0849e-04
Validation rmse logD = 0.673830
Validation R2 logD = 0.683835
Epoch 30
Train function
Loss = 4.0810e-03, PNorm = 35.7361, GNorm = 1.6732, lr_0 = 1.0849e-04
Loss = 4.4649e-03, PNorm = 35.7410, GNorm = 5.1109, lr_0 = 1.0849e-04
Loss = 4.2501e-03, PNorm = 35.7453, GNorm = 4.4107, lr_0 = 1.0849e-04
Loss = 4.2029e-03, PNorm = 35.7496, GNorm = 3.5513, lr_0 = 1.0849e-04
Loss = 3.7103e-03, PNorm = 35.7539, GNorm = 2.2158, lr_0 = 1.0849e-04
Validation rmse logD = 0.666882
Validation R2 logD = 0.690321
Epoch 31
Train function
Loss = 4.7594e-03, PNorm = 35.7585, GNorm = 3.3533, lr_0 = 1.0849e-04
Loss = 4.4080e-03, PNorm = 35.7627, GNorm = 4.9229, lr_0 = 1.0849e-04
Loss = 4.1166e-03, PNorm = 35.7670, GNorm = 6.9987, lr_0 = 1.0849e-04
Loss = 3.8691e-03, PNorm = 35.7711, GNorm = 3.6160, lr_0 = 1.0849e-04
Loss = 3.7555e-03, PNorm = 35.7752, GNorm = 6.6426, lr_0 = 1.0849e-04
Validation rmse logD = 0.704042
Validation R2 logD = 0.654848
Epoch 32
Train function
Loss = 7.2654e-03, PNorm = 35.7792, GNorm = 14.8556, lr_0 = 1.0849e-04
Loss = 3.4628e-03, PNorm = 35.7824, GNorm = 1.8464, lr_0 = 1.0849e-04
Loss = 4.3731e-03, PNorm = 35.7877, GNorm = 11.8947, lr_0 = 1.0849e-04
Loss = 4.3766e-03, PNorm = 35.7913, GNorm = 3.2884, lr_0 = 1.0849e-04
Loss = 3.5887e-03, PNorm = 35.7954, GNorm = 9.0851, lr_0 = 1.0849e-04
Loss = 3.9910e-03, PNorm = 35.7999, GNorm = 4.9250, lr_0 = 1.0849e-04
Validation rmse logD = 0.644434
Validation R2 logD = 0.710819
Epoch 33
Train function
Loss = 3.8228e-03, PNorm = 35.8032, GNorm = 5.4021, lr_0 = 1.0849e-04
Loss = 4.0175e-03, PNorm = 35.8079, GNorm = 4.1950, lr_0 = 1.0849e-04
Loss = 3.4879e-03, PNorm = 35.8117, GNorm = 3.6854, lr_0 = 1.0849e-04
Loss = 3.8061e-03, PNorm = 35.8167, GNorm = 2.0968, lr_0 = 1.0849e-04
Loss = 3.9299e-03, PNorm = 35.8216, GNorm = 2.6241, lr_0 = 1.0849e-04
Validation rmse logD = 0.647392
Validation R2 logD = 0.708158
Epoch 34
Train function
Loss = 3.4693e-03, PNorm = 35.8259, GNorm = 4.9789, lr_0 = 1.0849e-04
Loss = 3.8838e-03, PNorm = 35.8300, GNorm = 5.3224, lr_0 = 1.0849e-04
Loss = 3.6138e-03, PNorm = 35.8337, GNorm = 3.3776, lr_0 = 1.0849e-04
Loss = 4.2382e-03, PNorm = 35.8377, GNorm = 14.3629, lr_0 = 1.0849e-04
Loss = 3.5900e-03, PNorm = 35.8423, GNorm = 5.5865, lr_0 = 1.0849e-04
Validation rmse logD = 0.638180
Validation R2 logD = 0.716404
Epoch 35
Train function
Loss = 2.7223e-03, PNorm = 35.8458, GNorm = 2.3948, lr_0 = 1.0849e-04
Loss = 3.5340e-03, PNorm = 35.8494, GNorm = 2.3394, lr_0 = 1.0849e-04
Loss = 3.2786e-03, PNorm = 35.8531, GNorm = 3.9195, lr_0 = 1.0849e-04
Loss = 4.6448e-03, PNorm = 35.8569, GNorm = 5.6356, lr_0 = 1.0849e-04
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_wo_MolLogP'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD', 'logP'],
 'task_names': ['logD', 'logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=499, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 414,902
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_wo_MolLogP'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD'],
 'task_names': ['logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 3,541 | train size = 2,655 | val size = 886 | test size = 625
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=499, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=1, bias=True)
  )
)
Number of parameters = 414,601
Moving model to cuda
Epoch 0
Train function
Loss = 2.8252e-02, PNorm = 35.0058, GNorm = 6.3649, lr_0 = 1.0849e-04
Loss = 2.0975e-02, PNorm = 35.0054, GNorm = 6.2100, lr_0 = 1.0849e-04
Loss = 1.8422e-02, PNorm = 35.0061, GNorm = 3.1628, lr_0 = 1.0849e-04
Loss = 1.7571e-02, PNorm = 35.0074, GNorm = 2.2595, lr_0 = 1.0849e-04
Loss = 1.7731e-02, PNorm = 35.0085, GNorm = 3.0250, lr_0 = 1.0849e-04
Validation rmse logD = 1.085169
Validation R2 logD = 0.180012
Epoch 1
Train function
Loss = 1.5315e-02, PNorm = 35.0100, GNorm = 1.9139, lr_0 = 1.0849e-04
Loss = 1.5058e-02, PNorm = 35.0120, GNorm = 1.6984, lr_0 = 1.0849e-04
Loss = 1.5388e-02, PNorm = 35.0138, GNorm = 2.5091, lr_0 = 1.0849e-04
Loss = 1.5452e-02, PNorm = 35.0162, GNorm = 2.6036, lr_0 = 1.0849e-04
Loss = 1.5136e-02, PNorm = 35.0187, GNorm = 4.6189, lr_0 = 1.0849e-04
Validation rmse logD = 1.016627
Validation R2 logD = 0.280325
Epoch 2
Train function
Loss = 1.7563e-02, PNorm = 35.0224, GNorm = 6.7419, lr_0 = 1.0849e-04
Loss = 1.3424e-02, PNorm = 35.0258, GNorm = 2.5288, lr_0 = 1.0849e-04
Loss = 1.2971e-02, PNorm = 35.0288, GNorm = 2.0841, lr_0 = 1.0849e-04
Loss = 1.4302e-02, PNorm = 35.0313, GNorm = 2.5320, lr_0 = 1.0849e-04
Loss = 1.2332e-02, PNorm = 35.0337, GNorm = 2.7913, lr_0 = 1.0849e-04
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_2d_normalized_wo_MolLogP'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 11,710 | train size = 8,782 | val size = 2,928 | test size = 2,067
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=1, bias=True)
  )
)
Number of parameters = 354,901
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'one_out_crossval',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'one_out_crossval',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Loss = 1.7144e-02, PNorm = 34.0564, GNorm = 2.4991, lr_0 = 1.0200e-04
Loss = 2.0836e-02, PNorm = 34.0578, GNorm = 4.3161, lr_0 = 1.0200e-04
Loss = 1.6399e-02, PNorm = 34.0593, GNorm = 6.4932, lr_0 = 1.0200e-04
Loss = 1.7208e-02, PNorm = 34.0608, GNorm = 2.0625, lr_0 = 1.0200e-04
Loss = 1.7193e-02, PNorm = 34.0627, GNorm = 3.4313, lr_0 = 1.0200e-04
Loss = 1.6376e-02, PNorm = 34.0649, GNorm = 5.2201, lr_0 = 1.0200e-04
Loss = 1.4518e-02, PNorm = 34.0678, GNorm = 2.7939, lr_0 = 1.0200e-04
Loss = 1.5150e-02, PNorm = 34.0710, GNorm = 10.6618, lr_0 = 1.0200e-04
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_logd_Lip_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 2,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_logd_Lip_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP', 'logD'],
 'task_names': ['logP', 'logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 2
Splitting data with seed 0
Total size = 15,032 | train size = 11,274 | val size = 3,758 | test size = 2,653
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=2, bias=True)
  )
)
Number of parameters = 355,202
Moving model to cuda
Epoch 0
Train function
Loss = 1.7144e-02, PNorm = 34.0564, GNorm = 2.4991, lr_0 = 1.0200e-04
Loss = 2.0836e-02, PNorm = 34.0578, GNorm = 4.3161, lr_0 = 1.0200e-04
Loss = 1.6399e-02, PNorm = 34.0593, GNorm = 6.4932, lr_0 = 1.0200e-04
Loss = 1.7208e-02, PNorm = 34.0608, GNorm = 2.0625, lr_0 = 1.0200e-04
Loss = 1.7193e-02, PNorm = 34.0627, GNorm = 3.4313, lr_0 = 1.0200e-04
Loss = 1.6376e-02, PNorm = 34.0649, GNorm = 5.2201, lr_0 = 1.0200e-04
Loss = 1.4518e-02, PNorm = 34.0678, GNorm = 2.7939, lr_0 = 1.0200e-04
Loss = 1.5150e-02, PNorm = 34.0710, GNorm = 10.6618, lr_0 = 1.0200e-04
Loss = 1.3435e-02, PNorm = 34.0732, GNorm = 10.3551, lr_0 = 1.0200e-04
Loss = 1.1961e-02, PNorm = 34.0761, GNorm = 4.3627, lr_0 = 1.0200e-04
Loss = 1.2771e-02, PNorm = 34.0794, GNorm = 2.7107, lr_0 = 1.0200e-04
Loss = 1.3138e-02, PNorm = 34.0829, GNorm = 4.9068, lr_0 = 1.0200e-04
Loss = 1.3105e-02, PNorm = 34.0865, GNorm = 19.9499, lr_0 = 1.0200e-04
Loss = 1.1540e-02, PNorm = 34.0891, GNorm = 6.5058, lr_0 = 1.0200e-04
Loss = 1.1791e-02, PNorm = 34.0913, GNorm = 9.2823, lr_0 = 1.0200e-04
Loss = 1.0747e-02, PNorm = 34.0949, GNorm = 7.5681, lr_0 = 1.0200e-04
Loss = 1.1345e-02, PNorm = 34.0984, GNorm = 4.6826, lr_0 = 1.0200e-04
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 1,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 22,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 11,710 | train size = 11,177 | val size = 533 | test size = 2,067
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=1, bias=True)
  )
)
Number of parameters = 354,901
Moving model to cuda
Epoch 0
Train function
Loss = 1.6962e-02, PNorm = 34.0116, GNorm = 1.9700, lr_0 = 1.0202e-04
Loss = 1.7711e-02, PNorm = 34.0139, GNorm = 2.5146, lr_0 = 1.0202e-04
Loss = 1.5340e-02, PNorm = 34.0169, GNorm = 1.9349, lr_0 = 1.0202e-04
Loss = 1.5352e-02, PNorm = 34.0206, GNorm = 1.7695, lr_0 = 1.0202e-04
Loss = 1.4047e-02, PNorm = 34.0248, GNorm = 2.5131, lr_0 = 1.0202e-04
Loss = 1.3613e-02, PNorm = 34.0288, GNorm = 15.3018, lr_0 = 1.0202e-04
Loss = 1.2169e-02, PNorm = 34.0321, GNorm = 20.2297, lr_0 = 1.0202e-04
Loss = 1.2418e-02, PNorm = 34.0349, GNorm = 21.5383, lr_0 = 1.0202e-04
Loss = 1.0239e-02, PNorm = 34.0375, GNorm = 2.9857, lr_0 = 1.0202e-04
Loss = 1.0228e-02, PNorm = 34.0413, GNorm = 24.1786, lr_0 = 1.0202e-04
Loss = 8.8066e-03, PNorm = 34.0445, GNorm = 6.9430, lr_0 = 1.0202e-04
Loss = 1.0586e-02, PNorm = 34.0473, GNorm = 15.4734, lr_0 = 1.0202e-04
Loss = 1.0139e-02, PNorm = 34.0504, GNorm = 14.3769, lr_0 = 1.0202e-04
Loss = 1.0688e-02, PNorm = 34.0530, GNorm = 3.6074, lr_0 = 1.0202e-04
Loss = 9.4742e-03, PNorm = 34.0559, GNorm = 12.3202, lr_0 = 1.0202e-04
Loss = 9.0447e-03, PNorm = 34.0595, GNorm = 8.8030, lr_0 = 1.0202e-04
Loss = 7.7187e-03, PNorm = 34.0637, GNorm = 5.1046, lr_0 = 1.0202e-04
Loss = 6.7299e-03, PNorm = 34.0665, GNorm = 7.9823, lr_0 = 1.0202e-04
Loss = 8.8600e-03, PNorm = 34.0699, GNorm = 10.6906, lr_0 = 1.0202e-04
Loss = 6.9359e-03, PNorm = 34.0723, GNorm = 11.0408, lr_0 = 1.0202e-04
Loss = 8.2959e-03, PNorm = 34.0753, GNorm = 15.0221, lr_0 = 1.0202e-04
Loss = 6.3613e-03, PNorm = 34.0789, GNorm = 2.8511, lr_0 = 1.0202e-04
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 22,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 11,710 | train size = 11,177 | val size = 533 | test size = 2,067
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=1, bias=True)
  )
)
Number of parameters = 354,901
Moving model to cuda
Epoch 0
Train function
Loss = 1.6962e-02, PNorm = 34.0116, GNorm = 1.9700, lr_0 = 1.0202e-04
Loss = 1.7711e-02, PNorm = 34.0139, GNorm = 2.5146, lr_0 = 1.0202e-04
Loss = 1.5340e-02, PNorm = 34.0169, GNorm = 1.9349, lr_0 = 1.0202e-04
Loss = 1.5352e-02, PNorm = 34.0206, GNorm = 1.7695, lr_0 = 1.0202e-04
Loss = 1.4047e-02, PNorm = 34.0248, GNorm = 2.5131, lr_0 = 1.0202e-04
Loss = 1.3613e-02, PNorm = 34.0288, GNorm = 15.3018, lr_0 = 1.0202e-04
Loss = 1.2169e-02, PNorm = 34.0321, GNorm = 20.2297, lr_0 = 1.0202e-04
Loss = 1.2418e-02, PNorm = 34.0349, GNorm = 21.5383, lr_0 = 1.0202e-04
Loss = 1.0239e-02, PNorm = 34.0375, GNorm = 2.9857, lr_0 = 1.0202e-04
Loss = 1.0228e-02, PNorm = 34.0413, GNorm = 24.1786, lr_0 = 1.0202e-04
Loss = 8.8066e-03, PNorm = 34.0445, GNorm = 6.9430, lr_0 = 1.0202e-04
Loss = 1.0586e-02, PNorm = 34.0473, GNorm = 15.4734, lr_0 = 1.0202e-04
Loss = 1.0139e-02, PNorm = 34.0504, GNorm = 14.3769, lr_0 = 1.0202e-04
Loss = 1.0688e-02, PNorm = 34.0530, GNorm = 3.6074, lr_0 = 1.0202e-04
Loss = 9.4742e-03, PNorm = 34.0559, GNorm = 12.3202, lr_0 = 1.0202e-04
Loss = 9.0447e-03, PNorm = 34.0595, GNorm = 8.8030, lr_0 = 1.0202e-04
Loss = 7.7187e-03, PNorm = 34.0637, GNorm = 5.1046, lr_0 = 1.0202e-04
Loss = 6.7299e-03, PNorm = 34.0665, GNorm = 7.9823, lr_0 = 1.0202e-04
Loss = 8.8600e-03, PNorm = 34.0699, GNorm = 10.6906, lr_0 = 1.0202e-04
Loss = 6.9359e-03, PNorm = 34.0723, GNorm = 11.0408, lr_0 = 1.0202e-04
Loss = 8.2959e-03, PNorm = 34.0753, GNorm = 15.0221, lr_0 = 1.0202e-04
Loss = 6.3613e-03, PNorm = 34.0789, GNorm = 2.8511, lr_0 = 1.0202e-04
Validation rmse logP = 1.072882
Validation R2 logP = 0.662762
Epoch 1
Train function
Loss = 7.2795e-03, PNorm = 34.0831, GNorm = 13.3237, lr_0 = 1.0202e-04
Loss = 5.7231e-03, PNorm = 34.0863, GNorm = 11.5603, lr_0 = 1.0202e-04
Loss = 6.1968e-03, PNorm = 34.0891, GNorm = 2.5603, lr_0 = 1.0202e-04
Loss = 5.9255e-03, PNorm = 34.0920, GNorm = 5.8651, lr_0 = 1.0202e-04
Loss = 5.6853e-03, PNorm = 34.0951, GNorm = 12.2037, lr_0 = 1.0202e-04
Loss = 6.3334e-03, PNorm = 34.0980, GNorm = 16.1343, lr_0 = 1.0202e-04
Loss = 5.9102e-03, PNorm = 34.1014, GNorm = 2.8133, lr_0 = 1.0202e-04
Loss = 5.9135e-03, PNorm = 34.1044, GNorm = 5.5456, lr_0 = 1.0202e-04
Loss = 6.2343e-03, PNorm = 34.1070, GNorm = 24.4747, lr_0 = 1.0202e-04
Loss = 6.4061e-03, PNorm = 34.1101, GNorm = 8.4209, lr_0 = 1.0202e-04
Loss = 5.9010e-03, PNorm = 34.1133, GNorm = 4.6929, lr_0 = 1.0202e-04
Loss = 5.8978e-03, PNorm = 34.1157, GNorm = 1.8397, lr_0 = 1.0202e-04
Loss = 4.8629e-03, PNorm = 34.1193, GNorm = 12.3852, lr_0 = 1.0202e-04
Loss = 5.2887e-03, PNorm = 34.1220, GNorm = 3.0351, lr_0 = 1.0202e-04
Loss = 5.6460e-03, PNorm = 34.1251, GNorm = 10.5021, lr_0 = 1.0202e-04
Loss = 5.0585e-03, PNorm = 34.1283, GNorm = 14.9942, lr_0 = 1.0202e-04
Loss = 5.2266e-03, PNorm = 34.1316, GNorm = 3.9659, lr_0 = 1.0202e-04
Loss = 5.1374e-03, PNorm = 34.1346, GNorm = 7.2599, lr_0 = 1.0202e-04
Loss = 4.2866e-03, PNorm = 34.1371, GNorm = 2.4246, lr_0 = 1.0202e-04
Loss = 5.0471e-03, PNorm = 34.1391, GNorm = 5.6410, lr_0 = 1.0202e-04
Loss = 6.4947e-03, PNorm = 34.1414, GNorm = 28.0426, lr_0 = 1.0202e-04
Loss = 5.4595e-03, PNorm = 34.1438, GNorm = 17.6084, lr_0 = 1.0202e-04
Validation rmse logP = 0.952799
Validation R2 logP = 0.734029
Epoch 2
Train function
Loss = 4.0591e-03, PNorm = 34.1465, GNorm = 13.4926, lr_0 = 1.0202e-04
Loss = 4.9461e-03, PNorm = 34.1483, GNorm = 4.3153, lr_0 = 1.0202e-04
Loss = 6.4834e-03, PNorm = 34.1514, GNorm = 29.8196, lr_0 = 1.0202e-04
Loss = 5.1567e-03, PNorm = 34.1536, GNorm = 24.0175, lr_0 = 1.0202e-04
Loss = 5.2143e-03, PNorm = 34.1559, GNorm = 21.5289, lr_0 = 1.0202e-04
Loss = 4.2826e-03, PNorm = 34.1586, GNorm = 12.5335, lr_0 = 1.0202e-04
Loss = 4.2552e-03, PNorm = 34.1610, GNorm = 13.6019, lr_0 = 1.0202e-04
Loss = 4.2537e-03, PNorm = 34.1627, GNorm = 5.7467, lr_0 = 1.0202e-04
Loss = 4.2979e-03, PNorm = 34.1655, GNorm = 13.7221, lr_0 = 1.0202e-04
Loss = 4.4376e-03, PNorm = 34.1680, GNorm = 5.1456, lr_0 = 1.0202e-04
Loss = 4.6663e-03, PNorm = 34.1706, GNorm = 7.2569, lr_0 = 1.0202e-04
Loss = 4.8437e-03, PNorm = 34.1735, GNorm = 5.3926, lr_0 = 1.0202e-04
Loss = 4.3787e-03, PNorm = 34.1757, GNorm = 6.9339, lr_0 = 1.0202e-04
Loss = 4.5440e-03, PNorm = 34.1789, GNorm = 9.1141, lr_0 = 1.0202e-04
Loss = 4.2564e-03, PNorm = 34.1817, GNorm = 9.2156, lr_0 = 1.0202e-04
Loss = 5.0423e-03, PNorm = 34.1844, GNorm = 4.5616, lr_0 = 1.0202e-04
Loss = 4.7976e-03, PNorm = 34.1875, GNorm = 19.3830, lr_0 = 1.0202e-04
Loss = 5.0343e-03, PNorm = 34.1903, GNorm = 20.0355, lr_0 = 1.0202e-04
Loss = 4.3390e-03, PNorm = 34.1936, GNorm = 11.6235, lr_0 = 1.0202e-04
Loss = 4.2202e-03, PNorm = 34.1969, GNorm = 4.4785, lr_0 = 1.0202e-04
Loss = 4.4324e-03, PNorm = 34.1993, GNorm = 17.3218, lr_0 = 1.0202e-04
Loss = 3.6087e-03, PNorm = 34.2018, GNorm = 3.5564, lr_0 = 1.0202e-04
Loss = 3.8527e-03, PNorm = 34.2034, GNorm = 6.2614, lr_0 = 1.0202e-04
Loss = 8.2009e-03, PNorm = 34.2036, GNorm = 13.2461, lr_0 = 1.0202e-04
Validation rmse logP = 0.797979
Validation R2 logP = 0.813441
Epoch 3
Train function
Loss = 3.7717e-03, PNorm = 34.2059, GNorm = 19.5343, lr_0 = 1.0202e-04
Loss = 3.2107e-03, PNorm = 34.2082, GNorm = 8.0051, lr_0 = 1.0202e-04
Loss = 3.7141e-03, PNorm = 34.2105, GNorm = 10.1286, lr_0 = 1.0202e-04
Loss = 3.8431e-03, PNorm = 34.2130, GNorm = 6.5624, lr_0 = 1.0202e-04
Loss = 3.7374e-03, PNorm = 34.2151, GNorm = 7.1734, lr_0 = 1.0202e-04
Loss = 3.5630e-03, PNorm = 34.2173, GNorm = 9.4605, lr_0 = 1.0202e-04
Loss = 3.7486e-03, PNorm = 34.2191, GNorm = 10.6165, lr_0 = 1.0202e-04
Loss = 3.7997e-03, PNorm = 34.2213, GNorm = 15.5627, lr_0 = 1.0202e-04
Loss = 4.4427e-03, PNorm = 34.2242, GNorm = 9.7345, lr_0 = 1.0202e-04
Loss = 3.9561e-03, PNorm = 34.2272, GNorm = 10.3794, lr_0 = 1.0202e-04
Loss = 3.5046e-03, PNorm = 34.2302, GNorm = 6.0206, lr_0 = 1.0202e-04
Loss = 3.5212e-03, PNorm = 34.2330, GNorm = 2.5651, lr_0 = 1.0202e-04
Loss = 3.7255e-03, PNorm = 34.2354, GNorm = 5.9512, lr_0 = 1.0202e-04
Loss = 3.9815e-03, PNorm = 34.2385, GNorm = 3.1020, lr_0 = 1.0202e-04
Loss = 3.2432e-03, PNorm = 34.2406, GNorm = 2.8622, lr_0 = 1.0202e-04
Loss = 3.6389e-03, PNorm = 34.2430, GNorm = 3.2545, lr_0 = 1.0202e-04
Loss = 3.2603e-03, PNorm = 34.2456, GNorm = 4.0075, lr_0 = 1.0202e-04
Loss = 3.1388e-03, PNorm = 34.2479, GNorm = 6.0379, lr_0 = 1.0202e-04
Loss = 3.0270e-03, PNorm = 34.2503, GNorm = 11.9728, lr_0 = 1.0202e-04
Loss = 3.4843e-03, PNorm = 34.2532, GNorm = 14.8658, lr_0 = 1.0202e-04
Loss = 3.8106e-03, PNorm = 34.2560, GNorm = 6.3226, lr_0 = 1.0202e-04
Loss = 3.8647e-03, PNorm = 34.2590, GNorm = 12.4843, lr_0 = 1.0202e-04
Validation rmse logP = 0.756336
Validation R2 logP = 0.832405
Epoch 4
Train function
Loss = 3.2865e-03, PNorm = 34.2618, GNorm = 7.3038, lr_0 = 1.0202e-04
Loss = 3.1658e-03, PNorm = 34.2641, GNorm = 7.3418, lr_0 = 1.0202e-04
Loss = 3.9188e-03, PNorm = 34.2664, GNorm = 10.2681, lr_0 = 1.0202e-04
Loss = 4.0255e-03, PNorm = 34.2695, GNorm = 4.8124, lr_0 = 1.0202e-04
Loss = 3.5399e-03, PNorm = 34.2724, GNorm = 9.8008, lr_0 = 1.0202e-04
Loss = 3.2681e-03, PNorm = 34.2760, GNorm = 22.9998, lr_0 = 1.0202e-04
Loss = 3.2963e-03, PNorm = 34.2782, GNorm = 16.3140, lr_0 = 1.0202e-04
Loss = 3.8480e-03, PNorm = 34.2812, GNorm = 2.8546, lr_0 = 1.0202e-04
Loss = 3.6048e-03, PNorm = 34.2837, GNorm = 4.5581, lr_0 = 1.0202e-04
Loss = 3.2755e-03, PNorm = 34.2857, GNorm = 12.1508, lr_0 = 1.0202e-04
Loss = 2.8672e-03, PNorm = 34.2872, GNorm = 7.1904, lr_0 = 1.0202e-04
Loss = 2.8840e-03, PNorm = 34.2893, GNorm = 2.7303, lr_0 = 1.0202e-04
Loss = 3.5341e-03, PNorm = 34.2917, GNorm = 6.9984, lr_0 = 1.0202e-04
Loss = 3.1694e-03, PNorm = 34.2939, GNorm = 12.7283, lr_0 = 1.0202e-04
Loss = 3.5548e-03, PNorm = 34.2968, GNorm = 2.2534, lr_0 = 1.0202e-04
Loss = 2.7982e-03, PNorm = 34.3002, GNorm = 9.7782, lr_0 = 1.0202e-04
Loss = 2.7344e-03, PNorm = 34.3030, GNorm = 5.0989, lr_0 = 1.0202e-04
Loss = 3.4206e-03, PNorm = 34.3053, GNorm = 9.7466, lr_0 = 1.0202e-04
Loss = 2.9429e-03, PNorm = 34.3078, GNorm = 22.8522, lr_0 = 1.0202e-04
Loss = 3.3368e-03, PNorm = 34.3097, GNorm = 9.7237, lr_0 = 1.0202e-04
Loss = 2.5483e-03, PNorm = 34.3117, GNorm = 2.9441, lr_0 = 1.0202e-04
Loss = 3.0391e-03, PNorm = 34.3132, GNorm = 14.6314, lr_0 = 1.0202e-04
Validation rmse logP = 0.697964
Validation R2 logP = 0.857276
Epoch 5
Train function
Loss = 3.1989e-03, PNorm = 34.3152, GNorm = 11.2701, lr_0 = 1.0202e-04
Loss = 2.7344e-03, PNorm = 34.3178, GNorm = 9.3287, lr_0 = 1.0202e-04
Loss = 2.6354e-03, PNorm = 34.3194, GNorm = 5.9615, lr_0 = 1.0202e-04
Loss = 2.9003e-03, PNorm = 34.3205, GNorm = 7.0320, lr_0 = 1.0202e-04
Loss = 3.3162e-03, PNorm = 34.3232, GNorm = 7.1071, lr_0 = 1.0202e-04
Loss = 3.0543e-03, PNorm = 34.3253, GNorm = 17.7850, lr_0 = 1.0202e-04
Loss = 3.2978e-03, PNorm = 34.3280, GNorm = 3.4564, lr_0 = 1.0202e-04
Loss = 2.2399e-03, PNorm = 34.3304, GNorm = 1.9615, lr_0 = 1.0202e-04
Loss = 3.0849e-03, PNorm = 34.3325, GNorm = 6.3568, lr_0 = 1.0202e-04
Loss = 3.2551e-03, PNorm = 34.3354, GNorm = 3.9055, lr_0 = 1.0202e-04
Loss = 2.6285e-03, PNorm = 34.3379, GNorm = 11.1965, lr_0 = 1.0202e-04
Loss = 3.1084e-03, PNorm = 34.3416, GNorm = 12.3188, lr_0 = 1.0202e-04
Loss = 2.9397e-03, PNorm = 34.3446, GNorm = 3.6968, lr_0 = 1.0202e-04
Loss = 3.0876e-03, PNorm = 34.3462, GNorm = 11.4447, lr_0 = 1.0202e-04
Loss = 3.3324e-03, PNorm = 34.3489, GNorm = 12.5428, lr_0 = 1.0202e-04
Loss = 2.5103e-03, PNorm = 34.3514, GNorm = 5.4814, lr_0 = 1.0202e-04
Loss = 3.0134e-03, PNorm = 34.3530, GNorm = 8.6215, lr_0 = 1.0202e-04
Loss = 2.7628e-03, PNorm = 34.3558, GNorm = 8.9114, lr_0 = 1.0202e-04
Loss = 2.7533e-03, PNorm = 34.3575, GNorm = 11.3383, lr_0 = 1.0202e-04
Loss = 3.1697e-03, PNorm = 34.3592, GNorm = 2.8043, lr_0 = 1.0202e-04
Loss = 3.1156e-03, PNorm = 34.3614, GNorm = 12.7698, lr_0 = 1.0202e-04
Loss = 2.6227e-03, PNorm = 34.3635, GNorm = 5.3330, lr_0 = 1.0202e-04
Loss = 2.8897e-03, PNorm = 34.3649, GNorm = 15.4464, lr_0 = 1.0202e-04
Validation rmse logP = 0.682541
Validation R2 logP = 0.863514
Epoch 6
Train function
Loss = 2.9852e-03, PNorm = 34.3661, GNorm = 10.0019, lr_0 = 1.0202e-04
Loss = 3.0579e-03, PNorm = 34.3680, GNorm = 14.2912, lr_0 = 1.0202e-04
Loss = 3.0250e-03, PNorm = 34.3704, GNorm = 8.0587, lr_0 = 1.0202e-04
Loss = 3.2706e-03, PNorm = 34.3731, GNorm = 14.6280, lr_0 = 1.0202e-04
Loss = 2.7077e-03, PNorm = 34.3753, GNorm = 3.4745, lr_0 = 1.0202e-04
Loss = 3.4054e-03, PNorm = 34.3774, GNorm = 19.4799, lr_0 = 1.0202e-04
Loss = 3.3471e-03, PNorm = 34.3799, GNorm = 21.2980, lr_0 = 1.0202e-04
Loss = 3.3071e-03, PNorm = 34.3820, GNorm = 9.4480, lr_0 = 1.0202e-04
Loss = 3.2846e-03, PNorm = 34.3847, GNorm = 7.5348, lr_0 = 1.0202e-04
Loss = 2.4097e-03, PNorm = 34.3866, GNorm = 5.4242, lr_0 = 1.0202e-04
Loss = 3.4583e-03, PNorm = 34.3888, GNorm = 7.5994, lr_0 = 1.0202e-04
Loss = 3.3416e-03, PNorm = 34.3906, GNorm = 9.8175, lr_0 = 1.0202e-04
Loss = 2.7860e-03, PNorm = 34.3943, GNorm = 1.5419, lr_0 = 1.0202e-04
Loss = 2.5718e-03, PNorm = 34.3971, GNorm = 2.5425, lr_0 = 1.0202e-04
Loss = 3.1053e-03, PNorm = 34.3999, GNorm = 5.1989, lr_0 = 1.0202e-04
Loss = 2.7548e-03, PNorm = 34.4021, GNorm = 9.3951, lr_0 = 1.0202e-04
Loss = 2.6500e-03, PNorm = 34.4040, GNorm = 5.1972, lr_0 = 1.0202e-04
Loss = 3.2142e-03, PNorm = 34.4064, GNorm = 19.7762, lr_0 = 1.0202e-04
Loss = 2.3738e-03, PNorm = 34.4091, GNorm = 2.0508, lr_0 = 1.0202e-04
Loss = 2.5479e-03, PNorm = 34.4109, GNorm = 8.4160, lr_0 = 1.0202e-04
Loss = 3.0408e-03, PNorm = 34.4123, GNorm = 11.2221, lr_0 = 1.0202e-04
Loss = 2.7426e-03, PNorm = 34.4139, GNorm = 16.9465, lr_0 = 1.0202e-04
Validation rmse logP = 0.691108
Validation R2 logP = 0.860065
Epoch 7
Train function
Loss = 2.2515e-03, PNorm = 34.4155, GNorm = 5.9209, lr_0 = 1.0202e-04
Loss = 2.6032e-03, PNorm = 34.4179, GNorm = 6.8838, lr_0 = 1.0202e-04
Loss = 2.6855e-03, PNorm = 34.4203, GNorm = 2.1573, lr_0 = 1.0202e-04
Loss = 2.7388e-03, PNorm = 34.4224, GNorm = 2.8488, lr_0 = 1.0202e-04
Loss = 2.6798e-03, PNorm = 34.4246, GNorm = 21.6886, lr_0 = 1.0202e-04
Loss = 2.7997e-03, PNorm = 34.4268, GNorm = 6.3319, lr_0 = 1.0202e-04
Loss = 2.5188e-03, PNorm = 34.4292, GNorm = 11.0373, lr_0 = 1.0202e-04
Loss = 1.9722e-03, PNorm = 34.4319, GNorm = 9.4748, lr_0 = 1.0202e-04
Loss = 2.4370e-03, PNorm = 34.4343, GNorm = 8.7261, lr_0 = 1.0202e-04
Loss = 3.2064e-03, PNorm = 34.4351, GNorm = 6.8155, lr_0 = 1.0202e-04
Loss = 2.8827e-03, PNorm = 34.4367, GNorm = 5.6518, lr_0 = 1.0202e-04
Loss = 2.2967e-03, PNorm = 34.4391, GNorm = 5.4470, lr_0 = 1.0202e-04
Loss = 2.4570e-03, PNorm = 34.4411, GNorm = 7.0090, lr_0 = 1.0202e-04
Loss = 2.6633e-03, PNorm = 34.4436, GNorm = 7.4419, lr_0 = 1.0202e-04
Loss = 2.6556e-03, PNorm = 34.4464, GNorm = 2.4037, lr_0 = 1.0202e-04
Loss = 2.1511e-03, PNorm = 34.4486, GNorm = 5.6395, lr_0 = 1.0202e-04
Loss = 2.3855e-03, PNorm = 34.4513, GNorm = 6.4019, lr_0 = 1.0202e-04
Loss = 2.3963e-03, PNorm = 34.4546, GNorm = 3.9489, lr_0 = 1.0202e-04
Loss = 2.6873e-03, PNorm = 34.4567, GNorm = 3.7251, lr_0 = 1.0202e-04
Loss = 2.6219e-03, PNorm = 34.4588, GNorm = 15.6698, lr_0 = 1.0202e-04
Loss = 2.3101e-03, PNorm = 34.4604, GNorm = 7.2156, lr_0 = 1.0202e-04
Loss = 2.4426e-03, PNorm = 34.4614, GNorm = 8.4564, lr_0 = 1.0202e-04
Validation rmse logP = 0.651569
Validation R2 logP = 0.875619
Epoch 8
Train function
Loss = 3.1666e-03, PNorm = 34.4620, GNorm = 4.9646, lr_0 = 1.0202e-04
Loss = 3.0805e-03, PNorm = 34.4639, GNorm = 1.8045, lr_0 = 1.0202e-04
Loss = 2.1892e-03, PNorm = 34.4661, GNorm = 9.9123, lr_0 = 1.0202e-04
Loss = 2.5836e-03, PNorm = 34.4683, GNorm = 5.2240, lr_0 = 1.0202e-04
Loss = 3.1116e-03, PNorm = 34.4701, GNorm = 7.2207, lr_0 = 1.0202e-04
Loss = 3.0615e-03, PNorm = 34.4715, GNorm = 7.3988, lr_0 = 1.0202e-04
Loss = 2.9343e-03, PNorm = 34.4738, GNorm = 5.3676, lr_0 = 1.0202e-04
Loss = 2.6004e-03, PNorm = 34.4758, GNorm = 6.9349, lr_0 = 1.0202e-04
Loss = 2.3206e-03, PNorm = 34.4776, GNorm = 14.7550, lr_0 = 1.0202e-04
Loss = 2.4096e-03, PNorm = 34.4794, GNorm = 8.5342, lr_0 = 1.0202e-04
Loss = 2.5810e-03, PNorm = 34.4809, GNorm = 2.4157, lr_0 = 1.0202e-04
Loss = 2.6923e-03, PNorm = 34.4830, GNorm = 1.9639, lr_0 = 1.0202e-04
Loss = 2.4860e-03, PNorm = 34.4850, GNorm = 14.5644, lr_0 = 1.0202e-04
Loss = 2.2741e-03, PNorm = 34.4868, GNorm = 9.1844, lr_0 = 1.0202e-04
Loss = 2.2137e-03, PNorm = 34.4888, GNorm = 9.2757, lr_0 = 1.0202e-04
Loss = 2.4170e-03, PNorm = 34.4911, GNorm = 1.7999, lr_0 = 1.0202e-04
Loss = 2.4016e-03, PNorm = 34.4932, GNorm = 4.8762, lr_0 = 1.0202e-04
Loss = 2.4143e-03, PNorm = 34.4958, GNorm = 12.0987, lr_0 = 1.0202e-04
Loss = 2.1553e-03, PNorm = 34.4977, GNorm = 4.5991, lr_0 = 1.0202e-04
Loss = 2.0353e-03, PNorm = 34.5003, GNorm = 6.1554, lr_0 = 1.0202e-04
Loss = 2.1710e-03, PNorm = 34.5028, GNorm = 7.9403, lr_0 = 1.0202e-04
Loss = 2.0286e-03, PNorm = 34.5042, GNorm = 12.7519, lr_0 = 1.0202e-04
Loss = 2.6676e-03, PNorm = 34.5059, GNorm = 2.6337, lr_0 = 1.0202e-04
Validation rmse logP = 0.635728
Validation R2 logP = 0.881594
Epoch 9
Train function
Loss = 1.9912e-03, PNorm = 34.5075, GNorm = 10.1412, lr_0 = 1.0202e-04
Loss = 1.8957e-03, PNorm = 34.5091, GNorm = 6.1231, lr_0 = 1.0202e-04
Loss = 2.4864e-03, PNorm = 34.5112, GNorm = 5.7706, lr_0 = 1.0202e-04
Loss = 2.3700e-03, PNorm = 34.5138, GNorm = 6.8004, lr_0 = 1.0202e-04
Loss = 2.3631e-03, PNorm = 34.5159, GNorm = 4.3818, lr_0 = 1.0202e-04
Loss = 2.3281e-03, PNorm = 34.5182, GNorm = 6.5017, lr_0 = 1.0202e-04
Loss = 2.1021e-03, PNorm = 34.5206, GNorm = 13.1937, lr_0 = 1.0202e-04
Loss = 2.3028e-03, PNorm = 34.5229, GNorm = 3.3260, lr_0 = 1.0202e-04
Loss = 2.4047e-03, PNorm = 34.5250, GNorm = 4.0315, lr_0 = 1.0202e-04
Loss = 2.0701e-03, PNorm = 34.5262, GNorm = 1.9092, lr_0 = 1.0202e-04
Loss = 2.1181e-03, PNorm = 34.5277, GNorm = 9.0149, lr_0 = 1.0202e-04
Loss = 2.6042e-03, PNorm = 34.5289, GNorm = 21.7067, lr_0 = 1.0202e-04
Loss = 2.6440e-03, PNorm = 34.5308, GNorm = 24.8861, lr_0 = 1.0202e-04
Loss = 2.3531e-03, PNorm = 34.5323, GNorm = 21.0915, lr_0 = 1.0202e-04
Loss = 2.7788e-03, PNorm = 34.5349, GNorm = 7.7533, lr_0 = 1.0202e-04
Loss = 2.3434e-03, PNorm = 34.5367, GNorm = 4.1132, lr_0 = 1.0202e-04
Loss = 2.9164e-03, PNorm = 34.5384, GNorm = 6.7316, lr_0 = 1.0202e-04
Loss = 2.2281e-03, PNorm = 34.5407, GNorm = 1.8412, lr_0 = 1.0202e-04
Loss = 1.9231e-03, PNorm = 34.5427, GNorm = 4.1811, lr_0 = 1.0202e-04
Loss = 2.2444e-03, PNorm = 34.5452, GNorm = 13.2182, lr_0 = 1.0202e-04
Loss = 2.9547e-03, PNorm = 34.5474, GNorm = 6.9509, lr_0 = 1.0202e-04
Loss = 1.9817e-03, PNorm = 34.5492, GNorm = 9.1436, lr_0 = 1.0202e-04
Validation rmse logP = 0.698034
Validation R2 logP = 0.857247
Epoch 10
Train function
Loss = 2.2539e-03, PNorm = 34.5509, GNorm = 13.9033, lr_0 = 1.0202e-04
Loss = 2.6022e-03, PNorm = 34.5529, GNorm = 21.8443, lr_0 = 1.0202e-04
Loss = 2.4055e-03, PNorm = 34.5543, GNorm = 15.4362, lr_0 = 1.0202e-04
Loss = 2.3282e-03, PNorm = 34.5562, GNorm = 8.5713, lr_0 = 1.0202e-04
Loss = 2.2453e-03, PNorm = 34.5583, GNorm = 2.8780, lr_0 = 1.0202e-04
Loss = 1.9133e-03, PNorm = 34.5599, GNorm = 7.8898, lr_0 = 1.0202e-04
Loss = 1.9005e-03, PNorm = 34.5616, GNorm = 2.7575, lr_0 = 1.0202e-04
Loss = 2.0195e-03, PNorm = 34.5630, GNorm = 3.6265, lr_0 = 1.0202e-04
Loss = 2.3060e-03, PNorm = 34.5646, GNorm = 8.5349, lr_0 = 1.0202e-04
Loss = 1.9424e-03, PNorm = 34.5667, GNorm = 3.6374, lr_0 = 1.0202e-04
Loss = 2.1162e-03, PNorm = 34.5687, GNorm = 8.2113, lr_0 = 1.0202e-04
Loss = 2.0303e-03, PNorm = 34.5708, GNorm = 4.8846, lr_0 = 1.0202e-04
Loss = 2.1683e-03, PNorm = 34.5732, GNorm = 5.8664, lr_0 = 1.0202e-04
Loss = 2.2538e-03, PNorm = 34.5749, GNorm = 2.5460, lr_0 = 1.0202e-04
Loss = 2.3971e-03, PNorm = 34.5761, GNorm = 15.1854, lr_0 = 1.0202e-04
Loss = 2.0572e-03, PNorm = 34.5774, GNorm = 2.7524, lr_0 = 1.0202e-04
Loss = 2.0556e-03, PNorm = 34.5788, GNorm = 15.9419, lr_0 = 1.0202e-04
Loss = 2.0441e-03, PNorm = 34.5808, GNorm = 10.5023, lr_0 = 1.0202e-04
Loss = 1.8072e-03, PNorm = 34.5830, GNorm = 6.1022, lr_0 = 1.0202e-04
Loss = 1.9734e-03, PNorm = 34.5853, GNorm = 6.2333, lr_0 = 1.0202e-04
Loss = 1.9241e-03, PNorm = 34.5867, GNorm = 2.2894, lr_0 = 1.0202e-04
Loss = 1.9727e-03, PNorm = 34.5884, GNorm = 2.0201, lr_0 = 1.0202e-04
Validation rmse logP = 0.648227
Validation R2 logP = 0.876892
Epoch 11
Train function
Loss = 2.7758e-03, PNorm = 34.5913, GNorm = 10.0083, lr_0 = 1.0202e-04
Loss = 1.9846e-03, PNorm = 34.5929, GNorm = 3.0146, lr_0 = 1.0202e-04
Loss = 2.1518e-03, PNorm = 34.5953, GNorm = 9.4307, lr_0 = 1.0202e-04
Loss = 1.8596e-03, PNorm = 34.5974, GNorm = 8.7576, lr_0 = 1.0202e-04
Loss = 2.0212e-03, PNorm = 34.5993, GNorm = 10.7892, lr_0 = 1.0202e-04
Loss = 2.3654e-03, PNorm = 34.6008, GNorm = 3.4196, lr_0 = 1.0202e-04
Loss = 1.8296e-03, PNorm = 34.6032, GNorm = 11.7694, lr_0 = 1.0202e-04
Loss = 2.1366e-03, PNorm = 34.6052, GNorm = 14.4856, lr_0 = 1.0202e-04
Loss = 2.3706e-03, PNorm = 34.6073, GNorm = 4.6136, lr_0 = 1.0202e-04
Loss = 2.5413e-03, PNorm = 34.6092, GNorm = 4.2518, lr_0 = 1.0202e-04
Loss = 1.6437e-03, PNorm = 34.6109, GNorm = 11.6936, lr_0 = 1.0202e-04
Loss = 2.0205e-03, PNorm = 34.6127, GNorm = 9.1068, lr_0 = 1.0202e-04
Loss = 2.6145e-03, PNorm = 34.6135, GNorm = 5.2516, lr_0 = 1.0202e-04
Loss = 2.2495e-03, PNorm = 34.6151, GNorm = 11.9134, lr_0 = 1.0202e-04
Loss = 1.6727e-03, PNorm = 34.6162, GNorm = 7.2560, lr_0 = 1.0202e-04
Loss = 2.0825e-03, PNorm = 34.6188, GNorm = 2.7044, lr_0 = 1.0202e-04
Loss = 2.5131e-03, PNorm = 34.6199, GNorm = 5.1918, lr_0 = 1.0202e-04
Loss = 2.3716e-03, PNorm = 34.6215, GNorm = 2.3169, lr_0 = 1.0202e-04
Loss = 2.0519e-03, PNorm = 34.6233, GNorm = 3.5205, lr_0 = 1.0202e-04
Loss = 2.0851e-03, PNorm = 34.6253, GNorm = 2.0271, lr_0 = 1.0202e-04
Loss = 1.9647e-03, PNorm = 34.6272, GNorm = 7.8722, lr_0 = 1.0202e-04
Loss = 1.8638e-03, PNorm = 34.6287, GNorm = 14.6014, lr_0 = 1.0202e-04
Loss = 2.3684e-03, PNorm = 34.6315, GNorm = 11.7314, lr_0 = 1.0202e-04
Validation rmse logP = 0.668902
Validation R2 logP = 0.868913
Epoch 12
Train function
Loss = 2.2304e-03, PNorm = 34.6333, GNorm = 10.8768, lr_0 = 1.0202e-04
Loss = 2.1407e-03, PNorm = 34.6361, GNorm = 11.8890, lr_0 = 1.0202e-04
Loss = 1.7447e-03, PNorm = 34.6381, GNorm = 3.7508, lr_0 = 1.0202e-04
Loss = 2.1368e-03, PNorm = 34.6406, GNorm = 2.6977, lr_0 = 1.0202e-04
Loss = 2.3562e-03, PNorm = 34.6430, GNorm = 11.1073, lr_0 = 1.0202e-04
Loss = 1.8079e-03, PNorm = 34.6444, GNorm = 2.2668, lr_0 = 1.0202e-04
Loss = 1.8601e-03, PNorm = 34.6466, GNorm = 2.4574, lr_0 = 1.0202e-04
Loss = 2.2749e-03, PNorm = 34.6483, GNorm = 3.6925, lr_0 = 1.0202e-04
Loss = 2.0241e-03, PNorm = 34.6503, GNorm = 4.8700, lr_0 = 1.0202e-04
Loss = 2.1762e-03, PNorm = 34.6520, GNorm = 18.2637, lr_0 = 1.0202e-04
Loss = 1.8496e-03, PNorm = 34.6533, GNorm = 8.9301, lr_0 = 1.0202e-04
Loss = 2.2133e-03, PNorm = 34.6553, GNorm = 3.1932, lr_0 = 1.0202e-04
Loss = 1.7783e-03, PNorm = 34.6577, GNorm = 2.4422, lr_0 = 1.0202e-04
Loss = 1.8751e-03, PNorm = 34.6599, GNorm = 7.4344, lr_0 = 1.0202e-04
Loss = 1.7302e-03, PNorm = 34.6614, GNorm = 8.5270, lr_0 = 1.0202e-04
Loss = 2.1009e-03, PNorm = 34.6627, GNorm = 2.5352, lr_0 = 1.0202e-04
Loss = 2.0782e-03, PNorm = 34.6645, GNorm = 5.4619, lr_0 = 1.0202e-04
Loss = 1.8450e-03, PNorm = 34.6660, GNorm = 2.0707, lr_0 = 1.0202e-04
Loss = 1.8031e-03, PNorm = 34.6682, GNorm = 3.1014, lr_0 = 1.0202e-04
Loss = 2.2399e-03, PNorm = 34.6702, GNorm = 12.0752, lr_0 = 1.0202e-04
Loss = 2.1646e-03, PNorm = 34.6717, GNorm = 3.2188, lr_0 = 1.0202e-04
Loss = 1.8274e-03, PNorm = 34.6731, GNorm = 6.6491, lr_0 = 1.0202e-04
Validation rmse logP = 0.683463
Validation R2 logP = 0.863145
Epoch 13
Train function
Loss = 2.5113e-03, PNorm = 34.6747, GNorm = 17.8675, lr_0 = 1.0202e-04
Loss = 2.1347e-03, PNorm = 34.6767, GNorm = 8.8378, lr_0 = 1.0202e-04
Loss = 2.3977e-03, PNorm = 34.6792, GNorm = 13.1585, lr_0 = 1.0202e-04
Loss = 1.6043e-03, PNorm = 34.6815, GNorm = 2.4968, lr_0 = 1.0202e-04
Loss = 1.6647e-03, PNorm = 34.6834, GNorm = 5.4518, lr_0 = 1.0202e-04
Loss = 1.6781e-03, PNorm = 34.6848, GNorm = 2.2426, lr_0 = 1.0202e-04
Loss = 1.7890e-03, PNorm = 34.6867, GNorm = 3.0335, lr_0 = 1.0202e-04
Loss = 1.9869e-03, PNorm = 34.6887, GNorm = 7.0111, lr_0 = 1.0202e-04
Loss = 1.6098e-03, PNorm = 34.6902, GNorm = 4.5938, lr_0 = 1.0202e-04
Loss = 2.2426e-03, PNorm = 34.6918, GNorm = 2.4871, lr_0 = 1.0202e-04
Loss = 1.5574e-03, PNorm = 34.6934, GNorm = 6.2523, lr_0 = 1.0202e-04
Loss = 1.7738e-03, PNorm = 34.6954, GNorm = 14.2954, lr_0 = 1.0202e-04
Loss = 2.0543e-03, PNorm = 34.6968, GNorm = 6.4621, lr_0 = 1.0202e-04
Loss = 1.8122e-03, PNorm = 34.6992, GNorm = 2.5775, lr_0 = 1.0202e-04
Loss = 1.8410e-03, PNorm = 34.7013, GNorm = 2.2227, lr_0 = 1.0202e-04
Loss = 2.0147e-03, PNorm = 34.7027, GNorm = 18.1662, lr_0 = 1.0202e-04
Loss = 2.1624e-03, PNorm = 34.7041, GNorm = 16.5033, lr_0 = 1.0202e-04
Loss = 2.3295e-03, PNorm = 34.7057, GNorm = 7.4862, lr_0 = 1.0202e-04
Loss = 1.9151e-03, PNorm = 34.7075, GNorm = 7.0291, lr_0 = 1.0202e-04
Loss = 2.3262e-03, PNorm = 34.7094, GNorm = 4.2976, lr_0 = 1.0202e-04
Loss = 1.9630e-03, PNorm = 34.7121, GNorm = 4.5897, lr_0 = 1.0202e-04
Loss = 1.7967e-03, PNorm = 34.7144, GNorm = 2.2145, lr_0 = 1.0202e-04
Validation rmse logP = 0.604799
Validation R2 logP = 0.892835
Epoch 14
Train function
Loss = 1.4707e-03, PNorm = 34.7159, GNorm = 4.1474, lr_0 = 1.0202e-04
Loss = 2.1726e-03, PNorm = 34.7184, GNorm = 7.6761, lr_0 = 1.0202e-04
Loss = 1.8393e-03, PNorm = 34.7211, GNorm = 4.0447, lr_0 = 1.0202e-04
Loss = 1.9051e-03, PNorm = 34.7237, GNorm = 10.3174, lr_0 = 1.0202e-04
Loss = 2.2780e-03, PNorm = 34.7258, GNorm = 6.7663, lr_0 = 1.0202e-04
Loss = 1.9445e-03, PNorm = 34.7272, GNorm = 15.8815, lr_0 = 1.0202e-04
Loss = 1.8845e-03, PNorm = 34.7289, GNorm = 8.0203, lr_0 = 1.0202e-04
Loss = 1.4340e-03, PNorm = 34.7302, GNorm = 8.5415, lr_0 = 1.0202e-04
Loss = 1.7589e-03, PNorm = 34.7312, GNorm = 5.1615, lr_0 = 1.0202e-04
Loss = 1.8304e-03, PNorm = 34.7328, GNorm = 14.4622, lr_0 = 1.0202e-04
Loss = 2.1924e-03, PNorm = 34.7345, GNorm = 10.8440, lr_0 = 1.0202e-04
Loss = 2.1751e-03, PNorm = 34.7358, GNorm = 7.6006, lr_0 = 1.0202e-04
Loss = 1.5570e-03, PNorm = 34.7377, GNorm = 6.0366, lr_0 = 1.0202e-04
Loss = 2.2767e-03, PNorm = 34.7402, GNorm = 7.9395, lr_0 = 1.0202e-04
Loss = 1.7418e-03, PNorm = 34.7433, GNorm = 8.8371, lr_0 = 1.0202e-04
Loss = 1.4400e-03, PNorm = 34.7446, GNorm = 3.2600, lr_0 = 1.0202e-04
Loss = 1.8967e-03, PNorm = 34.7460, GNorm = 4.4806, lr_0 = 1.0202e-04
Loss = 2.2718e-03, PNorm = 34.7482, GNorm = 11.6909, lr_0 = 1.0202e-04
Loss = 2.0914e-03, PNorm = 34.7499, GNorm = 3.9399, lr_0 = 1.0202e-04
Loss = 1.6264e-03, PNorm = 34.7522, GNorm = 3.1621, lr_0 = 1.0202e-04
Loss = 1.6495e-03, PNorm = 34.7534, GNorm = 15.5212, lr_0 = 1.0202e-04
Loss = 2.0572e-03, PNorm = 34.7549, GNorm = 14.0834, lr_0 = 1.0202e-04
Loss = 2.4299e-03, PNorm = 34.7563, GNorm = 21.3157, lr_0 = 1.0202e-04
Validation rmse logP = 0.728515
Validation R2 logP = 0.844508
Epoch 15
Train function
Loss = 2.2787e-03, PNorm = 34.7586, GNorm = 2.7803, lr_0 = 1.0202e-04
Loss = 2.4307e-03, PNorm = 34.7610, GNorm = 8.9113, lr_0 = 1.0202e-04
Loss = 1.7426e-03, PNorm = 34.7622, GNorm = 3.3662, lr_0 = 1.0202e-04
Loss = 1.4560e-03, PNorm = 34.7635, GNorm = 3.0934, lr_0 = 1.0202e-04
Loss = 1.7517e-03, PNorm = 34.7656, GNorm = 2.7319, lr_0 = 1.0202e-04
Loss = 1.9035e-03, PNorm = 34.7676, GNorm = 5.3989, lr_0 = 1.0202e-04
Loss = 1.9358e-03, PNorm = 34.7701, GNorm = 1.9884, lr_0 = 1.0202e-04
Loss = 1.7012e-03, PNorm = 34.7727, GNorm = 16.1676, lr_0 = 1.0202e-04
Loss = 1.7995e-03, PNorm = 34.7753, GNorm = 3.8468, lr_0 = 1.0202e-04
Loss = 1.9722e-03, PNorm = 34.7785, GNorm = 7.0851, lr_0 = 1.0202e-04
Loss = 1.9690e-03, PNorm = 34.7799, GNorm = 10.4136, lr_0 = 1.0202e-04
Loss = 2.0345e-03, PNorm = 34.7822, GNorm = 11.4839, lr_0 = 1.0202e-04
Loss = 1.9095e-03, PNorm = 34.7835, GNorm = 3.2823, lr_0 = 1.0202e-04
Loss = 2.0718e-03, PNorm = 34.7861, GNorm = 5.2363, lr_0 = 1.0202e-04
Loss = 1.7039e-03, PNorm = 34.7873, GNorm = 5.5315, lr_0 = 1.0202e-04
Loss = 1.6596e-03, PNorm = 34.7892, GNorm = 6.6152, lr_0 = 1.0202e-04
Loss = 1.8724e-03, PNorm = 34.7908, GNorm = 2.1654, lr_0 = 1.0202e-04
Loss = 1.8889e-03, PNorm = 34.7925, GNorm = 9.7988, lr_0 = 1.0202e-04
Loss = 1.7867e-03, PNorm = 34.7941, GNorm = 3.6947, lr_0 = 1.0202e-04
Loss = 1.9304e-03, PNorm = 34.7958, GNorm = 4.3770, lr_0 = 1.0202e-04
Loss = 1.8656e-03, PNorm = 34.7975, GNorm = 8.0428, lr_0 = 1.0202e-04
Loss = 2.3132e-03, PNorm = 34.7993, GNorm = 3.6183, lr_0 = 1.0202e-04
Validation rmse logP = 0.589096
Validation R2 logP = 0.898327
Epoch 16
Train function
Loss = 1.9962e-03, PNorm = 34.8010, GNorm = 8.3367, lr_0 = 1.0202e-04
Loss = 1.6306e-03, PNorm = 34.8026, GNorm = 10.5656, lr_0 = 1.0202e-04
Loss = 1.8341e-03, PNorm = 34.8052, GNorm = 12.0174, lr_0 = 1.0202e-04
Loss = 1.8875e-03, PNorm = 34.8077, GNorm = 9.4188, lr_0 = 1.0202e-04
Loss = 1.7984e-03, PNorm = 34.8094, GNorm = 2.8301, lr_0 = 1.0202e-04
Loss = 1.3605e-03, PNorm = 34.8105, GNorm = 1.9936, lr_0 = 1.0202e-04
Loss = 1.6952e-03, PNorm = 34.8128, GNorm = 2.5000, lr_0 = 1.0202e-04
Loss = 1.8834e-03, PNorm = 34.8155, GNorm = 9.4729, lr_0 = 1.0202e-04
Loss = 2.0716e-03, PNorm = 34.8177, GNorm = 2.4743, lr_0 = 1.0202e-04
Loss = 1.3485e-03, PNorm = 34.8205, GNorm = 1.0776, lr_0 = 1.0202e-04
Loss = 1.5885e-03, PNorm = 34.8221, GNorm = 5.8863, lr_0 = 1.0202e-04
Loss = 1.5822e-03, PNorm = 34.8236, GNorm = 1.6773, lr_0 = 1.0202e-04
Loss = 1.5756e-03, PNorm = 34.8246, GNorm = 3.3846, lr_0 = 1.0202e-04
Loss = 1.5987e-03, PNorm = 34.8260, GNorm = 5.1443, lr_0 = 1.0202e-04
Loss = 1.5553e-03, PNorm = 34.8279, GNorm = 8.5947, lr_0 = 1.0202e-04
Loss = 1.5886e-03, PNorm = 34.8301, GNorm = 3.2062, lr_0 = 1.0202e-04
Loss = 1.4994e-03, PNorm = 34.8321, GNorm = 2.2368, lr_0 = 1.0202e-04
Loss = 1.9492e-03, PNorm = 34.8345, GNorm = 15.3372, lr_0 = 1.0202e-04
Loss = 2.0504e-03, PNorm = 34.8358, GNorm = 12.6934, lr_0 = 1.0202e-04
Loss = 1.6622e-03, PNorm = 34.8380, GNorm = 9.0972, lr_0 = 1.0202e-04
Loss = 2.1565e-03, PNorm = 34.8399, GNorm = 5.9624, lr_0 = 1.0202e-04
Loss = 2.1232e-03, PNorm = 34.8418, GNorm = 2.7836, lr_0 = 1.0202e-04
Loss = 2.4055e-03, PNorm = 34.8437, GNorm = 13.6814, lr_0 = 1.0202e-04
Validation rmse logP = 0.561454
Validation R2 logP = 0.907645
Epoch 17
Train function
Loss = 1.9327e-03, PNorm = 34.8462, GNorm = 8.3127, lr_0 = 1.0202e-04
Loss = 1.5447e-03, PNorm = 34.8480, GNorm = 9.7930, lr_0 = 1.0202e-04
Loss = 1.7489e-03, PNorm = 34.8496, GNorm = 2.8745, lr_0 = 1.0202e-04
Loss = 1.4151e-03, PNorm = 34.8511, GNorm = 1.5820, lr_0 = 1.0202e-04
Loss = 1.8789e-03, PNorm = 34.8531, GNorm = 3.8713, lr_0 = 1.0202e-04
Loss = 1.6267e-03, PNorm = 34.8552, GNorm = 5.5371, lr_0 = 1.0202e-04
Loss = 1.3344e-03, PNorm = 34.8576, GNorm = 1.8693, lr_0 = 1.0202e-04
Loss = 1.4471e-03, PNorm = 34.8593, GNorm = 3.7551, lr_0 = 1.0202e-04
Loss = 1.7709e-03, PNorm = 34.8613, GNorm = 10.4332, lr_0 = 1.0202e-04
Loss = 1.9135e-03, PNorm = 34.8630, GNorm = 2.9872, lr_0 = 1.0202e-04
Loss = 1.9064e-03, PNorm = 34.8648, GNorm = 6.6241, lr_0 = 1.0202e-04
Loss = 1.4685e-03, PNorm = 34.8663, GNorm = 6.6782, lr_0 = 1.0202e-04
Loss = 1.7179e-03, PNorm = 34.8688, GNorm = 13.9869, lr_0 = 1.0202e-04
Loss = 1.9514e-03, PNorm = 34.8709, GNorm = 8.7660, lr_0 = 1.0202e-04
Loss = 1.8685e-03, PNorm = 34.8731, GNorm = 14.0009, lr_0 = 1.0202e-04
Loss = 2.2895e-03, PNorm = 34.8743, GNorm = 21.7919, lr_0 = 1.0202e-04
Loss = 1.9258e-03, PNorm = 34.8765, GNorm = 2.2524, lr_0 = 1.0202e-04
Loss = 1.8096e-03, PNorm = 34.8781, GNorm = 2.4477, lr_0 = 1.0202e-04
Loss = 2.0028e-03, PNorm = 34.8797, GNorm = 13.6166, lr_0 = 1.0202e-04
Loss = 2.1884e-03, PNorm = 34.8820, GNorm = 7.1715, lr_0 = 1.0202e-04
Loss = 1.7238e-03, PNorm = 34.8837, GNorm = 6.2272, lr_0 = 1.0202e-04
Loss = 1.8742e-03, PNorm = 34.8855, GNorm = 2.8046, lr_0 = 1.0202e-04
Validation rmse logP = 0.625071
Validation R2 logP = 0.885530
Epoch 18
Train function
Loss = 1.8544e-03, PNorm = 34.8872, GNorm = 8.8633, lr_0 = 1.0202e-04
Loss = 1.5808e-03, PNorm = 34.8894, GNorm = 3.5671, lr_0 = 1.0202e-04
Loss = 1.4607e-03, PNorm = 34.8914, GNorm = 8.9149, lr_0 = 1.0202e-04
Loss = 1.6615e-03, PNorm = 34.8934, GNorm = 7.7711, lr_0 = 1.0202e-04
Loss = 1.4797e-03, PNorm = 34.8953, GNorm = 5.5752, lr_0 = 1.0202e-04
Loss = 1.4262e-03, PNorm = 34.8978, GNorm = 11.7597, lr_0 = 1.0202e-04
Loss = 1.4961e-03, PNorm = 34.8996, GNorm = 2.0430, lr_0 = 1.0202e-04
Loss = 1.5541e-03, PNorm = 34.9017, GNorm = 7.2057, lr_0 = 1.0202e-04
Loss = 1.9396e-03, PNorm = 34.9036, GNorm = 6.0754, lr_0 = 1.0202e-04
Loss = 2.0845e-03, PNorm = 34.9051, GNorm = 5.6046, lr_0 = 1.0202e-04
Loss = 1.9415e-03, PNorm = 34.9055, GNorm = 16.6493, lr_0 = 1.0202e-04
Loss = 1.6220e-03, PNorm = 34.9077, GNorm = 10.4097, lr_0 = 1.0202e-04
Loss = 1.6303e-03, PNorm = 34.9090, GNorm = 2.4563, lr_0 = 1.0202e-04
Loss = 1.7342e-03, PNorm = 34.9106, GNorm = 12.7126, lr_0 = 1.0202e-04
Loss = 1.8821e-03, PNorm = 34.9132, GNorm = 3.1851, lr_0 = 1.0202e-04
Loss = 1.5927e-03, PNorm = 34.9161, GNorm = 4.0352, lr_0 = 1.0202e-04
Loss = 1.7162e-03, PNorm = 34.9179, GNorm = 13.9819, lr_0 = 1.0202e-04
Loss = 1.8956e-03, PNorm = 34.9184, GNorm = 16.8191, lr_0 = 1.0202e-04
Loss = 1.7797e-03, PNorm = 34.9201, GNorm = 6.2240, lr_0 = 1.0202e-04
Loss = 2.0101e-03, PNorm = 34.9223, GNorm = 17.0358, lr_0 = 1.0202e-04
Loss = 2.6852e-03, PNorm = 34.9235, GNorm = 3.8080, lr_0 = 1.0202e-04
Loss = 2.0175e-03, PNorm = 34.9255, GNorm = 5.7570, lr_0 = 1.0202e-04
Validation rmse logP = 0.572752
Validation R2 logP = 0.903891
Epoch 19
Train function
Loss = 1.3872e-03, PNorm = 34.9277, GNorm = 6.2109, lr_0 = 1.0202e-04
Loss = 1.5506e-03, PNorm = 34.9289, GNorm = 1.1733, lr_0 = 1.0202e-04
Loss = 1.5250e-03, PNorm = 34.9307, GNorm = 6.4893, lr_0 = 1.0202e-04
Loss = 1.7390e-03, PNorm = 34.9319, GNorm = 5.8875, lr_0 = 1.0202e-04
Loss = 1.7085e-03, PNorm = 34.9343, GNorm = 2.2862, lr_0 = 1.0202e-04
Loss = 1.4698e-03, PNorm = 34.9363, GNorm = 2.2109, lr_0 = 1.0202e-04
Loss = 1.5396e-03, PNorm = 34.9379, GNorm = 10.0668, lr_0 = 1.0202e-04
Loss = 1.8264e-03, PNorm = 34.9402, GNorm = 6.4604, lr_0 = 1.0202e-04
Loss = 1.7042e-03, PNorm = 34.9424, GNorm = 2.4785, lr_0 = 1.0202e-04
Loss = 1.7607e-03, PNorm = 34.9440, GNorm = 5.9111, lr_0 = 1.0202e-04
Loss = 1.5177e-03, PNorm = 34.9461, GNorm = 2.9631, lr_0 = 1.0202e-04
Loss = 1.3649e-03, PNorm = 34.9477, GNorm = 5.8855, lr_0 = 1.0202e-04
Loss = 1.3798e-03, PNorm = 34.9492, GNorm = 4.7162, lr_0 = 1.0202e-04
Loss = 2.1123e-03, PNorm = 34.9513, GNorm = 5.1881, lr_0 = 1.0202e-04
Loss = 1.5799e-03, PNorm = 34.9535, GNorm = 3.8408, lr_0 = 1.0202e-04
Loss = 1.7704e-03, PNorm = 34.9553, GNorm = 5.4486, lr_0 = 1.0202e-04
Loss = 1.3532e-03, PNorm = 34.9563, GNorm = 4.7671, lr_0 = 1.0202e-04
Loss = 1.0292e-03, PNorm = 34.9584, GNorm = 3.7850, lr_0 = 1.0202e-04
Loss = 1.7812e-03, PNorm = 34.9605, GNorm = 5.3025, lr_0 = 1.0202e-04
Loss = 1.8278e-03, PNorm = 34.9624, GNorm = 1.7612, lr_0 = 1.0202e-04
Loss = 1.4476e-03, PNorm = 34.9641, GNorm = 1.6266, lr_0 = 1.0202e-04
Loss = 1.4508e-03, PNorm = 34.9657, GNorm = 2.5511, lr_0 = 1.0202e-04
Loss = 1.2807e-03, PNorm = 34.9672, GNorm = 2.5310, lr_0 = 1.0202e-04
Loss = 5.2202e-03, PNorm = 34.9673, GNorm = 10.8515, lr_0 = 1.0202e-04
Validation rmse logP = 0.543162
Validation R2 logP = 0.913565
Epoch 20
Train function
Loss = 1.2268e-03, PNorm = 34.9696, GNorm = 5.1734, lr_0 = 1.0202e-04
Loss = 1.4909e-03, PNorm = 34.9713, GNorm = 4.1226, lr_0 = 1.0202e-04
Loss = 1.2410e-03, PNorm = 34.9726, GNorm = 1.7448, lr_0 = 1.0202e-04
Loss = 1.2238e-03, PNorm = 34.9744, GNorm = 9.0096, lr_0 = 1.0202e-04
Loss = 1.5409e-03, PNorm = 34.9767, GNorm = 23.5756, lr_0 = 1.0202e-04
Loss = 1.7237e-03, PNorm = 34.9784, GNorm = 14.6043, lr_0 = 1.0202e-04
Loss = 1.8835e-03, PNorm = 34.9803, GNorm = 8.8917, lr_0 = 1.0202e-04
Loss = 1.5488e-03, PNorm = 34.9817, GNorm = 11.8216, lr_0 = 1.0202e-04
Loss = 2.2315e-03, PNorm = 34.9838, GNorm = 2.4790, lr_0 = 1.0202e-04
Loss = 1.6290e-03, PNorm = 34.9867, GNorm = 2.6348, lr_0 = 1.0202e-04
Loss = 1.6249e-03, PNorm = 34.9891, GNorm = 2.5846, lr_0 = 1.0202e-04
Loss = 1.6307e-03, PNorm = 34.9917, GNorm = 5.3097, lr_0 = 1.0202e-04
Loss = 1.5126e-03, PNorm = 34.9940, GNorm = 3.5760, lr_0 = 1.0202e-04
Loss = 2.0086e-03, PNorm = 34.9951, GNorm = 19.0544, lr_0 = 1.0202e-04
Loss = 1.4616e-03, PNorm = 34.9960, GNorm = 4.3148, lr_0 = 1.0202e-04
Loss = 1.3567e-03, PNorm = 34.9978, GNorm = 4.6704, lr_0 = 1.0202e-04
Loss = 1.5379e-03, PNorm = 34.9997, GNorm = 9.3504, lr_0 = 1.0202e-04
Loss = 1.6062e-03, PNorm = 35.0015, GNorm = 11.2562, lr_0 = 1.0202e-04
Loss = 1.5424e-03, PNorm = 35.0028, GNorm = 2.5121, lr_0 = 1.0202e-04
Loss = 2.0305e-03, PNorm = 35.0038, GNorm = 5.4758, lr_0 = 1.0202e-04
Loss = 1.6702e-03, PNorm = 35.0059, GNorm = 2.8869, lr_0 = 1.0202e-04
Loss = 1.7329e-03, PNorm = 35.0084, GNorm = 5.5193, lr_0 = 1.0202e-04
Validation rmse logP = 0.553311
Validation R2 logP = 0.910304
Epoch 21
Train function
Loss = 1.5994e-03, PNorm = 35.0103, GNorm = 5.5797, lr_0 = 1.0202e-04
Loss = 1.6372e-03, PNorm = 35.0123, GNorm = 7.3100, lr_0 = 1.0202e-04
Loss = 1.6504e-03, PNorm = 35.0138, GNorm = 4.4846, lr_0 = 1.0202e-04
Loss = 1.6751e-03, PNorm = 35.0161, GNorm = 5.8466, lr_0 = 1.0202e-04
Loss = 1.5080e-03, PNorm = 35.0181, GNorm = 6.4787, lr_0 = 1.0202e-04
Loss = 1.5178e-03, PNorm = 35.0206, GNorm = 8.7784, lr_0 = 1.0202e-04
Loss = 1.6076e-03, PNorm = 35.0225, GNorm = 1.9271, lr_0 = 1.0202e-04
Loss = 1.4306e-03, PNorm = 35.0249, GNorm = 6.9036, lr_0 = 1.0202e-04
Loss = 1.9758e-03, PNorm = 35.0263, GNorm = 8.3988, lr_0 = 1.0202e-04
Loss = 1.4976e-03, PNorm = 35.0284, GNorm = 6.6290, lr_0 = 1.0202e-04
Loss = 1.7228e-03, PNorm = 35.0300, GNorm = 1.9808, lr_0 = 1.0202e-04
Loss = 1.5849e-03, PNorm = 35.0329, GNorm = 15.2841, lr_0 = 1.0202e-04
Loss = 1.8464e-03, PNorm = 35.0345, GNorm = 8.6690, lr_0 = 1.0202e-04
Loss = 1.3906e-03, PNorm = 35.0365, GNorm = 2.3183, lr_0 = 1.0202e-04
Loss = 1.4623e-03, PNorm = 35.0378, GNorm = 3.1176, lr_0 = 1.0202e-04
Loss = 1.5695e-03, PNorm = 35.0394, GNorm = 12.2906, lr_0 = 1.0202e-04
Loss = 1.6265e-03, PNorm = 35.0414, GNorm = 2.5848, lr_0 = 1.0202e-04
Loss = 1.2869e-03, PNorm = 35.0434, GNorm = 5.2601, lr_0 = 1.0202e-04
Loss = 1.4861e-03, PNorm = 35.0443, GNorm = 12.3905, lr_0 = 1.0202e-04
Loss = 1.4111e-03, PNorm = 35.0458, GNorm = 5.6097, lr_0 = 1.0202e-04
Loss = 1.5830e-03, PNorm = 35.0479, GNorm = 6.6528, lr_0 = 1.0202e-04
Loss = 1.3302e-03, PNorm = 35.0495, GNorm = 10.1960, lr_0 = 1.0202e-04
Validation rmse logP = 0.564275
Validation R2 logP = 0.906715
Epoch 22
Train function
Loss = 2.1536e-03, PNorm = 35.0513, GNorm = 14.8310, lr_0 = 1.0202e-04
Loss = 1.5729e-03, PNorm = 35.0535, GNorm = 7.3271, lr_0 = 1.0202e-04
Loss = 1.2294e-03, PNorm = 35.0554, GNorm = 3.1047, lr_0 = 1.0202e-04
Loss = 1.5530e-03, PNorm = 35.0579, GNorm = 7.6349, lr_0 = 1.0202e-04
Loss = 1.2814e-03, PNorm = 35.0598, GNorm = 2.7029, lr_0 = 1.0202e-04
Loss = 1.4955e-03, PNorm = 35.0618, GNorm = 8.5445, lr_0 = 1.0202e-04
Loss = 1.8337e-03, PNorm = 35.0635, GNorm = 8.2268, lr_0 = 1.0202e-04
Loss = 1.5711e-03, PNorm = 35.0655, GNorm = 3.1919, lr_0 = 1.0202e-04
Loss = 1.3924e-03, PNorm = 35.0670, GNorm = 10.3656, lr_0 = 1.0202e-04
Loss = 1.4714e-03, PNorm = 35.0687, GNorm = 5.8806, lr_0 = 1.0202e-04
Loss = 1.5339e-03, PNorm = 35.0706, GNorm = 10.9837, lr_0 = 1.0202e-04
Loss = 1.0958e-03, PNorm = 35.0723, GNorm = 11.0871, lr_0 = 1.0202e-04
Loss = 2.1164e-03, PNorm = 35.0736, GNorm = 6.2730, lr_0 = 1.0202e-04
Loss = 1.3715e-03, PNorm = 35.0755, GNorm = 3.6206, lr_0 = 1.0202e-04
Loss = 1.5149e-03, PNorm = 35.0768, GNorm = 2.7498, lr_0 = 1.0202e-04
Loss = 1.5967e-03, PNorm = 35.0788, GNorm = 6.4407, lr_0 = 1.0202e-04
Loss = 1.3608e-03, PNorm = 35.0801, GNorm = 3.0078, lr_0 = 1.0202e-04
Loss = 1.3171e-03, PNorm = 35.0817, GNorm = 3.5988, lr_0 = 1.0202e-04
Loss = 1.1812e-03, PNorm = 35.0831, GNorm = 1.5708, lr_0 = 1.0202e-04
Loss = 1.6898e-03, PNorm = 35.0850, GNorm = 9.6522, lr_0 = 1.0202e-04
Loss = 1.2107e-03, PNorm = 35.0869, GNorm = 2.0005, lr_0 = 1.0202e-04
Loss = 1.4304e-03, PNorm = 35.0886, GNorm = 7.9463, lr_0 = 1.0202e-04
Loss = 1.4788e-03, PNorm = 35.0899, GNorm = 5.1720, lr_0 = 1.0202e-04
Validation rmse logP = 0.572165
Validation R2 logP = 0.904087
Epoch 23
Train function
Loss = 1.1035e-03, PNorm = 35.0915, GNorm = 10.6612, lr_0 = 1.0202e-04
Loss = 1.6246e-03, PNorm = 35.0933, GNorm = 11.1153, lr_0 = 1.0202e-04
Loss = 1.4985e-03, PNorm = 35.0950, GNorm = 2.3093, lr_0 = 1.0202e-04
Loss = 1.3528e-03, PNorm = 35.0969, GNorm = 4.5431, lr_0 = 1.0202e-04
Loss = 1.4390e-03, PNorm = 35.0982, GNorm = 7.5210, lr_0 = 1.0202e-04
Loss = 1.5967e-03, PNorm = 35.1004, GNorm = 9.7566, lr_0 = 1.0202e-04
Loss = 1.2783e-03, PNorm = 35.1015, GNorm = 8.7633, lr_0 = 1.0202e-04
Loss = 1.1046e-03, PNorm = 35.1028, GNorm = 3.6301, lr_0 = 1.0202e-04
Loss = 1.3073e-03, PNorm = 35.1045, GNorm = 1.8119, lr_0 = 1.0202e-04
Loss = 1.3948e-03, PNorm = 35.1066, GNorm = 2.9081, lr_0 = 1.0202e-04
Loss = 1.4328e-03, PNorm = 35.1087, GNorm = 6.9548, lr_0 = 1.0202e-04
Loss = 1.5566e-03, PNorm = 35.1113, GNorm = 9.8195, lr_0 = 1.0202e-04
Loss = 1.3411e-03, PNorm = 35.1133, GNorm = 6.3526, lr_0 = 1.0202e-04
Loss = 1.3882e-03, PNorm = 35.1151, GNorm = 1.8056, lr_0 = 1.0202e-04
Loss = 1.5191e-03, PNorm = 35.1171, GNorm = 2.3945, lr_0 = 1.0202e-04
Loss = 1.4467e-03, PNorm = 35.1200, GNorm = 1.8162, lr_0 = 1.0202e-04
Loss = 1.4622e-03, PNorm = 35.1219, GNorm = 3.8575, lr_0 = 1.0202e-04
Loss = 1.2928e-03, PNorm = 35.1237, GNorm = 2.9463, lr_0 = 1.0202e-04
Loss = 1.5974e-03, PNorm = 35.1243, GNorm = 9.9270, lr_0 = 1.0202e-04
Loss = 1.3900e-03, PNorm = 35.1259, GNorm = 1.9845, lr_0 = 1.0202e-04
Loss = 1.6659e-03, PNorm = 35.1287, GNorm = 12.7060, lr_0 = 1.0202e-04
Loss = 1.4160e-03, PNorm = 35.1310, GNorm = 11.1520, lr_0 = 1.0202e-04
Validation rmse logP = 0.597122
Validation R2 logP = 0.895538
Epoch 24
Train function
Loss = 1.7373e-03, PNorm = 35.1330, GNorm = 17.0343, lr_0 = 1.0202e-04
Loss = 1.7219e-03, PNorm = 35.1348, GNorm = 18.1156, lr_0 = 1.0202e-04
Loss = 1.7993e-03, PNorm = 35.1367, GNorm = 8.5576, lr_0 = 1.0202e-04
Loss = 2.1844e-03, PNorm = 35.1375, GNorm = 6.8287, lr_0 = 1.0202e-04
Loss = 1.7846e-03, PNorm = 35.1399, GNorm = 6.1723, lr_0 = 1.0202e-04
Loss = 1.4913e-03, PNorm = 35.1412, GNorm = 7.3008, lr_0 = 1.0202e-04
Loss = 1.1341e-03, PNorm = 35.1427, GNorm = 4.4713, lr_0 = 1.0202e-04
Loss = 1.2273e-03, PNorm = 35.1450, GNorm = 1.1049, lr_0 = 1.0202e-04
Loss = 1.4218e-03, PNorm = 35.1475, GNorm = 9.5602, lr_0 = 1.0202e-04
Loss = 1.7941e-03, PNorm = 35.1493, GNorm = 8.4405, lr_0 = 1.0202e-04
Loss = 1.4338e-03, PNorm = 35.1524, GNorm = 2.2171, lr_0 = 1.0202e-04
Loss = 1.2940e-03, PNorm = 35.1543, GNorm = 3.1199, lr_0 = 1.0202e-04
Loss = 1.1945e-03, PNorm = 35.1563, GNorm = 2.8551, lr_0 = 1.0202e-04
Loss = 1.3032e-03, PNorm = 35.1580, GNorm = 5.7935, lr_0 = 1.0202e-04
Loss = 1.6329e-03, PNorm = 35.1601, GNorm = 3.4704, lr_0 = 1.0202e-04
Loss = 1.3769e-03, PNorm = 35.1614, GNorm = 7.2869, lr_0 = 1.0202e-04
Loss = 1.2710e-03, PNorm = 35.1627, GNorm = 7.7177, lr_0 = 1.0202e-04
Loss = 1.6335e-03, PNorm = 35.1645, GNorm = 3.3539, lr_0 = 1.0202e-04
Loss = 1.6891e-03, PNorm = 35.1658, GNorm = 1.6819, lr_0 = 1.0202e-04
Loss = 1.3217e-03, PNorm = 35.1679, GNorm = 3.4557, lr_0 = 1.0202e-04
Loss = 1.1798e-03, PNorm = 35.1696, GNorm = 5.2216, lr_0 = 1.0202e-04
Loss = 1.5941e-03, PNorm = 35.1717, GNorm = 13.5528, lr_0 = 1.0202e-04
Validation rmse logP = 0.605574
Validation R2 logP = 0.892560
Epoch 25
Train function
Loss = 1.4871e-03, PNorm = 35.1731, GNorm = 10.2472, lr_0 = 1.0202e-04
Loss = 1.6353e-03, PNorm = 35.1746, GNorm = 6.3389, lr_0 = 1.0202e-04
Loss = 1.5254e-03, PNorm = 35.1771, GNorm = 9.0797, lr_0 = 1.0202e-04
Loss = 1.3856e-03, PNorm = 35.1793, GNorm = 2.9625, lr_0 = 1.0202e-04
Loss = 1.4083e-03, PNorm = 35.1813, GNorm = 7.2772, lr_0 = 1.0202e-04
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 22,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 22,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 22,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 22,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 11,710 | train size = 11,177 | val size = 533 | test size = 2,067
Fitting scaler
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 4 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/logp_wo_averaging_train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 300,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 22,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 4,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_341/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/logp_wo_averaging_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': 'smiles',
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logP'],
 'task_names': ['logP'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 11,710 | train size = 11,177 | val size = 533 | test size = 2,067
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=1, bias=True)
  )
)
Number of parameters = 354,901
Moving model to cuda
Epoch 0
Train function
Loss = 1.6962e-02, PNorm = 34.0116, GNorm = 1.9700, lr_0 = 1.0202e-04
Loss = 1.7711e-02, PNorm = 34.0139, GNorm = 2.5146, lr_0 = 1.0202e-04
Loss = 1.5340e-02, PNorm = 34.0169, GNorm = 1.9349, lr_0 = 1.0202e-04
Loss = 1.5352e-02, PNorm = 34.0206, GNorm = 1.7695, lr_0 = 1.0202e-04
Loss = 1.4047e-02, PNorm = 34.0248, GNorm = 2.5131, lr_0 = 1.0202e-04
Loss = 1.3613e-02, PNorm = 34.0288, GNorm = 15.3018, lr_0 = 1.0202e-04
Loss = 1.2169e-02, PNorm = 34.0321, GNorm = 20.2297, lr_0 = 1.0202e-04
Loss = 1.2418e-02, PNorm = 34.0349, GNorm = 21.5383, lr_0 = 1.0202e-04
Loss = 1.0239e-02, PNorm = 34.0375, GNorm = 2.9857, lr_0 = 1.0202e-04
Loss = 1.0228e-02, PNorm = 34.0413, GNorm = 24.1786, lr_0 = 1.0202e-04
Loss = 8.8066e-03, PNorm = 34.0445, GNorm = 6.9430, lr_0 = 1.0202e-04
Loss = 1.0586e-02, PNorm = 34.0473, GNorm = 15.4734, lr_0 = 1.0202e-04
Loss = 1.0139e-02, PNorm = 34.0504, GNorm = 14.3769, lr_0 = 1.0202e-04
Loss = 1.0688e-02, PNorm = 34.0530, GNorm = 3.6074, lr_0 = 1.0202e-04
Loss = 9.4742e-03, PNorm = 34.0559, GNorm = 12.3202, lr_0 = 1.0202e-04
Loss = 9.0447e-03, PNorm = 34.0595, GNorm = 8.8030, lr_0 = 1.0202e-04
Loss = 7.7187e-03, PNorm = 34.0637, GNorm = 5.1046, lr_0 = 1.0202e-04
Loss = 6.7299e-03, PNorm = 34.0665, GNorm = 7.9823, lr_0 = 1.0202e-04
Loss = 8.8600e-03, PNorm = 34.0699, GNorm = 10.6906, lr_0 = 1.0202e-04
Loss = 6.9359e-03, PNorm = 34.0723, GNorm = 11.0408, lr_0 = 1.0202e-04
Loss = 8.2959e-03, PNorm = 34.0753, GNorm = 15.0221, lr_0 = 1.0202e-04
Loss = 6.3613e-03, PNorm = 34.0789, GNorm = 2.8511, lr_0 = 1.0202e-04
Validation rmse logP = 1.072882
Validation R2 logP = 0.662762
Epoch 1
Train function
Loss = 7.2795e-03, PNorm = 34.0831, GNorm = 13.3237, lr_0 = 1.0202e-04
Loss = 5.7231e-03, PNorm = 34.0863, GNorm = 11.5603, lr_0 = 1.0202e-04
Loss = 6.1968e-03, PNorm = 34.0891, GNorm = 2.5603, lr_0 = 1.0202e-04
Loss = 5.9255e-03, PNorm = 34.0920, GNorm = 5.8651, lr_0 = 1.0202e-04
Loss = 5.6853e-03, PNorm = 34.0951, GNorm = 12.2037, lr_0 = 1.0202e-04
Loss = 6.3334e-03, PNorm = 34.0980, GNorm = 16.1343, lr_0 = 1.0202e-04
Loss = 5.9102e-03, PNorm = 34.1014, GNorm = 2.8133, lr_0 = 1.0202e-04
Loss = 5.9135e-03, PNorm = 34.1044, GNorm = 5.5456, lr_0 = 1.0202e-04
Loss = 6.2343e-03, PNorm = 34.1070, GNorm = 24.4747, lr_0 = 1.0202e-04
Loss = 6.4061e-03, PNorm = 34.1101, GNorm = 8.4209, lr_0 = 1.0202e-04
Loss = 5.9010e-03, PNorm = 34.1133, GNorm = 4.6929, lr_0 = 1.0202e-04
Loss = 5.8978e-03, PNorm = 34.1157, GNorm = 1.8397, lr_0 = 1.0202e-04
Loss = 4.8629e-03, PNorm = 34.1193, GNorm = 12.3852, lr_0 = 1.0202e-04
Loss = 5.2887e-03, PNorm = 34.1220, GNorm = 3.0351, lr_0 = 1.0202e-04
Loss = 5.6460e-03, PNorm = 34.1251, GNorm = 10.5021, lr_0 = 1.0202e-04
Loss = 5.0585e-03, PNorm = 34.1283, GNorm = 14.9942, lr_0 = 1.0202e-04
Loss = 5.2266e-03, PNorm = 34.1316, GNorm = 3.9659, lr_0 = 1.0202e-04
Loss = 5.1374e-03, PNorm = 34.1346, GNorm = 7.2599, lr_0 = 1.0202e-04
Loss = 4.2866e-03, PNorm = 34.1371, GNorm = 2.4246, lr_0 = 1.0202e-04
Loss = 5.0471e-03, PNorm = 34.1391, GNorm = 5.6410, lr_0 = 1.0202e-04
Loss = 6.4947e-03, PNorm = 34.1414, GNorm = 28.0426, lr_0 = 1.0202e-04
Loss = 5.4595e-03, PNorm = 34.1438, GNorm = 17.6084, lr_0 = 1.0202e-04
Validation rmse logP = 0.952799
Validation R2 logP = 0.734029
Epoch 2
Train function
Loss = 4.0591e-03, PNorm = 34.1465, GNorm = 13.4926, lr_0 = 1.0202e-04
Loss = 4.9461e-03, PNorm = 34.1483, GNorm = 4.3153, lr_0 = 1.0202e-04
Loss = 6.4834e-03, PNorm = 34.1514, GNorm = 29.8196, lr_0 = 1.0202e-04
Loss = 5.1567e-03, PNorm = 34.1536, GNorm = 24.0175, lr_0 = 1.0202e-04
Loss = 5.2143e-03, PNorm = 34.1559, GNorm = 21.5289, lr_0 = 1.0202e-04
Loss = 4.2826e-03, PNorm = 34.1586, GNorm = 12.5335, lr_0 = 1.0202e-04
Loss = 4.2552e-03, PNorm = 34.1610, GNorm = 13.6019, lr_0 = 1.0202e-04
Loss = 4.2537e-03, PNorm = 34.1627, GNorm = 5.7467, lr_0 = 1.0202e-04
Loss = 4.2979e-03, PNorm = 34.1655, GNorm = 13.7221, lr_0 = 1.0202e-04
Loss = 4.4376e-03, PNorm = 34.1680, GNorm = 5.1456, lr_0 = 1.0202e-04
Loss = 4.6663e-03, PNorm = 34.1706, GNorm = 7.2569, lr_0 = 1.0202e-04
Loss = 4.8437e-03, PNorm = 34.1735, GNorm = 5.3926, lr_0 = 1.0202e-04
Loss = 4.3787e-03, PNorm = 34.1757, GNorm = 6.9339, lr_0 = 1.0202e-04
Loss = 4.5440e-03, PNorm = 34.1789, GNorm = 9.1141, lr_0 = 1.0202e-04
Loss = 4.2564e-03, PNorm = 34.1817, GNorm = 9.2156, lr_0 = 1.0202e-04
Loss = 5.0423e-03, PNorm = 34.1844, GNorm = 4.5616, lr_0 = 1.0202e-04
Loss = 4.7976e-03, PNorm = 34.1875, GNorm = 19.3830, lr_0 = 1.0202e-04
Loss = 5.0343e-03, PNorm = 34.1903, GNorm = 20.0355, lr_0 = 1.0202e-04
Loss = 4.3390e-03, PNorm = 34.1936, GNorm = 11.6235, lr_0 = 1.0202e-04
Loss = 4.2202e-03, PNorm = 34.1969, GNorm = 4.4785, lr_0 = 1.0202e-04
Loss = 4.4324e-03, PNorm = 34.1993, GNorm = 17.3218, lr_0 = 1.0202e-04
Loss = 3.6087e-03, PNorm = 34.2018, GNorm = 3.5564, lr_0 = 1.0202e-04
Loss = 3.8527e-03, PNorm = 34.2034, GNorm = 6.2614, lr_0 = 1.0202e-04
Loss = 8.2009e-03, PNorm = 34.2036, GNorm = 13.2461, lr_0 = 1.0202e-04
Validation rmse logP = 0.797979
Validation R2 logP = 0.813441
Epoch 3
Train function
Loss = 3.7717e-03, PNorm = 34.2059, GNorm = 19.5343, lr_0 = 1.0202e-04
Loss = 3.2107e-03, PNorm = 34.2082, GNorm = 8.0051, lr_0 = 1.0202e-04
Loss = 3.7141e-03, PNorm = 34.2105, GNorm = 10.1286, lr_0 = 1.0202e-04
Loss = 3.8431e-03, PNorm = 34.2130, GNorm = 6.5624, lr_0 = 1.0202e-04
Loss = 3.7374e-03, PNorm = 34.2151, GNorm = 7.1734, lr_0 = 1.0202e-04
Loss = 3.5630e-03, PNorm = 34.2173, GNorm = 9.4605, lr_0 = 1.0202e-04
Loss = 3.7486e-03, PNorm = 34.2191, GNorm = 10.6165, lr_0 = 1.0202e-04
Loss = 3.7997e-03, PNorm = 34.2213, GNorm = 15.5627, lr_0 = 1.0202e-04
Loss = 4.4427e-03, PNorm = 34.2242, GNorm = 9.7345, lr_0 = 1.0202e-04
Loss = 3.9561e-03, PNorm = 34.2272, GNorm = 10.3794, lr_0 = 1.0202e-04
Loss = 3.5046e-03, PNorm = 34.2302, GNorm = 6.0206, lr_0 = 1.0202e-04
Loss = 3.5212e-03, PNorm = 34.2330, GNorm = 2.5651, lr_0 = 1.0202e-04
Loss = 3.7255e-03, PNorm = 34.2354, GNorm = 5.9512, lr_0 = 1.0202e-04
Loss = 3.9815e-03, PNorm = 34.2385, GNorm = 3.1020, lr_0 = 1.0202e-04
Loss = 3.2432e-03, PNorm = 34.2406, GNorm = 2.8622, lr_0 = 1.0202e-04
Loss = 3.6389e-03, PNorm = 34.2430, GNorm = 3.2545, lr_0 = 1.0202e-04
Loss = 3.2603e-03, PNorm = 34.2456, GNorm = 4.0075, lr_0 = 1.0202e-04
Loss = 3.1388e-03, PNorm = 34.2479, GNorm = 6.0379, lr_0 = 1.0202e-04
Loss = 3.0270e-03, PNorm = 34.2503, GNorm = 11.9728, lr_0 = 1.0202e-04
Loss = 3.4843e-03, PNorm = 34.2532, GNorm = 14.8658, lr_0 = 1.0202e-04
Loss = 3.8106e-03, PNorm = 34.2560, GNorm = 6.3226, lr_0 = 1.0202e-04
Loss = 3.8647e-03, PNorm = 34.2590, GNorm = 12.4843, lr_0 = 1.0202e-04
Validation rmse logP = 0.756336
Validation R2 logP = 0.832405
Epoch 4
Train function
Loss = 3.2865e-03, PNorm = 34.2618, GNorm = 7.3038, lr_0 = 1.0202e-04
Loss = 3.1658e-03, PNorm = 34.2641, GNorm = 7.3418, lr_0 = 1.0202e-04
Loss = 3.9188e-03, PNorm = 34.2664, GNorm = 10.2681, lr_0 = 1.0202e-04
Loss = 4.0255e-03, PNorm = 34.2695, GNorm = 4.8124, lr_0 = 1.0202e-04
Loss = 3.5399e-03, PNorm = 34.2724, GNorm = 9.8008, lr_0 = 1.0202e-04
Loss = 3.2681e-03, PNorm = 34.2760, GNorm = 22.9998, lr_0 = 1.0202e-04
Loss = 3.2963e-03, PNorm = 34.2782, GNorm = 16.3140, lr_0 = 1.0202e-04
Loss = 3.8480e-03, PNorm = 34.2812, GNorm = 2.8546, lr_0 = 1.0202e-04
Loss = 3.6048e-03, PNorm = 34.2837, GNorm = 4.5581, lr_0 = 1.0202e-04
Loss = 3.2755e-03, PNorm = 34.2857, GNorm = 12.1508, lr_0 = 1.0202e-04
Loss = 2.8672e-03, PNorm = 34.2872, GNorm = 7.1904, lr_0 = 1.0202e-04
Loss = 2.8840e-03, PNorm = 34.2893, GNorm = 2.7303, lr_0 = 1.0202e-04
Loss = 3.5341e-03, PNorm = 34.2917, GNorm = 6.9984, lr_0 = 1.0202e-04
Loss = 3.1694e-03, PNorm = 34.2939, GNorm = 12.7283, lr_0 = 1.0202e-04
Loss = 3.5548e-03, PNorm = 34.2968, GNorm = 2.2534, lr_0 = 1.0202e-04
Loss = 2.7982e-03, PNorm = 34.3002, GNorm = 9.7782, lr_0 = 1.0202e-04
Loss = 2.7344e-03, PNorm = 34.3030, GNorm = 5.0989, lr_0 = 1.0202e-04
Loss = 3.4206e-03, PNorm = 34.3053, GNorm = 9.7466, lr_0 = 1.0202e-04
Loss = 2.9429e-03, PNorm = 34.3078, GNorm = 22.8522, lr_0 = 1.0202e-04
Loss = 3.3368e-03, PNorm = 34.3097, GNorm = 9.7237, lr_0 = 1.0202e-04
Loss = 2.5483e-03, PNorm = 34.3117, GNorm = 2.9441, lr_0 = 1.0202e-04
Loss = 3.0391e-03, PNorm = 34.3132, GNorm = 14.6314, lr_0 = 1.0202e-04
Validation rmse logP = 0.697964
Validation R2 logP = 0.857276
Epoch 5
Train function
Loss = 3.1989e-03, PNorm = 34.3152, GNorm = 11.2701, lr_0 = 1.0202e-04
Loss = 2.7344e-03, PNorm = 34.3178, GNorm = 9.3287, lr_0 = 1.0202e-04
Loss = 2.6354e-03, PNorm = 34.3194, GNorm = 5.9615, lr_0 = 1.0202e-04
Loss = 2.9003e-03, PNorm = 34.3205, GNorm = 7.0320, lr_0 = 1.0202e-04
Loss = 3.3162e-03, PNorm = 34.3232, GNorm = 7.1071, lr_0 = 1.0202e-04
Loss = 3.0543e-03, PNorm = 34.3253, GNorm = 17.7850, lr_0 = 1.0202e-04
Loss = 3.2978e-03, PNorm = 34.3280, GNorm = 3.4564, lr_0 = 1.0202e-04
Loss = 2.2399e-03, PNorm = 34.3304, GNorm = 1.9615, lr_0 = 1.0202e-04
Loss = 3.0849e-03, PNorm = 34.3325, GNorm = 6.3568, lr_0 = 1.0202e-04
Loss = 3.2551e-03, PNorm = 34.3354, GNorm = 3.9055, lr_0 = 1.0202e-04
Loss = 2.6285e-03, PNorm = 34.3379, GNorm = 11.1965, lr_0 = 1.0202e-04
Loss = 3.1084e-03, PNorm = 34.3416, GNorm = 12.3188, lr_0 = 1.0202e-04
Loss = 2.9397e-03, PNorm = 34.3446, GNorm = 3.6968, lr_0 = 1.0202e-04
Loss = 3.0876e-03, PNorm = 34.3462, GNorm = 11.4447, lr_0 = 1.0202e-04
Loss = 3.3324e-03, PNorm = 34.3489, GNorm = 12.5428, lr_0 = 1.0202e-04
Loss = 2.5103e-03, PNorm = 34.3514, GNorm = 5.4814, lr_0 = 1.0202e-04
Loss = 3.0134e-03, PNorm = 34.3530, GNorm = 8.6215, lr_0 = 1.0202e-04
Loss = 2.7628e-03, PNorm = 34.3558, GNorm = 8.9114, lr_0 = 1.0202e-04
Loss = 2.7533e-03, PNorm = 34.3575, GNorm = 11.3383, lr_0 = 1.0202e-04
Loss = 3.1697e-03, PNorm = 34.3592, GNorm = 2.8043, lr_0 = 1.0202e-04
Loss = 3.1156e-03, PNorm = 34.3614, GNorm = 12.7698, lr_0 = 1.0202e-04
Loss = 2.6227e-03, PNorm = 34.3635, GNorm = 5.3330, lr_0 = 1.0202e-04
Loss = 2.8897e-03, PNorm = 34.3649, GNorm = 15.4464, lr_0 = 1.0202e-04
Validation rmse logP = 0.682541
Validation R2 logP = 0.863514
Epoch 6
Train function
Loss = 2.9852e-03, PNorm = 34.3661, GNorm = 10.0019, lr_0 = 1.0202e-04
Loss = 3.0579e-03, PNorm = 34.3680, GNorm = 14.2912, lr_0 = 1.0202e-04
Loss = 3.0250e-03, PNorm = 34.3704, GNorm = 8.0587, lr_0 = 1.0202e-04
Loss = 3.2706e-03, PNorm = 34.3731, GNorm = 14.6280, lr_0 = 1.0202e-04
Loss = 2.7077e-03, PNorm = 34.3753, GNorm = 3.4745, lr_0 = 1.0202e-04
Loss = 3.4054e-03, PNorm = 34.3774, GNorm = 19.4799, lr_0 = 1.0202e-04
Loss = 3.3471e-03, PNorm = 34.3799, GNorm = 21.2980, lr_0 = 1.0202e-04
Loss = 3.3071e-03, PNorm = 34.3820, GNorm = 9.4480, lr_0 = 1.0202e-04
Loss = 3.2846e-03, PNorm = 34.3847, GNorm = 7.5348, lr_0 = 1.0202e-04
Loss = 2.4097e-03, PNorm = 34.3866, GNorm = 5.4242, lr_0 = 1.0202e-04
Loss = 3.4583e-03, PNorm = 34.3888, GNorm = 7.5994, lr_0 = 1.0202e-04
Loss = 3.3416e-03, PNorm = 34.3906, GNorm = 9.8175, lr_0 = 1.0202e-04
Loss = 2.7860e-03, PNorm = 34.3943, GNorm = 1.5419, lr_0 = 1.0202e-04
Loss = 2.5718e-03, PNorm = 34.3971, GNorm = 2.5425, lr_0 = 1.0202e-04
Loss = 3.1053e-03, PNorm = 34.3999, GNorm = 5.1989, lr_0 = 1.0202e-04
Loss = 2.7548e-03, PNorm = 34.4021, GNorm = 9.3951, lr_0 = 1.0202e-04
Loss = 2.6500e-03, PNorm = 34.4040, GNorm = 5.1972, lr_0 = 1.0202e-04
Loss = 3.2142e-03, PNorm = 34.4064, GNorm = 19.7762, lr_0 = 1.0202e-04
Loss = 2.3738e-03, PNorm = 34.4091, GNorm = 2.0508, lr_0 = 1.0202e-04
Loss = 2.5479e-03, PNorm = 34.4109, GNorm = 8.4160, lr_0 = 1.0202e-04
Loss = 3.0408e-03, PNorm = 34.4123, GNorm = 11.2221, lr_0 = 1.0202e-04
Loss = 2.7426e-03, PNorm = 34.4139, GNorm = 16.9465, lr_0 = 1.0202e-04
Validation rmse logP = 0.691108
Validation R2 logP = 0.860065
Epoch 7
Train function
Loss = 2.2515e-03, PNorm = 34.4155, GNorm = 5.9209, lr_0 = 1.0202e-04
Loss = 2.6032e-03, PNorm = 34.4179, GNorm = 6.8838, lr_0 = 1.0202e-04
Loss = 2.6855e-03, PNorm = 34.4203, GNorm = 2.1573, lr_0 = 1.0202e-04
Loss = 2.7388e-03, PNorm = 34.4224, GNorm = 2.8488, lr_0 = 1.0202e-04
Loss = 2.6798e-03, PNorm = 34.4246, GNorm = 21.6886, lr_0 = 1.0202e-04
Loss = 2.7997e-03, PNorm = 34.4268, GNorm = 6.3319, lr_0 = 1.0202e-04
Loss = 2.5188e-03, PNorm = 34.4292, GNorm = 11.0373, lr_0 = 1.0202e-04
Loss = 1.9722e-03, PNorm = 34.4319, GNorm = 9.4748, lr_0 = 1.0202e-04
Loss = 2.4370e-03, PNorm = 34.4343, GNorm = 8.7261, lr_0 = 1.0202e-04
Loss = 3.2064e-03, PNorm = 34.4351, GNorm = 6.8155, lr_0 = 1.0202e-04
Loss = 2.8827e-03, PNorm = 34.4367, GNorm = 5.6518, lr_0 = 1.0202e-04
Loss = 2.2967e-03, PNorm = 34.4391, GNorm = 5.4470, lr_0 = 1.0202e-04
Loss = 2.4570e-03, PNorm = 34.4411, GNorm = 7.0090, lr_0 = 1.0202e-04
Loss = 2.6633e-03, PNorm = 34.4436, GNorm = 7.4419, lr_0 = 1.0202e-04
Loss = 2.6556e-03, PNorm = 34.4464, GNorm = 2.4037, lr_0 = 1.0202e-04
Loss = 2.1511e-03, PNorm = 34.4486, GNorm = 5.6395, lr_0 = 1.0202e-04
Loss = 2.3855e-03, PNorm = 34.4513, GNorm = 6.4019, lr_0 = 1.0202e-04
Loss = 2.3963e-03, PNorm = 34.4546, GNorm = 3.9489, lr_0 = 1.0202e-04
Loss = 2.6873e-03, PNorm = 34.4567, GNorm = 3.7251, lr_0 = 1.0202e-04
Loss = 2.6219e-03, PNorm = 34.4588, GNorm = 15.6698, lr_0 = 1.0202e-04
Loss = 2.3101e-03, PNorm = 34.4604, GNorm = 7.2156, lr_0 = 1.0202e-04
Loss = 2.4426e-03, PNorm = 34.4614, GNorm = 8.4564, lr_0 = 1.0202e-04
Validation rmse logP = 0.651569
Validation R2 logP = 0.875619
Epoch 8
Train function
Loss = 3.1666e-03, PNorm = 34.4620, GNorm = 4.9646, lr_0 = 1.0202e-04
Loss = 3.0805e-03, PNorm = 34.4639, GNorm = 1.8045, lr_0 = 1.0202e-04
Loss = 2.1892e-03, PNorm = 34.4661, GNorm = 9.9123, lr_0 = 1.0202e-04
Loss = 2.5836e-03, PNorm = 34.4683, GNorm = 5.2240, lr_0 = 1.0202e-04
Loss = 3.1116e-03, PNorm = 34.4701, GNorm = 7.2207, lr_0 = 1.0202e-04
Loss = 3.0615e-03, PNorm = 34.4715, GNorm = 7.3988, lr_0 = 1.0202e-04
Loss = 2.9343e-03, PNorm = 34.4738, GNorm = 5.3676, lr_0 = 1.0202e-04
Loss = 2.6004e-03, PNorm = 34.4758, GNorm = 6.9349, lr_0 = 1.0202e-04
Loss = 2.3206e-03, PNorm = 34.4776, GNorm = 14.7550, lr_0 = 1.0202e-04
Loss = 2.4096e-03, PNorm = 34.4794, GNorm = 8.5342, lr_0 = 1.0202e-04
Loss = 2.5810e-03, PNorm = 34.4809, GNorm = 2.4157, lr_0 = 1.0202e-04
Loss = 2.6923e-03, PNorm = 34.4830, GNorm = 1.9639, lr_0 = 1.0202e-04
Loss = 2.4860e-03, PNorm = 34.4850, GNorm = 14.5644, lr_0 = 1.0202e-04
Loss = 2.2741e-03, PNorm = 34.4868, GNorm = 9.1844, lr_0 = 1.0202e-04
Loss = 2.2137e-03, PNorm = 34.4888, GNorm = 9.2757, lr_0 = 1.0202e-04
Loss = 2.4170e-03, PNorm = 34.4911, GNorm = 1.7999, lr_0 = 1.0202e-04
Loss = 2.4016e-03, PNorm = 34.4932, GNorm = 4.8762, lr_0 = 1.0202e-04
Loss = 2.4143e-03, PNorm = 34.4958, GNorm = 12.0987, lr_0 = 1.0202e-04
Loss = 2.1553e-03, PNorm = 34.4977, GNorm = 4.5991, lr_0 = 1.0202e-04
Loss = 2.0353e-03, PNorm = 34.5003, GNorm = 6.1554, lr_0 = 1.0202e-04
Loss = 2.1710e-03, PNorm = 34.5028, GNorm = 7.9403, lr_0 = 1.0202e-04
Loss = 2.0286e-03, PNorm = 34.5042, GNorm = 12.7519, lr_0 = 1.0202e-04
Loss = 2.6676e-03, PNorm = 34.5059, GNorm = 2.6337, lr_0 = 1.0202e-04
Validation rmse logP = 0.635728
Validation R2 logP = 0.881594
Epoch 9
Train function
Loss = 1.9912e-03, PNorm = 34.5075, GNorm = 10.1412, lr_0 = 1.0202e-04
Loss = 1.8957e-03, PNorm = 34.5091, GNorm = 6.1231, lr_0 = 1.0202e-04
Loss = 2.4864e-03, PNorm = 34.5112, GNorm = 5.7706, lr_0 = 1.0202e-04
Loss = 2.3700e-03, PNorm = 34.5138, GNorm = 6.8004, lr_0 = 1.0202e-04
Loss = 2.3631e-03, PNorm = 34.5159, GNorm = 4.3818, lr_0 = 1.0202e-04
Loss = 2.3281e-03, PNorm = 34.5182, GNorm = 6.5017, lr_0 = 1.0202e-04
Loss = 2.1021e-03, PNorm = 34.5206, GNorm = 13.1937, lr_0 = 1.0202e-04
Loss = 2.3028e-03, PNorm = 34.5229, GNorm = 3.3260, lr_0 = 1.0202e-04
Loss = 2.4047e-03, PNorm = 34.5250, GNorm = 4.0315, lr_0 = 1.0202e-04
Loss = 2.0701e-03, PNorm = 34.5262, GNorm = 1.9092, lr_0 = 1.0202e-04
Loss = 2.1181e-03, PNorm = 34.5277, GNorm = 9.0149, lr_0 = 1.0202e-04
Loss = 2.6042e-03, PNorm = 34.5289, GNorm = 21.7067, lr_0 = 1.0202e-04
Loss = 2.6440e-03, PNorm = 34.5308, GNorm = 24.8861, lr_0 = 1.0202e-04
Loss = 2.3531e-03, PNorm = 34.5323, GNorm = 21.0915, lr_0 = 1.0202e-04
Loss = 2.7788e-03, PNorm = 34.5349, GNorm = 7.7533, lr_0 = 1.0202e-04
Loss = 2.3434e-03, PNorm = 34.5367, GNorm = 4.1132, lr_0 = 1.0202e-04
Loss = 2.9164e-03, PNorm = 34.5384, GNorm = 6.7316, lr_0 = 1.0202e-04
Loss = 2.2281e-03, PNorm = 34.5407, GNorm = 1.8412, lr_0 = 1.0202e-04
Loss = 1.9231e-03, PNorm = 34.5427, GNorm = 4.1811, lr_0 = 1.0202e-04
Loss = 2.2444e-03, PNorm = 34.5452, GNorm = 13.2182, lr_0 = 1.0202e-04
Loss = 2.9547e-03, PNorm = 34.5474, GNorm = 6.9509, lr_0 = 1.0202e-04
Loss = 1.9817e-03, PNorm = 34.5492, GNorm = 9.1436, lr_0 = 1.0202e-04
Validation rmse logP = 0.698034
Validation R2 logP = 0.857247
Epoch 10
Train function
Loss = 2.2539e-03, PNorm = 34.5509, GNorm = 13.9033, lr_0 = 1.0202e-04
Loss = 2.6022e-03, PNorm = 34.5529, GNorm = 21.8443, lr_0 = 1.0202e-04
Loss = 2.4055e-03, PNorm = 34.5543, GNorm = 15.4362, lr_0 = 1.0202e-04
Loss = 2.3282e-03, PNorm = 34.5562, GNorm = 8.5713, lr_0 = 1.0202e-04
Loss = 2.2453e-03, PNorm = 34.5583, GNorm = 2.8780, lr_0 = 1.0202e-04
Loss = 1.9133e-03, PNorm = 34.5599, GNorm = 7.8898, lr_0 = 1.0202e-04
Loss = 1.9005e-03, PNorm = 34.5616, GNorm = 2.7575, lr_0 = 1.0202e-04
Loss = 2.0195e-03, PNorm = 34.5630, GNorm = 3.6265, lr_0 = 1.0202e-04
Loss = 2.3060e-03, PNorm = 34.5646, GNorm = 8.5349, lr_0 = 1.0202e-04
Loss = 1.9424e-03, PNorm = 34.5667, GNorm = 3.6374, lr_0 = 1.0202e-04
Loss = 2.1162e-03, PNorm = 34.5687, GNorm = 8.2113, lr_0 = 1.0202e-04
Loss = 2.0303e-03, PNorm = 34.5708, GNorm = 4.8846, lr_0 = 1.0202e-04
Loss = 2.1683e-03, PNorm = 34.5732, GNorm = 5.8664, lr_0 = 1.0202e-04
Loss = 2.2538e-03, PNorm = 34.5749, GNorm = 2.5460, lr_0 = 1.0202e-04
Loss = 2.3971e-03, PNorm = 34.5761, GNorm = 15.1854, lr_0 = 1.0202e-04
Loss = 2.0572e-03, PNorm = 34.5774, GNorm = 2.7524, lr_0 = 1.0202e-04
Loss = 2.0556e-03, PNorm = 34.5788, GNorm = 15.9419, lr_0 = 1.0202e-04
Loss = 2.0441e-03, PNorm = 34.5808, GNorm = 10.5023, lr_0 = 1.0202e-04
Loss = 1.8072e-03, PNorm = 34.5830, GNorm = 6.1022, lr_0 = 1.0202e-04
Loss = 1.9734e-03, PNorm = 34.5853, GNorm = 6.2333, lr_0 = 1.0202e-04
Loss = 1.9241e-03, PNorm = 34.5867, GNorm = 2.2894, lr_0 = 1.0202e-04
Loss = 1.9727e-03, PNorm = 34.5884, GNorm = 2.0201, lr_0 = 1.0202e-04
Validation rmse logP = 0.648227
Validation R2 logP = 0.876892
Epoch 11
Train function
Loss = 2.7758e-03, PNorm = 34.5913, GNorm = 10.0083, lr_0 = 1.0202e-04
Loss = 1.9846e-03, PNorm = 34.5929, GNorm = 3.0146, lr_0 = 1.0202e-04
Loss = 2.1518e-03, PNorm = 34.5953, GNorm = 9.4307, lr_0 = 1.0202e-04
Loss = 1.8596e-03, PNorm = 34.5974, GNorm = 8.7576, lr_0 = 1.0202e-04
Loss = 2.0212e-03, PNorm = 34.5993, GNorm = 10.7892, lr_0 = 1.0202e-04
Loss = 2.3654e-03, PNorm = 34.6008, GNorm = 3.4196, lr_0 = 1.0202e-04
Loss = 1.8296e-03, PNorm = 34.6032, GNorm = 11.7694, lr_0 = 1.0202e-04
Loss = 2.1366e-03, PNorm = 34.6052, GNorm = 14.4856, lr_0 = 1.0202e-04
Loss = 2.3706e-03, PNorm = 34.6073, GNorm = 4.6136, lr_0 = 1.0202e-04
Loss = 2.5413e-03, PNorm = 34.6092, GNorm = 4.2518, lr_0 = 1.0202e-04
Loss = 1.6437e-03, PNorm = 34.6109, GNorm = 11.6936, lr_0 = 1.0202e-04
Loss = 2.0205e-03, PNorm = 34.6127, GNorm = 9.1068, lr_0 = 1.0202e-04
Loss = 2.6145e-03, PNorm = 34.6135, GNorm = 5.2516, lr_0 = 1.0202e-04
Loss = 2.2495e-03, PNorm = 34.6151, GNorm = 11.9134, lr_0 = 1.0202e-04
Loss = 1.6727e-03, PNorm = 34.6162, GNorm = 7.2560, lr_0 = 1.0202e-04
Loss = 2.0825e-03, PNorm = 34.6188, GNorm = 2.7044, lr_0 = 1.0202e-04
Loss = 2.5131e-03, PNorm = 34.6199, GNorm = 5.1918, lr_0 = 1.0202e-04
Loss = 2.3716e-03, PNorm = 34.6215, GNorm = 2.3169, lr_0 = 1.0202e-04
Loss = 2.0519e-03, PNorm = 34.6233, GNorm = 3.5205, lr_0 = 1.0202e-04
Loss = 2.0851e-03, PNorm = 34.6253, GNorm = 2.0271, lr_0 = 1.0202e-04
Loss = 1.9647e-03, PNorm = 34.6272, GNorm = 7.8722, lr_0 = 1.0202e-04
Loss = 1.8638e-03, PNorm = 34.6287, GNorm = 14.6014, lr_0 = 1.0202e-04
Loss = 2.3684e-03, PNorm = 34.6315, GNorm = 11.7314, lr_0 = 1.0202e-04
Validation rmse logP = 0.668902
Validation R2 logP = 0.868913
Epoch 12
Train function
Loss = 2.2304e-03, PNorm = 34.6333, GNorm = 10.8768, lr_0 = 1.0202e-04
Loss = 2.1407e-03, PNorm = 34.6361, GNorm = 11.8890, lr_0 = 1.0202e-04
Loss = 1.7447e-03, PNorm = 34.6381, GNorm = 3.7508, lr_0 = 1.0202e-04
Loss = 2.1368e-03, PNorm = 34.6406, GNorm = 2.6977, lr_0 = 1.0202e-04
Loss = 2.3562e-03, PNorm = 34.6430, GNorm = 11.1073, lr_0 = 1.0202e-04
Loss = 1.8079e-03, PNorm = 34.6444, GNorm = 2.2668, lr_0 = 1.0202e-04
Loss = 1.8601e-03, PNorm = 34.6466, GNorm = 2.4574, lr_0 = 1.0202e-04
Loss = 2.2749e-03, PNorm = 34.6483, GNorm = 3.6925, lr_0 = 1.0202e-04
Loss = 2.0241e-03, PNorm = 34.6503, GNorm = 4.8700, lr_0 = 1.0202e-04
Loss = 2.1762e-03, PNorm = 34.6520, GNorm = 18.2637, lr_0 = 1.0202e-04
Loss = 1.8496e-03, PNorm = 34.6533, GNorm = 8.9301, lr_0 = 1.0202e-04
Loss = 2.2133e-03, PNorm = 34.6553, GNorm = 3.1932, lr_0 = 1.0202e-04
Loss = 1.7783e-03, PNorm = 34.6577, GNorm = 2.4422, lr_0 = 1.0202e-04
Loss = 1.8751e-03, PNorm = 34.6599, GNorm = 7.4344, lr_0 = 1.0202e-04
Loss = 1.7302e-03, PNorm = 34.6614, GNorm = 8.5270, lr_0 = 1.0202e-04
Loss = 2.1009e-03, PNorm = 34.6627, GNorm = 2.5352, lr_0 = 1.0202e-04
Loss = 2.0782e-03, PNorm = 34.6645, GNorm = 5.4619, lr_0 = 1.0202e-04
Loss = 1.8450e-03, PNorm = 34.6660, GNorm = 2.0707, lr_0 = 1.0202e-04
Loss = 1.8031e-03, PNorm = 34.6682, GNorm = 3.1014, lr_0 = 1.0202e-04
Loss = 2.2399e-03, PNorm = 34.6702, GNorm = 12.0752, lr_0 = 1.0202e-04
Loss = 2.1646e-03, PNorm = 34.6717, GNorm = 3.2188, lr_0 = 1.0202e-04
Loss = 1.8274e-03, PNorm = 34.6731, GNorm = 6.6491, lr_0 = 1.0202e-04
Validation rmse logP = 0.683463
Validation R2 logP = 0.863145
Epoch 13
Train function
Loss = 2.5113e-03, PNorm = 34.6747, GNorm = 17.8675, lr_0 = 1.0202e-04
Loss = 2.1347e-03, PNorm = 34.6767, GNorm = 8.8378, lr_0 = 1.0202e-04
Loss = 2.3977e-03, PNorm = 34.6792, GNorm = 13.1585, lr_0 = 1.0202e-04
Loss = 1.6043e-03, PNorm = 34.6815, GNorm = 2.4968, lr_0 = 1.0202e-04
Loss = 1.6647e-03, PNorm = 34.6834, GNorm = 5.4518, lr_0 = 1.0202e-04
Loss = 1.6781e-03, PNorm = 34.6848, GNorm = 2.2426, lr_0 = 1.0202e-04
Loss = 1.7890e-03, PNorm = 34.6867, GNorm = 3.0335, lr_0 = 1.0202e-04
Loss = 1.9869e-03, PNorm = 34.6887, GNorm = 7.0111, lr_0 = 1.0202e-04
Loss = 1.6098e-03, PNorm = 34.6902, GNorm = 4.5938, lr_0 = 1.0202e-04
Loss = 2.2426e-03, PNorm = 34.6918, GNorm = 2.4871, lr_0 = 1.0202e-04
Loss = 1.5574e-03, PNorm = 34.6934, GNorm = 6.2523, lr_0 = 1.0202e-04
Loss = 1.7738e-03, PNorm = 34.6954, GNorm = 14.2954, lr_0 = 1.0202e-04
Loss = 2.0543e-03, PNorm = 34.6968, GNorm = 6.4621, lr_0 = 1.0202e-04
Loss = 1.8122e-03, PNorm = 34.6992, GNorm = 2.5775, lr_0 = 1.0202e-04
Loss = 1.8410e-03, PNorm = 34.7013, GNorm = 2.2227, lr_0 = 1.0202e-04
Loss = 2.0147e-03, PNorm = 34.7027, GNorm = 18.1662, lr_0 = 1.0202e-04
Loss = 2.1624e-03, PNorm = 34.7041, GNorm = 16.5033, lr_0 = 1.0202e-04
Loss = 2.3295e-03, PNorm = 34.7057, GNorm = 7.4862, lr_0 = 1.0202e-04
Loss = 1.9151e-03, PNorm = 34.7075, GNorm = 7.0291, lr_0 = 1.0202e-04
Loss = 2.3262e-03, PNorm = 34.7094, GNorm = 4.2976, lr_0 = 1.0202e-04
Loss = 1.9630e-03, PNorm = 34.7121, GNorm = 4.5897, lr_0 = 1.0202e-04
Loss = 1.7967e-03, PNorm = 34.7144, GNorm = 2.2145, lr_0 = 1.0202e-04
Validation rmse logP = 0.604799
Validation R2 logP = 0.892835
Epoch 14
Train function
Loss = 1.4707e-03, PNorm = 34.7159, GNorm = 4.1474, lr_0 = 1.0202e-04
Loss = 2.1726e-03, PNorm = 34.7184, GNorm = 7.6761, lr_0 = 1.0202e-04
Loss = 1.8393e-03, PNorm = 34.7211, GNorm = 4.0447, lr_0 = 1.0202e-04
Loss = 1.9051e-03, PNorm = 34.7237, GNorm = 10.3174, lr_0 = 1.0202e-04
Loss = 2.2780e-03, PNorm = 34.7258, GNorm = 6.7663, lr_0 = 1.0202e-04
Loss = 1.9445e-03, PNorm = 34.7272, GNorm = 15.8815, lr_0 = 1.0202e-04
Loss = 1.8845e-03, PNorm = 34.7289, GNorm = 8.0203, lr_0 = 1.0202e-04
Loss = 1.4340e-03, PNorm = 34.7302, GNorm = 8.5415, lr_0 = 1.0202e-04
Loss = 1.7589e-03, PNorm = 34.7312, GNorm = 5.1615, lr_0 = 1.0202e-04
Loss = 1.8304e-03, PNorm = 34.7328, GNorm = 14.4622, lr_0 = 1.0202e-04
Loss = 2.1924e-03, PNorm = 34.7345, GNorm = 10.8440, lr_0 = 1.0202e-04
Loss = 2.1751e-03, PNorm = 34.7358, GNorm = 7.6006, lr_0 = 1.0202e-04
Loss = 1.5570e-03, PNorm = 34.7377, GNorm = 6.0366, lr_0 = 1.0202e-04
Loss = 2.2767e-03, PNorm = 34.7402, GNorm = 7.9395, lr_0 = 1.0202e-04
Loss = 1.7418e-03, PNorm = 34.7433, GNorm = 8.8371, lr_0 = 1.0202e-04
Loss = 1.4400e-03, PNorm = 34.7446, GNorm = 3.2600, lr_0 = 1.0202e-04
Loss = 1.8967e-03, PNorm = 34.7460, GNorm = 4.4806, lr_0 = 1.0202e-04
Loss = 2.2718e-03, PNorm = 34.7482, GNorm = 11.6909, lr_0 = 1.0202e-04
Loss = 2.0914e-03, PNorm = 34.7499, GNorm = 3.9399, lr_0 = 1.0202e-04
Loss = 1.6264e-03, PNorm = 34.7522, GNorm = 3.1621, lr_0 = 1.0202e-04
Loss = 1.6495e-03, PNorm = 34.7534, GNorm = 15.5212, lr_0 = 1.0202e-04
Loss = 2.0572e-03, PNorm = 34.7549, GNorm = 14.0834, lr_0 = 1.0202e-04
Loss = 2.4299e-03, PNorm = 34.7563, GNorm = 21.3157, lr_0 = 1.0202e-04
Validation rmse logP = 0.728515
Validation R2 logP = 0.844508
Epoch 15
Train function
Loss = 2.2787e-03, PNorm = 34.7586, GNorm = 2.7803, lr_0 = 1.0202e-04
Loss = 2.4307e-03, PNorm = 34.7610, GNorm = 8.9113, lr_0 = 1.0202e-04
Loss = 1.7426e-03, PNorm = 34.7622, GNorm = 3.3662, lr_0 = 1.0202e-04
Loss = 1.4560e-03, PNorm = 34.7635, GNorm = 3.0934, lr_0 = 1.0202e-04
Loss = 1.7517e-03, PNorm = 34.7656, GNorm = 2.7319, lr_0 = 1.0202e-04
Loss = 1.9035e-03, PNorm = 34.7676, GNorm = 5.3989, lr_0 = 1.0202e-04
Loss = 1.9358e-03, PNorm = 34.7701, GNorm = 1.9884, lr_0 = 1.0202e-04
Loss = 1.7012e-03, PNorm = 34.7727, GNorm = 16.1676, lr_0 = 1.0202e-04
Loss = 1.7995e-03, PNorm = 34.7753, GNorm = 3.8468, lr_0 = 1.0202e-04
Loss = 1.9722e-03, PNorm = 34.7785, GNorm = 7.0851, lr_0 = 1.0202e-04
Loss = 1.9690e-03, PNorm = 34.7799, GNorm = 10.4136, lr_0 = 1.0202e-04
Loss = 2.0345e-03, PNorm = 34.7822, GNorm = 11.4839, lr_0 = 1.0202e-04
Loss = 1.9095e-03, PNorm = 34.7835, GNorm = 3.2823, lr_0 = 1.0202e-04
Loss = 2.0718e-03, PNorm = 34.7861, GNorm = 5.2363, lr_0 = 1.0202e-04
Loss = 1.7039e-03, PNorm = 34.7873, GNorm = 5.5315, lr_0 = 1.0202e-04
Loss = 1.6596e-03, PNorm = 34.7892, GNorm = 6.6152, lr_0 = 1.0202e-04
Loss = 1.8724e-03, PNorm = 34.7908, GNorm = 2.1654, lr_0 = 1.0202e-04
Loss = 1.8889e-03, PNorm = 34.7925, GNorm = 9.7988, lr_0 = 1.0202e-04
Loss = 1.7867e-03, PNorm = 34.7941, GNorm = 3.6947, lr_0 = 1.0202e-04
Loss = 1.9304e-03, PNorm = 34.7958, GNorm = 4.3770, lr_0 = 1.0202e-04
Loss = 1.8656e-03, PNorm = 34.7975, GNorm = 8.0428, lr_0 = 1.0202e-04
Loss = 2.3132e-03, PNorm = 34.7993, GNorm = 3.6183, lr_0 = 1.0202e-04
Validation rmse logP = 0.589096
Validation R2 logP = 0.898327
Epoch 16
Train function
Loss = 1.9962e-03, PNorm = 34.8010, GNorm = 8.3367, lr_0 = 1.0202e-04
Loss = 1.6306e-03, PNorm = 34.8026, GNorm = 10.5656, lr_0 = 1.0202e-04
Loss = 1.8341e-03, PNorm = 34.8052, GNorm = 12.0174, lr_0 = 1.0202e-04
Loss = 1.8875e-03, PNorm = 34.8077, GNorm = 9.4188, lr_0 = 1.0202e-04
Loss = 1.7984e-03, PNorm = 34.8094, GNorm = 2.8301, lr_0 = 1.0202e-04
Loss = 1.3605e-03, PNorm = 34.8105, GNorm = 1.9936, lr_0 = 1.0202e-04
Loss = 1.6952e-03, PNorm = 34.8128, GNorm = 2.5000, lr_0 = 1.0202e-04
Loss = 1.8834e-03, PNorm = 34.8155, GNorm = 9.4729, lr_0 = 1.0202e-04
Loss = 2.0716e-03, PNorm = 34.8177, GNorm = 2.4743, lr_0 = 1.0202e-04
Loss = 1.3485e-03, PNorm = 34.8205, GNorm = 1.0776, lr_0 = 1.0202e-04
Loss = 1.5885e-03, PNorm = 34.8221, GNorm = 5.8863, lr_0 = 1.0202e-04
Loss = 1.5822e-03, PNorm = 34.8236, GNorm = 1.6773, lr_0 = 1.0202e-04
Loss = 1.5756e-03, PNorm = 34.8246, GNorm = 3.3846, lr_0 = 1.0202e-04
Loss = 1.5987e-03, PNorm = 34.8260, GNorm = 5.1443, lr_0 = 1.0202e-04
Loss = 1.5553e-03, PNorm = 34.8279, GNorm = 8.5947, lr_0 = 1.0202e-04
Loss = 1.5886e-03, PNorm = 34.8301, GNorm = 3.2062, lr_0 = 1.0202e-04
Loss = 1.4994e-03, PNorm = 34.8321, GNorm = 2.2368, lr_0 = 1.0202e-04
Loss = 1.9492e-03, PNorm = 34.8345, GNorm = 15.3372, lr_0 = 1.0202e-04
Loss = 2.0504e-03, PNorm = 34.8358, GNorm = 12.6934, lr_0 = 1.0202e-04
Loss = 1.6622e-03, PNorm = 34.8380, GNorm = 9.0972, lr_0 = 1.0202e-04
Loss = 2.1565e-03, PNorm = 34.8399, GNorm = 5.9624, lr_0 = 1.0202e-04
Loss = 2.1232e-03, PNorm = 34.8418, GNorm = 2.7836, lr_0 = 1.0202e-04
Loss = 2.4055e-03, PNorm = 34.8437, GNorm = 13.6814, lr_0 = 1.0202e-04
Validation rmse logP = 0.561454
Validation R2 logP = 0.907645
Epoch 17
Train function
Loss = 1.9327e-03, PNorm = 34.8462, GNorm = 8.3127, lr_0 = 1.0202e-04
Loss = 1.5447e-03, PNorm = 34.8480, GNorm = 9.7930, lr_0 = 1.0202e-04
Loss = 1.7489e-03, PNorm = 34.8496, GNorm = 2.8745, lr_0 = 1.0202e-04
Loss = 1.4151e-03, PNorm = 34.8511, GNorm = 1.5820, lr_0 = 1.0202e-04
Loss = 1.8789e-03, PNorm = 34.8531, GNorm = 3.8713, lr_0 = 1.0202e-04
Loss = 1.6267e-03, PNorm = 34.8552, GNorm = 5.5371, lr_0 = 1.0202e-04
Loss = 1.3344e-03, PNorm = 34.8576, GNorm = 1.8693, lr_0 = 1.0202e-04
Loss = 1.4471e-03, PNorm = 34.8593, GNorm = 3.7551, lr_0 = 1.0202e-04
Loss = 1.7709e-03, PNorm = 34.8613, GNorm = 10.4332, lr_0 = 1.0202e-04
Loss = 1.9135e-03, PNorm = 34.8630, GNorm = 2.9872, lr_0 = 1.0202e-04
Loss = 1.9064e-03, PNorm = 34.8648, GNorm = 6.6241, lr_0 = 1.0202e-04
Loss = 1.4685e-03, PNorm = 34.8663, GNorm = 6.6782, lr_0 = 1.0202e-04
Loss = 1.7179e-03, PNorm = 34.8688, GNorm = 13.9869, lr_0 = 1.0202e-04
Loss = 1.9514e-03, PNorm = 34.8709, GNorm = 8.7660, lr_0 = 1.0202e-04
Loss = 1.8685e-03, PNorm = 34.8731, GNorm = 14.0009, lr_0 = 1.0202e-04
Loss = 2.2895e-03, PNorm = 34.8743, GNorm = 21.7919, lr_0 = 1.0202e-04
Loss = 1.9258e-03, PNorm = 34.8765, GNorm = 2.2524, lr_0 = 1.0202e-04
Loss = 1.8096e-03, PNorm = 34.8781, GNorm = 2.4477, lr_0 = 1.0202e-04
Loss = 2.0028e-03, PNorm = 34.8797, GNorm = 13.6166, lr_0 = 1.0202e-04
Loss = 2.1884e-03, PNorm = 34.8820, GNorm = 7.1715, lr_0 = 1.0202e-04
Loss = 1.7238e-03, PNorm = 34.8837, GNorm = 6.2272, lr_0 = 1.0202e-04
Loss = 1.8742e-03, PNorm = 34.8855, GNorm = 2.8046, lr_0 = 1.0202e-04
Validation rmse logP = 0.625071
Validation R2 logP = 0.885530
Epoch 18
Train function
Loss = 1.8544e-03, PNorm = 34.8872, GNorm = 8.8633, lr_0 = 1.0202e-04
Loss = 1.5808e-03, PNorm = 34.8894, GNorm = 3.5671, lr_0 = 1.0202e-04
Loss = 1.4607e-03, PNorm = 34.8914, GNorm = 8.9149, lr_0 = 1.0202e-04
Loss = 1.6615e-03, PNorm = 34.8934, GNorm = 7.7711, lr_0 = 1.0202e-04
Loss = 1.4797e-03, PNorm = 34.8953, GNorm = 5.5752, lr_0 = 1.0202e-04
Loss = 1.4262e-03, PNorm = 34.8978, GNorm = 11.7597, lr_0 = 1.0202e-04
Loss = 1.4961e-03, PNorm = 34.8996, GNorm = 2.0430, lr_0 = 1.0202e-04
Loss = 1.5541e-03, PNorm = 34.9017, GNorm = 7.2057, lr_0 = 1.0202e-04
Loss = 1.9396e-03, PNorm = 34.9036, GNorm = 6.0754, lr_0 = 1.0202e-04
Loss = 2.0845e-03, PNorm = 34.9051, GNorm = 5.6046, lr_0 = 1.0202e-04
Loss = 1.9415e-03, PNorm = 34.9055, GNorm = 16.6493, lr_0 = 1.0202e-04
Loss = 1.6220e-03, PNorm = 34.9077, GNorm = 10.4097, lr_0 = 1.0202e-04
Loss = 1.6303e-03, PNorm = 34.9090, GNorm = 2.4563, lr_0 = 1.0202e-04
Loss = 1.7342e-03, PNorm = 34.9106, GNorm = 12.7126, lr_0 = 1.0202e-04
Loss = 1.8821e-03, PNorm = 34.9132, GNorm = 3.1851, lr_0 = 1.0202e-04
Loss = 1.5927e-03, PNorm = 34.9161, GNorm = 4.0352, lr_0 = 1.0202e-04
Loss = 1.7162e-03, PNorm = 34.9179, GNorm = 13.9819, lr_0 = 1.0202e-04
Loss = 1.8956e-03, PNorm = 34.9184, GNorm = 16.8191, lr_0 = 1.0202e-04
Loss = 1.7797e-03, PNorm = 34.9201, GNorm = 6.2240, lr_0 = 1.0202e-04
Loss = 2.0101e-03, PNorm = 34.9223, GNorm = 17.0358, lr_0 = 1.0202e-04
Loss = 2.6852e-03, PNorm = 34.9235, GNorm = 3.8080, lr_0 = 1.0202e-04
Loss = 2.0175e-03, PNorm = 34.9255, GNorm = 5.7570, lr_0 = 1.0202e-04
Validation rmse logP = 0.572752
Validation R2 logP = 0.903891
Epoch 19
Train function
Loss = 1.3872e-03, PNorm = 34.9277, GNorm = 6.2109, lr_0 = 1.0202e-04
Loss = 1.5506e-03, PNorm = 34.9289, GNorm = 1.1733, lr_0 = 1.0202e-04
Loss = 1.5250e-03, PNorm = 34.9307, GNorm = 6.4893, lr_0 = 1.0202e-04
Loss = 1.7390e-03, PNorm = 34.9319, GNorm = 5.8875, lr_0 = 1.0202e-04
Loss = 1.7085e-03, PNorm = 34.9343, GNorm = 2.2862, lr_0 = 1.0202e-04
Loss = 1.4698e-03, PNorm = 34.9363, GNorm = 2.2109, lr_0 = 1.0202e-04
Loss = 1.5396e-03, PNorm = 34.9379, GNorm = 10.0668, lr_0 = 1.0202e-04
Loss = 1.8264e-03, PNorm = 34.9402, GNorm = 6.4604, lr_0 = 1.0202e-04
Loss = 1.7042e-03, PNorm = 34.9424, GNorm = 2.4785, lr_0 = 1.0202e-04
Loss = 1.7607e-03, PNorm = 34.9440, GNorm = 5.9111, lr_0 = 1.0202e-04
Loss = 1.5177e-03, PNorm = 34.9461, GNorm = 2.9631, lr_0 = 1.0202e-04
Loss = 1.3649e-03, PNorm = 34.9477, GNorm = 5.8855, lr_0 = 1.0202e-04
Loss = 1.3798e-03, PNorm = 34.9492, GNorm = 4.7162, lr_0 = 1.0202e-04
Loss = 2.1123e-03, PNorm = 34.9513, GNorm = 5.1881, lr_0 = 1.0202e-04
Loss = 1.5799e-03, PNorm = 34.9535, GNorm = 3.8408, lr_0 = 1.0202e-04
Loss = 1.7704e-03, PNorm = 34.9553, GNorm = 5.4486, lr_0 = 1.0202e-04
Loss = 1.3532e-03, PNorm = 34.9563, GNorm = 4.7671, lr_0 = 1.0202e-04
Loss = 1.0292e-03, PNorm = 34.9584, GNorm = 3.7850, lr_0 = 1.0202e-04
Loss = 1.7812e-03, PNorm = 34.9605, GNorm = 5.3025, lr_0 = 1.0202e-04
Loss = 1.8278e-03, PNorm = 34.9624, GNorm = 1.7612, lr_0 = 1.0202e-04
Loss = 1.4476e-03, PNorm = 34.9641, GNorm = 1.6266, lr_0 = 1.0202e-04
Loss = 1.4508e-03, PNorm = 34.9657, GNorm = 2.5511, lr_0 = 1.0202e-04
Loss = 1.2807e-03, PNorm = 34.9672, GNorm = 2.5310, lr_0 = 1.0202e-04
Loss = 5.2202e-03, PNorm = 34.9673, GNorm = 10.8515, lr_0 = 1.0202e-04
Validation rmse logP = 0.543162
Validation R2 logP = 0.913565
Epoch 20
Train function
Loss = 1.2268e-03, PNorm = 34.9696, GNorm = 5.1734, lr_0 = 1.0202e-04
Loss = 1.4909e-03, PNorm = 34.9713, GNorm = 4.1226, lr_0 = 1.0202e-04
Loss = 1.2410e-03, PNorm = 34.9726, GNorm = 1.7448, lr_0 = 1.0202e-04
Loss = 1.2238e-03, PNorm = 34.9744, GNorm = 9.0096, lr_0 = 1.0202e-04
Loss = 1.5409e-03, PNorm = 34.9767, GNorm = 23.5756, lr_0 = 1.0202e-04
Loss = 1.7237e-03, PNorm = 34.9784, GNorm = 14.6043, lr_0 = 1.0202e-04
Loss = 1.8835e-03, PNorm = 34.9803, GNorm = 8.8917, lr_0 = 1.0202e-04
Loss = 1.5488e-03, PNorm = 34.9817, GNorm = 11.8216, lr_0 = 1.0202e-04
Loss = 2.2315e-03, PNorm = 34.9838, GNorm = 2.4790, lr_0 = 1.0202e-04
Loss = 1.6290e-03, PNorm = 34.9867, GNorm = 2.6348, lr_0 = 1.0202e-04
Loss = 1.6249e-03, PNorm = 34.9891, GNorm = 2.5846, lr_0 = 1.0202e-04
Loss = 1.6307e-03, PNorm = 34.9917, GNorm = 5.3097, lr_0 = 1.0202e-04
Loss = 1.5126e-03, PNorm = 34.9940, GNorm = 3.5760, lr_0 = 1.0202e-04
Loss = 2.0086e-03, PNorm = 34.9951, GNorm = 19.0544, lr_0 = 1.0202e-04
Loss = 1.4616e-03, PNorm = 34.9960, GNorm = 4.3148, lr_0 = 1.0202e-04
Loss = 1.3567e-03, PNorm = 34.9978, GNorm = 4.6704, lr_0 = 1.0202e-04
Loss = 1.5379e-03, PNorm = 34.9997, GNorm = 9.3504, lr_0 = 1.0202e-04
Loss = 1.6062e-03, PNorm = 35.0015, GNorm = 11.2562, lr_0 = 1.0202e-04
Loss = 1.5424e-03, PNorm = 35.0028, GNorm = 2.5121, lr_0 = 1.0202e-04
Loss = 2.0305e-03, PNorm = 35.0038, GNorm = 5.4758, lr_0 = 1.0202e-04
Loss = 1.6702e-03, PNorm = 35.0059, GNorm = 2.8869, lr_0 = 1.0202e-04
Loss = 1.7329e-03, PNorm = 35.0084, GNorm = 5.5193, lr_0 = 1.0202e-04
Validation rmse logP = 0.553311
Validation R2 logP = 0.910304
Epoch 21
Train function
Loss = 1.5994e-03, PNorm = 35.0103, GNorm = 5.5797, lr_0 = 1.0202e-04
Loss = 1.6372e-03, PNorm = 35.0123, GNorm = 7.3100, lr_0 = 1.0202e-04
Loss = 1.6504e-03, PNorm = 35.0138, GNorm = 4.4846, lr_0 = 1.0202e-04
Loss = 1.6751e-03, PNorm = 35.0161, GNorm = 5.8466, lr_0 = 1.0202e-04
Loss = 1.5080e-03, PNorm = 35.0181, GNorm = 6.4787, lr_0 = 1.0202e-04
Loss = 1.5178e-03, PNorm = 35.0206, GNorm = 8.7784, lr_0 = 1.0202e-04
Loss = 1.6076e-03, PNorm = 35.0225, GNorm = 1.9271, lr_0 = 1.0202e-04
Loss = 1.4306e-03, PNorm = 35.0249, GNorm = 6.9036, lr_0 = 1.0202e-04
Loss = 1.9758e-03, PNorm = 35.0263, GNorm = 8.3988, lr_0 = 1.0202e-04
Loss = 1.4976e-03, PNorm = 35.0284, GNorm = 6.6290, lr_0 = 1.0202e-04
Loss = 1.7228e-03, PNorm = 35.0300, GNorm = 1.9808, lr_0 = 1.0202e-04
Loss = 1.5849e-03, PNorm = 35.0329, GNorm = 15.2841, lr_0 = 1.0202e-04
Loss = 1.8464e-03, PNorm = 35.0345, GNorm = 8.6690, lr_0 = 1.0202e-04
Loss = 1.3906e-03, PNorm = 35.0365, GNorm = 2.3183, lr_0 = 1.0202e-04
Loss = 1.4623e-03, PNorm = 35.0378, GNorm = 3.1176, lr_0 = 1.0202e-04
Loss = 1.5695e-03, PNorm = 35.0394, GNorm = 12.2906, lr_0 = 1.0202e-04
Loss = 1.6265e-03, PNorm = 35.0414, GNorm = 2.5848, lr_0 = 1.0202e-04
Loss = 1.2869e-03, PNorm = 35.0434, GNorm = 5.2601, lr_0 = 1.0202e-04
Loss = 1.4861e-03, PNorm = 35.0443, GNorm = 12.3905, lr_0 = 1.0202e-04
Loss = 1.4111e-03, PNorm = 35.0458, GNorm = 5.6097, lr_0 = 1.0202e-04
Loss = 1.5830e-03, PNorm = 35.0479, GNorm = 6.6528, lr_0 = 1.0202e-04
Loss = 1.3302e-03, PNorm = 35.0495, GNorm = 10.1960, lr_0 = 1.0202e-04
Validation rmse logP = 0.564275
Validation R2 logP = 0.906715
Epoch 22
Train function
Loss = 2.1536e-03, PNorm = 35.0513, GNorm = 14.8310, lr_0 = 1.0202e-04
Loss = 1.5729e-03, PNorm = 35.0535, GNorm = 7.3271, lr_0 = 1.0202e-04
Loss = 1.2294e-03, PNorm = 35.0554, GNorm = 3.1047, lr_0 = 1.0202e-04
Loss = 1.5530e-03, PNorm = 35.0579, GNorm = 7.6349, lr_0 = 1.0202e-04
Loss = 1.2814e-03, PNorm = 35.0598, GNorm = 2.7029, lr_0 = 1.0202e-04
Loss = 1.4955e-03, PNorm = 35.0618, GNorm = 8.5445, lr_0 = 1.0202e-04
Loss = 1.8337e-03, PNorm = 35.0635, GNorm = 8.2268, lr_0 = 1.0202e-04
Loss = 1.5711e-03, PNorm = 35.0655, GNorm = 3.1919, lr_0 = 1.0202e-04
Loss = 1.3924e-03, PNorm = 35.0670, GNorm = 10.3656, lr_0 = 1.0202e-04
Loss = 1.4714e-03, PNorm = 35.0687, GNorm = 5.8806, lr_0 = 1.0202e-04
Loss = 1.5339e-03, PNorm = 35.0706, GNorm = 10.9837, lr_0 = 1.0202e-04
Loss = 1.0958e-03, PNorm = 35.0723, GNorm = 11.0871, lr_0 = 1.0202e-04
Loss = 2.1164e-03, PNorm = 35.0736, GNorm = 6.2730, lr_0 = 1.0202e-04
Loss = 1.3715e-03, PNorm = 35.0755, GNorm = 3.6206, lr_0 = 1.0202e-04
Loss = 1.5149e-03, PNorm = 35.0768, GNorm = 2.7498, lr_0 = 1.0202e-04
Loss = 1.5967e-03, PNorm = 35.0788, GNorm = 6.4407, lr_0 = 1.0202e-04
Loss = 1.3608e-03, PNorm = 35.0801, GNorm = 3.0078, lr_0 = 1.0202e-04
Loss = 1.3171e-03, PNorm = 35.0817, GNorm = 3.5988, lr_0 = 1.0202e-04
Loss = 1.1812e-03, PNorm = 35.0831, GNorm = 1.5708, lr_0 = 1.0202e-04
Loss = 1.6898e-03, PNorm = 35.0850, GNorm = 9.6522, lr_0 = 1.0202e-04
Loss = 1.2107e-03, PNorm = 35.0869, GNorm = 2.0005, lr_0 = 1.0202e-04
Loss = 1.4304e-03, PNorm = 35.0886, GNorm = 7.9463, lr_0 = 1.0202e-04
Loss = 1.4788e-03, PNorm = 35.0899, GNorm = 5.1720, lr_0 = 1.0202e-04
Validation rmse logP = 0.572165
Validation R2 logP = 0.904087
Epoch 23
Train function
Loss = 1.1035e-03, PNorm = 35.0915, GNorm = 10.6612, lr_0 = 1.0202e-04
Loss = 1.6246e-03, PNorm = 35.0933, GNorm = 11.1153, lr_0 = 1.0202e-04
Loss = 1.4985e-03, PNorm = 35.0950, GNorm = 2.3093, lr_0 = 1.0202e-04
Loss = 1.3528e-03, PNorm = 35.0969, GNorm = 4.5431, lr_0 = 1.0202e-04
Loss = 1.4390e-03, PNorm = 35.0982, GNorm = 7.5210, lr_0 = 1.0202e-04
Loss = 1.5967e-03, PNorm = 35.1004, GNorm = 9.7566, lr_0 = 1.0202e-04
Loss = 1.2783e-03, PNorm = 35.1015, GNorm = 8.7633, lr_0 = 1.0202e-04
Loss = 1.1046e-03, PNorm = 35.1028, GNorm = 3.6301, lr_0 = 1.0202e-04
Loss = 1.3073e-03, PNorm = 35.1045, GNorm = 1.8119, lr_0 = 1.0202e-04
Loss = 1.3948e-03, PNorm = 35.1066, GNorm = 2.9081, lr_0 = 1.0202e-04
Loss = 1.4328e-03, PNorm = 35.1087, GNorm = 6.9548, lr_0 = 1.0202e-04
Loss = 1.5566e-03, PNorm = 35.1113, GNorm = 9.8195, lr_0 = 1.0202e-04
Loss = 1.3411e-03, PNorm = 35.1133, GNorm = 6.3526, lr_0 = 1.0202e-04
Loss = 1.3882e-03, PNorm = 35.1151, GNorm = 1.8056, lr_0 = 1.0202e-04
Loss = 1.5191e-03, PNorm = 35.1171, GNorm = 2.3945, lr_0 = 1.0202e-04
Loss = 1.4467e-03, PNorm = 35.1200, GNorm = 1.8162, lr_0 = 1.0202e-04
Loss = 1.4622e-03, PNorm = 35.1219, GNorm = 3.8575, lr_0 = 1.0202e-04
Loss = 1.2928e-03, PNorm = 35.1237, GNorm = 2.9463, lr_0 = 1.0202e-04
Loss = 1.5974e-03, PNorm = 35.1243, GNorm = 9.9270, lr_0 = 1.0202e-04
Loss = 1.3900e-03, PNorm = 35.1259, GNorm = 1.9845, lr_0 = 1.0202e-04
Loss = 1.6659e-03, PNorm = 35.1287, GNorm = 12.7060, lr_0 = 1.0202e-04
Loss = 1.4160e-03, PNorm = 35.1310, GNorm = 11.1520, lr_0 = 1.0202e-04
Validation rmse logP = 0.597122
Validation R2 logP = 0.895538
Epoch 24
Train function
Loss = 1.7373e-03, PNorm = 35.1330, GNorm = 17.0343, lr_0 = 1.0202e-04
Loss = 1.7219e-03, PNorm = 35.1348, GNorm = 18.1156, lr_0 = 1.0202e-04
Loss = 1.7993e-03, PNorm = 35.1367, GNorm = 8.5576, lr_0 = 1.0202e-04
Loss = 2.1844e-03, PNorm = 35.1375, GNorm = 6.8287, lr_0 = 1.0202e-04
Loss = 1.7846e-03, PNorm = 35.1399, GNorm = 6.1723, lr_0 = 1.0202e-04
Loss = 1.4913e-03, PNorm = 35.1412, GNorm = 7.3008, lr_0 = 1.0202e-04
Loss = 1.1341e-03, PNorm = 35.1427, GNorm = 4.4713, lr_0 = 1.0202e-04
Loss = 1.2273e-03, PNorm = 35.1450, GNorm = 1.1049, lr_0 = 1.0202e-04
Loss = 1.4218e-03, PNorm = 35.1475, GNorm = 9.5602, lr_0 = 1.0202e-04
Loss = 1.7941e-03, PNorm = 35.1493, GNorm = 8.4405, lr_0 = 1.0202e-04
Loss = 1.4338e-03, PNorm = 35.1524, GNorm = 2.2171, lr_0 = 1.0202e-04
Loss = 1.2940e-03, PNorm = 35.1543, GNorm = 3.1199, lr_0 = 1.0202e-04
Loss = 1.1945e-03, PNorm = 35.1563, GNorm = 2.8551, lr_0 = 1.0202e-04
Loss = 1.3032e-03, PNorm = 35.1580, GNorm = 5.7935, lr_0 = 1.0202e-04
Loss = 1.6329e-03, PNorm = 35.1601, GNorm = 3.4704, lr_0 = 1.0202e-04
Loss = 1.3769e-03, PNorm = 35.1614, GNorm = 7.2869, lr_0 = 1.0202e-04
Loss = 1.2710e-03, PNorm = 35.1627, GNorm = 7.7177, lr_0 = 1.0202e-04
Loss = 1.6335e-03, PNorm = 35.1645, GNorm = 3.3539, lr_0 = 1.0202e-04
Loss = 1.6891e-03, PNorm = 35.1658, GNorm = 1.6819, lr_0 = 1.0202e-04
Loss = 1.3217e-03, PNorm = 35.1679, GNorm = 3.4557, lr_0 = 1.0202e-04
Loss = 1.1798e-03, PNorm = 35.1696, GNorm = 5.2216, lr_0 = 1.0202e-04
Loss = 1.5941e-03, PNorm = 35.1717, GNorm = 13.5528, lr_0 = 1.0202e-04
Validation rmse logP = 0.605574
Validation R2 logP = 0.892560
Epoch 25
Train function
Loss = 1.4871e-03, PNorm = 35.1731, GNorm = 10.2472, lr_0 = 1.0202e-04
Loss = 1.6353e-03, PNorm = 35.1746, GNorm = 6.3389, lr_0 = 1.0202e-04
Loss = 1.5254e-03, PNorm = 35.1771, GNorm = 9.0797, lr_0 = 1.0202e-04
Loss = 1.3856e-03, PNorm = 35.1793, GNorm = 2.9625, lr_0 = 1.0202e-04
Loss = 1.4083e-03, PNorm = 35.1813, GNorm = 7.2772, lr_0 = 1.0202e-04
Loss = 1.6106e-03, PNorm = 35.1836, GNorm = 5.4823, lr_0 = 1.0202e-04
Loss = 1.6128e-03, PNorm = 35.1857, GNorm = 3.0313, lr_0 = 1.0202e-04
Loss = 1.4688e-03, PNorm = 35.1880, GNorm = 16.7136, lr_0 = 1.0202e-04
Loss = 1.4889e-03, PNorm = 35.1893, GNorm = 6.6692, lr_0 = 1.0202e-04
Loss = 1.6035e-03, PNorm = 35.1914, GNorm = 3.8697, lr_0 = 1.0202e-04
Loss = 1.2090e-03, PNorm = 35.1927, GNorm = 3.8551, lr_0 = 1.0202e-04
Loss = 1.2087e-03, PNorm = 35.1945, GNorm = 3.2791, lr_0 = 1.0202e-04
Loss = 1.2431e-03, PNorm = 35.1957, GNorm = 3.5777, lr_0 = 1.0202e-04
Loss = 1.4379e-03, PNorm = 35.1972, GNorm = 2.3117, lr_0 = 1.0202e-04
Loss = 1.6122e-03, PNorm = 35.1989, GNorm = 3.9446, lr_0 = 1.0202e-04
Loss = 1.7148e-03, PNorm = 35.2014, GNorm = 9.6865, lr_0 = 1.0202e-04
Loss = 1.4181e-03, PNorm = 35.2035, GNorm = 1.7731, lr_0 = 1.0202e-04
Loss = 1.2272e-03, PNorm = 35.2056, GNorm = 5.1995, lr_0 = 1.0202e-04
Loss = 1.6266e-03, PNorm = 35.2074, GNorm = 4.7166, lr_0 = 1.0202e-04
Loss = 1.3144e-03, PNorm = 35.2094, GNorm = 7.8344, lr_0 = 1.0202e-04
Loss = 1.4832e-03, PNorm = 35.2109, GNorm = 11.7667, lr_0 = 1.0202e-04
Loss = 1.5748e-03, PNorm = 35.2120, GNorm = 9.3176, lr_0 = 1.0202e-04
Loss = 1.1293e-03, PNorm = 35.2136, GNorm = 5.6028, lr_0 = 1.0202e-04
Validation rmse logP = 0.521197
Validation R2 logP = 0.920414
Epoch 26
Train function
Loss = 1.4403e-03, PNorm = 35.2157, GNorm = 8.0386, lr_0 = 1.0202e-04
Loss = 1.7370e-03, PNorm = 35.2173, GNorm = 10.7696, lr_0 = 1.0202e-04
Loss = 1.7544e-03, PNorm = 35.2198, GNorm = 2.3600, lr_0 = 1.0202e-04
Loss = 1.4174e-03, PNorm = 35.2217, GNorm = 6.0367, lr_0 = 1.0202e-04
Loss = 1.3738e-03, PNorm = 35.2248, GNorm = 3.1110, lr_0 = 1.0202e-04
Loss = 1.2951e-03, PNorm = 35.2262, GNorm = 5.9284, lr_0 = 1.0202e-04
Loss = 1.5783e-03, PNorm = 35.2279, GNorm = 10.3322, lr_0 = 1.0202e-04
Loss = 1.1482e-03, PNorm = 35.2291, GNorm = 4.1234, lr_0 = 1.0202e-04
Loss = 1.3669e-03, PNorm = 35.2299, GNorm = 12.9817, lr_0 = 1.0202e-04
Loss = 1.7431e-03, PNorm = 35.2313, GNorm = 6.7368, lr_0 = 1.0202e-04
Loss = 1.1984e-03, PNorm = 35.2340, GNorm = 7.3338, lr_0 = 1.0202e-04
Loss = 1.3056e-03, PNorm = 35.2356, GNorm = 2.5135, lr_0 = 1.0202e-04
Loss = 1.1927e-03, PNorm = 35.2373, GNorm = 4.8639, lr_0 = 1.0202e-04
Loss = 1.1982e-03, PNorm = 35.2399, GNorm = 4.1493, lr_0 = 1.0202e-04
Loss = 1.6497e-03, PNorm = 35.2417, GNorm = 2.4565, lr_0 = 1.0202e-04
Loss = 1.2739e-03, PNorm = 35.2437, GNorm = 5.0338, lr_0 = 1.0202e-04
Loss = 1.0909e-03, PNorm = 35.2456, GNorm = 1.4050, lr_0 = 1.0202e-04
Loss = 1.3093e-03, PNorm = 35.2475, GNorm = 5.4046, lr_0 = 1.0202e-04
Loss = 1.1539e-03, PNorm = 35.2497, GNorm = 4.9831, lr_0 = 1.0202e-04
Loss = 1.2871e-03, PNorm = 35.2518, GNorm = 1.8923, lr_0 = 1.0202e-04
Loss = 1.2619e-03, PNorm = 35.2536, GNorm = 7.5779, lr_0 = 1.0202e-04
Loss = 1.3582e-03, PNorm = 35.2552, GNorm = 2.5896, lr_0 = 1.0202e-04
Validation rmse logP = 0.511455
Validation R2 logP = 0.923361
Epoch 27
Train function
Loss = 8.2176e-04, PNorm = 35.2561, GNorm = 4.0327, lr_0 = 1.0202e-04
Loss = 1.0743e-03, PNorm = 35.2574, GNorm = 1.6218, lr_0 = 1.0202e-04
Loss = 1.0580e-03, PNorm = 35.2586, GNorm = 3.8196, lr_0 = 1.0202e-04
Loss = 1.1991e-03, PNorm = 35.2600, GNorm = 3.4696, lr_0 = 1.0202e-04
Loss = 1.1513e-03, PNorm = 35.2615, GNorm = 3.2847, lr_0 = 1.0202e-04
Loss = 1.4636e-03, PNorm = 35.2631, GNorm = 5.2408, lr_0 = 1.0202e-04
Loss = 1.5669e-03, PNorm = 35.2649, GNorm = 2.2544, lr_0 = 1.0202e-04
Loss = 1.3033e-03, PNorm = 35.2659, GNorm = 10.4337, lr_0 = 1.0202e-04
Loss = 1.3749e-03, PNorm = 35.2675, GNorm = 6.0200, lr_0 = 1.0202e-04
Loss = 1.6386e-03, PNorm = 35.2690, GNorm = 11.8837, lr_0 = 1.0202e-04
Loss = 1.4480e-03, PNorm = 35.2705, GNorm = 3.3755, lr_0 = 1.0202e-04
Loss = 1.5326e-03, PNorm = 35.2725, GNorm = 2.3414, lr_0 = 1.0202e-04
Loss = 1.2028e-03, PNorm = 35.2748, GNorm = 1.9540, lr_0 = 1.0202e-04
Loss = 1.3327e-03, PNorm = 35.2768, GNorm = 4.4854, lr_0 = 1.0202e-04
Loss = 1.0964e-03, PNorm = 35.2786, GNorm = 2.5758, lr_0 = 1.0202e-04
Loss = 1.2806e-03, PNorm = 35.2803, GNorm = 2.6530, lr_0 = 1.0202e-04
Loss = 9.9163e-04, PNorm = 35.2817, GNorm = 4.3030, lr_0 = 1.0202e-04
Loss = 1.2641e-03, PNorm = 35.2841, GNorm = 6.2031, lr_0 = 1.0202e-04
Loss = 1.3001e-03, PNorm = 35.2866, GNorm = 3.3852, lr_0 = 1.0202e-04
Loss = 1.4017e-03, PNorm = 35.2893, GNorm = 7.5261, lr_0 = 1.0202e-04
Loss = 1.2071e-03, PNorm = 35.2919, GNorm = 8.1840, lr_0 = 1.0202e-04
Loss = 1.2455e-03, PNorm = 35.2943, GNorm = 10.4479, lr_0 = 1.0202e-04
Validation rmse logP = 0.517864
Validation R2 logP = 0.921429
Epoch 28
Train function
Loss = 7.7189e-04, PNorm = 35.2953, GNorm = 3.1450, lr_0 = 1.0202e-04
Loss = 1.3399e-03, PNorm = 35.2975, GNorm = 11.4868, lr_0 = 1.0202e-04
Loss = 1.3814e-03, PNorm = 35.2993, GNorm = 3.9397, lr_0 = 1.0202e-04
Loss = 1.1267e-03, PNorm = 35.3005, GNorm = 4.2107, lr_0 = 1.0202e-04
Loss = 1.2954e-03, PNorm = 35.3017, GNorm = 3.6045, lr_0 = 1.0202e-04
Loss = 1.4698e-03, PNorm = 35.3041, GNorm = 2.4020, lr_0 = 1.0202e-04
Loss = 1.4060e-03, PNorm = 35.3063, GNorm = 2.6961, lr_0 = 1.0202e-04
Loss = 1.1966e-03, PNorm = 35.3084, GNorm = 2.9350, lr_0 = 1.0202e-04
Loss = 1.5701e-03, PNorm = 35.3092, GNorm = 4.2124, lr_0 = 1.0202e-04
Loss = 1.4497e-03, PNorm = 35.3109, GNorm = 2.6034, lr_0 = 1.0202e-04
Loss = 1.4831e-03, PNorm = 35.3121, GNorm = 5.9224, lr_0 = 1.0202e-04
Loss = 1.0872e-03, PNorm = 35.3142, GNorm = 10.1728, lr_0 = 1.0202e-04
Loss = 1.1151e-03, PNorm = 35.3160, GNorm = 1.7703, lr_0 = 1.0202e-04
Loss = 1.2362e-03, PNorm = 35.3179, GNorm = 6.7119, lr_0 = 1.0202e-04
Loss = 9.6593e-04, PNorm = 35.3196, GNorm = 5.5336, lr_0 = 1.0202e-04
Loss = 1.6479e-03, PNorm = 35.3224, GNorm = 5.6284, lr_0 = 1.0202e-04
Loss = 1.4239e-03, PNorm = 35.3246, GNorm = 5.2907, lr_0 = 1.0202e-04
Loss = 1.1927e-03, PNorm = 35.3269, GNorm = 2.7000, lr_0 = 1.0202e-04
Loss = 1.3564e-03, PNorm = 35.3288, GNorm = 7.7538, lr_0 = 1.0202e-04
Loss = 1.0952e-03, PNorm = 35.3303, GNorm = 7.3615, lr_0 = 1.0202e-04
Loss = 1.3078e-03, PNorm = 35.3313, GNorm = 10.3857, lr_0 = 1.0202e-04
Loss = 1.3180e-03, PNorm = 35.3334, GNorm = 6.7470, lr_0 = 1.0202e-04
Loss = 1.2290e-03, PNorm = 35.3348, GNorm = 2.9711, lr_0 = 1.0202e-04
Validation rmse logP = 0.514811
Validation R2 logP = 0.922352
Epoch 29
Train function
Loss = 9.8636e-04, PNorm = 35.3358, GNorm = 9.0988, lr_0 = 1.0202e-04
Loss = 1.5699e-03, PNorm = 35.3371, GNorm = 6.3369, lr_0 = 1.0202e-04
Loss = 1.4458e-03, PNorm = 35.3398, GNorm = 2.9808, lr_0 = 1.0202e-04
Loss = 1.0211e-03, PNorm = 35.3414, GNorm = 2.0945, lr_0 = 1.0202e-04
Loss = 1.3190e-03, PNorm = 35.3431, GNorm = 3.1068, lr_0 = 1.0202e-04
Loss = 1.1971e-03, PNorm = 35.3447, GNorm = 1.3723, lr_0 = 1.0202e-04
Loss = 1.4595e-03, PNorm = 35.3470, GNorm = 1.9094, lr_0 = 1.0202e-04
Loss = 1.1899e-03, PNorm = 35.3482, GNorm = 4.3040, lr_0 = 1.0202e-04
Loss = 1.3157e-03, PNorm = 35.3499, GNorm = 5.1759, lr_0 = 1.0202e-04
Loss = 1.1344e-03, PNorm = 35.3513, GNorm = 4.6066, lr_0 = 1.0202e-04
Loss = 1.2700e-03, PNorm = 35.3533, GNorm = 1.5828, lr_0 = 1.0202e-04
Loss = 1.4214e-03, PNorm = 35.3556, GNorm = 2.7774, lr_0 = 1.0202e-04
Loss = 1.4248e-03, PNorm = 35.3573, GNorm = 11.1465, lr_0 = 1.0202e-04
Loss = 1.4591e-03, PNorm = 35.3594, GNorm = 6.9354, lr_0 = 1.0202e-04
Loss = 1.1539e-03, PNorm = 35.3606, GNorm = 10.3457, lr_0 = 1.0202e-04
Loss = 1.4021e-03, PNorm = 35.3627, GNorm = 10.1121, lr_0 = 1.0202e-04
Loss = 1.9281e-03, PNorm = 35.3641, GNorm = 10.6587, lr_0 = 1.0202e-04
Loss = 1.5055e-03, PNorm = 35.3664, GNorm = 4.3565, lr_0 = 1.0202e-04
Loss = 1.4628e-03, PNorm = 35.3687, GNorm = 5.2760, lr_0 = 1.0202e-04
Loss = 1.1293e-03, PNorm = 35.3708, GNorm = 9.6086, lr_0 = 1.0202e-04
Loss = 1.3943e-03, PNorm = 35.3737, GNorm = 16.5703, lr_0 = 1.0202e-04
Loss = 1.2439e-03, PNorm = 35.3753, GNorm = 1.8852, lr_0 = 1.0202e-04
Validation rmse logP = 0.505446
Validation R2 logP = 0.925152
Epoch 30
Train function
Loss = 1.2901e-03, PNorm = 35.3768, GNorm = 2.6306, lr_0 = 1.0202e-04
Loss = 1.0864e-03, PNorm = 35.3786, GNorm = 4.5190, lr_0 = 1.0202e-04
Loss = 1.4407e-03, PNorm = 35.3811, GNorm = 4.3401, lr_0 = 1.0202e-04
Loss = 1.1510e-03, PNorm = 35.3825, GNorm = 4.0631, lr_0 = 1.0202e-04
Loss = 1.0534e-03, PNorm = 35.3843, GNorm = 7.9416, lr_0 = 1.0202e-04
Loss = 1.1101e-03, PNorm = 35.3857, GNorm = 7.5740, lr_0 = 1.0202e-04
Loss = 1.0451e-03, PNorm = 35.3884, GNorm = 5.5419, lr_0 = 1.0202e-04
Loss = 1.2053e-03, PNorm = 35.3902, GNorm = 9.3325, lr_0 = 1.0202e-04
Loss = 9.9593e-04, PNorm = 35.3913, GNorm = 6.7201, lr_0 = 1.0202e-04
Loss = 1.8610e-03, PNorm = 35.3938, GNorm = 2.6149, lr_0 = 1.0202e-04
Loss = 1.3256e-03, PNorm = 35.3954, GNorm = 4.0096, lr_0 = 1.0202e-04
Loss = 1.1318e-03, PNorm = 35.3977, GNorm = 4.4449, lr_0 = 1.0202e-04
Loss = 1.2052e-03, PNorm = 35.3995, GNorm = 1.9714, lr_0 = 1.0202e-04
Loss = 1.1808e-03, PNorm = 35.4012, GNorm = 2.5660, lr_0 = 1.0202e-04
Loss = 1.1667e-03, PNorm = 35.4041, GNorm = 4.0639, lr_0 = 1.0202e-04
Loss = 1.5912e-03, PNorm = 35.4064, GNorm = 7.0831, lr_0 = 1.0202e-04
Loss = 1.2834e-03, PNorm = 35.4088, GNorm = 6.4731, lr_0 = 1.0202e-04
Loss = 1.3257e-03, PNorm = 35.4103, GNorm = 5.0893, lr_0 = 1.0202e-04
Loss = 1.3544e-03, PNorm = 35.4130, GNorm = 2.5407, lr_0 = 1.0202e-04
Loss = 1.2145e-03, PNorm = 35.4139, GNorm = 2.0010, lr_0 = 1.0202e-04
Loss = 1.3771e-03, PNorm = 35.4157, GNorm = 3.8847, lr_0 = 1.0202e-04
Loss = 1.0335e-03, PNorm = 35.4178, GNorm = 3.2162, lr_0 = 1.0202e-04
Validation rmse logP = 0.525791
Validation R2 logP = 0.919005
Epoch 31
Train function
Loss = 7.8898e-04, PNorm = 35.4199, GNorm = 1.8418, lr_0 = 1.0202e-04
Loss = 1.0692e-03, PNorm = 35.4214, GNorm = 6.8035, lr_0 = 1.0202e-04
Loss = 1.4993e-03, PNorm = 35.4224, GNorm = 15.0864, lr_0 = 1.0202e-04
Loss = 1.0957e-03, PNorm = 35.4239, GNorm = 5.2809, lr_0 = 1.0202e-04
Loss = 1.0698e-03, PNorm = 35.4264, GNorm = 3.2423, lr_0 = 1.0202e-04
Loss = 1.1200e-03, PNorm = 35.4291, GNorm = 7.2525, lr_0 = 1.0202e-04
Loss = 1.1641e-03, PNorm = 35.4310, GNorm = 6.5849, lr_0 = 1.0202e-04
Loss = 1.3903e-03, PNorm = 35.4338, GNorm = 5.0745, lr_0 = 1.0202e-04
Loss = 1.1802e-03, PNorm = 35.4351, GNorm = 2.7159, lr_0 = 1.0202e-04
Loss = 1.5914e-03, PNorm = 35.4376, GNorm = 3.1094, lr_0 = 1.0202e-04
Loss = 1.2394e-03, PNorm = 35.4394, GNorm = 11.1848, lr_0 = 1.0202e-04
Loss = 1.1161e-03, PNorm = 35.4405, GNorm = 3.4570, lr_0 = 1.0202e-04
Loss = 1.3459e-03, PNorm = 35.4417, GNorm = 2.6391, lr_0 = 1.0202e-04
Loss = 1.3055e-03, PNorm = 35.4434, GNorm = 4.1923, lr_0 = 1.0202e-04
Loss = 1.3310e-03, PNorm = 35.4449, GNorm = 4.2838, lr_0 = 1.0202e-04
Loss = 1.3907e-03, PNorm = 35.4470, GNorm = 2.8706, lr_0 = 1.0202e-04
Loss = 1.4447e-03, PNorm = 35.4477, GNorm = 2.5344, lr_0 = 1.0202e-04
Loss = 1.3567e-03, PNorm = 35.4496, GNorm = 7.3737, lr_0 = 1.0202e-04
Loss = 9.5364e-04, PNorm = 35.4508, GNorm = 4.7905, lr_0 = 1.0202e-04
Loss = 9.5468e-04, PNorm = 35.4525, GNorm = 1.9758, lr_0 = 1.0202e-04
Loss = 1.1521e-03, PNorm = 35.4542, GNorm = 2.4939, lr_0 = 1.0202e-04
Loss = 1.2788e-03, PNorm = 35.4562, GNorm = 6.3960, lr_0 = 1.0202e-04
Loss = 1.0606e-03, PNorm = 35.4586, GNorm = 2.5755, lr_0 = 1.0202e-04
Validation rmse logP = 0.516308
Validation R2 logP = 0.921900
Epoch 32
Train function
Loss = 1.1889e-03, PNorm = 35.4607, GNorm = 5.2269, lr_0 = 1.0202e-04
Loss = 1.0994e-03, PNorm = 35.4623, GNorm = 5.2170, lr_0 = 1.0202e-04
Loss = 1.3793e-03, PNorm = 35.4637, GNorm = 16.1790, lr_0 = 1.0202e-04
Loss = 1.8535e-03, PNorm = 35.4659, GNorm = 19.5552, lr_0 = 1.0202e-04
Loss = 1.3968e-03, PNorm = 35.4686, GNorm = 6.7200, lr_0 = 1.0202e-04
Loss = 1.6268e-03, PNorm = 35.4704, GNorm = 1.6456, lr_0 = 1.0202e-04
Loss = 1.3343e-03, PNorm = 35.4726, GNorm = 4.5928, lr_0 = 1.0202e-04
Loss = 1.4898e-03, PNorm = 35.4739, GNorm = 2.0092, lr_0 = 1.0202e-04
Loss = 1.1640e-03, PNorm = 35.4760, GNorm = 5.7849, lr_0 = 1.0202e-04
Loss = 1.3142e-03, PNorm = 35.4780, GNorm = 8.4922, lr_0 = 1.0202e-04
Loss = 1.1087e-03, PNorm = 35.4797, GNorm = 2.2324, lr_0 = 1.0202e-04
Loss = 9.7111e-04, PNorm = 35.4811, GNorm = 4.2234, lr_0 = 1.0202e-04
Loss = 1.0976e-03, PNorm = 35.4827, GNorm = 3.6126, lr_0 = 1.0202e-04
Loss = 1.1656e-03, PNorm = 35.4847, GNorm = 2.4645, lr_0 = 1.0202e-04
Loss = 1.0670e-03, PNorm = 35.4866, GNorm = 5.6283, lr_0 = 1.0202e-04
Loss = 9.6196e-04, PNorm = 35.4885, GNorm = 4.8640, lr_0 = 1.0202e-04
Loss = 1.4287e-03, PNorm = 35.4912, GNorm = 5.2276, lr_0 = 1.0202e-04
Loss = 1.2559e-03, PNorm = 35.4921, GNorm = 7.0435, lr_0 = 1.0202e-04
Loss = 1.1816e-03, PNorm = 35.4945, GNorm = 6.9628, lr_0 = 1.0202e-04
Loss = 1.1049e-03, PNorm = 35.4969, GNorm = 2.5938, lr_0 = 1.0202e-04
Loss = 1.2150e-03, PNorm = 35.4989, GNorm = 7.1083, lr_0 = 1.0202e-04
Loss = 1.4898e-03, PNorm = 35.5006, GNorm = 5.5126, lr_0 = 1.0202e-04
Validation rmse logP = 0.543991
Validation R2 logP = 0.913301
Epoch 33
Train function
Loss = 1.2073e-03, PNorm = 35.5017, GNorm = 3.3707, lr_0 = 1.0202e-04
Loss = 1.1424e-03, PNorm = 35.5018, GNorm = 7.3205, lr_0 = 1.0202e-04
Loss = 1.2931e-03, PNorm = 35.5036, GNorm = 3.4552, lr_0 = 1.0202e-04
Loss = 1.0057e-03, PNorm = 35.5063, GNorm = 1.7693, lr_0 = 1.0202e-04
Loss = 1.0028e-03, PNorm = 35.5082, GNorm = 2.9097, lr_0 = 1.0202e-04
Loss = 9.0259e-04, PNorm = 35.5092, GNorm = 1.7384, lr_0 = 1.0202e-04
Loss = 1.1825e-03, PNorm = 35.5116, GNorm = 5.3617, lr_0 = 1.0202e-04
Loss = 1.2089e-03, PNorm = 35.5136, GNorm = 3.3415, lr_0 = 1.0202e-04
Loss = 1.2221e-03, PNorm = 35.5174, GNorm = 4.7446, lr_0 = 1.0202e-04
Loss = 1.4576e-03, PNorm = 35.5198, GNorm = 8.3500, lr_0 = 1.0202e-04
Loss = 1.2716e-03, PNorm = 35.5213, GNorm = 4.9135, lr_0 = 1.0202e-04
Loss = 1.1786e-03, PNorm = 35.5230, GNorm = 5.5134, lr_0 = 1.0202e-04
Loss = 1.2013e-03, PNorm = 35.5245, GNorm = 6.5806, lr_0 = 1.0202e-04
Loss = 1.5105e-03, PNorm = 35.5261, GNorm = 12.2231, lr_0 = 1.0202e-04
Loss = 1.1736e-03, PNorm = 35.5275, GNorm = 2.9398, lr_0 = 1.0202e-04
Loss = 1.1849e-03, PNorm = 35.5291, GNorm = 2.4413, lr_0 = 1.0202e-04
Loss = 1.1078e-03, PNorm = 35.5314, GNorm = 2.8095, lr_0 = 1.0202e-04
Loss = 8.7221e-04, PNorm = 35.5327, GNorm = 4.2878, lr_0 = 1.0202e-04
Loss = 9.7051e-04, PNorm = 35.5339, GNorm = 4.1847, lr_0 = 1.0202e-04
Loss = 1.2267e-03, PNorm = 35.5355, GNorm = 1.7767, lr_0 = 1.0202e-04
Loss = 1.2094e-03, PNorm = 35.5372, GNorm = 2.6306, lr_0 = 1.0202e-04
Loss = 1.0210e-03, PNorm = 35.5388, GNorm = 1.3722, lr_0 = 1.0202e-04
Loss = 1.3126e-03, PNorm = 35.5409, GNorm = 3.7514, lr_0 = 1.0202e-04
Validation rmse logP = 0.537631
Validation R2 logP = 0.915316
Epoch 34
Train function
Loss = 1.0090e-03, PNorm = 35.5421, GNorm = 4.1618, lr_0 = 1.0202e-04
Loss = 1.0597e-03, PNorm = 35.5433, GNorm = 5.7506, lr_0 = 1.0202e-04
Loss = 1.0359e-03, PNorm = 35.5452, GNorm = 2.8033, lr_0 = 1.0202e-04
Loss = 1.2957e-03, PNorm = 35.5480, GNorm = 2.7792, lr_0 = 1.0202e-04
Loss = 1.0681e-03, PNorm = 35.5502, GNorm = 3.5424, lr_0 = 1.0202e-04
Loss = 1.2395e-03, PNorm = 35.5519, GNorm = 2.7999, lr_0 = 1.0202e-04
Loss = 9.4370e-04, PNorm = 35.5532, GNorm = 3.6983, lr_0 = 1.0202e-04
Loss = 8.9871e-04, PNorm = 35.5545, GNorm = 1.6138, lr_0 = 1.0202e-04
Loss = 1.2348e-03, PNorm = 35.5561, GNorm = 9.6911, lr_0 = 1.0202e-04
Loss = 1.2895e-03, PNorm = 35.5576, GNorm = 10.3137, lr_0 = 1.0202e-04
Loss = 1.2801e-03, PNorm = 35.5597, GNorm = 9.9453, lr_0 = 1.0202e-04
Loss = 1.1365e-03, PNorm = 35.5611, GNorm = 2.7165, lr_0 = 1.0202e-04
Loss = 1.0568e-03, PNorm = 35.5632, GNorm = 2.1279, lr_0 = 1.0202e-04
Loss = 1.2623e-03, PNorm = 35.5661, GNorm = 7.3126, lr_0 = 1.0202e-04
Loss = 1.0749e-03, PNorm = 35.5685, GNorm = 4.8549, lr_0 = 1.0202e-04
Loss = 1.2114e-03, PNorm = 35.5701, GNorm = 4.2752, lr_0 = 1.0202e-04
Loss = 9.2364e-04, PNorm = 35.5720, GNorm = 4.4043, lr_0 = 1.0202e-04
Loss = 1.1464e-03, PNorm = 35.5738, GNorm = 8.1454, lr_0 = 1.0202e-04
Loss = 1.3053e-03, PNorm = 35.5751, GNorm = 3.2330, lr_0 = 1.0202e-04
Loss = 1.3698e-03, PNorm = 35.5769, GNorm = 12.0802, lr_0 = 1.0202e-04
Loss = 1.4009e-03, PNorm = 35.5789, GNorm = 2.9876, lr_0 = 1.0202e-04
Loss = 9.3219e-04, PNorm = 35.5802, GNorm = 5.1130, lr_0 = 1.0202e-04
Validation rmse logP = 0.496040
Validation R2 logP = 0.927911
Epoch 35
Train function
Loss = 1.1457e-03, PNorm = 35.5824, GNorm = 2.9748, lr_0 = 1.0202e-04
Loss = 1.1491e-03, PNorm = 35.5833, GNorm = 5.9472, lr_0 = 1.0202e-04
Loss = 1.0900e-03, PNorm = 35.5847, GNorm = 3.3322, lr_0 = 1.0202e-04
Loss = 1.2569e-03, PNorm = 35.5854, GNorm = 9.9967, lr_0 = 1.0202e-04
Loss = 1.2329e-03, PNorm = 35.5876, GNorm = 5.4912, lr_0 = 1.0202e-04
Loss = 1.6094e-03, PNorm = 35.5891, GNorm = 6.7010, lr_0 = 1.0202e-04
Loss = 1.3459e-03, PNorm = 35.5910, GNorm = 5.9438, lr_0 = 1.0202e-04
Loss = 1.2116e-03, PNorm = 35.5937, GNorm = 3.2328, lr_0 = 1.0202e-04
Loss = 1.1467e-03, PNorm = 35.5965, GNorm = 2.1302, lr_0 = 1.0202e-04
Loss = 9.2819e-04, PNorm = 35.5989, GNorm = 2.6087, lr_0 = 1.0202e-04
Loss = 1.1070e-03, PNorm = 35.5998, GNorm = 3.5128, lr_0 = 1.0202e-04
Loss = 9.5650e-04, PNorm = 35.6024, GNorm = 2.8703, lr_0 = 1.0202e-04
Loss = 1.0125e-03, PNorm = 35.6044, GNorm = 9.5020, lr_0 = 1.0202e-04
Loss = 1.2170e-03, PNorm = 35.6056, GNorm = 9.2989, lr_0 = 1.0202e-04
Loss = 1.2268e-03, PNorm = 35.6068, GNorm = 9.3226, lr_0 = 1.0202e-04
Loss = 1.1046e-03, PNorm = 35.6088, GNorm = 3.4781, lr_0 = 1.0202e-04
Loss = 1.0743e-03, PNorm = 35.6108, GNorm = 3.0213, lr_0 = 1.0202e-04
Loss = 1.1403e-03, PNorm = 35.6126, GNorm = 5.1527, lr_0 = 1.0202e-04
Loss = 1.0376e-03, PNorm = 35.6144, GNorm = 8.6570, lr_0 = 1.0202e-04
Loss = 1.0329e-03, PNorm = 35.6163, GNorm = 1.8047, lr_0 = 1.0202e-04
Loss = 8.9684e-04, PNorm = 35.6177, GNorm = 1.6749, lr_0 = 1.0202e-04
Loss = 1.0494e-03, PNorm = 35.6203, GNorm = 1.6240, lr_0 = 1.0202e-04
Validation rmse logP = 0.510657
Validation R2 logP = 0.923601
Epoch 36
Train function
Loss = 9.4126e-04, PNorm = 35.6221, GNorm = 5.7619, lr_0 = 1.0202e-04
Loss = 1.1793e-03, PNorm = 35.6251, GNorm = 4.7224, lr_0 = 1.0202e-04
Loss = 9.5210e-04, PNorm = 35.6271, GNorm = 7.7626, lr_0 = 1.0202e-04
Loss = 8.9532e-04, PNorm = 35.6296, GNorm = 4.5230, lr_0 = 1.0202e-04
Loss = 8.5113e-04, PNorm = 35.6318, GNorm = 2.2568, lr_0 = 1.0202e-04
Loss = 1.2131e-03, PNorm = 35.6332, GNorm = 5.9528, lr_0 = 1.0202e-04
Loss = 1.4847e-03, PNorm = 35.6359, GNorm = 12.2361, lr_0 = 1.0202e-04
Loss = 1.0655e-03, PNorm = 35.6378, GNorm = 3.4081, lr_0 = 1.0202e-04
Loss = 1.3905e-03, PNorm = 35.6398, GNorm = 1.8718, lr_0 = 1.0202e-04
Loss = 1.0365e-03, PNorm = 35.6404, GNorm = 4.7666, lr_0 = 1.0202e-04
Loss = 1.1372e-03, PNorm = 35.6412, GNorm = 5.6734, lr_0 = 1.0202e-04
Loss = 1.0363e-03, PNorm = 35.6419, GNorm = 3.8939, lr_0 = 1.0202e-04
Loss = 1.0523e-03, PNorm = 35.6435, GNorm = 3.4086, lr_0 = 1.0202e-04
Loss = 1.0299e-03, PNorm = 35.6452, GNorm = 8.1462, lr_0 = 1.0202e-04
Loss = 9.9618e-04, PNorm = 35.6466, GNorm = 2.3904, lr_0 = 1.0202e-04
Loss = 1.0699e-03, PNorm = 35.6479, GNorm = 6.3178, lr_0 = 1.0202e-04
Loss = 1.0879e-03, PNorm = 35.6500, GNorm = 1.5325, lr_0 = 1.0202e-04
Loss = 1.0252e-03, PNorm = 35.6515, GNorm = 2.9870, lr_0 = 1.0202e-04
Loss = 1.1886e-03, PNorm = 35.6527, GNorm = 5.6130, lr_0 = 1.0202e-04
Loss = 1.0106e-03, PNorm = 35.6541, GNorm = 6.4724, lr_0 = 1.0202e-04
Loss = 1.0451e-03, PNorm = 35.6563, GNorm = 4.4075, lr_0 = 1.0202e-04
Loss = 1.1469e-03, PNorm = 35.6578, GNorm = 8.3979, lr_0 = 1.0202e-04
Loss = 1.4985e-03, PNorm = 35.6594, GNorm = 12.9279, lr_0 = 1.0202e-04
Loss = 1.7999e-03, PNorm = 35.6594, GNorm = 4.0901, lr_0 = 1.0202e-04
Validation rmse logP = 0.591166
Validation R2 logP = 0.897611
Epoch 37
Train function
Loss = 1.4477e-03, PNorm = 35.6614, GNorm = 2.6806, lr_0 = 1.0202e-04
Loss = 1.2897e-03, PNorm = 35.6633, GNorm = 5.9319, lr_0 = 1.0202e-04
Loss = 1.3963e-03, PNorm = 35.6654, GNorm = 4.2151, lr_0 = 1.0202e-04
Loss = 1.0781e-03, PNorm = 35.6678, GNorm = 8.1221, lr_0 = 1.0202e-04
Loss = 1.1557e-03, PNorm = 35.6698, GNorm = 3.5927, lr_0 = 1.0202e-04
Loss = 1.3761e-03, PNorm = 35.6724, GNorm = 6.8353, lr_0 = 1.0202e-04
Loss = 9.6968e-04, PNorm = 35.6740, GNorm = 4.6409, lr_0 = 1.0202e-04
Loss = 1.1618e-03, PNorm = 35.6745, GNorm = 5.2332, lr_0 = 1.0202e-04
Loss = 1.1213e-03, PNorm = 35.6762, GNorm = 9.4280, lr_0 = 1.0202e-04
Loss = 1.2786e-03, PNorm = 35.6776, GNorm = 9.6105, lr_0 = 1.0202e-04
Loss = 1.1695e-03, PNorm = 35.6794, GNorm = 4.0684, lr_0 = 1.0202e-04
Loss = 1.2333e-03, PNorm = 35.6804, GNorm = 8.1038, lr_0 = 1.0202e-04
Loss = 1.1375e-03, PNorm = 35.6814, GNorm = 5.0240, lr_0 = 1.0202e-04
Loss = 1.0722e-03, PNorm = 35.6832, GNorm = 4.7117, lr_0 = 1.0202e-04
Loss = 1.0807e-03, PNorm = 35.6847, GNorm = 7.3762, lr_0 = 1.0202e-04
Loss = 1.0254e-03, PNorm = 35.6868, GNorm = 2.6311, lr_0 = 1.0202e-04
Loss = 9.4176e-04, PNorm = 35.6880, GNorm = 4.2615, lr_0 = 1.0202e-04
Loss = 9.8063e-04, PNorm = 35.6911, GNorm = 1.3293, lr_0 = 1.0202e-04
Loss = 1.1478e-03, PNorm = 35.6929, GNorm = 8.1458, lr_0 = 1.0202e-04
Loss = 1.2208e-03, PNorm = 35.6951, GNorm = 10.7609, lr_0 = 1.0202e-04
Loss = 9.6777e-04, PNorm = 35.6960, GNorm = 2.5352, lr_0 = 1.0202e-04
Loss = 8.8119e-04, PNorm = 35.6976, GNorm = 1.2915, lr_0 = 1.0202e-04
Validation rmse logP = 0.528869
Validation R2 logP = 0.918054
Epoch 38
Train function
Loss = 1.0834e-03, PNorm = 35.6987, GNorm = 4.1417, lr_0 = 1.0202e-04
Loss = 1.3313e-03, PNorm = 35.7006, GNorm = 2.6362, lr_0 = 1.0202e-04
Loss = 1.0666e-03, PNorm = 35.7026, GNorm = 2.4214, lr_0 = 1.0202e-04
Loss = 1.0954e-03, PNorm = 35.7036, GNorm = 3.9622, lr_0 = 1.0202e-04
Loss = 9.6182e-04, PNorm = 35.7046, GNorm = 4.2130, lr_0 = 1.0202e-04
Loss = 1.0022e-03, PNorm = 35.7059, GNorm = 10.2295, lr_0 = 1.0202e-04
Loss = 1.0268e-03, PNorm = 35.7082, GNorm = 7.0192, lr_0 = 1.0202e-04
Loss = 1.0171e-03, PNorm = 35.7099, GNorm = 4.0624, lr_0 = 1.0202e-04
Loss = 9.8645e-04, PNorm = 35.7121, GNorm = 2.6648, lr_0 = 1.0202e-04
Loss = 8.7035e-04, PNorm = 35.7140, GNorm = 6.9065, lr_0 = 1.0202e-04
Loss = 9.6998e-04, PNorm = 35.7162, GNorm = 1.4944, lr_0 = 1.0202e-04
Loss = 9.3598e-04, PNorm = 35.7185, GNorm = 4.3458, lr_0 = 1.0202e-04
Loss = 9.0600e-04, PNorm = 35.7208, GNorm = 10.8101, lr_0 = 1.0202e-04
Loss = 9.9711e-04, PNorm = 35.7222, GNorm = 5.5364, lr_0 = 1.0202e-04
Loss = 1.0931e-03, PNorm = 35.7233, GNorm = 7.3046, lr_0 = 1.0202e-04
Loss = 8.9262e-04, PNorm = 35.7259, GNorm = 2.0915, lr_0 = 1.0202e-04
Loss = 1.3102e-03, PNorm = 35.7276, GNorm = 4.6728, lr_0 = 1.0202e-04
Loss = 1.1847e-03, PNorm = 35.7296, GNorm = 5.5717, lr_0 = 1.0202e-04
Loss = 1.0534e-03, PNorm = 35.7320, GNorm = 1.8979, lr_0 = 1.0202e-04
Loss = 1.0438e-03, PNorm = 35.7334, GNorm = 11.8705, lr_0 = 1.0202e-04
Loss = 1.2901e-03, PNorm = 35.7340, GNorm = 1.9817, lr_0 = 1.0202e-04
Loss = 1.2032e-03, PNorm = 35.7352, GNorm = 2.6179, lr_0 = 1.0202e-04
Validation rmse logP = 0.601094
Validation R2 logP = 0.894144
Epoch 39
Train function
Loss = 1.4593e-03, PNorm = 35.7378, GNorm = 4.7971, lr_0 = 1.0202e-04
Loss = 1.1834e-03, PNorm = 35.7404, GNorm = 9.1501, lr_0 = 1.0202e-04
Loss = 1.1675e-03, PNorm = 35.7429, GNorm = 5.8446, lr_0 = 1.0202e-04
Loss = 1.2256e-03, PNorm = 35.7454, GNorm = 6.4731, lr_0 = 1.0202e-04
Loss = 1.3612e-03, PNorm = 35.7468, GNorm = 1.9347, lr_0 = 1.0202e-04
Loss = 1.1829e-03, PNorm = 35.7485, GNorm = 5.9007, lr_0 = 1.0202e-04
Loss = 1.1580e-03, PNorm = 35.7499, GNorm = 2.2038, lr_0 = 1.0202e-04
Loss = 8.1099e-04, PNorm = 35.7512, GNorm = 4.4400, lr_0 = 1.0202e-04
Loss = 1.1934e-03, PNorm = 35.7533, GNorm = 3.8936, lr_0 = 1.0202e-04
Loss = 1.1057e-03, PNorm = 35.7547, GNorm = 3.6845, lr_0 = 1.0202e-04
Loss = 9.2331e-04, PNorm = 35.7554, GNorm = 1.7008, lr_0 = 1.0202e-04
Loss = 9.2235e-04, PNorm = 35.7574, GNorm = 4.3527, lr_0 = 1.0202e-04
Loss = 1.1446e-03, PNorm = 35.7585, GNorm = 3.6560, lr_0 = 1.0202e-04
Loss = 9.7473e-04, PNorm = 35.7596, GNorm = 7.7063, lr_0 = 1.0202e-04
Loss = 1.0131e-03, PNorm = 35.7615, GNorm = 1.7021, lr_0 = 1.0202e-04
Loss = 9.8746e-04, PNorm = 35.7633, GNorm = 6.7297, lr_0 = 1.0202e-04
Loss = 9.8414e-04, PNorm = 35.7645, GNorm = 4.4352, lr_0 = 1.0202e-04
Loss = 9.9799e-04, PNorm = 35.7665, GNorm = 4.9685, lr_0 = 1.0202e-04
Loss = 9.7782e-04, PNorm = 35.7678, GNorm = 5.1079, lr_0 = 1.0202e-04
Loss = 1.2975e-03, PNorm = 35.7703, GNorm = 1.4800, lr_0 = 1.0202e-04
Loss = 1.1545e-03, PNorm = 35.7732, GNorm = 2.3446, lr_0 = 1.0202e-04
Loss = 8.7037e-04, PNorm = 35.7747, GNorm = 3.1720, lr_0 = 1.0202e-04
Loss = 8.9724e-04, PNorm = 35.7768, GNorm = 4.7197, lr_0 = 1.0202e-04
Validation rmse logP = 0.494919
Validation R2 logP = 0.928237
Epoch 40
Train function
Loss = 1.0445e-03, PNorm = 35.7783, GNorm = 1.8524, lr_0 = 1.0202e-04
Loss = 8.3132e-04, PNorm = 35.7803, GNorm = 2.4441, lr_0 = 1.0202e-04
Loss = 1.0575e-03, PNorm = 35.7821, GNorm = 2.5234, lr_0 = 1.0202e-04
Loss = 7.7916e-04, PNorm = 35.7841, GNorm = 5.6586, lr_0 = 1.0202e-04
Loss = 1.0362e-03, PNorm = 35.7855, GNorm = 1.4177, lr_0 = 1.0202e-04
Loss = 1.1135e-03, PNorm = 35.7877, GNorm = 2.3904, lr_0 = 1.0202e-04
Loss = 8.3053e-04, PNorm = 35.7893, GNorm = 4.1426, lr_0 = 1.0202e-04
Loss = 9.2005e-04, PNorm = 35.7909, GNorm = 4.5140, lr_0 = 1.0202e-04
Loss = 1.0348e-03, PNorm = 35.7932, GNorm = 2.9203, lr_0 = 1.0202e-04
Loss = 9.1801e-04, PNorm = 35.7949, GNorm = 7.9817, lr_0 = 1.0202e-04
Loss = 8.8542e-04, PNorm = 35.7971, GNorm = 4.7221, lr_0 = 1.0202e-04
Loss = 1.0472e-03, PNorm = 35.7982, GNorm = 3.3529, lr_0 = 1.0202e-04
Loss = 1.1400e-03, PNorm = 35.8002, GNorm = 6.4501, lr_0 = 1.0202e-04
Loss = 1.2763e-03, PNorm = 35.8021, GNorm = 10.0600, lr_0 = 1.0202e-04
Loss = 1.3656e-03, PNorm = 35.8045, GNorm = 6.7973, lr_0 = 1.0202e-04
Loss = 1.2691e-03, PNorm = 35.8070, GNorm = 4.5199, lr_0 = 1.0202e-04
Loss = 9.5594e-04, PNorm = 35.8083, GNorm = 6.5636, lr_0 = 1.0202e-04
Loss = 1.0406e-03, PNorm = 35.8087, GNorm = 7.9929, lr_0 = 1.0202e-04
Loss = 1.1235e-03, PNorm = 35.8114, GNorm = 6.7362, lr_0 = 1.0202e-04
Loss = 1.0603e-03, PNorm = 35.8129, GNorm = 2.4217, lr_0 = 1.0202e-04
Loss = 9.1870e-04, PNorm = 35.8145, GNorm = 7.7506, lr_0 = 1.0202e-04
Loss = 1.1562e-03, PNorm = 35.8164, GNorm = 8.4493, lr_0 = 1.0202e-04
Validation rmse logP = 0.495879
Validation R2 logP = 0.927958
Epoch 41
Train function
Loss = 8.1644e-04, PNorm = 35.8182, GNorm = 6.2538, lr_0 = 1.0202e-04
Loss = 1.0848e-03, PNorm = 35.8202, GNorm = 2.9968, lr_0 = 1.0202e-04
Loss = 8.6840e-04, PNorm = 35.8222, GNorm = 3.3785, lr_0 = 1.0202e-04
Loss = 8.5690e-04, PNorm = 35.8244, GNorm = 1.4980, lr_0 = 1.0202e-04
Loss = 8.2730e-04, PNorm = 35.8273, GNorm = 2.6541, lr_0 = 1.0202e-04
Loss = 1.1523e-03, PNorm = 35.8294, GNorm = 4.7303, lr_0 = 1.0202e-04
Loss = 9.2865e-04, PNorm = 35.8305, GNorm = 5.1980, lr_0 = 1.0202e-04
Loss = 8.4794e-04, PNorm = 35.8321, GNorm = 2.4460, lr_0 = 1.0202e-04
Loss = 1.0510e-03, PNorm = 35.8337, GNorm = 12.7737, lr_0 = 1.0202e-04
Loss = 1.1942e-03, PNorm = 35.8355, GNorm = 5.0735, lr_0 = 1.0202e-04
Loss = 9.8945e-04, PNorm = 35.8375, GNorm = 9.9776, lr_0 = 1.0202e-04
Loss = 1.3727e-03, PNorm = 35.8393, GNorm = 1.7769, lr_0 = 1.0202e-04
Loss = 9.9945e-04, PNorm = 35.8398, GNorm = 2.8107, lr_0 = 1.0202e-04
Loss = 9.7561e-04, PNorm = 35.8424, GNorm = 6.6416, lr_0 = 1.0202e-04
Loss = 1.1640e-03, PNorm = 35.8434, GNorm = 2.2695, lr_0 = 1.0202e-04
Loss = 1.4024e-03, PNorm = 35.8454, GNorm = 1.1271, lr_0 = 1.0202e-04
Loss = 1.1197e-03, PNorm = 35.8465, GNorm = 3.2831, lr_0 = 1.0202e-04
Loss = 8.7295e-04, PNorm = 35.8481, GNorm = 6.0868, lr_0 = 1.0202e-04
Loss = 1.0116e-03, PNorm = 35.8495, GNorm = 3.3641, lr_0 = 1.0202e-04
Loss = 1.0204e-03, PNorm = 35.8509, GNorm = 4.6329, lr_0 = 1.0202e-04
Loss = 8.6012e-04, PNorm = 35.8529, GNorm = 1.8296, lr_0 = 1.0202e-04
Loss = 8.3729e-04, PNorm = 35.8546, GNorm = 4.0959, lr_0 = 1.0202e-04
Validation rmse logP = 0.499493
Validation R2 logP = 0.926904
Epoch 42
Train function
Loss = 6.4809e-04, PNorm = 35.8562, GNorm = 2.7452, lr_0 = 1.0202e-04
Loss = 1.2082e-03, PNorm = 35.8588, GNorm = 3.0187, lr_0 = 1.0202e-04
Loss = 1.0046e-03, PNorm = 35.8603, GNorm = 1.4177, lr_0 = 1.0202e-04
Loss = 1.0648e-03, PNorm = 35.8625, GNorm = 5.0430, lr_0 = 1.0202e-04
Loss = 7.9273e-04, PNorm = 35.8637, GNorm = 4.5919, lr_0 = 1.0202e-04
Loss = 1.4394e-03, PNorm = 35.8660, GNorm = 12.0614, lr_0 = 1.0202e-04
Loss = 9.4248e-04, PNorm = 35.8679, GNorm = 0.9821, lr_0 = 1.0202e-04
Loss = 1.0790e-03, PNorm = 35.8700, GNorm = 2.4035, lr_0 = 1.0202e-04
Loss = 1.0072e-03, PNorm = 35.8721, GNorm = 2.7174, lr_0 = 1.0202e-04
Loss = 9.4871e-04, PNorm = 35.8739, GNorm = 2.7815, lr_0 = 1.0202e-04
Loss = 9.7124e-04, PNorm = 35.8760, GNorm = 1.5746, lr_0 = 1.0202e-04
Loss = 9.9194e-04, PNorm = 35.8775, GNorm = 9.9791, lr_0 = 1.0202e-04
Loss = 9.6096e-04, PNorm = 35.8798, GNorm = 3.1227, lr_0 = 1.0202e-04
Loss = 9.4546e-04, PNorm = 35.8815, GNorm = 1.1391, lr_0 = 1.0202e-04
Loss = 1.0464e-03, PNorm = 35.8832, GNorm = 9.8534, lr_0 = 1.0202e-04
Loss = 1.1224e-03, PNorm = 35.8843, GNorm = 7.7752, lr_0 = 1.0202e-04
Loss = 9.8216e-04, PNorm = 35.8859, GNorm = 1.9972, lr_0 = 1.0202e-04
Loss = 9.3429e-04, PNorm = 35.8876, GNorm = 9.9501, lr_0 = 1.0202e-04
Loss = 1.2955e-03, PNorm = 35.8880, GNorm = 9.1236, lr_0 = 1.0202e-04
Loss = 1.1724e-03, PNorm = 35.8903, GNorm = 3.4381, lr_0 = 1.0202e-04
Loss = 1.3279e-03, PNorm = 35.8914, GNorm = 1.6232, lr_0 = 1.0202e-04
Loss = 9.6068e-04, PNorm = 35.8927, GNorm = 8.7494, lr_0 = 1.0202e-04
Loss = 1.1143e-03, PNorm = 35.8951, GNorm = 13.9069, lr_0 = 1.0202e-04
Validation rmse logP = 0.641818
Validation R2 logP = 0.879314
Epoch 43
Train function
Loss = 1.5440e-03, PNorm = 35.8962, GNorm = 12.1268, lr_0 = 1.0202e-04
Loss = 9.8096e-04, PNorm = 35.8992, GNorm = 6.1660, lr_0 = 1.0202e-04
Loss = 9.4788e-04, PNorm = 35.9010, GNorm = 3.9406, lr_0 = 1.0202e-04
Loss = 7.9406e-04, PNorm = 35.9030, GNorm = 4.5375, lr_0 = 1.0202e-04
Loss = 8.3982e-04, PNorm = 35.9046, GNorm = 3.7978, lr_0 = 1.0202e-04
Loss = 7.0103e-04, PNorm = 35.9065, GNorm = 3.7104, lr_0 = 1.0202e-04
Loss = 9.9824e-04, PNorm = 35.9080, GNorm = 1.4116, lr_0 = 1.0202e-04
Loss = 8.1828e-04, PNorm = 35.9101, GNorm = 2.2564, lr_0 = 1.0202e-04
Loss = 8.9785e-04, PNorm = 35.9121, GNorm = 2.0286, lr_0 = 1.0202e-04
Loss = 9.3985e-04, PNorm = 35.9136, GNorm = 1.4211, lr_0 = 1.0202e-04
Loss = 9.7024e-04, PNorm = 35.9162, GNorm = 5.8170, lr_0 = 1.0202e-04
Loss = 8.7875e-04, PNorm = 35.9180, GNorm = 5.8854, lr_0 = 1.0202e-04
Loss = 9.4477e-04, PNorm = 35.9203, GNorm = 2.2456, lr_0 = 1.0202e-04
Loss = 6.8238e-04, PNorm = 35.9217, GNorm = 2.8930, lr_0 = 1.0202e-04
Loss = 1.0889e-03, PNorm = 35.9240, GNorm = 1.5109, lr_0 = 1.0202e-04
Loss = 1.0613e-03, PNorm = 35.9261, GNorm = 2.1210, lr_0 = 1.0202e-04
Loss = 9.4992e-04, PNorm = 35.9273, GNorm = 1.6668, lr_0 = 1.0202e-04
Loss = 9.4062e-04, PNorm = 35.9293, GNorm = 3.8779, lr_0 = 1.0202e-04
Loss = 9.8638e-04, PNorm = 35.9303, GNorm = 2.3533, lr_0 = 1.0202e-04
Loss = 1.1149e-03, PNorm = 35.9318, GNorm = 1.3271, lr_0 = 1.0202e-04
Loss = 1.1919e-03, PNorm = 35.9335, GNorm = 7.5452, lr_0 = 1.0202e-04
Loss = 1.0190e-03, PNorm = 35.9349, GNorm = 2.0157, lr_0 = 1.0202e-04
Validation rmse logP = 0.519733
Validation R2 logP = 0.920861
Epoch 44
Train function
Loss = 1.1562e-03, PNorm = 35.9361, GNorm = 5.4317, lr_0 = 1.0202e-04
Loss = 1.0946e-03, PNorm = 35.9381, GNorm = 2.6949, lr_0 = 1.0202e-04
Loss = 9.1462e-04, PNorm = 35.9402, GNorm = 1.6282, lr_0 = 1.0202e-04
Loss = 1.0406e-03, PNorm = 35.9421, GNorm = 2.3252, lr_0 = 1.0202e-04
Loss = 9.8650e-04, PNorm = 35.9439, GNorm = 1.3890, lr_0 = 1.0202e-04
Loss = 9.0302e-04, PNorm = 35.9457, GNorm = 1.5382, lr_0 = 1.0202e-04
Loss = 8.0189e-04, PNorm = 35.9479, GNorm = 8.1070, lr_0 = 1.0202e-04
Loss = 8.1520e-04, PNorm = 35.9496, GNorm = 4.5102, lr_0 = 1.0202e-04
Loss = 7.6107e-04, PNorm = 35.9514, GNorm = 3.5099, lr_0 = 1.0202e-04
Loss = 8.8916e-04, PNorm = 35.9523, GNorm = 2.8868, lr_0 = 1.0202e-04
Loss = 8.1921e-04, PNorm = 35.9540, GNorm = 5.4497, lr_0 = 1.0202e-04
Loss = 7.0809e-04, PNorm = 35.9553, GNorm = 2.8271, lr_0 = 1.0202e-04
Loss = 8.4602e-04, PNorm = 35.9567, GNorm = 1.4266, lr_0 = 1.0202e-04
Loss = 8.7718e-04, PNorm = 35.9581, GNorm = 9.3276, lr_0 = 1.0202e-04
Loss = 1.4076e-03, PNorm = 35.9591, GNorm = 9.3872, lr_0 = 1.0202e-04
Loss = 1.2833e-03, PNorm = 35.9606, GNorm = 5.8086, lr_0 = 1.0202e-04
Loss = 1.2562e-03, PNorm = 35.9622, GNorm = 1.7893, lr_0 = 1.0202e-04
Loss = 1.0571e-03, PNorm = 35.9638, GNorm = 2.3072, lr_0 = 1.0202e-04
Loss = 1.0439e-03, PNorm = 35.9662, GNorm = 5.2395, lr_0 = 1.0202e-04
Loss = 1.0286e-03, PNorm = 35.9689, GNorm = 2.3931, lr_0 = 1.0202e-04
Loss = 8.7864e-04, PNorm = 35.9713, GNorm = 5.5747, lr_0 = 1.0202e-04
Loss = 7.7959e-04, PNorm = 35.9731, GNorm = 1.5477, lr_0 = 1.0202e-04
Validation rmse logP = 0.531791
Validation R2 logP = 0.917146
Epoch 45
Train function
Loss = 1.0526e-03, PNorm = 35.9742, GNorm = 6.9668, lr_0 = 1.0202e-04
Loss = 8.1502e-04, PNorm = 35.9756, GNorm = 2.8538, lr_0 = 1.0202e-04
Loss = 1.0319e-03, PNorm = 35.9771, GNorm = 1.4964, lr_0 = 1.0202e-04
Loss = 1.1362e-03, PNorm = 35.9789, GNorm = 3.0219, lr_0 = 1.0202e-04
Loss = 7.9279e-04, PNorm = 35.9811, GNorm = 1.6371, lr_0 = 1.0202e-04
Loss = 7.9606e-04, PNorm = 35.9829, GNorm = 2.5444, lr_0 = 1.0202e-04
Loss = 1.0156e-03, PNorm = 35.9842, GNorm = 6.8157, lr_0 = 1.0202e-04
Loss = 8.9975e-04, PNorm = 35.9851, GNorm = 4.1237, lr_0 = 1.0202e-04
Loss = 1.0909e-03, PNorm = 35.9864, GNorm = 3.9375, lr_0 = 1.0202e-04
Loss = 1.1644e-03, PNorm = 35.9892, GNorm = 3.3423, lr_0 = 1.0202e-04
Loss = 9.4972e-04, PNorm = 35.9915, GNorm = 2.2661, lr_0 = 1.0202e-04
Loss = 8.2321e-04, PNorm = 35.9932, GNorm = 3.0137, lr_0 = 1.0202e-04
Loss = 9.1627e-04, PNorm = 35.9949, GNorm = 3.1349, lr_0 = 1.0202e-04
Loss = 7.7492e-04, PNorm = 35.9972, GNorm = 5.5596, lr_0 = 1.0202e-04
Loss = 1.0371e-03, PNorm = 36.0001, GNorm = 2.4061, lr_0 = 1.0202e-04
Loss = 7.8580e-04, PNorm = 36.0020, GNorm = 1.5736, lr_0 = 1.0202e-04
Loss = 9.0388e-04, PNorm = 36.0037, GNorm = 3.9461, lr_0 = 1.0202e-04
Loss = 7.6697e-04, PNorm = 36.0051, GNorm = 4.1264, lr_0 = 1.0202e-04
Loss = 7.9516e-04, PNorm = 36.0070, GNorm = 3.2699, lr_0 = 1.0202e-04
Loss = 9.2001e-04, PNorm = 36.0082, GNorm = 6.9459, lr_0 = 1.0202e-04
Loss = 1.2641e-03, PNorm = 36.0098, GNorm = 7.3129, lr_0 = 1.0202e-04
Loss = 7.7450e-04, PNorm = 36.0119, GNorm = 4.1725, lr_0 = 1.0202e-04
Loss = 1.0737e-03, PNorm = 36.0121, GNorm = 2.0338, lr_0 = 1.0202e-04
Validation rmse logP = 0.581868
Validation R2 logP = 0.900807
Epoch 46
Train function
Loss = 1.1500e-03, PNorm = 36.0131, GNorm = 6.9619, lr_0 = 1.0202e-04
Loss = 1.0561e-03, PNorm = 36.0154, GNorm = 1.3082, lr_0 = 1.0202e-04
Loss = 1.0495e-03, PNorm = 36.0179, GNorm = 2.1607, lr_0 = 1.0202e-04
Loss = 1.0133e-03, PNorm = 36.0202, GNorm = 7.4492, lr_0 = 1.0202e-04
Loss = 1.1251e-03, PNorm = 36.0215, GNorm = 7.5940, lr_0 = 1.0202e-04
Loss = 1.0091e-03, PNorm = 36.0236, GNorm = 4.7335, lr_0 = 1.0202e-04
Loss = 9.7133e-04, PNorm = 36.0264, GNorm = 4.1542, lr_0 = 1.0202e-04
Loss = 1.0276e-03, PNorm = 36.0286, GNorm = 3.2766, lr_0 = 1.0202e-04
Loss = 1.0628e-03, PNorm = 36.0298, GNorm = 7.8080, lr_0 = 1.0202e-04
Loss = 8.6051e-04, PNorm = 36.0318, GNorm = 2.7811, lr_0 = 1.0202e-04
Loss = 7.8938e-04, PNorm = 36.0328, GNorm = 1.7232, lr_0 = 1.0202e-04
Loss = 8.6911e-04, PNorm = 36.0348, GNorm = 1.7068, lr_0 = 1.0202e-04
Loss = 8.8158e-04, PNorm = 36.0373, GNorm = 2.6735, lr_0 = 1.0202e-04
Loss = 8.7611e-04, PNorm = 36.0392, GNorm = 4.8662, lr_0 = 1.0202e-04
Loss = 7.9989e-04, PNorm = 36.0405, GNorm = 6.2776, lr_0 = 1.0202e-04
Loss = 1.0965e-03, PNorm = 36.0412, GNorm = 2.0841, lr_0 = 1.0202e-04
Loss = 9.9947e-04, PNorm = 36.0423, GNorm = 1.6731, lr_0 = 1.0202e-04
Loss = 8.9286e-04, PNorm = 36.0432, GNorm = 1.1191, lr_0 = 1.0202e-04
Loss = 7.9553e-04, PNorm = 36.0447, GNorm = 1.5929, lr_0 = 1.0202e-04
Loss = 9.9075e-04, PNorm = 36.0467, GNorm = 6.3095, lr_0 = 1.0202e-04
Loss = 1.1505e-03, PNorm = 36.0484, GNorm = 7.0107, lr_0 = 1.0202e-04
Loss = 1.2486e-03, PNorm = 36.0507, GNorm = 9.8726, lr_0 = 1.0202e-04
Validation rmse logP = 0.491486
Validation R2 logP = 0.929229
Epoch 47
Train function
Loss = 7.8984e-04, PNorm = 36.0523, GNorm = 2.1284, lr_0 = 1.0202e-04
Loss = 7.8736e-04, PNorm = 36.0540, GNorm = 4.4219, lr_0 = 1.0202e-04
Loss = 8.0411e-04, PNorm = 36.0561, GNorm = 2.1009, lr_0 = 1.0202e-04
Loss = 7.9750e-04, PNorm = 36.0577, GNorm = 4.5906, lr_0 = 1.0202e-04
Loss = 8.3293e-04, PNorm = 36.0588, GNorm = 5.4469, lr_0 = 1.0202e-04
Loss = 8.6440e-04, PNorm = 36.0601, GNorm = 2.3381, lr_0 = 1.0202e-04
Loss = 8.8731e-04, PNorm = 36.0621, GNorm = 3.3746, lr_0 = 1.0202e-04
Loss = 1.2713e-03, PNorm = 36.0637, GNorm = 5.1830, lr_0 = 1.0202e-04
Loss = 9.4132e-04, PNorm = 36.0646, GNorm = 2.6984, lr_0 = 1.0202e-04
Loss = 8.6937e-04, PNorm = 36.0671, GNorm = 5.9807, lr_0 = 1.0202e-04
Loss = 8.3215e-04, PNorm = 36.0696, GNorm = 1.5209, lr_0 = 1.0202e-04
Loss = 9.0352e-04, PNorm = 36.0721, GNorm = 2.4345, lr_0 = 1.0202e-04
Loss = 8.3554e-04, PNorm = 36.0744, GNorm = 4.2496, lr_0 = 1.0202e-04
Loss = 6.1864e-04, PNorm = 36.0764, GNorm = 1.9237, lr_0 = 1.0202e-04
Loss = 9.5788e-04, PNorm = 36.0780, GNorm = 1.4938, lr_0 = 1.0202e-04
Loss = 8.7535e-04, PNorm = 36.0796, GNorm = 2.3176, lr_0 = 1.0202e-04
Loss = 1.1132e-03, PNorm = 36.0806, GNorm = 7.5918, lr_0 = 1.0202e-04
Loss = 8.3942e-04, PNorm = 36.0821, GNorm = 1.5286, lr_0 = 1.0202e-04
Loss = 7.2139e-04, PNorm = 36.0839, GNorm = 4.1892, lr_0 = 1.0202e-04
Loss = 8.2649e-04, PNorm = 36.0855, GNorm = 6.3752, lr_0 = 1.0202e-04
Loss = 8.1145e-04, PNorm = 36.0863, GNorm = 2.0120, lr_0 = 1.0202e-04
Loss = 9.5547e-04, PNorm = 36.0878, GNorm = 2.0497, lr_0 = 1.0202e-04
Validation rmse logP = 0.502833
Validation R2 logP = 0.925923
Epoch 48
Train function
Loss = 1.6558e-03, PNorm = 36.0898, GNorm = 8.2468, lr_0 = 1.0202e-04
Loss = 8.5602e-04, PNorm = 36.0912, GNorm = 6.0275, lr_0 = 1.0202e-04
Loss = 7.7314e-04, PNorm = 36.0930, GNorm = 2.7816, lr_0 = 1.0202e-04
Loss = 9.5147e-04, PNorm = 36.0949, GNorm = 6.4213, lr_0 = 1.0202e-04
Loss = 9.2562e-04, PNorm = 36.0968, GNorm = 2.7681, lr_0 = 1.0202e-04
Loss = 7.6727e-04, PNorm = 36.0983, GNorm = 1.2858, lr_0 = 1.0202e-04
Loss = 7.1567e-04, PNorm = 36.0998, GNorm = 2.4009, lr_0 = 1.0202e-04
Loss = 7.9299e-04, PNorm = 36.1023, GNorm = 1.9578, lr_0 = 1.0202e-04
Loss = 9.5466e-04, PNorm = 36.1048, GNorm = 2.7101, lr_0 = 1.0202e-04
Loss = 6.6655e-04, PNorm = 36.1071, GNorm = 1.6990, lr_0 = 1.0202e-04
Loss = 1.0804e-03, PNorm = 36.1082, GNorm = 1.8334, lr_0 = 1.0202e-04
Loss = 1.1420e-03, PNorm = 36.1100, GNorm = 14.2145, lr_0 = 1.0202e-04
Loss = 1.2175e-03, PNorm = 36.1120, GNorm = 10.2265, lr_0 = 1.0202e-04
Loss = 1.0842e-03, PNorm = 36.1138, GNorm = 4.3497, lr_0 = 1.0202e-04
Loss = 9.2560e-04, PNorm = 36.1147, GNorm = 2.1595, lr_0 = 1.0202e-04
Loss = 9.0053e-04, PNorm = 36.1157, GNorm = 2.1250, lr_0 = 1.0202e-04
Loss = 1.0421e-03, PNorm = 36.1167, GNorm = 5.8152, lr_0 = 1.0202e-04
Loss = 8.2309e-04, PNorm = 36.1184, GNorm = 3.9795, lr_0 = 1.0202e-04
Loss = 8.6772e-04, PNorm = 36.1193, GNorm = 1.2999, lr_0 = 1.0202e-04
Loss = 7.7054e-04, PNorm = 36.1210, GNorm = 4.7668, lr_0 = 1.0202e-04
Loss = 9.1442e-04, PNorm = 36.1224, GNorm = 4.7271, lr_0 = 1.0202e-04
Loss = 7.9972e-04, PNorm = 36.1241, GNorm = 5.5574, lr_0 = 1.0202e-04
Loss = 8.6152e-04, PNorm = 36.1254, GNorm = 5.2847, lr_0 = 1.0202e-04
Validation rmse logP = 0.527797
Validation R2 logP = 0.918386
Epoch 49
Train function
Loss = 1.1249e-03, PNorm = 36.1256, GNorm = 8.6106, lr_0 = 1.0202e-04
Loss = 1.1916e-03, PNorm = 36.1284, GNorm = 5.7618, lr_0 = 1.0202e-04
Loss = 8.5630e-04, PNorm = 36.1306, GNorm = 4.8156, lr_0 = 1.0202e-04
Loss = 8.4088e-04, PNorm = 36.1329, GNorm = 4.6748, lr_0 = 1.0202e-04
Loss = 7.3809e-04, PNorm = 36.1352, GNorm = 2.8950, lr_0 = 1.0202e-04
Loss = 8.2832e-04, PNorm = 36.1361, GNorm = 1.1259, lr_0 = 1.0202e-04
Loss = 7.7675e-04, PNorm = 36.1380, GNorm = 2.1141, lr_0 = 1.0202e-04
Loss = 7.1887e-04, PNorm = 36.1396, GNorm = 2.4438, lr_0 = 1.0202e-04
Loss = 8.6577e-04, PNorm = 36.1410, GNorm = 1.8948, lr_0 = 1.0202e-04
Loss = 8.8014e-04, PNorm = 36.1423, GNorm = 1.8152, lr_0 = 1.0202e-04
Loss = 8.0585e-04, PNorm = 36.1434, GNorm = 4.7600, lr_0 = 1.0202e-04
Loss = 7.5825e-04, PNorm = 36.1442, GNorm = 2.0226, lr_0 = 1.0202e-04
Loss = 8.8809e-04, PNorm = 36.1457, GNorm = 4.0873, lr_0 = 1.0202e-04
Loss = 9.2558e-04, PNorm = 36.1480, GNorm = 3.4935, lr_0 = 1.0202e-04
Loss = 8.8476e-04, PNorm = 36.1501, GNorm = 7.2439, lr_0 = 1.0202e-04
Loss = 8.6410e-04, PNorm = 36.1518, GNorm = 1.5793, lr_0 = 1.0202e-04
Loss = 8.5419e-04, PNorm = 36.1543, GNorm = 1.5572, lr_0 = 1.0202e-04
Loss = 8.0197e-04, PNorm = 36.1555, GNorm = 2.7019, lr_0 = 1.0202e-04
Loss = 9.2810e-04, PNorm = 36.1574, GNorm = 3.5991, lr_0 = 1.0202e-04
Loss = 9.6832e-04, PNorm = 36.1588, GNorm = 1.7598, lr_0 = 1.0202e-04
Loss = 9.1984e-04, PNorm = 36.1597, GNorm = 1.2502, lr_0 = 1.0202e-04
Loss = 8.5337e-04, PNorm = 36.1613, GNorm = 1.9643, lr_0 = 1.0202e-04
Validation rmse logP = 0.493824
Validation R2 logP = 0.928554
Epoch 50
Train function
Loss = 1.0354e-03, PNorm = 36.1632, GNorm = 7.2087, lr_0 = 1.0202e-04
Loss = 8.8327e-04, PNorm = 36.1648, GNorm = 5.7901, lr_0 = 1.0202e-04
Loss = 1.1235e-03, PNorm = 36.1663, GNorm = 8.1963, lr_0 = 1.0202e-04
Loss = 7.8335e-04, PNorm = 36.1676, GNorm = 5.0008, lr_0 = 1.0202e-04
Loss = 7.2789e-04, PNorm = 36.1695, GNorm = 3.9096, lr_0 = 1.0202e-04
Loss = 7.7836e-04, PNorm = 36.1708, GNorm = 5.5417, lr_0 = 1.0202e-04
Loss = 9.3561e-04, PNorm = 36.1720, GNorm = 4.3345, lr_0 = 1.0202e-04
Loss = 7.7197e-04, PNorm = 36.1737, GNorm = 3.2651, lr_0 = 1.0202e-04
Loss = 8.0894e-04, PNorm = 36.1753, GNorm = 1.7353, lr_0 = 1.0202e-04
Loss = 8.6623e-04, PNorm = 36.1766, GNorm = 3.9705, lr_0 = 1.0202e-04
Loss = 9.0903e-04, PNorm = 36.1790, GNorm = 6.7439, lr_0 = 1.0202e-04
Loss = 9.5811e-04, PNorm = 36.1808, GNorm = 12.0212, lr_0 = 1.0202e-04
Loss = 1.1776e-03, PNorm = 36.1828, GNorm = 6.7818, lr_0 = 1.0202e-04
Loss = 9.6897e-04, PNorm = 36.1836, GNorm = 7.3757, lr_0 = 1.0202e-04
Loss = 7.6357e-04, PNorm = 36.1846, GNorm = 3.7565, lr_0 = 1.0202e-04
Loss = 8.9711e-04, PNorm = 36.1865, GNorm = 1.3891, lr_0 = 1.0202e-04
Loss = 8.0023e-04, PNorm = 36.1886, GNorm = 9.1808, lr_0 = 1.0202e-04
Loss = 8.7189e-04, PNorm = 36.1898, GNorm = 4.9712, lr_0 = 1.0202e-04
Loss = 9.3388e-04, PNorm = 36.1922, GNorm = 5.3358, lr_0 = 1.0202e-04
Loss = 9.6578e-04, PNorm = 36.1945, GNorm = 5.9512, lr_0 = 1.0202e-04
Loss = 1.1846e-03, PNorm = 36.1966, GNorm = 8.7087, lr_0 = 1.0202e-04
Loss = 1.0495e-03, PNorm = 36.1989, GNorm = 6.3403, lr_0 = 1.0202e-04
Loss = 8.8181e-04, PNorm = 36.2006, GNorm = 3.4758, lr_0 = 1.0202e-04
Loss = 7.1885e-04, PNorm = 36.2008, GNorm = 2.9090, lr_0 = 1.0202e-04
Validation rmse logP = 0.480829
Validation R2 logP = 0.932265
Epoch 51
Train function
Loss = 7.1186e-04, PNorm = 36.2026, GNorm = 4.2225, lr_0 = 1.0202e-04
Loss = 8.7978e-04, PNorm = 36.2039, GNorm = 2.5283, lr_0 = 1.0202e-04
Loss = 1.0411e-03, PNorm = 36.2053, GNorm = 4.3554, lr_0 = 1.0202e-04
Loss = 1.1098e-03, PNorm = 36.2069, GNorm = 4.0535, lr_0 = 1.0202e-04
Loss = 8.9618e-04, PNorm = 36.2089, GNorm = 7.1164, lr_0 = 1.0202e-04
Loss = 8.1470e-04, PNorm = 36.2115, GNorm = 3.8312, lr_0 = 1.0202e-04
Loss = 8.2869e-04, PNorm = 36.2135, GNorm = 3.6156, lr_0 = 1.0202e-04
Loss = 9.3940e-04, PNorm = 36.2153, GNorm = 7.6018, lr_0 = 1.0202e-04
Loss = 1.0354e-03, PNorm = 36.2168, GNorm = 12.6429, lr_0 = 1.0202e-04
Loss = 1.0069e-03, PNorm = 36.2186, GNorm = 1.8396, lr_0 = 1.0202e-04
Loss = 8.6756e-04, PNorm = 36.2196, GNorm = 3.4436, lr_0 = 1.0202e-04
Loss = 1.1350e-03, PNorm = 36.2214, GNorm = 9.4715, lr_0 = 1.0202e-04
Loss = 8.7118e-04, PNorm = 36.2230, GNorm = 2.8629, lr_0 = 1.0202e-04
Loss = 9.5148e-04, PNorm = 36.2246, GNorm = 2.5914, lr_0 = 1.0202e-04
Loss = 9.2519e-04, PNorm = 36.2264, GNorm = 4.9383, lr_0 = 1.0202e-04
Loss = 8.8211e-04, PNorm = 36.2275, GNorm = 5.8532, lr_0 = 1.0202e-04
Loss = 8.7905e-04, PNorm = 36.2279, GNorm = 5.6501, lr_0 = 1.0202e-04
Loss = 8.0916e-04, PNorm = 36.2292, GNorm = 3.1189, lr_0 = 1.0202e-04
Loss = 8.2052e-04, PNorm = 36.2309, GNorm = 1.5635, lr_0 = 1.0202e-04
Loss = 7.7829e-04, PNorm = 36.2327, GNorm = 2.6076, lr_0 = 1.0202e-04
Loss = 6.7649e-04, PNorm = 36.2349, GNorm = 2.4808, lr_0 = 1.0202e-04
Loss = 8.4323e-04, PNorm = 36.2362, GNorm = 3.9732, lr_0 = 1.0202e-04
Validation rmse logP = 0.503980
Validation R2 logP = 0.925585
Epoch 52
Train function
Loss = 7.2762e-04, PNorm = 36.2377, GNorm = 1.8018, lr_0 = 1.0202e-04
Loss = 7.4497e-04, PNorm = 36.2393, GNorm = 2.6968, lr_0 = 1.0202e-04
Loss = 7.6374e-04, PNorm = 36.2412, GNorm = 5.3703, lr_0 = 1.0202e-04
Loss = 8.3330e-04, PNorm = 36.2434, GNorm = 2.4461, lr_0 = 1.0202e-04
Loss = 6.9168e-04, PNorm = 36.2461, GNorm = 4.5465, lr_0 = 1.0202e-04
Loss = 8.8942e-04, PNorm = 36.2482, GNorm = 2.2218, lr_0 = 1.0202e-04
Loss = 1.0075e-03, PNorm = 36.2493, GNorm = 9.1794, lr_0 = 1.0202e-04
Loss = 1.0865e-03, PNorm = 36.2508, GNorm = 1.3565, lr_0 = 1.0202e-04
Loss = 8.5541e-04, PNorm = 36.2521, GNorm = 2.8167, lr_0 = 1.0202e-04
Loss = 7.8627e-04, PNorm = 36.2541, GNorm = 1.8179, lr_0 = 1.0202e-04
Loss = 6.7616e-04, PNorm = 36.2558, GNorm = 1.9732, lr_0 = 1.0202e-04
Loss = 5.7645e-04, PNorm = 36.2568, GNorm = 1.2597, lr_0 = 1.0202e-04
Loss = 7.5674e-04, PNorm = 36.2586, GNorm = 3.5779, lr_0 = 1.0202e-04
Loss = 1.0762e-03, PNorm = 36.2607, GNorm = 4.3996, lr_0 = 1.0202e-04
Loss = 8.5252e-04, PNorm = 36.2624, GNorm = 2.5919, lr_0 = 1.0202e-04
Loss = 8.6395e-04, PNorm = 36.2631, GNorm = 2.6840, lr_0 = 1.0202e-04
Loss = 8.5863e-04, PNorm = 36.2648, GNorm = 3.7432, lr_0 = 1.0202e-04
Loss = 9.5485e-04, PNorm = 36.2662, GNorm = 5.3634, lr_0 = 1.0202e-04
Loss = 7.0080e-04, PNorm = 36.2679, GNorm = 1.7503, lr_0 = 1.0202e-04
Loss = 8.3413e-04, PNorm = 36.2693, GNorm = 1.3868, lr_0 = 1.0202e-04
Loss = 8.8634e-04, PNorm = 36.2705, GNorm = 1.2250, lr_0 = 1.0202e-04
Loss = 8.3612e-04, PNorm = 36.2718, GNorm = 7.4829, lr_0 = 1.0202e-04
Validation rmse logP = 0.483575
Validation R2 logP = 0.931489
Epoch 53
Train function
Loss = 1.0705e-03, PNorm = 36.2733, GNorm = 7.8478, lr_0 = 1.0202e-04
Loss = 7.1556e-04, PNorm = 36.2751, GNorm = 1.8015, lr_0 = 1.0202e-04
Loss = 7.8729e-04, PNorm = 36.2773, GNorm = 2.6403, lr_0 = 1.0202e-04
Loss = 9.2862e-04, PNorm = 36.2787, GNorm = 8.6684, lr_0 = 1.0202e-04
Loss = 8.9784e-04, PNorm = 36.2812, GNorm = 5.9176, lr_0 = 1.0202e-04
Loss = 7.4543e-04, PNorm = 36.2834, GNorm = 4.4041, lr_0 = 1.0202e-04
Loss = 8.2475e-04, PNorm = 36.2859, GNorm = 1.8470, lr_0 = 1.0202e-04
Loss = 7.0189e-04, PNorm = 36.2877, GNorm = 2.4316, lr_0 = 1.0202e-04
Loss = 6.7200e-04, PNorm = 36.2893, GNorm = 3.9453, lr_0 = 1.0202e-04
Loss = 6.8455e-04, PNorm = 36.2903, GNorm = 3.4894, lr_0 = 1.0202e-04
Loss = 6.6767e-04, PNorm = 36.2919, GNorm = 3.0657, lr_0 = 1.0202e-04
Loss = 9.3296e-04, PNorm = 36.2937, GNorm = 4.4775, lr_0 = 1.0202e-04
Loss = 6.5977e-04, PNorm = 36.2952, GNorm = 4.6607, lr_0 = 1.0202e-04
Loss = 1.0115e-03, PNorm = 36.2968, GNorm = 7.5367, lr_0 = 1.0202e-04
Loss = 8.3687e-04, PNorm = 36.2978, GNorm = 4.5719, lr_0 = 1.0202e-04
Loss = 9.4670e-04, PNorm = 36.2993, GNorm = 3.0178, lr_0 = 1.0202e-04
Loss = 9.2556e-04, PNorm = 36.3012, GNorm = 4.8970, lr_0 = 1.0202e-04
Loss = 8.7498e-04, PNorm = 36.3034, GNorm = 1.6259, lr_0 = 1.0202e-04
Loss = 7.0794e-04, PNorm = 36.3055, GNorm = 1.3414, lr_0 = 1.0202e-04
Loss = 8.7384e-04, PNorm = 36.3070, GNorm = 7.3404, lr_0 = 1.0202e-04
Loss = 8.0332e-04, PNorm = 36.3091, GNorm = 1.5093, lr_0 = 1.0202e-04
Loss = 7.9031e-04, PNorm = 36.3108, GNorm = 2.1277, lr_0 = 1.0202e-04
Loss = 8.3367e-04, PNorm = 36.3116, GNorm = 2.0474, lr_0 = 1.0202e-04
Validation rmse logP = 0.501545
Validation R2 logP = 0.926303
Epoch 54
Train function
Loss = 7.0057e-04, PNorm = 36.3141, GNorm = 0.7717, lr_0 = 1.0202e-04
Loss = 8.1250e-04, PNorm = 36.3164, GNorm = 1.4634, lr_0 = 1.0202e-04
Loss = 7.9044e-04, PNorm = 36.3176, GNorm = 3.1664, lr_0 = 1.0202e-04
Loss = 8.7445e-04, PNorm = 36.3188, GNorm = 8.4311, lr_0 = 1.0202e-04
Loss = 1.1106e-03, PNorm = 36.3189, GNorm = 1.9340, lr_0 = 1.0202e-04
Loss = 8.8644e-04, PNorm = 36.3198, GNorm = 4.9167, lr_0 = 1.0202e-04
Loss = 6.6827e-04, PNorm = 36.3217, GNorm = 4.8456, lr_0 = 1.0202e-04
Loss = 7.2590e-04, PNorm = 36.3240, GNorm = 2.7564, lr_0 = 1.0202e-04
Loss = 9.5084e-04, PNorm = 36.3261, GNorm = 5.5124, lr_0 = 1.0202e-04
Loss = 7.4332e-04, PNorm = 36.3273, GNorm = 3.2061, lr_0 = 1.0202e-04
Loss = 7.2370e-04, PNorm = 36.3284, GNorm = 1.1567, lr_0 = 1.0202e-04
Loss = 8.8742e-04, PNorm = 36.3305, GNorm = 2.8377, lr_0 = 1.0202e-04
Loss = 7.8901e-04, PNorm = 36.3326, GNorm = 5.9703, lr_0 = 1.0202e-04
Loss = 1.0358e-03, PNorm = 36.3353, GNorm = 1.1637, lr_0 = 1.0202e-04
Loss = 8.2577e-04, PNorm = 36.3386, GNorm = 3.3921, lr_0 = 1.0202e-04
Loss = 6.1227e-04, PNorm = 36.3406, GNorm = 1.4590, lr_0 = 1.0202e-04
Loss = 7.5031e-04, PNorm = 36.3417, GNorm = 6.0934, lr_0 = 1.0202e-04
Loss = 8.3521e-04, PNorm = 36.3415, GNorm = 3.5815, lr_0 = 1.0202e-04
Loss = 7.0878e-04, PNorm = 36.3423, GNorm = 7.1242, lr_0 = 1.0202e-04
Loss = 7.8869e-04, PNorm = 36.3440, GNorm = 5.5101, lr_0 = 1.0202e-04
Loss = 8.1070e-04, PNorm = 36.3461, GNorm = 1.8480, lr_0 = 1.0202e-04
Loss = 8.6310e-04, PNorm = 36.3489, GNorm = 5.9749, lr_0 = 1.0202e-04
Validation rmse logP = 0.489520
Validation R2 logP = 0.929794
Epoch 55
Train function
Loss = 7.6450e-04, PNorm = 36.3508, GNorm = 2.3865, lr_0 = 1.0202e-04
Loss = 6.0972e-04, PNorm = 36.3525, GNorm = 4.0978, lr_0 = 1.0202e-04
Loss = 7.0745e-04, PNorm = 36.3536, GNorm = 6.4237, lr_0 = 1.0202e-04
Loss = 6.0558e-04, PNorm = 36.3544, GNorm = 4.4168, lr_0 = 1.0202e-04
Loss = 6.9731e-04, PNorm = 36.3558, GNorm = 3.1329, lr_0 = 1.0202e-04
Loss = 7.4082e-04, PNorm = 36.3569, GNorm = 4.5198, lr_0 = 1.0202e-04
Loss = 7.0144e-04, PNorm = 36.3584, GNorm = 2.3371, lr_0 = 1.0202e-04
Loss = 9.0170e-04, PNorm = 36.3595, GNorm = 3.7139, lr_0 = 1.0202e-04
Loss = 8.3483e-04, PNorm = 36.3605, GNorm = 1.7219, lr_0 = 1.0202e-04
Loss = 6.8179e-04, PNorm = 36.3624, GNorm = 1.2552, lr_0 = 1.0202e-04
Loss = 6.8035e-04, PNorm = 36.3643, GNorm = 2.0838, lr_0 = 1.0202e-04
Loss = 8.0313e-04, PNorm = 36.3665, GNorm = 2.6852, lr_0 = 1.0202e-04
Loss = 9.5292e-04, PNorm = 36.3686, GNorm = 3.6651, lr_0 = 1.0202e-04
Loss = 6.3242e-04, PNorm = 36.3702, GNorm = 0.9850, lr_0 = 1.0202e-04
Loss = 8.4751e-04, PNorm = 36.3706, GNorm = 5.2627, lr_0 = 1.0202e-04
Loss = 8.4198e-04, PNorm = 36.3717, GNorm = 3.2284, lr_0 = 1.0202e-04
Loss = 8.6582e-04, PNorm = 36.3730, GNorm = 1.7997, lr_0 = 1.0202e-04
Loss = 7.0164e-04, PNorm = 36.3743, GNorm = 1.9898, lr_0 = 1.0202e-04
Loss = 7.8614e-04, PNorm = 36.3763, GNorm = 4.1472, lr_0 = 1.0202e-04
Loss = 8.7110e-04, PNorm = 36.3786, GNorm = 7.4443, lr_0 = 1.0202e-04
Loss = 1.2003e-03, PNorm = 36.3807, GNorm = 8.6918, lr_0 = 1.0202e-04
Loss = 8.3850e-04, PNorm = 36.3832, GNorm = 4.1591, lr_0 = 1.0202e-04
Validation rmse logP = 0.515608
Validation R2 logP = 0.922112
Epoch 56
Train function
Loss = 9.0412e-04, PNorm = 36.3849, GNorm = 6.1159, lr_0 = 1.0202e-04
Loss = 6.0366e-04, PNorm = 36.3868, GNorm = 1.6267, lr_0 = 1.0202e-04
Loss = 7.4442e-04, PNorm = 36.3890, GNorm = 2.8313, lr_0 = 1.0202e-04
Loss = 7.4871e-04, PNorm = 36.3916, GNorm = 3.6406, lr_0 = 1.0202e-04
Loss = 8.7188e-04, PNorm = 36.3931, GNorm = 3.3915, lr_0 = 1.0202e-04
Loss = 8.2891e-04, PNorm = 36.3949, GNorm = 8.5647, lr_0 = 1.0202e-04
Loss = 8.6744e-04, PNorm = 36.3965, GNorm = 3.0999, lr_0 = 1.0202e-04
Loss = 7.7199e-04, PNorm = 36.3982, GNorm = 2.7039, lr_0 = 1.0202e-04
Loss = 9.0133e-04, PNorm = 36.4004, GNorm = 8.5874, lr_0 = 1.0202e-04
Loss = 8.6714e-04, PNorm = 36.4028, GNorm = 4.9122, lr_0 = 1.0202e-04
Loss = 9.2798e-04, PNorm = 36.4044, GNorm = 3.1324, lr_0 = 1.0202e-04
Loss = 8.1866e-04, PNorm = 36.4054, GNorm = 7.4312, lr_0 = 1.0202e-04
Loss = 8.5405e-04, PNorm = 36.4059, GNorm = 3.7975, lr_0 = 1.0202e-04
Loss = 8.9952e-04, PNorm = 36.4066, GNorm = 2.5908, lr_0 = 1.0202e-04
Loss = 8.3333e-04, PNorm = 36.4082, GNorm = 1.3806, lr_0 = 1.0202e-04
Loss = 7.7966e-04, PNorm = 36.4096, GNorm = 3.5717, lr_0 = 1.0202e-04
Loss = 7.9950e-04, PNorm = 36.4111, GNorm = 1.5206, lr_0 = 1.0202e-04
Loss = 7.2885e-04, PNorm = 36.4131, GNorm = 3.6866, lr_0 = 1.0202e-04
Loss = 8.4275e-04, PNorm = 36.4159, GNorm = 7.2034, lr_0 = 1.0202e-04
Loss = 8.0493e-04, PNorm = 36.4172, GNorm = 3.6243, lr_0 = 1.0202e-04
Loss = 6.7828e-04, PNorm = 36.4191, GNorm = 3.9805, lr_0 = 1.0202e-04
Loss = 8.2196e-04, PNorm = 36.4198, GNorm = 5.2547, lr_0 = 1.0202e-04
Loss = 9.1250e-04, PNorm = 36.4218, GNorm = 6.6651, lr_0 = 1.0202e-04
Validation rmse logP = 0.517760
Validation R2 logP = 0.921460
Epoch 57
Train function
Loss = 6.6302e-04, PNorm = 36.4240, GNorm = 3.6032, lr_0 = 1.0202e-04
Loss = 7.6333e-04, PNorm = 36.4258, GNorm = 2.9427, lr_0 = 1.0202e-04
Loss = 7.2205e-04, PNorm = 36.4276, GNorm = 1.8071, lr_0 = 1.0202e-04
Loss = 7.3509e-04, PNorm = 36.4297, GNorm = 1.2507, lr_0 = 1.0202e-04
Loss = 7.5362e-04, PNorm = 36.4313, GNorm = 1.1323, lr_0 = 1.0202e-04
Loss = 7.0489e-04, PNorm = 36.4331, GNorm = 5.4012, lr_0 = 1.0202e-04
Loss = 6.8157e-04, PNorm = 36.4343, GNorm = 3.3637, lr_0 = 1.0202e-04
Loss = 8.7897e-04, PNorm = 36.4375, GNorm = 1.9198, lr_0 = 1.0202e-04
Loss = 7.4074e-04, PNorm = 36.4398, GNorm = 4.1173, lr_0 = 1.0202e-04
Loss = 8.9548e-04, PNorm = 36.4416, GNorm = 5.3163, lr_0 = 1.0202e-04
Loss = 9.8005e-04, PNorm = 36.4428, GNorm = 9.5658, lr_0 = 1.0202e-04
Loss = 9.2139e-04, PNorm = 36.4449, GNorm = 4.7941, lr_0 = 1.0202e-04
Loss = 8.8386e-04, PNorm = 36.4473, GNorm = 4.7957, lr_0 = 1.0202e-04
Loss = 7.9307e-04, PNorm = 36.4487, GNorm = 2.4424, lr_0 = 1.0202e-04
Loss = 8.5949e-04, PNorm = 36.4500, GNorm = 2.3093, lr_0 = 1.0202e-04
Loss = 6.5100e-04, PNorm = 36.4517, GNorm = 1.9392, lr_0 = 1.0202e-04
Loss = 7.4273e-04, PNorm = 36.4535, GNorm = 2.4161, lr_0 = 1.0202e-04
Loss = 7.5427e-04, PNorm = 36.4543, GNorm = 7.9908, lr_0 = 1.0202e-04
Loss = 7.4623e-04, PNorm = 36.4559, GNorm = 2.6732, lr_0 = 1.0202e-04
Loss = 7.5224e-04, PNorm = 36.4577, GNorm = 1.8300, lr_0 = 1.0202e-04
Loss = 7.8987e-04, PNorm = 36.4583, GNorm = 1.3547, lr_0 = 1.0202e-04
Loss = 7.0826e-04, PNorm = 36.4596, GNorm = 7.7966, lr_0 = 1.0202e-04
Validation rmse logP = 0.466453
Validation R2 logP = 0.936255
Epoch 58
Train function
Loss = 8.2661e-04, PNorm = 36.4615, GNorm = 1.9360, lr_0 = 1.0202e-04
Loss = 7.0352e-04, PNorm = 36.4634, GNorm = 1.6049, lr_0 = 1.0202e-04
Loss = 6.4063e-04, PNorm = 36.4662, GNorm = 4.2406, lr_0 = 1.0202e-04
Loss = 9.7418e-04, PNorm = 36.4685, GNorm = 3.6126, lr_0 = 1.0202e-04
Loss = 8.2000e-04, PNorm = 36.4705, GNorm = 2.7706, lr_0 = 1.0202e-04
Loss = 8.3044e-04, PNorm = 36.4718, GNorm = 2.3406, lr_0 = 1.0202e-04
Loss = 6.4826e-04, PNorm = 36.4736, GNorm = 1.2590, lr_0 = 1.0202e-04
Loss = 8.3158e-04, PNorm = 36.4753, GNorm = 1.8687, lr_0 = 1.0202e-04
Loss = 5.6464e-04, PNorm = 36.4763, GNorm = 2.6011, lr_0 = 1.0202e-04
Loss = 6.4383e-04, PNorm = 36.4773, GNorm = 3.6040, lr_0 = 1.0202e-04
Loss = 6.3012e-04, PNorm = 36.4785, GNorm = 1.2938, lr_0 = 1.0202e-04
Loss = 6.3221e-04, PNorm = 36.4793, GNorm = 2.7377, lr_0 = 1.0202e-04
Loss = 6.8653e-04, PNorm = 36.4809, GNorm = 1.5339, lr_0 = 1.0202e-04
Loss = 7.5821e-04, PNorm = 36.4823, GNorm = 1.8197, lr_0 = 1.0202e-04
Loss = 6.4158e-04, PNorm = 36.4836, GNorm = 4.8601, lr_0 = 1.0202e-04
Loss = 7.3278e-04, PNorm = 36.4848, GNorm = 3.4926, lr_0 = 1.0202e-04
Loss = 6.5125e-04, PNorm = 36.4864, GNorm = 2.0033, lr_0 = 1.0202e-04
Loss = 6.6784e-04, PNorm = 36.4878, GNorm = 3.0471, lr_0 = 1.0202e-04
Loss = 5.3720e-04, PNorm = 36.4885, GNorm = 0.9788, lr_0 = 1.0202e-04
Loss = 7.2501e-04, PNorm = 36.4899, GNorm = 3.0860, lr_0 = 1.0202e-04
Loss = 9.2332e-04, PNorm = 36.4920, GNorm = 1.8577, lr_0 = 1.0202e-04
Loss = 1.0365e-03, PNorm = 36.4941, GNorm = 5.8379, lr_0 = 1.0202e-04
Validation rmse logP = 0.478436
Validation R2 logP = 0.932937
Epoch 59
Train function
Loss = 7.3108e-04, PNorm = 36.4966, GNorm = 4.7948, lr_0 = 1.0202e-04
Loss = 6.1844e-04, PNorm = 36.4993, GNorm = 1.8752, lr_0 = 1.0202e-04
Loss = 6.3577e-04, PNorm = 36.5009, GNorm = 2.4756, lr_0 = 1.0202e-04
Loss = 5.2617e-04, PNorm = 36.5027, GNorm = 1.6234, lr_0 = 1.0202e-04
Loss = 6.2300e-04, PNorm = 36.5040, GNorm = 2.4445, lr_0 = 1.0202e-04
Loss = 7.5142e-04, PNorm = 36.5053, GNorm = 2.5632, lr_0 = 1.0202e-04
Loss = 7.2819e-04, PNorm = 36.5066, GNorm = 2.2644, lr_0 = 1.0202e-04
Loss = 7.9980e-04, PNorm = 36.5081, GNorm = 3.9558, lr_0 = 1.0202e-04
Loss = 8.7620e-04, PNorm = 36.5104, GNorm = 6.8925, lr_0 = 1.0202e-04
Loss = 6.9955e-04, PNorm = 36.5124, GNorm = 5.0906, lr_0 = 1.0202e-04
Loss = 6.1441e-04, PNorm = 36.5148, GNorm = 4.0162, lr_0 = 1.0202e-04
Loss = 8.4508e-04, PNorm = 36.5163, GNorm = 1.2022, lr_0 = 1.0202e-04
Loss = 7.5297e-04, PNorm = 36.5168, GNorm = 4.0237, lr_0 = 1.0202e-04
Loss = 6.2872e-04, PNorm = 36.5175, GNorm = 1.3697, lr_0 = 1.0202e-04
Loss = 6.5518e-04, PNorm = 36.5177, GNorm = 1.6368, lr_0 = 1.0202e-04
Loss = 7.9708e-04, PNorm = 36.5182, GNorm = 9.3229, lr_0 = 1.0202e-04
Loss = 7.8486e-04, PNorm = 36.5199, GNorm = 4.7015, lr_0 = 1.0202e-04
Loss = 1.0394e-03, PNorm = 36.5209, GNorm = 3.0896, lr_0 = 1.0202e-04
Loss = 1.0917e-03, PNorm = 36.5225, GNorm = 15.8245, lr_0 = 1.0202e-04
Loss = 1.1300e-03, PNorm = 36.5239, GNorm = 3.8251, lr_0 = 1.0202e-04
Loss = 9.2883e-04, PNorm = 36.5260, GNorm = 4.9250, lr_0 = 1.0202e-04
Loss = 8.9042e-04, PNorm = 36.5286, GNorm = 2.9372, lr_0 = 1.0202e-04
Loss = 8.2294e-04, PNorm = 36.5309, GNorm = 5.9074, lr_0 = 1.0202e-04
Validation rmse logP = 0.521811
Validation R2 logP = 0.920227
Epoch 60
Train function
Loss = 7.8894e-04, PNorm = 36.5326, GNorm = 4.2649, lr_0 = 1.0202e-04
Loss = 6.4203e-04, PNorm = 36.5351, GNorm = 4.0518, lr_0 = 1.0202e-04
Loss = 7.8248e-04, PNorm = 36.5364, GNorm = 3.8458, lr_0 = 1.0202e-04
Loss = 6.9517e-04, PNorm = 36.5387, GNorm = 7.5931, lr_0 = 1.0202e-04
Loss = 6.5245e-04, PNorm = 36.5401, GNorm = 4.3905, lr_0 = 1.0202e-04
Loss = 7.2613e-04, PNorm = 36.5421, GNorm = 2.1265, lr_0 = 1.0202e-04
Loss = 6.9724e-04, PNorm = 36.5441, GNorm = 1.9847, lr_0 = 1.0202e-04
Loss = 7.0197e-04, PNorm = 36.5456, GNorm = 4.7752, lr_0 = 1.0202e-04
Loss = 6.3246e-04, PNorm = 36.5468, GNorm = 2.3364, lr_0 = 1.0202e-04
Loss = 7.2108e-04, PNorm = 36.5485, GNorm = 2.0607, lr_0 = 1.0202e-04
Loss = 6.1985e-04, PNorm = 36.5502, GNorm = 3.2900, lr_0 = 1.0202e-04
Loss = 6.2997e-04, PNorm = 36.5518, GNorm = 2.8607, lr_0 = 1.0202e-04
Loss = 8.3069e-04, PNorm = 36.5545, GNorm = 4.9840, lr_0 = 1.0202e-04
Loss = 7.4534e-04, PNorm = 36.5552, GNorm = 2.0480, lr_0 = 1.0202e-04
Loss = 6.9487e-04, PNorm = 36.5564, GNorm = 2.6884, lr_0 = 1.0202e-04
Loss = 7.1196e-04, PNorm = 36.5576, GNorm = 2.2079, lr_0 = 1.0202e-04
Loss = 9.3752e-04, PNorm = 36.5591, GNorm = 3.2083, lr_0 = 1.0202e-04
Loss = 9.1411e-04, PNorm = 36.5610, GNorm = 2.2185, lr_0 = 1.0202e-04
Loss = 5.6947e-04, PNorm = 36.5626, GNorm = 0.9154, lr_0 = 1.0202e-04
Loss = 7.4641e-04, PNorm = 36.5647, GNorm = 2.3836, lr_0 = 1.0202e-04
Loss = 6.7419e-04, PNorm = 36.5670, GNorm = 7.0838, lr_0 = 1.0202e-04
Loss = 7.2317e-04, PNorm = 36.5679, GNorm = 4.9091, lr_0 = 1.0202e-04
Validation rmse logP = 0.494094
Validation R2 logP = 0.928476
Epoch 61
Train function
Loss = 6.9346e-04, PNorm = 36.5692, GNorm = 4.5447, lr_0 = 1.0202e-04
Loss = 9.7055e-04, PNorm = 36.5710, GNorm = 4.1274, lr_0 = 1.0202e-04
Loss = 8.6237e-04, PNorm = 36.5740, GNorm = 3.3470, lr_0 = 1.0202e-04
Loss = 7.8684e-04, PNorm = 36.5752, GNorm = 3.4377, lr_0 = 1.0202e-04
Loss = 8.2367e-04, PNorm = 36.5778, GNorm = 2.5131, lr_0 = 1.0202e-04
Loss = 6.7519e-04, PNorm = 36.5795, GNorm = 2.2952, lr_0 = 1.0202e-04
Loss = 5.5917e-04, PNorm = 36.5802, GNorm = 3.4133, lr_0 = 1.0202e-04
Loss = 6.6756e-04, PNorm = 36.5818, GNorm = 4.2648, lr_0 = 1.0202e-04
Loss = 7.6924e-04, PNorm = 36.5820, GNorm = 5.9858, lr_0 = 1.0202e-04
Loss = 5.8541e-04, PNorm = 36.5834, GNorm = 4.8225, lr_0 = 1.0202e-04
Loss = 8.0449e-04, PNorm = 36.5853, GNorm = 2.8832, lr_0 = 1.0202e-04
Loss = 7.5934e-04, PNorm = 36.5868, GNorm = 1.7267, lr_0 = 1.0202e-04
Loss = 6.7380e-04, PNorm = 36.5894, GNorm = 3.2590, lr_0 = 1.0202e-04
Loss = 8.7906e-04, PNorm = 36.5913, GNorm = 1.9857, lr_0 = 1.0202e-04
Loss = 6.9279e-04, PNorm = 36.5932, GNorm = 2.7831, lr_0 = 1.0202e-04
Loss = 8.0185e-04, PNorm = 36.5942, GNorm = 3.2870, lr_0 = 1.0202e-04
Loss = 8.7855e-04, PNorm = 36.5962, GNorm = 2.3324, lr_0 = 1.0202e-04
Loss = 8.8445e-04, PNorm = 36.5978, GNorm = 2.1438, lr_0 = 1.0202e-04
Loss = 9.0910e-04, PNorm = 36.5987, GNorm = 3.7780, lr_0 = 1.0202e-04
Loss = 1.0526e-03, PNorm = 36.6001, GNorm = 6.6979, lr_0 = 1.0202e-04
Loss = 1.0011e-03, PNorm = 36.6016, GNorm = 2.5431, lr_0 = 1.0202e-04
Loss = 6.9747e-04, PNorm = 36.6033, GNorm = 4.3864, lr_0 = 1.0202e-04
Validation rmse logP = 0.473207
Validation R2 logP = 0.934395
Epoch 62
Train function
Loss = 3.8423e-04, PNorm = 36.6048, GNorm = 2.3871, lr_0 = 1.0202e-04
Loss = 7.3496e-04, PNorm = 36.6066, GNorm = 3.8022, lr_0 = 1.0202e-04
Loss = 7.1902e-04, PNorm = 36.6075, GNorm = 7.9703, lr_0 = 1.0202e-04
Loss = 8.1552e-04, PNorm = 36.6089, GNorm = 2.4887, lr_0 = 1.0202e-04
Loss = 1.0726e-03, PNorm = 36.6104, GNorm = 1.7494, lr_0 = 1.0202e-04
Loss = 8.5959e-04, PNorm = 36.6126, GNorm = 3.7270, lr_0 = 1.0202e-04
Loss = 9.2206e-04, PNorm = 36.6157, GNorm = 2.4873, lr_0 = 1.0202e-04
Loss = 8.5591e-04, PNorm = 36.6175, GNorm = 1.5300, lr_0 = 1.0202e-04
Loss = 8.0319e-04, PNorm = 36.6196, GNorm = 3.0170, lr_0 = 1.0202e-04
Loss = 6.3679e-04, PNorm = 36.6208, GNorm = 2.0233, lr_0 = 1.0202e-04
Loss = 7.0109e-04, PNorm = 36.6223, GNorm = 2.1339, lr_0 = 1.0202e-04
Loss = 6.1934e-04, PNorm = 36.6248, GNorm = 3.3517, lr_0 = 1.0202e-04
Loss = 8.4017e-04, PNorm = 36.6274, GNorm = 6.4211, lr_0 = 1.0202e-04
Loss = 7.6564e-04, PNorm = 36.6296, GNorm = 3.2626, lr_0 = 1.0202e-04
Loss = 5.9981e-04, PNorm = 36.6312, GNorm = 0.8891, lr_0 = 1.0202e-04
Loss = 6.6488e-04, PNorm = 36.6336, GNorm = 2.6153, lr_0 = 1.0202e-04
Loss = 8.1599e-04, PNorm = 36.6350, GNorm = 2.0383, lr_0 = 1.0202e-04
Loss = 7.0439e-04, PNorm = 36.6363, GNorm = 3.8085, lr_0 = 1.0202e-04
Loss = 7.7222e-04, PNorm = 36.6374, GNorm = 4.2636, lr_0 = 1.0202e-04
Loss = 6.2168e-04, PNorm = 36.6389, GNorm = 4.8086, lr_0 = 1.0202e-04
Loss = 5.4988e-04, PNorm = 36.6396, GNorm = 1.4036, lr_0 = 1.0202e-04
Loss = 6.6946e-04, PNorm = 36.6416, GNorm = 1.6072, lr_0 = 1.0202e-04
Loss = 7.5233e-04, PNorm = 36.6433, GNorm = 2.7978, lr_0 = 1.0202e-04
Validation rmse logP = 0.484281
Validation R2 logP = 0.931289
Epoch 63
Train function
Loss = 7.2855e-04, PNorm = 36.6454, GNorm = 5.6623, lr_0 = 1.0202e-04
Loss = 5.1161e-04, PNorm = 36.6472, GNorm = 1.6058, lr_0 = 1.0202e-04
Loss = 7.0904e-04, PNorm = 36.6487, GNorm = 1.0812, lr_0 = 1.0202e-04
Loss = 4.9029e-04, PNorm = 36.6497, GNorm = 2.2495, lr_0 = 1.0202e-04
Loss = 7.6971e-04, PNorm = 36.6505, GNorm = 1.9132, lr_0 = 1.0202e-04
Loss = 8.0870e-04, PNorm = 36.6524, GNorm = 3.8922, lr_0 = 1.0202e-04
Loss = 7.1451e-04, PNorm = 36.6546, GNorm = 4.6308, lr_0 = 1.0202e-04
Loss = 6.6525e-04, PNorm = 36.6569, GNorm = 1.2119, lr_0 = 1.0202e-04
Loss = 5.9513e-04, PNorm = 36.6577, GNorm = 1.4208, lr_0 = 1.0202e-04
Loss = 4.7843e-04, PNorm = 36.6590, GNorm = 1.3161, lr_0 = 1.0202e-04
Loss = 7.2025e-04, PNorm = 36.6599, GNorm = 2.8830, lr_0 = 1.0202e-04
Loss = 6.9503e-04, PNorm = 36.6615, GNorm = 3.9762, lr_0 = 1.0202e-04
Loss = 7.4035e-04, PNorm = 36.6634, GNorm = 3.9151, lr_0 = 1.0202e-04
Loss = 6.0627e-04, PNorm = 36.6655, GNorm = 2.2147, lr_0 = 1.0202e-04
Loss = 9.5202e-04, PNorm = 36.6673, GNorm = 2.4328, lr_0 = 1.0202e-04
Loss = 6.2884e-04, PNorm = 36.6693, GNorm = 1.8863, lr_0 = 1.0202e-04
Loss = 6.3096e-04, PNorm = 36.6708, GNorm = 1.0371, lr_0 = 1.0202e-04
Loss = 6.7716e-04, PNorm = 36.6728, GNorm = 4.3504, lr_0 = 1.0202e-04
Loss = 6.4731e-04, PNorm = 36.6735, GNorm = 1.5133, lr_0 = 1.0202e-04
Loss = 8.1228e-04, PNorm = 36.6746, GNorm = 2.9227, lr_0 = 1.0202e-04
Loss = 7.0844e-04, PNorm = 36.6751, GNorm = 1.5114, lr_0 = 1.0202e-04
Loss = 6.9426e-04, PNorm = 36.6752, GNorm = 5.1724, lr_0 = 1.0202e-04
Validation rmse logP = 0.471223
Validation R2 logP = 0.934944
Epoch 64
Train function
Loss = 6.7782e-04, PNorm = 36.6766, GNorm = 2.1282, lr_0 = 1.0202e-04
Loss = 5.6200e-04, PNorm = 36.6781, GNorm = 2.4929, lr_0 = 1.0202e-04
Loss = 6.0100e-04, PNorm = 36.6793, GNorm = 1.9775, lr_0 = 1.0202e-04
Loss = 6.1950e-04, PNorm = 36.6809, GNorm = 1.8189, lr_0 = 1.0202e-04
Loss = 7.1247e-04, PNorm = 36.6828, GNorm = 6.5251, lr_0 = 1.0202e-04
Loss = 6.6392e-04, PNorm = 36.6845, GNorm = 5.0954, lr_0 = 1.0202e-04
Loss = 7.5105e-04, PNorm = 36.6860, GNorm = 6.8785, lr_0 = 1.0202e-04
Loss = 7.1036e-04, PNorm = 36.6882, GNorm = 2.9344, lr_0 = 1.0202e-04
Loss = 6.9944e-04, PNorm = 36.6903, GNorm = 6.0402, lr_0 = 1.0202e-04
Loss = 7.4853e-04, PNorm = 36.6917, GNorm = 3.5685, lr_0 = 1.0202e-04
Loss = 7.2145e-04, PNorm = 36.6935, GNorm = 4.1480, lr_0 = 1.0202e-04
Loss = 8.1148e-04, PNorm = 36.6950, GNorm = 4.9461, lr_0 = 1.0202e-04
Loss = 6.6460e-04, PNorm = 36.6972, GNorm = 1.8406, lr_0 = 1.0202e-04
Loss = 7.5225e-04, PNorm = 36.6995, GNorm = 0.8916, lr_0 = 1.0202e-04
Loss = 7.4023e-04, PNorm = 36.7015, GNorm = 2.3315, lr_0 = 1.0202e-04
Loss = 7.1545e-04, PNorm = 36.7028, GNorm = 2.8999, lr_0 = 1.0202e-04
Loss = 6.7827e-04, PNorm = 36.7040, GNorm = 1.4790, lr_0 = 1.0202e-04
Loss = 5.8213e-04, PNorm = 36.7053, GNorm = 1.6292, lr_0 = 1.0202e-04
Loss = 6.2534e-04, PNorm = 36.7058, GNorm = 1.9692, lr_0 = 1.0202e-04
Loss = 6.9454e-04, PNorm = 36.7067, GNorm = 6.1986, lr_0 = 1.0202e-04
Loss = 1.0793e-03, PNorm = 36.7080, GNorm = 3.8438, lr_0 = 1.0202e-04
Loss = 7.5344e-04, PNorm = 36.7097, GNorm = 1.6592, lr_0 = 1.0202e-04
Loss = 6.6306e-04, PNorm = 36.7120, GNorm = 2.4968, lr_0 = 1.0202e-04
Validation rmse logP = 0.490890
Validation R2 logP = 0.929401
Epoch 65
Train function
Loss = 6.0759e-04, PNorm = 36.7128, GNorm = 2.0067, lr_0 = 1.0202e-04
Loss = 6.9104e-04, PNorm = 36.7151, GNorm = 3.5362, lr_0 = 1.0202e-04
Loss = 6.8247e-04, PNorm = 36.7160, GNorm = 4.8819, lr_0 = 1.0202e-04
Loss = 8.0144e-04, PNorm = 36.7173, GNorm = 3.9225, lr_0 = 1.0202e-04
Loss = 7.3990e-04, PNorm = 36.7191, GNorm = 2.6729, lr_0 = 1.0202e-04
Loss = 6.5447e-04, PNorm = 36.7208, GNorm = 5.5504, lr_0 = 1.0202e-04
Loss = 6.5469e-04, PNorm = 36.7224, GNorm = 1.1218, lr_0 = 1.0202e-04
Loss = 6.7212e-04, PNorm = 36.7239, GNorm = 3.9913, lr_0 = 1.0202e-04
Loss = 7.2060e-04, PNorm = 36.7255, GNorm = 10.2700, lr_0 = 1.0202e-04
Loss = 5.8941e-04, PNorm = 36.7271, GNorm = 3.5545, lr_0 = 1.0202e-04
Loss = 6.9252e-04, PNorm = 36.7289, GNorm = 3.5333, lr_0 = 1.0202e-04
Loss = 5.0415e-04, PNorm = 36.7299, GNorm = 1.6322, lr_0 = 1.0202e-04
Loss = 6.5650e-04, PNorm = 36.7312, GNorm = 2.6763, lr_0 = 1.0202e-04
Loss = 5.7376e-04, PNorm = 36.7320, GNorm = 2.4230, lr_0 = 1.0202e-04
Loss = 6.2142e-04, PNorm = 36.7336, GNorm = 3.3775, lr_0 = 1.0202e-04
Loss = 7.2507e-04, PNorm = 36.7354, GNorm = 2.9965, lr_0 = 1.0202e-04
Loss = 6.4672e-04, PNorm = 36.7365, GNorm = 3.3312, lr_0 = 1.0202e-04
Loss = 7.9140e-04, PNorm = 36.7387, GNorm = 2.2795, lr_0 = 1.0202e-04
Loss = 7.0555e-04, PNorm = 36.7410, GNorm = 6.7741, lr_0 = 1.0202e-04
Loss = 7.7345e-04, PNorm = 36.7431, GNorm = 2.5280, lr_0 = 1.0202e-04
Loss = 6.6962e-04, PNorm = 36.7442, GNorm = 6.7180, lr_0 = 1.0202e-04
Loss = 8.0015e-04, PNorm = 36.7448, GNorm = 5.1663, lr_0 = 1.0202e-04
Validation rmse logP = 0.481672
Validation R2 logP = 0.932027
Epoch 66
Train function
Loss = 7.8841e-04, PNorm = 36.7465, GNorm = 1.8800, lr_0 = 1.0202e-04
Loss = 6.7577e-04, PNorm = 36.7474, GNorm = 6.5016, lr_0 = 1.0202e-04
Loss = 1.0253e-03, PNorm = 36.7489, GNorm = 8.4795, lr_0 = 1.0202e-04
Loss = 7.7117e-04, PNorm = 36.7500, GNorm = 2.1013, lr_0 = 1.0202e-04
Loss = 5.3665e-04, PNorm = 36.7521, GNorm = 4.5798, lr_0 = 1.0202e-04
Loss = 5.1003e-04, PNorm = 36.7529, GNorm = 1.6982, lr_0 = 1.0202e-04
Loss = 7.3958e-04, PNorm = 36.7548, GNorm = 4.3465, lr_0 = 1.0202e-04
Loss = 7.1370e-04, PNorm = 36.7574, GNorm = 3.6939, lr_0 = 1.0202e-04
Loss = 7.1907e-04, PNorm = 36.7591, GNorm = 2.2458, lr_0 = 1.0202e-04
Loss = 7.1341e-04, PNorm = 36.7611, GNorm = 2.3482, lr_0 = 1.0202e-04
Loss = 6.2210e-04, PNorm = 36.7625, GNorm = 2.6726, lr_0 = 1.0202e-04
Loss = 5.0274e-04, PNorm = 36.7647, GNorm = 2.5992, lr_0 = 1.0202e-04
Loss = 6.2060e-04, PNorm = 36.7659, GNorm = 2.9733, lr_0 = 1.0202e-04
Loss = 5.3921e-04, PNorm = 36.7671, GNorm = 3.1804, lr_0 = 1.0202e-04
Loss = 6.6484e-04, PNorm = 36.7688, GNorm = 1.1991, lr_0 = 1.0202e-04
Loss = 8.2630e-04, PNorm = 36.7698, GNorm = 8.4658, lr_0 = 1.0202e-04
Loss = 6.9871e-04, PNorm = 36.7710, GNorm = 3.0056, lr_0 = 1.0202e-04
Loss = 6.5889e-04, PNorm = 36.7722, GNorm = 3.0187, lr_0 = 1.0202e-04
Loss = 6.7161e-04, PNorm = 36.7745, GNorm = 1.9414, lr_0 = 1.0202e-04
Loss = 6.9448e-04, PNorm = 36.7765, GNorm = 3.3553, lr_0 = 1.0202e-04
Loss = 8.3099e-04, PNorm = 36.7779, GNorm = 2.2759, lr_0 = 1.0202e-04
Loss = 5.7234e-04, PNorm = 36.7797, GNorm = 2.8152, lr_0 = 1.0202e-04
Validation rmse logP = 0.476265
Validation R2 logP = 0.933545
Epoch 67
Train function
Loss = 6.8297e-04, PNorm = 36.7813, GNorm = 2.0576, lr_0 = 1.0202e-04
Loss = 6.1170e-04, PNorm = 36.7826, GNorm = 5.3438, lr_0 = 1.0202e-04
Loss = 6.4121e-04, PNorm = 36.7839, GNorm = 7.0772, lr_0 = 1.0202e-04
Loss = 6.4296e-04, PNorm = 36.7856, GNorm = 5.0074, lr_0 = 1.0202e-04
Loss = 5.4858e-04, PNorm = 36.7877, GNorm = 1.1959, lr_0 = 1.0202e-04
Loss = 5.6274e-04, PNorm = 36.7900, GNorm = 1.7198, lr_0 = 1.0202e-04
Loss = 6.0696e-04, PNorm = 36.7918, GNorm = 1.4917, lr_0 = 1.0202e-04
Loss = 6.5844e-04, PNorm = 36.7936, GNorm = 3.2795, lr_0 = 1.0202e-04
Loss = 5.8980e-04, PNorm = 36.7950, GNorm = 2.0747, lr_0 = 1.0202e-04
Loss = 5.8285e-04, PNorm = 36.7964, GNorm = 4.3334, lr_0 = 1.0202e-04
Loss = 6.7250e-04, PNorm = 36.7980, GNorm = 4.6538, lr_0 = 1.0202e-04
Loss = 7.7348e-04, PNorm = 36.7998, GNorm = 6.2465, lr_0 = 1.0202e-04
Loss = 6.3872e-04, PNorm = 36.8031, GNorm = 1.5128, lr_0 = 1.0202e-04
Loss = 6.2535e-04, PNorm = 36.8051, GNorm = 1.0015, lr_0 = 1.0202e-04
Loss = 7.5815e-04, PNorm = 36.8067, GNorm = 4.1969, lr_0 = 1.0202e-04
Loss = 8.7930e-04, PNorm = 36.8084, GNorm = 2.1876, lr_0 = 1.0202e-04
Loss = 6.5936e-04, PNorm = 36.8092, GNorm = 1.8317, lr_0 = 1.0202e-04
Loss = 6.3369e-04, PNorm = 36.8101, GNorm = 2.6481, lr_0 = 1.0202e-04
Loss = 6.5336e-04, PNorm = 36.8113, GNorm = 4.0576, lr_0 = 1.0202e-04
Loss = 5.7729e-04, PNorm = 36.8122, GNorm = 2.5302, lr_0 = 1.0202e-04
Loss = 6.4518e-04, PNorm = 36.8135, GNorm = 2.7562, lr_0 = 1.0202e-04
Loss = 7.7734e-04, PNorm = 36.8137, GNorm = 3.5550, lr_0 = 1.0202e-04
Loss = 8.3229e-04, PNorm = 36.8144, GNorm = 4.0617, lr_0 = 1.0202e-04
Loss = 1.1546e-03, PNorm = 36.8146, GNorm = 7.7178, lr_0 = 1.0202e-04
Validation rmse logP = 0.461443
Validation R2 logP = 0.937617
Epoch 68
Train function
Loss = 6.0546e-04, PNorm = 36.8165, GNorm = 6.8823, lr_0 = 1.0202e-04
Loss = 6.1729e-04, PNorm = 36.8189, GNorm = 1.7713, lr_0 = 1.0202e-04
Loss = 5.3608e-04, PNorm = 36.8209, GNorm = 3.0291, lr_0 = 1.0202e-04
Loss = 8.0465e-04, PNorm = 36.8229, GNorm = 7.5116, lr_0 = 1.0202e-04
Loss = 6.2582e-04, PNorm = 36.8255, GNorm = 1.2087, lr_0 = 1.0202e-04
Loss = 8.7314e-04, PNorm = 36.8268, GNorm = 1.7080, lr_0 = 1.0202e-04
Loss = 7.6334e-04, PNorm = 36.8296, GNorm = 1.9252, lr_0 = 1.0202e-04
Loss = 7.2917e-04, PNorm = 36.8309, GNorm = 2.9333, lr_0 = 1.0202e-04
Loss = 6.3695e-04, PNorm = 36.8328, GNorm = 3.8921, lr_0 = 1.0202e-04
Loss = 6.6152e-04, PNorm = 36.8337, GNorm = 1.5739, lr_0 = 1.0202e-04
Loss = 6.7701e-04, PNorm = 36.8356, GNorm = 1.2501, lr_0 = 1.0202e-04
Loss = 5.0744e-04, PNorm = 36.8372, GNorm = 5.4307, lr_0 = 1.0202e-04
Loss = 7.3831e-04, PNorm = 36.8390, GNorm = 3.3757, lr_0 = 1.0202e-04
Loss = 6.5199e-04, PNorm = 36.8414, GNorm = 3.0667, lr_0 = 1.0202e-04
Loss = 6.9796e-04, PNorm = 36.8425, GNorm = 3.4048, lr_0 = 1.0202e-04
Loss = 6.7799e-04, PNorm = 36.8430, GNorm = 2.0858, lr_0 = 1.0202e-04
Loss = 7.6684e-04, PNorm = 36.8442, GNorm = 6.3594, lr_0 = 1.0202e-04
