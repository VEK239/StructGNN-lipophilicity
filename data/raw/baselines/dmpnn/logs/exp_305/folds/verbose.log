Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_305/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logD'],
 'task_names': ['logD'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': False,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_305/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=895, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,222,401
Moving model to cuda
Epoch 0
Train function
Loss = 1.6663e-02, PNorm = 52.5080, GNorm = 3.6680, lr_0 = 4.8077e-04
Validation rmse logS = 1.157555
Validation R2 logS = 0.705495
Epoch 1
Train function
Loss = 7.1012e-03, PNorm = 52.5711, GNorm = 10.3615, lr_0 = 8.6154e-04
Validation rmse logS = 1.088776
Validation R2 logS = 0.739453
Epoch 2
Train function
Loss = 4.4841e-03, PNorm = 52.6574, GNorm = 2.9145, lr_0 = 9.8743e-04
Loss = 4.7527e-03, PNorm = 52.7248, GNorm = 2.3064, lr_0 = 9.6974e-04
Validation rmse logS = 1.089961
Validation R2 logS = 0.738885
Epoch 3
Train function
Loss = 3.9872e-03, PNorm = 52.7726, GNorm = 2.7758, lr_0 = 9.5237e-04
Validation rmse logS = 0.821254
Validation R2 logS = 0.851760
Epoch 4
Train function
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_305/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Loss = 2.4305e-03, PNorm = 52.8158, GNorm = 0.5660, lr_0 = 9.3363e-04
Validation rmse logS = 0.769584
Validation R2 logS = 0.869827
Epoch 5
Train function
Loss = 2.3931e-03, PNorm = 52.8540, GNorm = 1.1955, lr_0 = 9.1690e-04
Loss = 1.9837e-03, PNorm = 52.8877, GNorm = 1.0757, lr_0 = 9.0048e-04
Loss = 4.1940e-03, PNorm = 52.8909, GNorm = 1.2382, lr_0 = 8.9885e-04
Validation rmse logS = 0.720280
Validation R2 logS = 0.885972
Epoch 6
Train function
Loss = 1.8743e-03, PNorm = 52.9224, GNorm = 0.6198, lr_0 = 8.8275e-04
Validation rmse logS = 0.697116
Validation R2 logS = 0.893188
Epoch 7
Train function
Loss = 1.5080e-03, PNorm = 52.9604, GNorm = 1.2535, lr_0 = 8.6694e-04
Validation rmse logS = 0.685664
Validation R2 logS = 0.896669
Epoch 8
Train function
Loss = 1.9984e-03, PNorm = 52.9932, GNorm = 1.1011, lr_0 = 8.4988e-04
Loss = 1.6440e-03, PNorm = 53.0244, GNorm = 0.6856, lr_0 = 8.3466e-04
Validation rmse logS = 0.764081
Validation R2 logS = 0.871682
Epoch 9
Train function
Loss = 1.5434e-03, PNorm = 53.0584, GNorm = 1.8605, lr_0 = 8.1971e-04
Validation rmse logS = 0.665339
Validation R2 logS = 0.902704
Epoch 10
Train function
Loss = 1.4950e-03, PNorm = 53.0967, GNorm = 0.7084, lr_0 = 8.0357e-04
Validation rmse logS = 0.628702
Validation R2 logS = 0.913124
Epoch 11
Train function
Loss = 1.0921e-03, PNorm = 53.1213, GNorm = 0.4012, lr_0 = 7.8918e-04
Fold 0
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': None,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_305/folds/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=125, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,500,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.4278e-02, PNorm = 55.2298, GNorm = 11.9961, lr_0 = 4.8077e-04
Validation rmse logS = 1.203626
Validation R2 logS = 0.681586
Epoch 1
Train function
Loss = 6.8882e-03, PNorm = 55.2923, GNorm = 3.4466, lr_0 = 8.6154e-04
Validation rmse logS = 0.974743
Validation R2 logS = 0.791172
Epoch 2
Train function
Loss = 3.1968e-03, PNorm = 55.3855, GNorm = 1.5534, lr_0 = 9.8743e-04
Loss = 4.2274e-03, PNorm = 55.4498, GNorm = 2.2093, lr_0 = 9.6974e-04
Validation rmse logS = 0.970936
Validation R2 logS = 0.792800
Epoch 3
Train function
Loss = 3.3582e-03, PNorm = 55.5051, GNorm = 1.2712, lr_0 = 9.5237e-04
Validation rmse logS = 0.800743
Validation R2 logS = 0.859072
Epoch 4
Train function
Loss = 2.5528e-03, PNorm = 55.5476, GNorm = 3.3705, lr_0 = 9.3363e-04
Validation rmse logS = 0.750277
Validation R2 logS = 0.876277
Epoch 5
Train function
Loss = 2.0859e-03, PNorm = 55.5849, GNorm = 0.9487, lr_0 = 9.1690e-04
Loss = 1.8722e-03, PNorm = 55.6212, GNorm = 1.1340, lr_0 = 9.0048e-04
Loss = 4.4965e-03, PNorm = 55.6247, GNorm = 1.1870, lr_0 = 8.9885e-04
Validation rmse logS = 0.718657
Validation R2 logS = 0.886485
Epoch 6
Train function
Loss = 1.7306e-03, PNorm = 55.6632, GNorm = 0.4779, lr_0 = 8.8275e-04
Validation rmse logS = 0.660741
Validation R2 logS = 0.904044
Epoch 7
Train function
Loss = 1.5500e-03, PNorm = 55.7010, GNorm = 1.9389, lr_0 = 8.6694e-04
Validation rmse logS = 0.672728
Validation R2 logS = 0.900531
Epoch 8
Train function
Loss = 1.7939e-03, PNorm = 55.7363, GNorm = 0.9108, lr_0 = 8.4988e-04
Loss = 1.4763e-03, PNorm = 55.7728, GNorm = 2.3679, lr_0 = 8.3466e-04
Validation rmse logS = 0.699613
Validation R2 logS = 0.892422
Epoch 9
Train function
Loss = 1.3398e-03, PNorm = 55.8119, GNorm = 0.7058, lr_0 = 8.1971e-04
Validation rmse logS = 0.620349
Validation R2 logS = 0.915417
Epoch 10
Train function
Loss = 1.3156e-03, PNorm = 55.8559, GNorm = 0.4488, lr_0 = 8.0357e-04
Validation rmse logS = 0.626606
Validation R2 logS = 0.913702
Epoch 11
Train function
Loss = 1.0474e-03, PNorm = 55.8888, GNorm = 0.6909, lr_0 = 7.8918e-04
Loss = 1.1294e-03, PNorm = 55.9249, GNorm = 1.6918, lr_0 = 7.7504e-04
Validation rmse logS = 0.650803
Validation R2 logS = 0.906909
Epoch 12
Train function
Loss = 9.4750e-04, PNorm = 55.9589, GNorm = 1.5728, lr_0 = 7.5979e-04
Validation rmse logS = 0.634985
Validation R2 logS = 0.911379
Epoch 13
Train function
Loss = 1.0875e-03, PNorm = 55.9945, GNorm = 0.6131, lr_0 = 7.4618e-04
Validation rmse logS = 0.698353
Validation R2 logS = 0.892809
Epoch 14
Train function
Loss = 1.5660e-03, PNorm = 56.0350, GNorm = 2.7729, lr_0 = 7.3149e-04
Loss = 1.3775e-03, PNorm = 56.0694, GNorm = 0.5721, lr_0 = 7.1839e-04
Validation rmse logS = 0.613084
Validation R2 logS = 0.917387
Epoch 15
Train function
Loss = 8.9159e-04, PNorm = 56.1073, GNorm = 0.7162, lr_0 = 7.0552e-04
Validation rmse logS = 0.614547
Validation R2 logS = 0.916992
Epoch 16
Train function
Loss = 8.1318e-04, PNorm = 56.1432, GNorm = 0.4703, lr_0 = 6.9163e-04
Validation rmse logS = 0.640512
Validation R2 logS = 0.909830
Epoch 17
Train function
Loss = 9.9510e-04, PNorm = 56.1749, GNorm = 1.3937, lr_0 = 6.7924e-04
Loss = 8.4551e-04, PNorm = 56.2093, GNorm = 1.2809, lr_0 = 6.6708e-04
Validation rmse logS = 0.712031
Validation R2 logS = 0.888569
Epoch 18
Train function
Loss = 1.0198e-03, PNorm = 56.2458, GNorm = 1.8843, lr_0 = 6.5395e-04
Validation rmse logS = 0.845584
Validation R2 logS = 0.842847
Epoch 19
Train function
Loss = 1.3220e-03, PNorm = 56.2826, GNorm = 2.6951, lr_0 = 6.4223e-04
Validation rmse logS = 0.675041
Validation R2 logS = 0.899846
Epoch 20
Train function
Loss = 1.0580e-03, PNorm = 56.3158, GNorm = 1.6040, lr_0 = 6.2959e-04
Loss = 7.6004e-04, PNorm = 56.3480, GNorm = 0.4004, lr_0 = 6.1831e-04
Validation rmse logS = 0.606225
Validation R2 logS = 0.919225
Epoch 21
Train function
Loss = 6.0690e-04, PNorm = 56.3749, GNorm = 0.4794, lr_0 = 6.0724e-04
Validation rmse logS = 0.592548
Validation R2 logS = 0.922829
Epoch 22
Train function
Loss = 4.3964e-04, PNorm = 56.4029, GNorm = 0.5222, lr_0 = 5.9529e-04
Loss = 6.7559e-04, PNorm = 56.4273, GNorm = 0.9983, lr_0 = 5.8462e-04
Validation rmse logS = 0.616965
Validation R2 logS = 0.916338
Epoch 23
Train function
Loss = 6.6498e-04, PNorm = 56.4556, GNorm = 0.5254, lr_0 = 5.7415e-04
Validation rmse logS = 0.609013
Validation R2 logS = 0.918480
Epoch 24
Train function
Loss = 5.7658e-04, PNorm = 56.4824, GNorm = 0.3314, lr_0 = 5.6285e-04
Validation rmse logS = 0.593143
Validation R2 logS = 0.922673
Epoch 25
Train function
Loss = 5.3142e-04, PNorm = 56.5090, GNorm = 0.7450, lr_0 = 5.5277e-04
Loss = 5.7430e-04, PNorm = 56.5362, GNorm = 1.0807, lr_0 = 5.4287e-04
Loss = 1.3501e-03, PNorm = 56.5385, GNorm = 0.5216, lr_0 = 5.4189e-04
Validation rmse logS = 0.586033
Validation R2 logS = 0.924516
Epoch 26
Train function
Loss = 4.7793e-04, PNorm = 56.5621, GNorm = 0.4077, lr_0 = 5.3218e-04
Validation rmse logS = 0.580758
Validation R2 logS = 0.925869
Epoch 27
Train function
Loss = 4.5846e-04, PNorm = 56.5897, GNorm = 0.8592, lr_0 = 5.2171e-04
Validation rmse logS = 0.595478
Validation R2 logS = 0.922064
Epoch 28
Train function
Loss = 6.4332e-04, PNorm = 56.6144, GNorm = 0.6332, lr_0 = 5.1236e-04
Loss = 3.4653e-04, PNorm = 56.6409, GNorm = 0.6036, lr_0 = 5.0318e-04
Loss = 5.5671e-04, PNorm = 56.6432, GNorm = 0.5320, lr_0 = 5.0228e-04
Validation rmse logS = 0.570325
Validation R2 logS = 0.928509
Epoch 29
Train function
Loss = 3.7669e-04, PNorm = 56.6659, GNorm = 0.3517, lr_0 = 4.9328e-04
Validation rmse logS = 0.581692
Validation R2 logS = 0.925630
Epoch 30
Train function
Loss = 3.6163e-04, PNorm = 56.6896, GNorm = 0.6039, lr_0 = 4.8444e-04
Validation rmse logS = 0.585337
Validation R2 logS = 0.924695
Epoch 31
Train function
Loss = 2.7500e-04, PNorm = 56.7127, GNorm = 0.2563, lr_0 = 4.7491e-04
Loss = 3.2677e-04, PNorm = 56.7360, GNorm = 0.6646, lr_0 = 4.6640e-04
Validation rmse logS = 0.585576
Validation R2 logS = 0.924634
Epoch 32
Train function
Loss = 3.2554e-04, PNorm = 56.7585, GNorm = 1.1258, lr_0 = 4.5805e-04
Validation rmse logS = 0.596206
Validation R2 logS = 0.921873
Epoch 33
Train function
Loss = 3.6217e-04, PNorm = 56.7831, GNorm = 0.4407, lr_0 = 4.4903e-04
Validation rmse logS = 0.597346
Validation R2 logS = 0.921574
Epoch 34
Train function
Loss = 2.6125e-04, PNorm = 56.8031, GNorm = 0.4497, lr_0 = 4.4099e-04
Loss = 3.4463e-04, PNorm = 56.8257, GNorm = 0.7539, lr_0 = 4.3309e-04
Validation rmse logS = 0.590262
Validation R2 logS = 0.923423
Epoch 35
Train function
Loss = 3.5163e-04, PNorm = 56.8471, GNorm = 1.4001, lr_0 = 4.2456e-04
Validation rmse logS = 0.604205
Validation R2 logS = 0.919762
Epoch 36
Train function
Loss = 3.0061e-04, PNorm = 56.8694, GNorm = 0.3351, lr_0 = 4.1696e-04
Validation rmse logS = 0.594054
Validation R2 logS = 0.922436
Epoch 37
Train function
Loss = 2.4313e-04, PNorm = 56.8917, GNorm = 0.4165, lr_0 = 4.0875e-04
Loss = 2.6188e-04, PNorm = 56.9091, GNorm = 0.2894, lr_0 = 4.0143e-04
Validation rmse logS = 0.576244
Validation R2 logS = 0.927017
Epoch 38
Train function
Loss = 2.3711e-04, PNorm = 56.9288, GNorm = 0.5810, lr_0 = 3.9424e-04
Validation rmse logS = 0.599853
Validation R2 logS = 0.920914
Epoch 39
Train function
Loss = 2.6943e-04, PNorm = 56.9468, GNorm = 0.8764, lr_0 = 3.8648e-04
Validation rmse logS = 0.597729
Validation R2 logS = 0.921473
Epoch 40
Train function
Loss = 2.6902e-04, PNorm = 56.9641, GNorm = 0.9591, lr_0 = 3.7956e-04
Loss = 2.8636e-04, PNorm = 56.9810, GNorm = 0.4484, lr_0 = 3.7276e-04
Validation rmse logS = 0.601894
Validation R2 logS = 0.920375
Epoch 41
Train function
Loss = 2.3907e-04, PNorm = 56.9990, GNorm = 0.3548, lr_0 = 3.6542e-04
Validation rmse logS = 0.597102
Validation R2 logS = 0.921638
Epoch 42
Train function
Loss = 1.9910e-04, PNorm = 57.0146, GNorm = 0.3727, lr_0 = 3.5888e-04
Validation rmse logS = 0.585845
Validation R2 logS = 0.924565
Epoch 43
Train function
Loss = 1.2314e-04, PNorm = 57.0284, GNorm = 0.1930, lr_0 = 3.5181e-04
Loss = 1.8789e-04, PNorm = 57.0424, GNorm = 0.2705, lr_0 = 3.4551e-04
Validation rmse logS = 0.580747
Validation R2 logS = 0.925872
Epoch 44
Train function
Loss = 2.0884e-04, PNorm = 57.0576, GNorm = 0.2757, lr_0 = 3.3932e-04
Validation rmse logS = 0.583110
Validation R2 logS = 0.925267
Epoch 45
Train function
Loss = 1.9529e-04, PNorm = 57.0717, GNorm = 0.2925, lr_0 = 3.3264e-04
Loss = 1.9534e-04, PNorm = 57.0846, GNorm = 0.6466, lr_0 = 3.2668e-04
Validation rmse logS = 0.579948
Validation R2 logS = 0.926076
Epoch 46
Train function
Loss = 1.5838e-04, PNorm = 57.0986, GNorm = 0.4973, lr_0 = 3.2083e-04
Validation rmse logS = 0.586740
Validation R2 logS = 0.924334
Epoch 47
Train function
Loss = 1.9816e-04, PNorm = 57.1152, GNorm = 0.5388, lr_0 = 3.1452e-04
Validation rmse logS = 0.604612
Validation R2 logS = 0.919654
Epoch 48
Train function
Loss = 1.9157e-04, PNorm = 57.1279, GNorm = 0.3749, lr_0 = 3.0888e-04
Loss = 2.1671e-04, PNorm = 57.1414, GNorm = 0.7151, lr_0 = 3.0335e-04
Loss = 4.5129e-04, PNorm = 57.1422, GNorm = 0.3889, lr_0 = 3.0280e-04
Validation rmse logS = 0.593629
Validation R2 logS = 0.922547
Epoch 49
Train function
Loss = 1.7652e-04, PNorm = 57.1540, GNorm = 0.2746, lr_0 = 2.9738e-04
Validation rmse logS = 0.598875
Validation R2 logS = 0.921172
Epoch 50
Train function
Loss = 1.8725e-04, PNorm = 57.1648, GNorm = 0.9971, lr_0 = 2.9205e-04
Validation rmse logS = 0.584216
Validation R2 logS = 0.924984
Epoch 51
Train function
Loss = 1.2199e-04, PNorm = 57.1779, GNorm = 0.5814, lr_0 = 2.8630e-04
Loss = 1.7987e-04, PNorm = 57.1875, GNorm = 0.6530, lr_0 = 2.8118e-04
Loss = 1.6431e-04, PNorm = 57.1886, GNorm = 0.2955, lr_0 = 2.8067e-04
Validation rmse logS = 0.588646
Validation R2 logS = 0.923842
Epoch 52
Train function
Loss = 1.6500e-04, PNorm = 57.1997, GNorm = 0.6634, lr_0 = 2.7564e-04
Validation rmse logS = 0.586664
Validation R2 logS = 0.924354
Epoch 53
Train function
Loss = 1.1847e-04, PNorm = 57.2092, GNorm = 0.4242, lr_0 = 2.7070e-04
Validation rmse logS = 0.587126
Validation R2 logS = 0.924234
Epoch 54
Train function
Loss = 8.7715e-05, PNorm = 57.2200, GNorm = 0.1570, lr_0 = 2.6538e-04
Loss = 1.3829e-04, PNorm = 57.2280, GNorm = 0.4179, lr_0 = 2.6062e-04
Validation rmse logS = 0.588533
Validation R2 logS = 0.923871
Epoch 55
Train function
Loss = 1.2633e-04, PNorm = 57.2376, GNorm = 0.2058, lr_0 = 2.5595e-04
Validation rmse logS = 0.592140
Validation R2 logS = 0.922935
Epoch 56
Train function
Loss = 1.2206e-04, PNorm = 57.2464, GNorm = 0.2080, lr_0 = 2.5092e-04
Validation rmse logS = 0.605238
Validation R2 logS = 0.919488
Epoch 57
Train function
Loss = 8.8039e-05, PNorm = 57.2558, GNorm = 0.2791, lr_0 = 2.4642e-04
Loss = 1.2916e-04, PNorm = 57.2639, GNorm = 0.2397, lr_0 = 2.4201e-04
Validation rmse logS = 0.592446
Validation R2 logS = 0.922855
Epoch 58
Train function
Loss = 1.2701e-04, PNorm = 57.2720, GNorm = 0.6360, lr_0 = 2.3724e-04
Validation rmse logS = 0.584666
Validation R2 logS = 0.924868
Epoch 59
Train function
Loss = 1.0418e-04, PNorm = 57.2821, GNorm = 0.2147, lr_0 = 2.3300e-04
Validation rmse logS = 0.587284
Validation R2 logS = 0.924194
Epoch 60
Train function
Loss = 1.2493e-04, PNorm = 57.2907, GNorm = 0.3305, lr_0 = 2.2841e-04
Loss = 9.7343e-05, PNorm = 57.2987, GNorm = 0.3866, lr_0 = 2.2432e-04
Validation rmse logS = 0.590454
Validation R2 logS = 0.923373
Epoch 61
Train function
Loss = 1.0634e-04, PNorm = 57.3066, GNorm = 0.1364, lr_0 = 2.2030e-04
Validation rmse logS = 0.594212
Validation R2 logS = 0.922395
Epoch 62
Train function
Loss = 1.0318e-04, PNorm = 57.3144, GNorm = 0.2411, lr_0 = 2.1596e-04
Validation rmse logS = 0.586113
Validation R2 logS = 0.924496
Epoch 63
Train function
Loss = 1.0895e-04, PNorm = 57.3219, GNorm = 0.3486, lr_0 = 2.1210e-04
Loss = 9.1529e-05, PNorm = 57.3285, GNorm = 0.2176, lr_0 = 2.0830e-04
Validation rmse logS = 0.596074
Validation R2 logS = 0.921907
Epoch 64
Train function
Loss = 7.5414e-05, PNorm = 57.3346, GNorm = 0.4949, lr_0 = 2.0420e-04
Validation rmse logS = 0.595266
Validation R2 logS = 0.922119
Epoch 65
Train function
Loss = 9.2286e-05, PNorm = 57.3417, GNorm = 0.5252, lr_0 = 2.0054e-04
Validation rmse logS = 0.599204
Validation R2 logS = 0.921085
Epoch 66
Train function
Loss = 1.1984e-04, PNorm = 57.3493, GNorm = 0.6325, lr_0 = 1.9659e-04
Loss = 9.5016e-05, PNorm = 57.3548, GNorm = 0.3936, lr_0 = 1.9307e-04
Validation rmse logS = 0.589504
Validation R2 logS = 0.923619
Epoch 67
Train function
Loss = 1.1315e-04, PNorm = 57.3620, GNorm = 0.3588, lr_0 = 1.8961e-04
Validation rmse logS = 0.590146
Validation R2 logS = 0.923453
Epoch 68
Train function
Loss = 7.4330e-05, PNorm = 57.3673, GNorm = 0.1308, lr_0 = 1.8588e-04
Loss = 1.1023e-04, PNorm = 57.3727, GNorm = 1.0716, lr_0 = 1.8255e-04
Validation rmse logS = 0.587195
Validation R2 logS = 0.924217
Epoch 69
Train function
Loss = 9.1662e-05, PNorm = 57.3788, GNorm = 0.3749, lr_0 = 1.7928e-04
Validation rmse logS = 0.598617
Validation R2 logS = 0.921240
Epoch 70
Train function
Loss = 8.3833e-05, PNorm = 57.3848, GNorm = 0.1485, lr_0 = 1.7575e-04
Validation rmse logS = 0.589285
Validation R2 logS = 0.923676
Epoch 71
Train function
Loss = 1.0154e-04, PNorm = 57.3907, GNorm = 0.3424, lr_0 = 1.7260e-04
Loss = 8.6593e-05, PNorm = 57.3956, GNorm = 0.2604, lr_0 = 1.6951e-04
Loss = 8.3211e-05, PNorm = 57.3960, GNorm = 0.1758, lr_0 = 1.6921e-04
Validation rmse logS = 0.590967
Validation R2 logS = 0.923240
Epoch 72
Train function
Loss = 8.1970e-05, PNorm = 57.4020, GNorm = 0.1743, lr_0 = 1.6617e-04
Validation rmse logS = 0.590791
Validation R2 logS = 0.923286
Epoch 73
Train function
Loss = 7.5416e-05, PNorm = 57.4059, GNorm = 0.1816, lr_0 = 1.6320e-04
Validation rmse logS = 0.593009
Validation R2 logS = 0.922709
Epoch 74
Train function
Loss = 8.9074e-05, PNorm = 57.4117, GNorm = 0.1686, lr_0 = 1.5999e-04
Loss = 6.5936e-05, PNorm = 57.4166, GNorm = 0.1696, lr_0 = 1.5712e-04
Validation rmse logS = 0.593525
Validation R2 logS = 0.922574
Epoch 75
Train function
Loss = 5.7676e-05, PNorm = 57.4221, GNorm = 0.3028, lr_0 = 1.5431e-04
Validation rmse logS = 0.595313
Validation R2 logS = 0.922107
Epoch 76
Train function
Loss = 4.5413e-05, PNorm = 57.4267, GNorm = 0.1340, lr_0 = 1.5127e-04
Validation rmse logS = 0.596924
Validation R2 logS = 0.921684
Epoch 77
Train function
Loss = 5.6774e-05, PNorm = 57.4311, GNorm = 0.2512, lr_0 = 1.4829e-04
Loss = 7.2400e-05, PNorm = 57.4356, GNorm = 0.2791, lr_0 = 1.4563e-04
Validation rmse logS = 0.592134
Validation R2 logS = 0.922936
Epoch 78
Train function
Loss = 7.6313e-05, PNorm = 57.4406, GNorm = 0.2144, lr_0 = 1.4303e-04
Validation rmse logS = 0.601646
Validation R2 logS = 0.920441
Epoch 79
Train function
Loss = 7.6503e-05, PNorm = 57.4444, GNorm = 0.4459, lr_0 = 1.4021e-04
Validation rmse logS = 0.597191
Validation R2 logS = 0.921615
Epoch 80
Train function
Loss = 3.6161e-05, PNorm = 57.4486, GNorm = 0.2815, lr_0 = 1.3770e-04
Loss = 7.7406e-05, PNorm = 57.4521, GNorm = 0.2245, lr_0 = 1.3523e-04
Validation rmse logS = 0.596332
Validation R2 logS = 0.921840
Epoch 81
Train function
Loss = 6.4347e-05, PNorm = 57.4564, GNorm = 0.2834, lr_0 = 1.3257e-04
Validation rmse logS = 0.588653
Validation R2 logS = 0.923840
Epoch 82
Train function
Loss = 5.8771e-05, PNorm = 57.4601, GNorm = 0.1454, lr_0 = 1.3020e-04
Validation rmse logS = 0.601106
Validation R2 logS = 0.920583
Epoch 83
Train function
Loss = 7.2972e-05, PNorm = 57.4638, GNorm = 0.5651, lr_0 = 1.2763e-04
Loss = 6.2646e-05, PNorm = 57.4680, GNorm = 0.1378, lr_0 = 1.2535e-04
Validation rmse logS = 0.592863
Validation R2 logS = 0.922747
Epoch 84
Train function
Loss = 4.9487e-05, PNorm = 57.4718, GNorm = 0.1565, lr_0 = 1.2310e-04
Validation rmse logS = 0.598690
Validation R2 logS = 0.921221
Epoch 85
Train function
Loss = 5.5449e-05, PNorm = 57.4762, GNorm = 0.2171, lr_0 = 1.2068e-04
Validation rmse logS = 0.593868
Validation R2 logS = 0.922484
Epoch 86
Train function
Loss = 2.7813e-05, PNorm = 57.4796, GNorm = 0.1691, lr_0 = 1.1852e-04
Loss = 6.9442e-05, PNorm = 57.4825, GNorm = 0.3295, lr_0 = 1.1639e-04
Validation rmse logS = 0.591970
Validation R2 logS = 0.922979
Epoch 87
Train function
Loss = 5.9179e-05, PNorm = 57.4866, GNorm = 0.1564, lr_0 = 1.1410e-04
Validation rmse logS = 0.593929
Validation R2 logS = 0.922468
Epoch 88
Train function
Loss = 4.8526e-05, PNorm = 57.4897, GNorm = 0.1356, lr_0 = 1.1206e-04
Validation rmse logS = 0.597073
Validation R2 logS = 0.921645
Epoch 89
Train function
Loss = 2.1761e-05, PNorm = 57.4928, GNorm = 0.1503, lr_0 = 1.0985e-04
Loss = 5.8502e-05, PNorm = 57.4960, GNorm = 0.1875, lr_0 = 1.0789e-04
Validation rmse logS = 0.593990
Validation R2 logS = 0.922453
Epoch 90
Train function
Loss = 4.2215e-05, PNorm = 57.4991, GNorm = 0.1162, lr_0 = 1.0595e-04
Validation rmse logS = 0.589803
Validation R2 logS = 0.923542
Epoch 91
Train function
Loss = 4.7935e-05, PNorm = 57.5017, GNorm = 0.1751, lr_0 = 1.0387e-04
Loss = 5.2244e-05, PNorm = 57.5045, GNorm = 0.2412, lr_0 = 1.0201e-04
Validation rmse logS = 0.596704
Validation R2 logS = 0.921742
Epoch 92
Train function
Loss = 4.8954e-05, PNorm = 57.5072, GNorm = 0.2016, lr_0 = 1.0018e-04
Validation rmse logS = 0.593342
Validation R2 logS = 0.922622
Epoch 93
Train function
Loss = 6.0738e-05, PNorm = 57.5104, GNorm = 0.1408, lr_0 = 1.0000e-04
Validation rmse logS = 0.591953
Validation R2 logS = 0.922984
Epoch 94
Train function
Loss = 7.1135e-05, PNorm = 57.5137, GNorm = 0.3201, lr_0 = 1.0000e-04
Loss = 4.0369e-05, PNorm = 57.5163, GNorm = 0.1308, lr_0 = 1.0000e-04
Loss = 2.7855e-04, PNorm = 57.5167, GNorm = 0.3330, lr_0 = 1.0000e-04
Validation rmse logS = 0.595222
Validation R2 logS = 0.922130
Epoch 95
Train function
Loss = 4.2127e-05, PNorm = 57.5198, GNorm = 0.1708, lr_0 = 1.0000e-04
Validation rmse logS = 0.595789
Validation R2 logS = 0.921982
Epoch 96
Train function
Loss = 4.6116e-05, PNorm = 57.5227, GNorm = 0.2936, lr_0 = 1.0000e-04
Validation rmse logS = 0.600642
Validation R2 logS = 0.920706
Epoch 97
Train function
Loss = 5.2481e-05, PNorm = 57.5255, GNorm = 0.2947, lr_0 = 1.0000e-04
Loss = 5.3318e-05, PNorm = 57.5285, GNorm = 0.1798, lr_0 = 1.0000e-04
Validation rmse logS = 0.597259
Validation R2 logS = 0.921597
Epoch 98
Train function
Loss = 4.7720e-05, PNorm = 57.5309, GNorm = 0.2245, lr_0 = 1.0000e-04
Validation rmse logS = 0.601032
Validation R2 logS = 0.920603
Epoch 99
Train function
Loss = 4.8115e-05, PNorm = 57.5340, GNorm = 0.0993, lr_0 = 1.0000e-04
Validation rmse logS = 0.593432
Validation R2 logS = 0.922598
Model 0 best validation rmse = 0.570325 on epoch 28
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "substructures_encoder.encoder.cached_zero_vector".
Loading pretrained parameter "substructures_encoder.encoder.W_o.weight".
Loading pretrained parameter "substructures_encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.611049
Model 0 test R2 logS = 0.902389
Ensemble test rmse  logS= 0.611049
Ensemble test R2  logS= 0.902389
Fold 1
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_305/folds/fold_1',
 'save_smiles_splits': False,
 'seed': 1,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 674,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 1
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=125, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,500,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.6005e-02, PNorm = 55.2280, GNorm = 10.5405, lr_0 = 4.8077e-04
Validation rmse logS = 1.335837
Validation R2 logS = 0.577045
Epoch 1
Train function
Loss = 7.7318e-03, PNorm = 55.2939, GNorm = 4.3725, lr_0 = 8.6154e-04
Validation rmse logS = 0.998707
Validation R2 logS = 0.763591
Epoch 2
Train function
Loss = 4.6142e-03, PNorm = 55.3921, GNorm = 3.4265, lr_0 = 9.8743e-04
Loss = 3.1902e-03, PNorm = 55.4579, GNorm = 1.5659, lr_0 = 9.6974e-04
Validation rmse logS = 0.891284
Validation R2 logS = 0.811713
Epoch 3
Train function
Loss = 2.9609e-03, PNorm = 55.5073, GNorm = 1.6651, lr_0 = 9.5237e-04
Validation rmse logS = 0.813054
Validation R2 logS = 0.843315
Epoch 4
Train function
Loss = 2.1802e-03, PNorm = 55.5559, GNorm = 1.5766, lr_0 = 9.3363e-04
Validation rmse logS = 0.765994
Validation R2 logS = 0.860928
Epoch 5
Train function
Loss = 2.5727e-03, PNorm = 55.5926, GNorm = 0.9144, lr_0 = 9.1690e-04
Loss = 2.0979e-03, PNorm = 55.6287, GNorm = 1.5305, lr_0 = 9.0048e-04
Loss = 2.7228e-03, PNorm = 55.6309, GNorm = 1.5563, lr_0 = 8.9885e-04
Validation rmse logS = 0.721673
Validation R2 logS = 0.876556
Epoch 6
Train function
Loss = 1.6977e-03, PNorm = 55.6609, GNorm = 1.0564, lr_0 = 8.8275e-04
Validation rmse logS = 0.777430
Validation R2 logS = 0.856745
Epoch 7
Train function
Loss = 1.9205e-03, PNorm = 55.6898, GNorm = 2.1120, lr_0 = 8.6694e-04
Validation rmse logS = 0.688159
Validation R2 logS = 0.887756
Epoch 8
Train function
Loss = 1.3649e-03, PNorm = 55.7303, GNorm = 0.6165, lr_0 = 8.4988e-04
Loss = 1.4018e-03, PNorm = 55.7696, GNorm = 0.4853, lr_0 = 8.3466e-04
Validation rmse logS = 0.666197
Validation R2 logS = 0.894806
Epoch 9
Train function
Loss = 1.3113e-03, PNorm = 55.8037, GNorm = 0.5304, lr_0 = 8.1971e-04
Validation rmse logS = 0.728540
Validation R2 logS = 0.874196
Epoch 10
Train function
Loss = 1.3228e-03, PNorm = 55.8403, GNorm = 0.4809, lr_0 = 8.0357e-04
Validation rmse logS = 0.733108
Validation R2 logS = 0.872614
Epoch 11
Train function
Loss = 2.5651e-03, PNorm = 55.8753, GNorm = 4.2190, lr_0 = 7.8918e-04
Loss = 1.6740e-03, PNorm = 55.9150, GNorm = 0.9586, lr_0 = 7.7504e-04
Validation rmse logS = 0.690239
Validation R2 logS = 0.887076
Epoch 12
Train function
Loss = 1.2982e-03, PNorm = 55.9553, GNorm = 0.4555, lr_0 = 7.5979e-04
Validation rmse logS = 0.644557
Validation R2 logS = 0.901529
Epoch 13
Train function
Loss = 1.0583e-03, PNorm = 55.9881, GNorm = 0.4323, lr_0 = 7.4618e-04
Validation rmse logS = 0.640025
Validation R2 logS = 0.902909
Epoch 14
Train function
Loss = 8.8884e-04, PNorm = 56.0216, GNorm = 0.4132, lr_0 = 7.3149e-04
Loss = 1.0790e-03, PNorm = 56.0566, GNorm = 1.7023, lr_0 = 7.1839e-04
Validation rmse logS = 0.697175
Validation R2 logS = 0.884795
Epoch 15
Train function
Loss = 1.0684e-03, PNorm = 56.0882, GNorm = 1.0838, lr_0 = 7.0552e-04
Validation rmse logS = 0.623863
Validation R2 logS = 0.907750
Epoch 16
Train function
Loss = 9.8544e-04, PNorm = 56.1287, GNorm = 1.7658, lr_0 = 6.9163e-04
Validation rmse logS = 0.754734
Validation R2 logS = 0.864987
Epoch 17
Train function
Loss = 2.4931e-03, PNorm = 56.1639, GNorm = 3.6157, lr_0 = 6.7924e-04
Loss = 1.0254e-03, PNorm = 56.1991, GNorm = 0.8288, lr_0 = 6.6708e-04
Validation rmse logS = 0.616432
Validation R2 logS = 0.909935
Epoch 18
Train function
Loss = 9.7490e-04, PNorm = 56.2352, GNorm = 1.1953, lr_0 = 6.5395e-04
Validation rmse logS = 0.613683
Validation R2 logS = 0.910736
Epoch 19
Train function
Loss = 6.5316e-04, PNorm = 56.2620, GNorm = 1.0816, lr_0 = 6.4223e-04
Validation rmse logS = 0.613206
Validation R2 logS = 0.910875
Epoch 20
Train function
Loss = 8.7175e-04, PNorm = 56.2912, GNorm = 0.5340, lr_0 = 6.2959e-04
Loss = 6.1068e-04, PNorm = 56.3188, GNorm = 0.4418, lr_0 = 6.1831e-04
Validation rmse logS = 0.605127
Validation R2 logS = 0.913208
Epoch 21
Train function
Loss = 6.1216e-04, PNorm = 56.3535, GNorm = 0.4380, lr_0 = 6.0724e-04
Validation rmse logS = 0.604318
Validation R2 logS = 0.913440
Epoch 22
Train function
Loss = 5.7479e-04, PNorm = 56.3852, GNorm = 0.7078, lr_0 = 5.9529e-04
Loss = 6.7694e-04, PNorm = 56.4103, GNorm = 1.2548, lr_0 = 5.8462e-04
Validation rmse logS = 0.610101
Validation R2 logS = 0.911775
Epoch 23
Train function
Loss = 7.0334e-04, PNorm = 56.4380, GNorm = 0.7291, lr_0 = 5.7415e-04
Validation rmse logS = 0.669306
Validation R2 logS = 0.893822
Epoch 24
Train function
Loss = 7.2276e-04, PNorm = 56.4721, GNorm = 1.7730, lr_0 = 5.6285e-04
Validation rmse logS = 0.612043
Validation R2 logS = 0.911213
Epoch 25
Train function
Loss = 6.1545e-04, PNorm = 56.5038, GNorm = 1.2393, lr_0 = 5.5277e-04
Loss = 5.2807e-04, PNorm = 56.5361, GNorm = 0.7854, lr_0 = 5.4287e-04
Loss = 7.3017e-04, PNorm = 56.5384, GNorm = 0.3242, lr_0 = 5.4189e-04
Validation rmse logS = 0.604399
Validation R2 logS = 0.913417
Epoch 26
Train function
Loss = 5.0793e-04, PNorm = 56.5648, GNorm = 0.8713, lr_0 = 5.3218e-04
Validation rmse logS = 0.601631
Validation R2 logS = 0.914208
Epoch 27
Train function
Loss = 4.3595e-04, PNorm = 56.5897, GNorm = 0.4493, lr_0 = 5.2171e-04
Validation rmse logS = 0.607921
Validation R2 logS = 0.912405
Epoch 28
Train function
Loss = 2.9067e-04, PNorm = 56.6149, GNorm = 0.5376, lr_0 = 5.1236e-04
Loss = 4.3046e-04, PNorm = 56.6381, GNorm = 0.4061, lr_0 = 5.0318e-04
Loss = 1.2750e-03, PNorm = 56.6403, GNorm = 1.1756, lr_0 = 5.0228e-04
Validation rmse logS = 0.609143
Validation R2 logS = 0.912052
Epoch 29
Train function
Loss = 4.3535e-04, PNorm = 56.6638, GNorm = 0.4750, lr_0 = 4.9328e-04
Validation rmse logS = 0.629205
Validation R2 logS = 0.906164
Epoch 30
Train function
Loss = 4.6067e-04, PNorm = 56.6923, GNorm = 0.9232, lr_0 = 4.8444e-04
Validation rmse logS = 0.621780
Validation R2 logS = 0.908365
Epoch 31
Train function
Loss = 3.2632e-04, PNorm = 56.7174, GNorm = 0.3726, lr_0 = 4.7491e-04
Loss = 4.1993e-04, PNorm = 56.7432, GNorm = 0.9523, lr_0 = 4.6640e-04
Validation rmse logS = 0.625196
Validation R2 logS = 0.907356
Epoch 32
Train function
Loss = 4.0568e-04, PNorm = 56.7666, GNorm = 0.8343, lr_0 = 4.5805e-04
Validation rmse logS = 0.600112
Validation R2 logS = 0.914640
Epoch 33
Train function
Loss = 3.2305e-04, PNorm = 56.7913, GNorm = 0.2748, lr_0 = 4.4903e-04
Validation rmse logS = 0.593192
Validation R2 logS = 0.916598
Epoch 34
Train function
Loss = 2.2938e-04, PNorm = 56.8121, GNorm = 0.4395, lr_0 = 4.4099e-04
Loss = 3.5268e-04, PNorm = 56.8303, GNorm = 0.9028, lr_0 = 4.3309e-04
Validation rmse logS = 0.601802
Validation R2 logS = 0.914159
Epoch 35
Train function
Loss = 3.1355e-04, PNorm = 56.8545, GNorm = 0.2269, lr_0 = 4.2456e-04
Validation rmse logS = 0.609735
Validation R2 logS = 0.911881
Epoch 36
Train function
Loss = 3.3495e-04, PNorm = 56.8730, GNorm = 0.5094, lr_0 = 4.1696e-04
Validation rmse logS = 0.609279
Validation R2 logS = 0.912013
Epoch 37
Train function
Loss = 2.6112e-04, PNorm = 56.8901, GNorm = 0.3377, lr_0 = 4.0875e-04
Loss = 3.5729e-04, PNorm = 56.9068, GNorm = 0.6592, lr_0 = 4.0143e-04
Validation rmse logS = 0.601265
Validation R2 logS = 0.914312
Epoch 38
Train function
Loss = 2.6941e-04, PNorm = 56.9267, GNorm = 0.5631, lr_0 = 3.9424e-04
Validation rmse logS = 0.610856
Validation R2 logS = 0.911557
Epoch 39
Train function
Loss = 2.3653e-04, PNorm = 56.9443, GNorm = 0.4329, lr_0 = 3.8648e-04
Validation rmse logS = 0.604501
Validation R2 logS = 0.913387
Epoch 40
Train function
Loss = 3.6416e-04, PNorm = 56.9604, GNorm = 0.9990, lr_0 = 3.7956e-04
Loss = 2.4213e-04, PNorm = 56.9735, GNorm = 0.2942, lr_0 = 3.7276e-04
Validation rmse logS = 0.612618
Validation R2 logS = 0.911046
Epoch 41
Train function
Loss = 1.9886e-04, PNorm = 56.9884, GNorm = 0.2614, lr_0 = 3.6542e-04
Validation rmse logS = 0.600145
Validation R2 logS = 0.914631
Epoch 42
Train function
Loss = 2.8142e-04, PNorm = 57.0009, GNorm = 0.2434, lr_0 = 3.5888e-04
Validation rmse logS = 0.608342
Validation R2 logS = 0.912283
Epoch 43
Train function
Loss = 1.1637e-04, PNorm = 57.0158, GNorm = 0.2421, lr_0 = 3.5181e-04
Loss = 1.8494e-04, PNorm = 57.0298, GNorm = 0.1630, lr_0 = 3.4551e-04
Validation rmse logS = 0.599310
Validation R2 logS = 0.914869
Epoch 44
Train function
Loss = 1.5653e-04, PNorm = 57.0418, GNorm = 0.2069, lr_0 = 3.3932e-04
Validation rmse logS = 0.597443
Validation R2 logS = 0.915398
Epoch 45
Train function
Loss = 1.8824e-04, PNorm = 57.0521, GNorm = 0.5118, lr_0 = 3.3264e-04
Loss = 2.2015e-04, PNorm = 57.0643, GNorm = 0.6819, lr_0 = 3.2668e-04
Validation rmse logS = 0.608396
Validation R2 logS = 0.912268
Epoch 46
Train function
Loss = 1.7553e-04, PNorm = 57.0747, GNorm = 0.3490, lr_0 = 3.2083e-04
Validation rmse logS = 0.596304
Validation R2 logS = 0.915720
Epoch 47
Train function
Loss = 1.3615e-04, PNorm = 57.0869, GNorm = 0.2338, lr_0 = 3.1452e-04
Validation rmse logS = 0.597207
Validation R2 logS = 0.915465
Epoch 48
Train function
Loss = 1.0685e-04, PNorm = 57.0974, GNorm = 0.2875, lr_0 = 3.0888e-04
Loss = 1.5210e-04, PNorm = 57.1078, GNorm = 0.2323, lr_0 = 3.0335e-04
Loss = 4.8640e-04, PNorm = 57.1093, GNorm = 0.5795, lr_0 = 3.0280e-04
Validation rmse logS = 0.604351
Validation R2 logS = 0.913430
Epoch 49
Train function
Loss = 1.5370e-04, PNorm = 57.1207, GNorm = 0.7822, lr_0 = 2.9738e-04
Validation rmse logS = 0.592014
Validation R2 logS = 0.916929
Epoch 50
Train function
Loss = 2.4857e-04, PNorm = 57.1340, GNorm = 1.3367, lr_0 = 2.9205e-04
Validation rmse logS = 0.610629
Validation R2 logS = 0.911623
Epoch 51
Train function
Loss = 1.9871e-04, PNorm = 57.1465, GNorm = 0.2227, lr_0 = 2.8630e-04
Loss = 1.8799e-04, PNorm = 57.1584, GNorm = 0.7378, lr_0 = 2.8118e-04
Loss = 8.8842e-05, PNorm = 57.1596, GNorm = 0.2244, lr_0 = 2.8067e-04
Validation rmse logS = 0.602119
Validation R2 logS = 0.914069
Epoch 52
Train function
Loss = 1.9617e-04, PNorm = 57.1682, GNorm = 0.2267, lr_0 = 2.7564e-04
Validation rmse logS = 0.604144
Validation R2 logS = 0.913490
Epoch 53
Train function
Loss = 1.8301e-04, PNorm = 57.1786, GNorm = 0.2945, lr_0 = 2.7070e-04
Validation rmse logS = 0.602495
Validation R2 logS = 0.913961
Epoch 54
Train function
Loss = 1.5421e-04, PNorm = 57.1913, GNorm = 0.6594, lr_0 = 2.6538e-04
Loss = 1.2949e-04, PNorm = 57.2019, GNorm = 0.2119, lr_0 = 2.6062e-04
Validation rmse logS = 0.602430
Validation R2 logS = 0.913980
Epoch 55
Train function
Loss = 1.7376e-04, PNorm = 57.2096, GNorm = 0.3196, lr_0 = 2.5595e-04
Validation rmse logS = 0.602303
Validation R2 logS = 0.914016
Epoch 56
Train function
Loss = 1.3147e-04, PNorm = 57.2174, GNorm = 0.3385, lr_0 = 2.5092e-04
Validation rmse logS = 0.609087
Validation R2 logS = 0.912068
Epoch 57
Train function
Loss = 1.6864e-04, PNorm = 57.2239, GNorm = 0.5999, lr_0 = 2.4642e-04
Loss = 1.2304e-04, PNorm = 57.2316, GNorm = 0.2141, lr_0 = 2.4201e-04
Validation rmse logS = 0.601626
Validation R2 logS = 0.914209
Epoch 58
Train function
Loss = 1.0184e-04, PNorm = 57.2399, GNorm = 0.2184, lr_0 = 2.3724e-04
Validation rmse logS = 0.597210
Validation R2 logS = 0.915464
Epoch 59
Train function
Loss = 9.5273e-05, PNorm = 57.2485, GNorm = 0.4906, lr_0 = 2.3300e-04
Validation rmse logS = 0.608145
Validation R2 logS = 0.912340
Epoch 60
Train function
Loss = 1.0010e-04, PNorm = 57.2560, GNorm = 0.4572, lr_0 = 2.2841e-04
Loss = 1.1181e-04, PNorm = 57.2631, GNorm = 0.3549, lr_0 = 2.2432e-04
Validation rmse logS = 0.607098
Validation R2 logS = 0.912642
Epoch 61
Train function
Loss = 9.9164e-05, PNorm = 57.2712, GNorm = 0.2571, lr_0 = 2.2030e-04
Validation rmse logS = 0.599599
Validation R2 logS = 0.914786
Epoch 62
Train function
Loss = 9.7306e-05, PNorm = 57.2782, GNorm = 0.2646, lr_0 = 2.1596e-04
Validation rmse logS = 0.604270
Validation R2 logS = 0.913454
Epoch 63
Train function
Loss = 2.2355e-04, PNorm = 57.2844, GNorm = 0.6053, lr_0 = 2.1210e-04
Loss = 9.3423e-05, PNorm = 57.2905, GNorm = 0.2927, lr_0 = 2.0830e-04
Validation rmse logS = 0.598600
Validation R2 logS = 0.915070
Epoch 64
Train function
Loss = 1.0087e-04, PNorm = 57.2976, GNorm = 0.6549, lr_0 = 2.0420e-04
Validation rmse logS = 0.620899
Validation R2 logS = 0.908625
Epoch 65
Train function
Loss = 1.0559e-04, PNorm = 57.3046, GNorm = 0.3709, lr_0 = 2.0054e-04
Validation rmse logS = 0.609082
Validation R2 logS = 0.912070
Epoch 66
Train function
Loss = 1.4693e-04, PNorm = 57.3111, GNorm = 0.1725, lr_0 = 1.9659e-04
Loss = 6.7065e-05, PNorm = 57.3171, GNorm = 0.1992, lr_0 = 1.9307e-04
Validation rmse logS = 0.604743
Validation R2 logS = 0.913318
Epoch 67
Train function
Loss = 6.4668e-05, PNorm = 57.3216, GNorm = 0.1729, lr_0 = 1.8961e-04
Validation rmse logS = 0.605328
Validation R2 logS = 0.913150
Epoch 68
Train function
Loss = 7.7517e-05, PNorm = 57.3285, GNorm = 0.2543, lr_0 = 1.8588e-04
Loss = 7.2384e-05, PNorm = 57.3341, GNorm = 0.3497, lr_0 = 1.8255e-04
Validation rmse logS = 0.604371
Validation R2 logS = 0.913425
Epoch 69
Train function
Loss = 7.2201e-05, PNorm = 57.3391, GNorm = 0.2298, lr_0 = 1.7928e-04
Validation rmse logS = 0.604268
Validation R2 logS = 0.913454
Epoch 70
Train function
Loss = 6.9272e-05, PNorm = 57.3449, GNorm = 0.1864, lr_0 = 1.7575e-04
Validation rmse logS = 0.609924
Validation R2 logS = 0.911826
Epoch 71
Train function
Loss = 5.8425e-05, PNorm = 57.3500, GNorm = 0.1274, lr_0 = 1.7260e-04
Loss = 8.1278e-05, PNorm = 57.3550, GNorm = 0.4369, lr_0 = 1.6951e-04
Loss = 1.3582e-04, PNorm = 57.3554, GNorm = 0.1840, lr_0 = 1.6921e-04
Validation rmse logS = 0.612167
Validation R2 logS = 0.911177
Epoch 72
Train function
Loss = 7.1559e-05, PNorm = 57.3609, GNorm = 0.3945, lr_0 = 1.6617e-04
Validation rmse logS = 0.616153
Validation R2 logS = 0.910016
Epoch 73
Train function
Loss = 6.6548e-05, PNorm = 57.3655, GNorm = 0.1890, lr_0 = 1.6320e-04
Validation rmse logS = 0.602341
Validation R2 logS = 0.914005
Epoch 74
Train function
Loss = 6.3053e-05, PNorm = 57.3707, GNorm = 0.2532, lr_0 = 1.5999e-04
Loss = 6.2951e-05, PNorm = 57.3751, GNorm = 0.3289, lr_0 = 1.5712e-04
Validation rmse logS = 0.608005
Validation R2 logS = 0.912380
Epoch 75
Train function
Loss = 6.5915e-05, PNorm = 57.3800, GNorm = 0.2409, lr_0 = 1.5431e-04
Validation rmse logS = 0.609454
Validation R2 logS = 0.911962
Epoch 76
Train function
Loss = 7.4684e-05, PNorm = 57.3844, GNorm = 0.3516, lr_0 = 1.5127e-04
Validation rmse logS = 0.608496
Validation R2 logS = 0.912239
Epoch 77
Train function
Loss = 5.6703e-05, PNorm = 57.3892, GNorm = 0.1523, lr_0 = 1.4829e-04
Loss = 6.2039e-05, PNorm = 57.3931, GNorm = 0.3615, lr_0 = 1.4563e-04
Validation rmse logS = 0.612768
Validation R2 logS = 0.911002
Epoch 78
Train function
Loss = 6.6300e-05, PNorm = 57.3977, GNorm = 0.3131, lr_0 = 1.4303e-04
Validation rmse logS = 0.611548
Validation R2 logS = 0.911356
Epoch 79
Train function
Loss = 5.6391e-05, PNorm = 57.4020, GNorm = 0.3806, lr_0 = 1.4021e-04
Validation rmse logS = 0.605234
Validation R2 logS = 0.913177
Epoch 80
Train function
Loss = 7.8504e-05, PNorm = 57.4049, GNorm = 0.2373, lr_0 = 1.3770e-04
Loss = 5.2192e-05, PNorm = 57.4090, GNorm = 0.1845, lr_0 = 1.3523e-04
Validation rmse logS = 0.613703
Validation R2 logS = 0.910730
Epoch 81
Train function
Loss = 5.8235e-05, PNorm = 57.4136, GNorm = 0.1449, lr_0 = 1.3257e-04
Validation rmse logS = 0.610898
Validation R2 logS = 0.911545
Epoch 82
Train function
Loss = 4.7819e-05, PNorm = 57.4166, GNorm = 0.1738, lr_0 = 1.3020e-04
Validation rmse logS = 0.609343
Validation R2 logS = 0.911994
Epoch 83
Train function
Loss = 5.0126e-05, PNorm = 57.4203, GNorm = 0.1908, lr_0 = 1.2763e-04
Loss = 5.3598e-05, PNorm = 57.4238, GNorm = 0.1019, lr_0 = 1.2535e-04
Validation rmse logS = 0.611408
Validation R2 logS = 0.911397
Epoch 84
Train function
Loss = 3.5528e-05, PNorm = 57.4265, GNorm = 0.1818, lr_0 = 1.2310e-04
Validation rmse logS = 0.608865
Validation R2 logS = 0.912132
Epoch 85
Train function
Loss = 2.9870e-05, PNorm = 57.4301, GNorm = 0.0961, lr_0 = 1.2068e-04
Validation rmse logS = 0.612925
Validation R2 logS = 0.910957
Epoch 86
Train function
Loss = 4.2944e-05, PNorm = 57.4333, GNorm = 0.1817, lr_0 = 1.1852e-04
Loss = 4.4889e-05, PNorm = 57.4361, GNorm = 0.1849, lr_0 = 1.1639e-04
Validation rmse logS = 0.607485
Validation R2 logS = 0.912530
Epoch 87
Train function
Loss = 5.4380e-05, PNorm = 57.4395, GNorm = 0.1413, lr_0 = 1.1410e-04
Validation rmse logS = 0.608469
Validation R2 logS = 0.912247
Epoch 88
Train function
Loss = 4.6203e-05, PNorm = 57.4430, GNorm = 0.2207, lr_0 = 1.1206e-04
Validation rmse logS = 0.604301
Validation R2 logS = 0.913445
Epoch 89
Train function
Loss = 7.8426e-05, PNorm = 57.4465, GNorm = 0.3054, lr_0 = 1.0985e-04
Loss = 5.3282e-05, PNorm = 57.4492, GNorm = 0.1438, lr_0 = 1.0789e-04
Validation rmse logS = 0.608947
Validation R2 logS = 0.912109
Epoch 90
Train function
Loss = 4.0558e-05, PNorm = 57.4519, GNorm = 0.2241, lr_0 = 1.0595e-04
Validation rmse logS = 0.620428
Validation R2 logS = 0.908763
Epoch 91
Train function
Loss = 5.2244e-05, PNorm = 57.4550, GNorm = 0.2491, lr_0 = 1.0387e-04
Loss = 5.7369e-05, PNorm = 57.4574, GNorm = 0.1086, lr_0 = 1.0201e-04
Validation rmse logS = 0.610325
Validation R2 logS = 0.911710
Epoch 92
Train function
Loss = 5.4242e-05, PNorm = 57.4603, GNorm = 0.3671, lr_0 = 1.0018e-04
Validation rmse logS = 0.619896
Validation R2 logS = 0.908920
Epoch 93
Train function
Loss = 4.3312e-05, PNorm = 57.4634, GNorm = 0.2720, lr_0 = 1.0000e-04
Validation rmse logS = 0.610223
Validation R2 logS = 0.911740
Epoch 94
Train function
Loss = 2.6463e-05, PNorm = 57.4656, GNorm = 0.2894, lr_0 = 1.0000e-04
Loss = 4.9117e-05, PNorm = 57.4683, GNorm = 0.2336, lr_0 = 1.0000e-04
Loss = 8.1877e-05, PNorm = 57.4686, GNorm = 0.3030, lr_0 = 1.0000e-04
Validation rmse logS = 0.612185
Validation R2 logS = 0.911172
Epoch 95
Train function
Loss = 3.8807e-05, PNorm = 57.4711, GNorm = 0.1802, lr_0 = 1.0000e-04
Validation rmse logS = 0.612854
Validation R2 logS = 0.910977
Epoch 96
Train function
Loss = 3.7530e-05, PNorm = 57.4736, GNorm = 0.1263, lr_0 = 1.0000e-04
Validation rmse logS = 0.615710
Validation R2 logS = 0.910146
Epoch 97
Train function
Loss = 3.6310e-05, PNorm = 57.4762, GNorm = 0.2539, lr_0 = 1.0000e-04
Loss = 4.2478e-05, PNorm = 57.4788, GNorm = 0.3799, lr_0 = 1.0000e-04
Validation rmse logS = 0.608304
Validation R2 logS = 0.912294
Epoch 98
Train function
Loss = 4.3733e-05, PNorm = 57.4809, GNorm = 0.2068, lr_0 = 1.0000e-04
Validation rmse logS = 0.608124
Validation R2 logS = 0.912346
Epoch 99
Train function
Loss = 3.3934e-05, PNorm = 57.4837, GNorm = 0.1670, lr_0 = 1.0000e-04
Validation rmse logS = 0.609752
Validation R2 logS = 0.911876
Model 0 best validation rmse = 0.592014 on epoch 49
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "substructures_encoder.encoder.cached_zero_vector".
Loading pretrained parameter "substructures_encoder.encoder.W_o.weight".
Loading pretrained parameter "substructures_encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.666252
Model 0 test R2 logS = 0.883955
Ensemble test rmse  logS= 0.666252
Ensemble test R2  logS= 0.883955
Fold 2
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_305/folds/fold_2',
 'save_smiles_splits': False,
 'seed': 2,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 674,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 2
Total size = 899 | train size = 674 | val size = 225 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=125, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,500,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.6145e-02, PNorm = 55.2300, GNorm = 6.1275, lr_0 = 4.8077e-04
Validation rmse logS = 1.302177
Validation R2 logS = 0.606981
Epoch 1
Train function
Loss = 7.3467e-03, PNorm = 55.3009, GNorm = 4.7513, lr_0 = 8.6154e-04
Validation rmse logS = 0.995151
Validation R2 logS = 0.770464
Epoch 2
Train function
Loss = 4.0316e-03, PNorm = 55.3881, GNorm = 1.8042, lr_0 = 9.8743e-04
Loss = 4.2436e-03, PNorm = 55.4555, GNorm = 2.2175, lr_0 = 9.6974e-04
Validation rmse logS = 0.845204
Validation R2 logS = 0.834424
Epoch 3
Train function
Loss = 3.0862e-03, PNorm = 55.5126, GNorm = 1.6292, lr_0 = 9.5237e-04
Validation rmse logS = 0.882858
Validation R2 logS = 0.819343
Epoch 4
Train function
Loss = 3.0570e-03, PNorm = 55.5701, GNorm = 1.0350, lr_0 = 9.3363e-04
Validation rmse logS = 0.798815
Validation R2 logS = 0.852101
Epoch 5
Train function
Loss = 2.3187e-03, PNorm = 55.6117, GNorm = 1.3501, lr_0 = 9.1690e-04
Loss = 2.2213e-03, PNorm = 55.6456, GNorm = 0.5455, lr_0 = 9.0048e-04
Loss = 5.6532e-03, PNorm = 55.6494, GNorm = 1.1245, lr_0 = 8.9885e-04
Validation rmse logS = 0.718393
Validation R2 logS = 0.880382
Epoch 6
Train function
Loss = 1.8070e-03, PNorm = 55.6812, GNorm = 0.8211, lr_0 = 8.8275e-04
Validation rmse logS = 0.702914
Validation R2 logS = 0.885481
Epoch 7
Train function
Loss = 1.8512e-03, PNorm = 55.7115, GNorm = 1.0012, lr_0 = 8.6694e-04
Validation rmse logS = 0.666057
Validation R2 logS = 0.897176
Epoch 8
Train function
Loss = 1.2488e-03, PNorm = 55.7484, GNorm = 1.4979, lr_0 = 8.4988e-04
Loss = 1.4775e-03, PNorm = 55.7823, GNorm = 2.1366, lr_0 = 8.3466e-04
Validation rmse logS = 0.688579
Validation R2 logS = 0.890104
Epoch 9
Train function
Loss = 1.3646e-03, PNorm = 55.8132, GNorm = 0.5128, lr_0 = 8.1971e-04
Validation rmse logS = 0.635289
Validation R2 logS = 0.906456
Epoch 10
Train function
Loss = 1.0972e-03, PNorm = 55.8547, GNorm = 0.5088, lr_0 = 8.0357e-04
Validation rmse logS = 0.626997
Validation R2 logS = 0.908882
Epoch 11
Train function
Loss = 1.1412e-03, PNorm = 55.8868, GNorm = 0.7458, lr_0 = 7.8918e-04
Loss = 1.3453e-03, PNorm = 55.9226, GNorm = 0.5207, lr_0 = 7.7504e-04
Validation rmse logS = 0.681919
Validation R2 logS = 0.892220
Epoch 12
Train function
Loss = 1.0466e-03, PNorm = 55.9636, GNorm = 0.9470, lr_0 = 7.5979e-04
Validation rmse logS = 0.658798
Validation R2 logS = 0.899404
Epoch 13
Train function
Loss = 1.0481e-03, PNorm = 55.9977, GNorm = 1.1974, lr_0 = 7.4618e-04
Validation rmse logS = 0.617180
Validation R2 logS = 0.911713
Epoch 14
Train function
Loss = 6.6299e-04, PNorm = 56.0348, GNorm = 0.4598, lr_0 = 7.3149e-04
Loss = 9.8640e-04, PNorm = 56.0648, GNorm = 0.9862, lr_0 = 7.1839e-04
Validation rmse logS = 0.626297
Validation R2 logS = 0.909085
Epoch 15
Train function
Loss = 8.1921e-04, PNorm = 56.1015, GNorm = 1.4669, lr_0 = 7.0552e-04
Validation rmse logS = 0.629238
Validation R2 logS = 0.908229
Epoch 16
Train function
Loss = 8.0311e-04, PNorm = 56.1387, GNorm = 0.3480, lr_0 = 6.9163e-04
Validation rmse logS = 0.592055
Validation R2 logS = 0.918755
Epoch 17
Train function
Loss = 7.4693e-04, PNorm = 56.1688, GNorm = 0.3143, lr_0 = 6.7924e-04
Loss = 8.8749e-04, PNorm = 56.1992, GNorm = 0.9026, lr_0 = 6.6708e-04
Validation rmse logS = 0.624488
Validation R2 logS = 0.909610
Epoch 18
Train function
Loss = 8.7742e-04, PNorm = 56.2331, GNorm = 0.6847, lr_0 = 6.5395e-04
Validation rmse logS = 0.615043
Validation R2 logS = 0.912323
Epoch 19
Train function
Loss = 9.7078e-04, PNorm = 56.2627, GNorm = 1.9095, lr_0 = 6.4223e-04
Validation rmse logS = 0.596415
Validation R2 logS = 0.917554
Epoch 20
Train function
Loss = 8.5382e-04, PNorm = 56.2975, GNorm = 1.1995, lr_0 = 6.2959e-04
Loss = 6.6716e-04, PNorm = 56.3223, GNorm = 0.8892, lr_0 = 6.1831e-04
Validation rmse logS = 0.603413
Validation R2 logS = 0.915608
Epoch 21
Train function
Loss = 7.0345e-04, PNorm = 56.3553, GNorm = 2.1460, lr_0 = 6.0724e-04
Validation rmse logS = 0.638939
Validation R2 logS = 0.905378
Epoch 22
Train function
Loss = 7.5673e-04, PNorm = 56.3890, GNorm = 0.3832, lr_0 = 5.9529e-04
Loss = 8.4045e-04, PNorm = 56.4192, GNorm = 0.4758, lr_0 = 5.8462e-04
Validation rmse logS = 0.629395
Validation R2 logS = 0.908184
Epoch 23
Train function
Loss = 7.0098e-04, PNorm = 56.4513, GNorm = 0.4605, lr_0 = 5.7415e-04
Validation rmse logS = 0.571883
Validation R2 logS = 0.924197
Epoch 24
Train function
Loss = 5.2566e-04, PNorm = 56.4801, GNorm = 0.5733, lr_0 = 5.6285e-04
Validation rmse logS = 0.567986
Validation R2 logS = 0.925226
Epoch 25
Train function
Loss = 4.9458e-04, PNorm = 56.5051, GNorm = 1.0907, lr_0 = 5.5277e-04
Loss = 5.7950e-04, PNorm = 56.5321, GNorm = 0.3071, lr_0 = 5.4287e-04
Loss = 1.1279e-03, PNorm = 56.5336, GNorm = 1.2362, lr_0 = 5.4189e-04
Validation rmse logS = 0.579723
Validation R2 logS = 0.922104
Epoch 26
Train function
Loss = 5.4323e-04, PNorm = 56.5595, GNorm = 0.9003, lr_0 = 5.3218e-04
Validation rmse logS = 0.570135
Validation R2 logS = 0.924660
Epoch 27
Train function
Loss = 3.8723e-04, PNorm = 56.5851, GNorm = 0.3287, lr_0 = 5.2171e-04
Validation rmse logS = 0.602295
Validation R2 logS = 0.915920
Epoch 28
Train function
Loss = 5.5099e-04, PNorm = 56.6053, GNorm = 0.3446, lr_0 = 5.1236e-04
Loss = 5.6188e-04, PNorm = 56.6282, GNorm = 0.9222, lr_0 = 5.0318e-04
Loss = 7.0664e-04, PNorm = 56.6302, GNorm = 0.9887, lr_0 = 5.0228e-04
Validation rmse logS = 0.581395
Validation R2 logS = 0.921654
Epoch 29
Train function
Loss = 4.4875e-04, PNorm = 56.6581, GNorm = 0.6434, lr_0 = 4.9328e-04
Validation rmse logS = 0.565412
Validation R2 logS = 0.925902
Epoch 30
Train function
Loss = 4.2284e-04, PNorm = 56.6811, GNorm = 0.3844, lr_0 = 4.8444e-04
Validation rmse logS = 0.574670
Validation R2 logS = 0.923456
Epoch 31
Train function
Loss = 3.3028e-04, PNorm = 56.7043, GNorm = 0.3549, lr_0 = 4.7491e-04
Loss = 3.8872e-04, PNorm = 56.7276, GNorm = 0.7408, lr_0 = 4.6640e-04
Validation rmse logS = 0.557262
Validation R2 logS = 0.928023
Epoch 32
Train function
Loss = 3.2305e-04, PNorm = 56.7486, GNorm = 0.2523, lr_0 = 4.5805e-04
Validation rmse logS = 0.558881
Validation R2 logS = 0.927605
Epoch 33
Train function
Loss = 3.3543e-04, PNorm = 56.7722, GNorm = 0.7270, lr_0 = 4.4903e-04
Validation rmse logS = 0.563067
Validation R2 logS = 0.926516
Epoch 34
Train function
Loss = 4.4612e-04, PNorm = 56.7921, GNorm = 0.5830, lr_0 = 4.4099e-04
Loss = 3.5135e-04, PNorm = 56.8138, GNorm = 0.2962, lr_0 = 4.3309e-04
Validation rmse logS = 0.567045
Validation R2 logS = 0.925474
Epoch 35
Train function
Loss = 3.5696e-04, PNorm = 56.8341, GNorm = 1.3551, lr_0 = 4.2456e-04
Validation rmse logS = 0.581533
Validation R2 logS = 0.921617
Epoch 36
Train function
Loss = 3.3851e-04, PNorm = 56.8525, GNorm = 0.6652, lr_0 = 4.1696e-04
Validation rmse logS = 0.567692
Validation R2 logS = 0.925304
Epoch 37
Train function
Loss = 3.0850e-04, PNorm = 56.8725, GNorm = 0.6677, lr_0 = 4.0875e-04
Loss = 3.1342e-04, PNorm = 56.8938, GNorm = 0.2466, lr_0 = 4.0143e-04
Validation rmse logS = 0.555602
Validation R2 logS = 0.928451
Epoch 38
Train function
Loss = 3.7266e-04, PNorm = 56.9108, GNorm = 0.5319, lr_0 = 3.9424e-04
Validation rmse logS = 0.588165
Validation R2 logS = 0.919819
Epoch 39
Train function
Loss = 2.4530e-04, PNorm = 56.9313, GNorm = 0.9543, lr_0 = 3.8648e-04
Validation rmse logS = 0.576603
Validation R2 logS = 0.922940
Epoch 40
Train function
Loss = 2.1685e-04, PNorm = 56.9470, GNorm = 0.5886, lr_0 = 3.7956e-04
Loss = 2.9659e-04, PNorm = 56.9670, GNorm = 0.5796, lr_0 = 3.7276e-04
Validation rmse logS = 0.556692
Validation R2 logS = 0.928171
Epoch 41
Train function
Loss = 2.0902e-04, PNorm = 56.9872, GNorm = 0.6343, lr_0 = 3.6542e-04
Validation rmse logS = 0.600360
Validation R2 logS = 0.916459
Epoch 42
Train function
Loss = 4.3943e-04, PNorm = 57.0050, GNorm = 1.0323, lr_0 = 3.5888e-04
Validation rmse logS = 0.580818
Validation R2 logS = 0.921809
Epoch 43
Train function
Loss = 3.2862e-04, PNorm = 57.0231, GNorm = 1.0709, lr_0 = 3.5181e-04
Loss = 2.7506e-04, PNorm = 57.0399, GNorm = 0.3002, lr_0 = 3.4551e-04
Validation rmse logS = 0.556147
Validation R2 logS = 0.928311
Epoch 44
Train function
Loss = 1.9389e-04, PNorm = 57.0547, GNorm = 0.6233, lr_0 = 3.3932e-04
Validation rmse logS = 0.556247
Validation R2 logS = 0.928285
Epoch 45
Train function
Loss = 2.0567e-04, PNorm = 57.0690, GNorm = 0.2347, lr_0 = 3.3264e-04
Loss = 2.2422e-04, PNorm = 57.0834, GNorm = 0.4283, lr_0 = 3.2668e-04
Validation rmse logS = 0.563993
Validation R2 logS = 0.926274
Epoch 46
Train function
Loss = 1.8712e-04, PNorm = 57.0956, GNorm = 0.3170, lr_0 = 3.2083e-04
Validation rmse logS = 0.555870
Validation R2 logS = 0.928382
Epoch 47
Train function
Loss = 2.0322e-04, PNorm = 57.1111, GNorm = 0.2687, lr_0 = 3.1452e-04
Validation rmse logS = 0.563083
Validation R2 logS = 0.926512
Epoch 48
Train function
Loss = 1.7281e-04, PNorm = 57.1249, GNorm = 0.5126, lr_0 = 3.0888e-04
Loss = 1.9614e-04, PNorm = 57.1378, GNorm = 0.1780, lr_0 = 3.0335e-04
Loss = 2.0086e-04, PNorm = 57.1389, GNorm = 0.3072, lr_0 = 3.0280e-04
Validation rmse logS = 0.566907
Validation R2 logS = 0.925510
Epoch 49
Train function
Loss = 1.6094e-04, PNorm = 57.1494, GNorm = 0.4989, lr_0 = 2.9738e-04
Validation rmse logS = 0.562976
Validation R2 logS = 0.926540
Epoch 50
Train function
Loss = 1.9085e-04, PNorm = 57.1612, GNorm = 0.6629, lr_0 = 2.9205e-04
Validation rmse logS = 0.565006
Validation R2 logS = 0.926009
Epoch 51
Train function
Loss = 2.3453e-04, PNorm = 57.1748, GNorm = 0.3554, lr_0 = 2.8630e-04
Loss = 1.6133e-04, PNorm = 57.1872, GNorm = 0.5125, lr_0 = 2.8118e-04
Loss = 2.7736e-04, PNorm = 57.1879, GNorm = 0.3207, lr_0 = 2.8067e-04
Validation rmse logS = 0.552293
Validation R2 logS = 0.929301
Epoch 52
Train function
Loss = 1.5992e-04, PNorm = 57.1987, GNorm = 0.5539, lr_0 = 2.7564e-04
Validation rmse logS = 0.551606
Validation R2 logS = 0.929477
Epoch 53
Train function
Loss = 1.3892e-04, PNorm = 57.2101, GNorm = 0.4464, lr_0 = 2.7070e-04
Validation rmse logS = 0.555732
Validation R2 logS = 0.928418
Epoch 54
Train function
Loss = 1.9158e-04, PNorm = 57.2204, GNorm = 1.0360, lr_0 = 2.6538e-04
Loss = 1.5978e-04, PNorm = 57.2313, GNorm = 0.3007, lr_0 = 2.6062e-04
Validation rmse logS = 0.553255
Validation R2 logS = 0.929054
Epoch 55
Train function
Loss = 1.3307e-04, PNorm = 57.2425, GNorm = 0.2122, lr_0 = 2.5595e-04
Validation rmse logS = 0.559790
Validation R2 logS = 0.927369
Epoch 56
Train function
Loss = 1.7204e-04, PNorm = 57.2530, GNorm = 0.1705, lr_0 = 2.5092e-04
Validation rmse logS = 0.554058
Validation R2 logS = 0.928849
Epoch 57
Train function
Loss = 1.1738e-04, PNorm = 57.2634, GNorm = 0.2209, lr_0 = 2.4642e-04
Loss = 1.2648e-04, PNorm = 57.2716, GNorm = 0.4227, lr_0 = 2.4201e-04
Validation rmse logS = 0.571016
Validation R2 logS = 0.924426
Epoch 58
Train function
Loss = 1.2063e-04, PNorm = 57.2812, GNorm = 0.1998, lr_0 = 2.3724e-04
Validation rmse logS = 0.569414
Validation R2 logS = 0.924850
Epoch 59
Train function
Loss = 1.1611e-04, PNorm = 57.2902, GNorm = 0.3251, lr_0 = 2.3300e-04
Validation rmse logS = 0.558855
Validation R2 logS = 0.927611
Epoch 60
Train function
Loss = 1.0903e-04, PNorm = 57.2993, GNorm = 0.2002, lr_0 = 2.2841e-04
Loss = 1.2591e-04, PNorm = 57.3092, GNorm = 0.2498, lr_0 = 2.2432e-04
Validation rmse logS = 0.557364
Validation R2 logS = 0.927997
Epoch 61
Train function
Loss = 1.0518e-04, PNorm = 57.3161, GNorm = 0.2862, lr_0 = 2.2030e-04
Validation rmse logS = 0.558900
Validation R2 logS = 0.927599
Epoch 62
Train function
Loss = 9.6529e-05, PNorm = 57.3258, GNorm = 0.4006, lr_0 = 2.1596e-04
Validation rmse logS = 0.562565
Validation R2 logS = 0.926647
Epoch 63
Train function
Loss = 1.2422e-04, PNorm = 57.3323, GNorm = 0.3289, lr_0 = 2.1210e-04
Loss = 9.9980e-05, PNorm = 57.3398, GNorm = 0.2924, lr_0 = 2.0830e-04
Validation rmse logS = 0.564891
Validation R2 logS = 0.926039
Epoch 64
Train function
Loss = 1.0317e-04, PNorm = 57.3475, GNorm = 0.4677, lr_0 = 2.0420e-04
Validation rmse logS = 0.563854
Validation R2 logS = 0.926310
Epoch 65
Train function
Loss = 1.0606e-04, PNorm = 57.3556, GNorm = 0.3136, lr_0 = 2.0054e-04
Validation rmse logS = 0.566062
Validation R2 logS = 0.925732
Epoch 66
Train function
Loss = 1.0177e-04, PNorm = 57.3657, GNorm = 0.1451, lr_0 = 1.9659e-04
Loss = 1.0659e-04, PNorm = 57.3722, GNorm = 0.4484, lr_0 = 1.9307e-04
Validation rmse logS = 0.560806
Validation R2 logS = 0.927105
Epoch 67
Train function
Loss = 1.0938e-04, PNorm = 57.3792, GNorm = 0.1430, lr_0 = 1.8961e-04
Validation rmse logS = 0.560792
Validation R2 logS = 0.927109
Epoch 68
Train function
Loss = 7.3080e-05, PNorm = 57.3857, GNorm = 0.0964, lr_0 = 1.8588e-04
Loss = 9.2549e-05, PNorm = 57.3926, GNorm = 0.1418, lr_0 = 1.8255e-04
Validation rmse logS = 0.560170
Validation R2 logS = 0.927270
Epoch 69
Train function
Loss = 7.9036e-05, PNorm = 57.3991, GNorm = 0.1102, lr_0 = 1.7928e-04
Validation rmse logS = 0.562458
Validation R2 logS = 0.926675
Epoch 70
Train function
Loss = 6.6284e-05, PNorm = 57.4064, GNorm = 0.0972, lr_0 = 1.7575e-04
Validation rmse logS = 0.567280
Validation R2 logS = 0.925412
Epoch 71
Train function
Loss = 8.0636e-05, PNorm = 57.4123, GNorm = 0.0929, lr_0 = 1.7260e-04
Loss = 6.9859e-05, PNorm = 57.4180, GNorm = 0.2635, lr_0 = 1.6951e-04
Loss = 7.0930e-05, PNorm = 57.4185, GNorm = 0.2145, lr_0 = 1.6921e-04
Validation rmse logS = 0.557813
Validation R2 logS = 0.927881
Epoch 72
Train function
Loss = 7.0939e-05, PNorm = 57.4235, GNorm = 0.2949, lr_0 = 1.6617e-04
Validation rmse logS = 0.561349
Validation R2 logS = 0.926964
Epoch 73
Train function
Loss = 6.1469e-05, PNorm = 57.4300, GNorm = 0.1307, lr_0 = 1.6320e-04
Validation rmse logS = 0.558202
Validation R2 logS = 0.927780
Epoch 74
Train function
Loss = 5.0367e-05, PNorm = 57.4357, GNorm = 0.1410, lr_0 = 1.5999e-04
Loss = 7.3312e-05, PNorm = 57.4401, GNorm = 0.1404, lr_0 = 1.5712e-04
Validation rmse logS = 0.564634
Validation R2 logS = 0.926106
Epoch 75
Train function
Loss = 8.3688e-05, PNorm = 57.4463, GNorm = 0.2757, lr_0 = 1.5431e-04
Validation rmse logS = 0.561668
Validation R2 logS = 0.926881
Epoch 76
Train function
Loss = 5.9640e-05, PNorm = 57.4525, GNorm = 0.3055, lr_0 = 1.5127e-04
Validation rmse logS = 0.559985
Validation R2 logS = 0.927318
Epoch 77
Train function
Loss = 9.1294e-05, PNorm = 57.4576, GNorm = 0.2275, lr_0 = 1.4829e-04
Loss = 6.2502e-05, PNorm = 57.4627, GNorm = 0.2914, lr_0 = 1.4563e-04
Validation rmse logS = 0.557535
Validation R2 logS = 0.927953
Epoch 78
Train function
Loss = 5.6135e-05, PNorm = 57.4673, GNorm = 0.1557, lr_0 = 1.4303e-04
Validation rmse logS = 0.565134
Validation R2 logS = 0.925975
Epoch 79
Train function
Loss = 5.6011e-05, PNorm = 57.4735, GNorm = 0.3334, lr_0 = 1.4021e-04
Validation rmse logS = 0.560238
Validation R2 logS = 0.927253
Epoch 80
Train function
Loss = 5.3902e-05, PNorm = 57.4786, GNorm = 0.1081, lr_0 = 1.3770e-04
Loss = 5.9016e-05, PNorm = 57.4823, GNorm = 0.2024, lr_0 = 1.3523e-04
Validation rmse logS = 0.561655
Validation R2 logS = 0.926884
Epoch 81
Train function
Loss = 5.9549e-05, PNorm = 57.4876, GNorm = 0.1150, lr_0 = 1.3257e-04
Validation rmse logS = 0.563853
Validation R2 logS = 0.926311
Epoch 82
Train function
Loss = 6.6575e-05, PNorm = 57.4923, GNorm = 0.4816, lr_0 = 1.3020e-04
Validation rmse logS = 0.560857
Validation R2 logS = 0.927091
Epoch 83
Train function
Loss = 3.1283e-05, PNorm = 57.4964, GNorm = 0.1472, lr_0 = 1.2763e-04
Loss = 7.0333e-05, PNorm = 57.5001, GNorm = 0.2566, lr_0 = 1.2535e-04
Validation rmse logS = 0.563235
Validation R2 logS = 0.926472
Epoch 84
Train function
Loss = 6.0258e-05, PNorm = 57.5042, GNorm = 0.1892, lr_0 = 1.2310e-04
Validation rmse logS = 0.559779
Validation R2 logS = 0.927372
Epoch 85
Train function
Loss = 5.0392e-05, PNorm = 57.5085, GNorm = 0.1969, lr_0 = 1.2068e-04
Validation rmse logS = 0.559246
Validation R2 logS = 0.927510
Epoch 86
Train function
Loss = 4.4043e-05, PNorm = 57.5118, GNorm = 0.1360, lr_0 = 1.1852e-04
Loss = 6.1352e-05, PNorm = 57.5154, GNorm = 0.2086, lr_0 = 1.1639e-04
Validation rmse logS = 0.564841
Validation R2 logS = 0.926052
Epoch 87
Train function
Loss = 5.4911e-05, PNorm = 57.5194, GNorm = 0.3298, lr_0 = 1.1410e-04
Validation rmse logS = 0.562992
Validation R2 logS = 0.926535
Epoch 88
Train function
Loss = 6.4617e-05, PNorm = 57.5225, GNorm = 0.2176, lr_0 = 1.1206e-04
Validation rmse logS = 0.560627
Validation R2 logS = 0.927151
Epoch 89
Train function
Loss = 6.0321e-05, PNorm = 57.5268, GNorm = 0.1608, lr_0 = 1.0985e-04
Loss = 4.8458e-05, PNorm = 57.5303, GNorm = 0.1130, lr_0 = 1.0789e-04
Validation rmse logS = 0.559847
Validation R2 logS = 0.927354
Epoch 90
Train function
Loss = 4.9649e-05, PNorm = 57.5332, GNorm = 0.4578, lr_0 = 1.0595e-04
Validation rmse logS = 0.562613
Validation R2 logS = 0.926634
Epoch 91
Train function
Loss = 4.2266e-05, PNorm = 57.5364, GNorm = 0.1262, lr_0 = 1.0387e-04
Loss = 5.3518e-05, PNorm = 57.5396, GNorm = 0.4390, lr_0 = 1.0201e-04
Validation rmse logS = 0.560069
Validation R2 logS = 0.927296
Epoch 92
Train function
Loss = 4.4392e-05, PNorm = 57.5425, GNorm = 0.1614, lr_0 = 1.0018e-04
Validation rmse logS = 0.562615
Validation R2 logS = 0.926634
Epoch 93
Train function
Loss = 3.5524e-05, PNorm = 57.5449, GNorm = 0.0854, lr_0 = 1.0000e-04
Validation rmse logS = 0.564263
Validation R2 logS = 0.926203
Epoch 94
Train function
Loss = 2.6368e-05, PNorm = 57.5480, GNorm = 0.1410, lr_0 = 1.0000e-04
Loss = 4.8207e-05, PNorm = 57.5503, GNorm = 0.1976, lr_0 = 1.0000e-04
Loss = 4.0217e-05, PNorm = 57.5506, GNorm = 0.1913, lr_0 = 1.0000e-04
Validation rmse logS = 0.563903
Validation R2 logS = 0.926297
Epoch 95
Train function
Loss = 4.8014e-05, PNorm = 57.5538, GNorm = 0.4142, lr_0 = 1.0000e-04
Validation rmse logS = 0.563876
Validation R2 logS = 0.926305
Epoch 96
Train function
Loss = 5.3859e-05, PNorm = 57.5568, GNorm = 0.5746, lr_0 = 1.0000e-04
Validation rmse logS = 0.565567
Validation R2 logS = 0.925862
Epoch 97
Train function
Loss = 3.3715e-05, PNorm = 57.5603, GNorm = 0.2979, lr_0 = 1.0000e-04
Loss = 4.8720e-05, PNorm = 57.5627, GNorm = 0.2875, lr_0 = 1.0000e-04
Validation rmse logS = 0.560763
Validation R2 logS = 0.927116
Epoch 98
Train function
Loss = 3.6589e-05, PNorm = 57.5657, GNorm = 0.0860, lr_0 = 1.0000e-04
Validation rmse logS = 0.563233
Validation R2 logS = 0.926472
Epoch 99
Train function
Loss = 3.4601e-05, PNorm = 57.5682, GNorm = 0.1275, lr_0 = 1.0000e-04
Validation rmse logS = 0.564115
Validation R2 logS = 0.926242
Model 0 best validation rmse = 0.551606 on epoch 52
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "substructures_encoder.encoder.cached_zero_vector".
Loading pretrained parameter "substructures_encoder.encoder.W_o.weight".
Loading pretrained parameter "substructures_encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.624855
Model 0 test R2 logS = 0.897928
Ensemble test rmse  logS= 0.624855
Ensemble test R2  logS= 0.897928
Fold 3
Command line
python ./scripts/SOTA/dmpnn/train.py --dataset_type regression --num_workers 0 --config_path_yaml ./params.yaml
Args
{'activation': 'ReLU',
 'additional_encoder': True,
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'class_balance': False,
 'config_path': None,
 'config_path_yaml': './params.yaml',
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': './data/raw/baselines/dmpnn/train_val_dataset.csv',
 'dataset_type': 'regression',
 'depth': 6,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 100,
 'features_generator': ['rdkit_wo_fragments_and_counts'],
 'features_only': False,
 'features_path': None,
 'features_scaling': False,
 'features_size': 95,
 'ffn_hidden_size': 800,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'grad_clip': None,
 'hidden_size': 800,
 'ignore_columns': None,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': True,
 'num_folds': 4,
 'num_lrs': 1,
 'num_tasks': 1,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'save_dir': './data/raw/baselines/dmpnn/logs/exp_305/folds/fold_3',
 'save_smiles_splits': False,
 'seed': 3,
 'separate_test_features_path': None,
 'separate_test_path': './data/3_final_data/split_data/esol_test.csv',
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'k-fold',
 'substructures_atom_messages': False,
 'substructures_depth': False,
 'substructures_hidden_size': 300,
 'substructures_merge': False,
 'substructures_undirected': False,
 'substructures_use_substructures': True,
 'symmetry_feature': False,
 'target_columns': ['logS'],
 'task_names': ['logS'],
 'test': False,
 'test_fold_index': None,
 'train_data_size': 674,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 3
Total size = 899 | train size = 675 | val size = 224 | test size = 159
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=800, bias=False)
      (W_h): Linear(in_features=800, out_features=800, bias=False)
      (W_o): Linear(in_features=933, out_features=800, bias=True)
    )
  )
  (substructures_encoder): SubstructureLayer(
    (encoder): SubstructureEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_o): Linear(in_features=125, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=1195, out_features=800, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=800, out_features=1, bias=True)
  )
)
Number of parameters = 2,500,201
Moving model to cuda
Epoch 0
Train function
Loss = 1.6814e-02, PNorm = 55.2287, GNorm = 11.5008, lr_0 = 4.8077e-04
Validation rmse logS = 1.224888
Validation R2 logS = 0.648124
Epoch 1
Train function
Loss = 6.1836e-03, PNorm = 55.2969, GNorm = 3.1943, lr_0 = 8.6154e-04
Validation rmse logS = 0.975474
Validation R2 logS = 0.776834
Epoch 2
Train function
Loss = 4.8160e-03, PNorm = 55.3790, GNorm = 1.4078, lr_0 = 9.8921e-04
Loss = 3.7330e-03, PNorm = 55.4397, GNorm = 0.9879, lr_0 = 9.7150e-04
Loss = 8.2520e-03, PNorm = 55.4447, GNorm = 1.8350, lr_0 = 9.6974e-04
Validation rmse logS = 0.946273
Validation R2 logS = 0.789995
Epoch 3
Train function
Loss = 3.1951e-03, PNorm = 55.4978, GNorm = 1.4900, lr_0 = 9.5237e-04
Validation rmse logS = 0.725944
Validation R2 logS = 0.876404
Epoch 4
Train function
Loss = 2.5184e-03, PNorm = 55.5450, GNorm = 0.7462, lr_0 = 9.3531e-04
Validation rmse logS = 0.698376
Validation R2 logS = 0.885613
Epoch 5
Train function
Loss = 1.7712e-03, PNorm = 55.5876, GNorm = 1.7977, lr_0 = 9.1690e-04
Loss = 2.1928e-03, PNorm = 55.6258, GNorm = 1.7733, lr_0 = 9.0048e-04
Validation rmse logS = 0.686437
Validation R2 logS = 0.889491
Epoch 6
Train function
Loss = 1.7086e-03, PNorm = 55.6638, GNorm = 0.8408, lr_0 = 8.8435e-04
Validation rmse logS = 0.681433
Validation R2 logS = 0.891096
Epoch 7
Train function
Loss = 1.4163e-03, PNorm = 55.7016, GNorm = 0.5083, lr_0 = 8.6694e-04
Validation rmse logS = 0.717277
Validation R2 logS = 0.879338
Epoch 8
Train function
Loss = 1.2158e-03, PNorm = 55.7314, GNorm = 0.8700, lr_0 = 8.5141e-04
Loss = 1.4137e-03, PNorm = 55.7653, GNorm = 1.8451, lr_0 = 8.3617e-04
Validation rmse logS = 0.640217
Validation R2 logS = 0.903872
Epoch 9
Train function
Loss = 1.4199e-03, PNorm = 55.8052, GNorm = 1.1471, lr_0 = 8.1971e-04
Validation rmse logS = 0.868509
Validation R2 logS = 0.823093
Epoch 10
Train function
Loss = 1.6042e-03, PNorm = 55.8356, GNorm = 1.0234, lr_0 = 8.0502e-04
Validation rmse logS = 0.647093
Validation R2 logS = 0.901796
Epoch 11
Train function
Loss = 1.1644e-03, PNorm = 55.8721, GNorm = 0.7740, lr_0 = 7.8918e-04
Loss = 1.1680e-03, PNorm = 55.9073, GNorm = 1.4330, lr_0 = 7.7504e-04
Validation rmse logS = 0.626511
Validation R2 logS = 0.907943
Epoch 12
Train function
Loss = 1.1096e-03, PNorm = 55.9383, GNorm = 0.9390, lr_0 = 7.6116e-04
Validation rmse logS = 0.637028
Validation R2 logS = 0.904827
Epoch 13
Train function
Loss = 1.0369e-03, PNorm = 55.9715, GNorm = 1.2670, lr_0 = 7.4618e-04
Validation rmse logS = 0.641870
Validation R2 logS = 0.903374
Epoch 14
Train function
Loss = 1.2463e-03, PNorm = 55.9998, GNorm = 0.7681, lr_0 = 7.3281e-04
Loss = 9.1910e-04, PNorm = 56.0341, GNorm = 0.9360, lr_0 = 7.1969e-04
Validation rmse logS = 0.623249
Validation R2 logS = 0.908900
Epoch 15
Train function
Loss = 9.2685e-04, PNorm = 56.0635, GNorm = 0.7959, lr_0 = 7.0552e-04
Validation rmse logS = 0.620852
Validation R2 logS = 0.909599
Epoch 16
Train function
Loss = 6.4948e-04, PNorm = 56.0908, GNorm = 0.6939, lr_0 = 6.9288e-04
Validation rmse logS = 0.630478
Validation R2 logS = 0.906774
Epoch 17
Train function
Loss = 8.7997e-04, PNorm = 56.1227, GNorm = 0.5107, lr_0 = 6.7924e-04
Loss = 8.2955e-04, PNorm = 56.1498, GNorm = 0.7066, lr_0 = 6.6708e-04
Validation rmse logS = 0.612620
Validation R2 logS = 0.911980
Epoch 18
Train function
Loss = 8.2357e-04, PNorm = 56.1814, GNorm = 1.1128, lr_0 = 6.5513e-04
Validation rmse logS = 0.652678
Validation R2 logS = 0.900093
Epoch 19
Train function
Loss = 7.5366e-04, PNorm = 56.2131, GNorm = 0.7419, lr_0 = 6.4223e-04
Loss = 8.9097e-04, PNorm = 56.2377, GNorm = 1.9938, lr_0 = 6.3073e-04
Validation rmse logS = 0.622079
Validation R2 logS = 0.909241
Epoch 20
Train function
Loss = 7.4613e-04, PNorm = 56.2672, GNorm = 0.3328, lr_0 = 6.1943e-04
Validation rmse logS = 0.668572
Validation R2 logS = 0.895168
Epoch 21
Train function
Loss = 6.3899e-04, PNorm = 56.2989, GNorm = 0.6325, lr_0 = 6.0724e-04
Validation rmse logS = 0.622897
Validation R2 logS = 0.909003
Epoch 22
Train function
Loss = 4.0439e-04, PNorm = 56.3282, GNorm = 0.6388, lr_0 = 5.9636e-04
Loss = 6.3596e-04, PNorm = 56.3539, GNorm = 0.6875, lr_0 = 5.8568e-04
Loss = 1.3536e-03, PNorm = 56.3571, GNorm = 0.3649, lr_0 = 5.8462e-04
Validation rmse logS = 0.607855
Validation R2 logS = 0.913344
Epoch 23
Train function
Loss = 5.3127e-04, PNorm = 56.3857, GNorm = 0.5849, lr_0 = 5.7415e-04
Validation rmse logS = 0.600650
Validation R2 logS = 0.915386
Epoch 24
Train function
Loss = 4.6582e-04, PNorm = 56.4131, GNorm = 0.3929, lr_0 = 5.6387e-04
Validation rmse logS = 0.651801
Validation R2 logS = 0.900361
Epoch 25
Train function
Loss = 8.8361e-04, PNorm = 56.4419, GNorm = 2.0314, lr_0 = 5.5277e-04
Loss = 7.9513e-04, PNorm = 56.4677, GNorm = 0.6525, lr_0 = 5.4287e-04
Validation rmse logS = 0.650359
Validation R2 logS = 0.900802
Epoch 26
Train function
Loss = 5.6754e-04, PNorm = 56.5018, GNorm = 0.3067, lr_0 = 5.3314e-04
Validation rmse logS = 0.641442
Validation R2 logS = 0.903504
Epoch 27
Train function
Loss = 5.5902e-04, PNorm = 56.5300, GNorm = 1.6073, lr_0 = 5.2265e-04
Validation rmse logS = 0.611721
Validation R2 logS = 0.912238
Epoch 28
Train function
Loss = 7.0383e-04, PNorm = 56.5560, GNorm = 1.0689, lr_0 = 5.1329e-04
Loss = 4.4231e-04, PNorm = 56.5781, GNorm = 0.8489, lr_0 = 5.0409e-04
Validation rmse logS = 0.648544
Validation R2 logS = 0.901355
Epoch 29
Train function
Loss = 5.8859e-04, PNorm = 56.6052, GNorm = 1.7962, lr_0 = 4.9417e-04
Validation rmse logS = 0.610384
Validation R2 logS = 0.912622
Epoch 30
Train function
Loss = 4.0501e-04, PNorm = 56.6309, GNorm = 0.2620, lr_0 = 4.8532e-04
Validation rmse logS = 0.605018
Validation R2 logS = 0.914151
Epoch 31
Train function
Loss = 4.7564e-04, PNorm = 56.6552, GNorm = 0.9697, lr_0 = 4.7577e-04
Loss = 3.9437e-04, PNorm = 56.6804, GNorm = 0.8916, lr_0 = 4.6725e-04
Validation rmse logS = 0.592032
Validation R2 logS = 0.917797
Epoch 32
Train function
Loss = 4.1637e-04, PNorm = 56.7006, GNorm = 0.5701, lr_0 = 4.5888e-04
Validation rmse logS = 0.598734
Validation R2 logS = 0.915925
Epoch 33
Train function
Loss = 3.1932e-04, PNorm = 56.7241, GNorm = 0.2375, lr_0 = 4.4984e-04
Validation rmse logS = 0.601646
Validation R2 logS = 0.915106
Epoch 34
Train function
Loss = 4.6023e-04, PNorm = 56.7474, GNorm = 0.5818, lr_0 = 4.4179e-04
Loss = 3.8209e-04, PNorm = 56.7657, GNorm = 1.2652, lr_0 = 4.3387e-04
Validation rmse logS = 0.645706
Validation R2 logS = 0.902216
Epoch 35
Train function
Loss = 4.4061e-04, PNorm = 56.7887, GNorm = 0.3992, lr_0 = 4.2533e-04
Validation rmse logS = 0.600570
Validation R2 logS = 0.915409
Epoch 36
Train function
Loss = 3.1444e-04, PNorm = 56.8087, GNorm = 0.9051, lr_0 = 4.1771e-04
Validation rmse logS = 0.597042
Validation R2 logS = 0.916400
Epoch 37
Train function
Loss = 3.5528e-04, PNorm = 56.8293, GNorm = 1.0494, lr_0 = 4.0949e-04
Loss = 3.7034e-04, PNorm = 56.8502, GNorm = 0.3373, lr_0 = 4.0216e-04
Validation rmse logS = 0.609165
Validation R2 logS = 0.912970
Epoch 38
Train function
Loss = 2.7687e-04, PNorm = 56.8718, GNorm = 0.3974, lr_0 = 3.9495e-04
Validation rmse logS = 0.595945
Validation R2 logS = 0.916707
Epoch 39
Train function
Loss = 3.1243e-04, PNorm = 56.8932, GNorm = 0.4744, lr_0 = 3.8718e-04
Loss = 2.7824e-04, PNorm = 56.9135, GNorm = 0.5918, lr_0 = 3.8024e-04
Validation rmse logS = 0.598653
Validation R2 logS = 0.915948
Epoch 40
Train function
Loss = 2.6143e-04, PNorm = 56.9288, GNorm = 0.5653, lr_0 = 3.7343e-04
Validation rmse logS = 0.606932
Validation R2 logS = 0.913607
Epoch 41
Train function
Loss = 3.3090e-04, PNorm = 56.9473, GNorm = 0.7674, lr_0 = 3.6608e-04
Validation rmse logS = 0.616813
Validation R2 logS = 0.910771
Epoch 42
Train function
Loss = 3.0551e-04, PNorm = 56.9653, GNorm = 0.3247, lr_0 = 3.5953e-04
Loss = 2.7986e-04, PNorm = 56.9850, GNorm = 0.3340, lr_0 = 3.5309e-04
Loss = 5.3894e-04, PNorm = 56.9866, GNorm = 0.7529, lr_0 = 3.5245e-04
Validation rmse logS = 0.609999
Validation R2 logS = 0.912732
Epoch 43
Train function
Loss = 2.5942e-04, PNorm = 57.0017, GNorm = 0.3400, lr_0 = 3.4614e-04
Validation rmse logS = 0.615044
Validation R2 logS = 0.911283
Epoch 44
Train function
Loss = 2.3177e-04, PNorm = 57.0209, GNorm = 0.1291, lr_0 = 3.3994e-04
Validation rmse logS = 0.599319
Validation R2 logS = 0.915761
Epoch 45
Train function
Loss = 2.2591e-04, PNorm = 57.0384, GNorm = 0.7474, lr_0 = 3.3324e-04
Loss = 2.0474e-04, PNorm = 57.0523, GNorm = 0.3951, lr_0 = 3.2728e-04
Validation rmse logS = 0.585904
Validation R2 logS = 0.919490
Epoch 46
Train function
Loss = 2.0524e-04, PNorm = 57.0662, GNorm = 0.2148, lr_0 = 3.2141e-04
Validation rmse logS = 0.595612
Validation R2 logS = 0.916800
Epoch 47
Train function
Loss = 2.1637e-04, PNorm = 57.0819, GNorm = 0.5453, lr_0 = 3.1509e-04
Validation rmse logS = 0.601304
Validation R2 logS = 0.915202
Epoch 48
Train function
Loss = 1.8372e-04, PNorm = 57.0970, GNorm = 0.6350, lr_0 = 3.0944e-04
Loss = 2.0672e-04, PNorm = 57.1112, GNorm = 0.3129, lr_0 = 3.0390e-04
Validation rmse logS = 0.594133
Validation R2 logS = 0.917213
Epoch 49
Train function
Loss = 2.0524e-04, PNorm = 57.1247, GNorm = 0.3400, lr_0 = 2.9792e-04
Validation rmse logS = 0.592533
Validation R2 logS = 0.917658
Epoch 50
Train function
Loss = 2.1669e-04, PNorm = 57.1393, GNorm = 0.7571, lr_0 = 2.9258e-04
Validation rmse logS = 0.589079
Validation R2 logS = 0.918615
Epoch 51
Train function
Loss = 2.4847e-04, PNorm = 57.1523, GNorm = 0.5833, lr_0 = 2.8682e-04
Loss = 1.7358e-04, PNorm = 57.1654, GNorm = 0.8373, lr_0 = 2.8169e-04
Validation rmse logS = 0.608121
Validation R2 logS = 0.913268
Epoch 52
Train function
Loss = 2.3979e-04, PNorm = 57.1744, GNorm = 0.8160, lr_0 = 2.7664e-04
Validation rmse logS = 0.590390
Validation R2 logS = 0.918252
Epoch 53
Train function
Loss = 2.3379e-04, PNorm = 57.1865, GNorm = 1.1283, lr_0 = 2.7119e-04
Validation rmse logS = 0.581915
Validation R2 logS = 0.920583
Epoch 54
Train function
Loss = 1.3158e-04, PNorm = 57.1983, GNorm = 0.5567, lr_0 = 2.6634e-04
Loss = 1.6930e-04, PNorm = 57.2098, GNorm = 0.5349, lr_0 = 2.6157e-04
Validation rmse logS = 0.591614
Validation R2 logS = 0.917913
Epoch 55
Train function
Loss = 1.7454e-04, PNorm = 57.2220, GNorm = 0.3428, lr_0 = 2.5642e-04
Validation rmse logS = 0.597762
Validation R2 logS = 0.916198
Epoch 56
Train function
Loss = 1.1518e-04, PNorm = 57.2323, GNorm = 0.3233, lr_0 = 2.5183e-04
Validation rmse logS = 0.586835
Validation R2 logS = 0.919234
Epoch 57
Train function
Loss = 9.6153e-05, PNorm = 57.2422, GNorm = 0.4239, lr_0 = 2.4687e-04
Loss = 1.3531e-04, PNorm = 57.2525, GNorm = 0.3682, lr_0 = 2.4245e-04
Validation rmse logS = 0.585721
Validation R2 logS = 0.919540
Epoch 58
Train function
Loss = 1.4194e-04, PNorm = 57.2608, GNorm = 0.3324, lr_0 = 2.3810e-04
Validation rmse logS = 0.592258
Validation R2 logS = 0.917734
Epoch 59
Train function
Loss = 1.2971e-04, PNorm = 57.2703, GNorm = 0.4612, lr_0 = 2.3342e-04
Loss = 1.3253e-04, PNorm = 57.2790, GNorm = 0.4785, lr_0 = 2.2924e-04
Validation rmse logS = 0.591669
Validation R2 logS = 0.917898
Epoch 60
Train function
Loss = 1.2484e-04, PNorm = 57.2867, GNorm = 0.2296, lr_0 = 2.2513e-04
Validation rmse logS = 0.588644
Validation R2 logS = 0.918735
Epoch 61
Train function
Loss = 1.1343e-04, PNorm = 57.2954, GNorm = 0.2040, lr_0 = 2.2070e-04
Validation rmse logS = 0.594598
Validation R2 logS = 0.917083
Epoch 62
Train function
Loss = 1.2052e-04, PNorm = 57.3035, GNorm = 0.2391, lr_0 = 2.1675e-04
Loss = 1.2226e-04, PNorm = 57.3109, GNorm = 0.2025, lr_0 = 2.1286e-04
Loss = 1.2690e-04, PNorm = 57.3118, GNorm = 0.2622, lr_0 = 2.1248e-04
Validation rmse logS = 0.601644
Validation R2 logS = 0.915106
Epoch 63
Train function
Loss = 1.0296e-04, PNorm = 57.3189, GNorm = 0.5494, lr_0 = 2.0867e-04
Validation rmse logS = 0.584194
Validation R2 logS = 0.919959
Epoch 64
Train function
Loss = 1.6044e-04, PNorm = 57.3262, GNorm = 1.1588, lr_0 = 2.0494e-04
Validation rmse logS = 0.597410
Validation R2 logS = 0.916297
Epoch 65
Train function
Loss = 1.4075e-04, PNorm = 57.3344, GNorm = 0.7837, lr_0 = 2.0090e-04
Loss = 1.3284e-04, PNorm = 57.3423, GNorm = 0.3480, lr_0 = 1.9730e-04
Validation rmse logS = 0.594689
Validation R2 logS = 0.917058
Epoch 66
Train function
Loss = 1.2957e-04, PNorm = 57.3500, GNorm = 0.2991, lr_0 = 1.9377e-04
Validation rmse logS = 0.593733
Validation R2 logS = 0.917324
Epoch 67
Train function
Loss = 1.3188e-04, PNorm = 57.3586, GNorm = 0.2118, lr_0 = 1.8995e-04
Validation rmse logS = 0.588764
Validation R2 logS = 0.918702
Epoch 68
Train function
Loss = 9.6291e-05, PNorm = 57.3653, GNorm = 0.1598, lr_0 = 1.8655e-04
Loss = 1.0104e-04, PNorm = 57.3714, GNorm = 0.3809, lr_0 = 1.8321e-04
Validation rmse logS = 0.592264
Validation R2 logS = 0.917733
Epoch 69
Train function
Loss = 1.1946e-04, PNorm = 57.3769, GNorm = 0.6164, lr_0 = 1.7960e-04
Validation rmse logS = 0.587140
Validation R2 logS = 0.919150
Epoch 70
Train function
Loss = 9.7867e-05, PNorm = 57.3830, GNorm = 0.2540, lr_0 = 1.7639e-04
Validation rmse logS = 0.593157
Validation R2 logS = 0.917484
Epoch 71
Train function
Loss = 5.9660e-05, PNorm = 57.3897, GNorm = 0.1717, lr_0 = 1.7292e-04
Loss = 8.9941e-05, PNorm = 57.3938, GNorm = 0.2203, lr_0 = 1.6982e-04
Validation rmse logS = 0.590747
Validation R2 logS = 0.918154
Epoch 72
Train function
Loss = 7.9996e-05, PNorm = 57.3998, GNorm = 0.2844, lr_0 = 1.6678e-04
Validation rmse logS = 0.595106
Validation R2 logS = 0.916941
Epoch 73
Train function
Loss = 7.2339e-05, PNorm = 57.4058, GNorm = 0.1962, lr_0 = 1.6349e-04
Validation rmse logS = 0.591262
Validation R2 logS = 0.918011
Epoch 74
Train function
Loss = 5.1044e-05, PNorm = 57.4109, GNorm = 0.2218, lr_0 = 1.6057e-04
Loss = 9.1413e-05, PNorm = 57.4170, GNorm = 0.3760, lr_0 = 1.5769e-04
Validation rmse logS = 0.593091
Validation R2 logS = 0.917503
Epoch 75
Train function
Loss = 6.9307e-05, PNorm = 57.4226, GNorm = 0.3312, lr_0 = 1.5459e-04
Validation rmse logS = 0.589873
Validation R2 logS = 0.918395
Epoch 76
Train function
Loss = 6.6734e-05, PNorm = 57.4275, GNorm = 0.2327, lr_0 = 1.5182e-04
Validation rmse logS = 0.591630
Validation R2 logS = 0.917909
Epoch 77
Train function
Loss = 1.1795e-04, PNorm = 57.4327, GNorm = 0.2978, lr_0 = 1.4883e-04
Loss = 6.1430e-05, PNorm = 57.4368, GNorm = 0.1583, lr_0 = 1.4616e-04
Validation rmse logS = 0.592193
Validation R2 logS = 0.917752
Epoch 78
Train function
Loss = 5.6011e-05, PNorm = 57.4417, GNorm = 0.1792, lr_0 = 1.4354e-04
Validation rmse logS = 0.592514
Validation R2 logS = 0.917663
Epoch 79
Train function
Loss = 5.1098e-05, PNorm = 57.4469, GNorm = 0.1237, lr_0 = 1.4072e-04
Loss = 8.3697e-05, PNorm = 57.4513, GNorm = 0.2228, lr_0 = 1.3820e-04
Validation rmse logS = 0.591096
Validation R2 logS = 0.918057
Epoch 80
Train function
Loss = 6.1862e-05, PNorm = 57.4554, GNorm = 0.2850, lr_0 = 1.3572e-04
Validation rmse logS = 0.593040
Validation R2 logS = 0.917517
Epoch 81
Train function
Loss = 6.2554e-05, PNorm = 57.4597, GNorm = 0.1736, lr_0 = 1.3305e-04
Validation rmse logS = 0.592014
Validation R2 logS = 0.917802
Epoch 82
Train function
Loss = 6.9367e-05, PNorm = 57.4638, GNorm = 0.3466, lr_0 = 1.3067e-04
Loss = 6.3936e-05, PNorm = 57.4676, GNorm = 0.4510, lr_0 = 1.2833e-04
Loss = 2.0920e-04, PNorm = 57.4680, GNorm = 0.4754, lr_0 = 1.2810e-04
Validation rmse logS = 0.589909
Validation R2 logS = 0.918386
Epoch 83
Train function
Loss = 6.6081e-05, PNorm = 57.4718, GNorm = 0.2802, lr_0 = 1.2580e-04
Validation rmse logS = 0.597234
Validation R2 logS = 0.916346
Epoch 84
Train function
Loss = 8.1776e-05, PNorm = 57.4760, GNorm = 0.2333, lr_0 = 1.2355e-04
Validation rmse logS = 0.597085
Validation R2 logS = 0.916388
Epoch 85
Train function
Loss = 4.9074e-05, PNorm = 57.4799, GNorm = 0.1817, lr_0 = 1.2112e-04
Loss = 6.4080e-05, PNorm = 57.4834, GNorm = 0.3959, lr_0 = 1.1895e-04
Validation rmse logS = 0.592959
Validation R2 logS = 0.917539
Epoch 86
Train function
Loss = 6.0318e-05, PNorm = 57.4873, GNorm = 0.5016, lr_0 = 1.1682e-04
Validation rmse logS = 0.593199
Validation R2 logS = 0.917473
Epoch 87
Train function
Loss = 6.4127e-05, PNorm = 57.4913, GNorm = 0.6136, lr_0 = 1.1452e-04
Validation rmse logS = 0.597449
Validation R2 logS = 0.916286
Epoch 88
Train function
Loss = 4.7376e-05, PNorm = 57.4945, GNorm = 0.2436, lr_0 = 1.1247e-04
Loss = 6.4547e-05, PNorm = 57.4975, GNorm = 0.1626, lr_0 = 1.1045e-04
Validation rmse logS = 0.595394
Validation R2 logS = 0.916861
Epoch 89
Train function
Loss = 5.2964e-05, PNorm = 57.5011, GNorm = 0.1448, lr_0 = 1.0828e-04
Validation rmse logS = 0.591833
Validation R2 logS = 0.917852
Epoch 90
Train function
Loss = 6.0056e-05, PNorm = 57.5041, GNorm = 0.1009, lr_0 = 1.0634e-04
Validation rmse logS = 0.590280
Validation R2 logS = 0.918283
Epoch 91
Train function
Loss = 3.7849e-05, PNorm = 57.5074, GNorm = 0.0881, lr_0 = 1.0424e-04
Loss = 5.4521e-05, PNorm = 57.5108, GNorm = 0.0841, lr_0 = 1.0238e-04
Validation rmse logS = 0.592751
Validation R2 logS = 0.917597
Epoch 92
Train function
Loss = 3.6213e-05, PNorm = 57.5137, GNorm = 0.3110, lr_0 = 1.0054e-04
Validation rmse logS = 0.594991
Validation R2 logS = 0.916973
Epoch 93
Train function
Loss = 5.5000e-05, PNorm = 57.5169, GNorm = 0.2088, lr_0 = 1.0000e-04
Validation rmse logS = 0.594759
Validation R2 logS = 0.917038
Epoch 94
Train function
Loss = 5.9170e-05, PNorm = 57.5197, GNorm = 0.1858, lr_0 = 1.0000e-04
Loss = 6.1909e-05, PNorm = 57.5225, GNorm = 0.1600, lr_0 = 1.0000e-04
Validation rmse logS = 0.589833
Validation R2 logS = 0.918407
Epoch 95
Train function
Loss = 4.7576e-05, PNorm = 57.5258, GNorm = 0.1925, lr_0 = 1.0000e-04
Validation rmse logS = 0.597749
Validation R2 logS = 0.916202
Epoch 96
Train function
Loss = 5.3624e-05, PNorm = 57.5287, GNorm = 0.2205, lr_0 = 1.0000e-04
Validation rmse logS = 0.596254
Validation R2 logS = 0.916620
Epoch 97
Train function
Loss = 4.9360e-05, PNorm = 57.5317, GNorm = 0.1269, lr_0 = 1.0000e-04
Loss = 5.5463e-05, PNorm = 57.5344, GNorm = 0.2348, lr_0 = 1.0000e-04
Validation rmse logS = 0.592567
Validation R2 logS = 0.917648
Epoch 98
Train function
Loss = 5.2323e-05, PNorm = 57.5370, GNorm = 0.4751, lr_0 = 1.0000e-04
Validation rmse logS = 0.604055
Validation R2 logS = 0.914424
Epoch 99
Train function
Loss = 4.7148e-05, PNorm = 57.5401, GNorm = 0.1836, lr_0 = 1.0000e-04
Loss = 6.1473e-05, PNorm = 57.5431, GNorm = 0.1359, lr_0 = 1.0000e-04
Validation rmse logS = 0.594535
Validation R2 logS = 0.917100
Model 0 best validation rmse = 0.581915 on epoch 53
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "substructures_encoder.encoder.cached_zero_vector".
Loading pretrained parameter "substructures_encoder.encoder.W_o.weight".
Loading pretrained parameter "substructures_encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Model 0 test rmse logS = 0.646455
Model 0 test R2 logS = 0.890749
Ensemble test rmse  logS= 0.646455
Ensemble test R2  logS= 0.890749
4-fold cross validation
	Seed 0 ==> test rmse = 0.611049
	Seed 0 ==> test R2 = 0.902389
	Seed 1 ==> test rmse = 0.666252
	Seed 1 ==> test R2 = 0.883955
	Seed 2 ==> test rmse = 0.624855
	Seed 2 ==> test R2 = 0.897928
	Seed 3 ==> test rmse = 0.646455
	Seed 3 ==> test R2 = 0.890749
Overall val rmse logS= 0.573965 +/- 0.015018
Overall val R2 logS = 0.923874 +/- 0.005290
Overall test rmse logS = 0.637153 +/- 0.021011
Overall test R2 logS = 0.893755 +/- 0.007018
Elapsed time = 0:24:47
